From bf6b95a50bf3dd1903de0fee63818fd0961ca269 Mon Sep 17 00:00:00 2001
From: sdodlapa <your-email@example.com>
Date: Wed, 17 Dec 2025 21:23:13 +0000
Subject: [PATCH 1/3] Add complete Week 1 learning path with interactive
 notebooks

- Add 4 interactive Jupyter notebooks for Week 1 (GPU basics, thread indexing, memory, error handling)
- Add Google Colab one-click launch badges to all notebooks
- Add 12-week streamlined curriculum (replacing 26-week version)
- Add GPU setup guide (SETUP-GPU.md) with cluster and Colab instructions
- Add cluster scripts (gpu-session.sh, start-jupyter-gpu.sh, setup-environment.sh)
- Add validation scripts (test.sh, Makefile) for practice exercises
- Add verify_cuda.py for testing GPU access
- Update main README.md to point to learning path
---
 README.md                                     |  25 +-
 learning-path/12-week-curriculum.md           | 321 +++++++++
 learning-path/README.md                       |  96 +++
 learning-path/SETUP-GPU.md                    | 291 +++++++++
 learning-path/week-01/README.md               |  65 ++
 learning-path/week-01/checkpoint-quiz.md      | 317 +++++++++
 learning-path/week-01/day-1-gpu-basics.ipynb  | 599 +++++++++++++++++
 .../week-01/day-2-thread-indexing.ipynb       | 614 ++++++++++++++++++
 .../week-01/day-3-memory-basics.ipynb         | 555 ++++++++++++++++
 .../week-01/day-4-error-handling.ipynb        | 537 +++++++++++++++
 .../01-foundations/ex01-device-query/Makefile |  31 +
 .../01-foundations/ex01-device-query/test.sh  |  62 ++
 .../01-foundations/ex02-hello-gpu/Makefile    |  31 +
 .../01-foundations/ex02-hello-gpu/test.sh     |  90 +++
 scripts/gpu-session.sh                        |  32 +
 scripts/setup-environment.sh                  |  51 ++
 scripts/start-jupyter-gpu.sh                  |  45 ++
 verify_cuda.py                                |  90 +++
 18 files changed, 3849 insertions(+), 3 deletions(-)
 create mode 100644 learning-path/12-week-curriculum.md
 create mode 100644 learning-path/README.md
 create mode 100644 learning-path/SETUP-GPU.md
 create mode 100644 learning-path/week-01/README.md
 create mode 100644 learning-path/week-01/checkpoint-quiz.md
 create mode 100644 learning-path/week-01/day-1-gpu-basics.ipynb
 create mode 100644 learning-path/week-01/day-2-thread-indexing.ipynb
 create mode 100644 learning-path/week-01/day-3-memory-basics.ipynb
 create mode 100644 learning-path/week-01/day-4-error-handling.ipynb
 create mode 100644 practice/01-foundations/ex01-device-query/Makefile
 create mode 100644 practice/01-foundations/ex01-device-query/test.sh
 create mode 100644 practice/01-foundations/ex02-hello-gpu/Makefile
 create mode 100644 practice/01-foundations/ex02-hello-gpu/test.sh
 create mode 100644 scripts/gpu-session.sh
 create mode 100644 scripts/setup-environment.sh
 create mode 100644 scripts/start-jupyter-gpu.sh
 create mode 100644 verify_cuda.py

diff --git a/README.md b/README.md
index 6ecf44b..898cf25 100644
--- a/README.md
+++ b/README.md
@@ -8,9 +8,28 @@ A personal CUDA programming learning repository with documentation and practice
 
 | Resource | Description |
 |----------|-------------|
-| üìñ **[CUDA Programming Guide](cuda-programming-guide/index.md)** | Full reference documentation (Table of Contents) |
-| ‚ö° **[Quick Reference Cheatsheet](notes/cuda-quick-reference.md)** | Common patterns, syntax, code snippets |
-| üî¨ **[Practice Examples](practice/)** | Hands-on CUDA code |
+| üéØ **[START HERE: Learning Path](learning-path/README.md)** | **Interactive notebooks for learning CUDA** |
+| üìÖ **[12-Week Curriculum](learning-path/12-week-curriculum.md)** | Structured learning plan |
+| üìñ **[CUDA Programming Guide](cuda-programming-guide/index.md)** | Full reference documentation |
+| ‚ö° **[Quick Reference Cheatsheet](notes/cuda-quick-reference.md)** | Common patterns & syntax |
+| üî¨ **[Practice Exercises](practice/)** | Standalone CUDA code exercises |
+
+---
+
+## üéì Learning Path (Recommended)
+
+The **[Learning Path](learning-path/README.md)** provides interactive Jupyter notebooks that combine theory, code examples, and exercises in one place.
+
+### Week 1: GPU Fundamentals (Available Now!)
+| Day | Notebook | Topics |
+|-----|----------|--------|
+| 1 | [GPU Basics](learning-path/week-01/day-1-gpu-basics.ipynb) | CPU vs GPU, device query, first kernel |
+| 2 | [Thread Indexing](learning-path/week-01/day-2-thread-indexing.ipynb) | 1D/2D indexing, grid-stride loops |
+| 3 | [Memory Basics](learning-path/week-01/day-3-memory-basics.ipynb) | Transfers, pinned memory, optimization |
+| 4 | [Error Handling](learning-path/week-01/day-4-error-handling.ipynb) | Debugging, common pitfalls |
+| 5 | [Checkpoint Quiz](learning-path/week-01/checkpoint-quiz.md) | Self-assessment |
+
+See the **[12-Week Curriculum](learning-path/12-week-curriculum.md)** for the complete plan.
 
 ---
 
diff --git a/learning-path/12-week-curriculum.md b/learning-path/12-week-curriculum.md
new file mode 100644
index 0000000..8b5eea9
--- /dev/null
+++ b/learning-path/12-week-curriculum.md
@@ -0,0 +1,321 @@
+# CUDA Learning: 12-Week MVP Curriculum
+
+> üéØ **Goal:** Become proficient in CUDA programming in 12 focused weeks  
+> ‚è±Ô∏è **Time commitment:** 4-6 hours per day, 5-6 days per week  
+> üìÖ **Total:** ~300 hours of focused learning
+
+---
+
+## Overview
+
+This is a **streamlined, achievable** curriculum that focuses on practical skills over comprehensive coverage. Master these 12 weeks and you'll be able to:
+
+- Write efficient CUDA kernels from scratch
+- Optimize GPU code using profiling tools
+- Handle real-world problems (image processing, matrix operations, ML primitives)
+- Understand multi-GPU and advanced patterns
+
+---
+
+## üìä Progress Tracker
+
+| Week | Focus | Status | Completed |
+|------|-------|--------|-----------|
+| 1 | GPU Fundamentals | ‚¨ú Not Started | |
+| 2 | Memory Patterns | ‚¨ú Not Started | |
+| 3 | Parallel Patterns I | ‚¨ú Not Started | |
+| 4 | Reduction & Atomics | ‚¨ú Not Started | |
+| 5 | Prefix Sum (Scan) | ‚¨ú Not Started | |
+| 6 | Matrix Operations | ‚¨ú Not Started | |
+| 7 | Memory Optimization | ‚¨ú Not Started | |
+| 8 | Profiling & Analysis | ‚¨ú Not Started | |
+| 9 | Streams & Concurrency | ‚¨ú Not Started | |
+| 10 | Advanced Patterns | ‚¨ú Not Started | |
+| 11 | Multi-GPU & Scaling | ‚¨ú Not Started | |
+| 12 | Capstone Project | ‚¨ú Not Started | |
+
+---
+
+## Week 1: GPU Fundamentals
+
+### Learning Goals
+- Understand CPU vs GPU architecture
+- Write and launch basic CUDA kernels
+- Master thread indexing (1D, 2D, 3D)
+- Handle memory transfers
+
+### Daily Schedule
+| Day | Topic | Materials |
+|-----|-------|-----------|
+| 1 | GPU basics, device query | [day-1-gpu-basics.ipynb](week-01/day-1-gpu-basics.ipynb) |
+| 2 | Thread indexing | [day-2-thread-indexing.ipynb](week-01/day-2-thread-indexing.ipynb) |
+| 3 | Memory management | [day-3-memory-basics.ipynb](week-01/day-3-memory-basics.ipynb) |
+| 4 | Error handling & debugging | [day-4-error-handling.ipynb](week-01/day-4-error-handling.ipynb) |
+| 5 | Practice & Quiz | Exercises + [checkpoint-quiz.md](week-01/checkpoint-quiz.md) |
+
+### Deliverables
+- [ ] All notebook exercises completed
+- [ ] Quiz score ‚â• 25/30
+- [ ] Can write a kernel from scratch without reference
+
+---
+
+## Week 2: Memory Patterns & Optimization
+
+### Learning Goals
+- Understand coalesced memory access
+- Use shared memory effectively
+- Avoid bank conflicts
+- Profile memory behavior
+
+### Topics
+1. Global memory access patterns
+2. Memory coalescing rules
+3. Shared memory introduction
+4. Shared memory bank conflicts
+5. Constant memory and texture memory
+
+### Project
+**Image Filter:** Implement Gaussian blur using shared memory tiling
+
+---
+
+## Week 3: Parallel Patterns I (Vector Operations)
+
+### Learning Goals
+- Implement grid-stride loops professionally
+- Handle edge cases and arbitrary sizes
+- Vector operations at scale
+- Fused operations for better performance
+
+### Topics
+1. Vector addition/subtraction/multiplication
+2. Vector dot product (setup for reduction)
+3. SAXPY and BLAS-like operations
+4. Multiple operations per thread
+5. Fusing operations to reduce memory traffic
+
+### Project
+**Vector Math Library:** Complete library with add, sub, mul, div, dot, norm, scale
+
+---
+
+## Week 4: Reduction & Atomics
+
+### Learning Goals
+- Implement tree reduction
+- Understand warp-level primitives
+- Use atomic operations correctly
+- Combine reduction techniques
+
+### Topics
+1. Naive reduction (with divergence)
+2. Sequential addressing (no divergence)
+3. Warp shuffle reduction
+4. Atomic operations
+5. Combining techniques for optimal reduction
+
+### Project
+**Statistical Functions:** sum, mean, min, max, variance implemented on GPU
+
+---
+
+## Week 5: Prefix Sum (Scan)
+
+### Learning Goals
+- Understand scan as fundamental parallel primitive
+- Implement work-efficient scan
+- Handle arrays larger than block size
+- Apply scan to real problems
+
+### Topics
+1. Inclusive vs exclusive scan
+2. Hillis-Steele algorithm
+3. Blelloch algorithm
+4. Large array scan with multiple blocks
+5. Applications: stream compaction, radix sort
+
+### Project
+**Stream Compaction:** Filter array to keep only positive values
+
+---
+
+## Week 6: Matrix Operations
+
+### Learning Goals
+- Implement optimized matrix multiply
+- Understand tiling strategies
+- Achieve good performance relative to cuBLAS
+- Matrix transpose optimization
+
+### Topics
+1. Naive matrix multiply
+2. Tiled matrix multiply with shared memory
+3. Rectangular matrix handling
+4. Matrix transpose (naive vs coalesced)
+5. Comparison with cuBLAS
+
+### Project
+**Matrix Library:** multiply, transpose, add with 80%+ cuBLAS performance
+
+---
+
+## Week 7: Memory Optimization Deep Dive
+
+### Learning Goals
+- Master occupancy analysis
+- Reduce register pressure
+- Optimize shared memory usage
+- Understand L1/L2 cache behavior
+
+### Topics
+1. Occupancy calculator usage
+2. Register spilling and local memory
+3. Shared memory configurations
+4. Cache optimization strategies
+5. Unified memory and prefetching
+
+### Project
+Optimize Week 6 matrix multiply to 95%+ cuBLAS performance
+
+---
+
+## Week 8: Profiling & Analysis
+
+### Learning Goals
+- Master Nsight Compute
+- Use Nsight Systems for timeline analysis
+- Understand roofline model
+- Identify and fix bottlenecks
+
+### Topics
+1. Nsight Compute basics
+2. Key metrics: bandwidth, occupancy, instruction mix
+3. Nsight Systems for system-wide analysis
+4. Roofline analysis
+5. Systematic optimization workflow
+
+### Project
+Profile and optimize a provided slow kernel by 10x
+
+---
+
+## Week 9: Streams & Concurrency
+
+### Learning Goals
+- Overlap computation and transfers
+- Use multiple streams effectively
+- Understand CUDA events
+- Implement async patterns
+
+### Topics
+1. CUDA streams introduction
+2. Async memory operations
+3. Overlap patterns (H2D, kernel, D2H)
+4. Events for timing and synchronization
+5. Multi-stream best practices
+
+### Project
+**Pipeline Processing:** Overlap data loading with processing for image batch
+
+---
+
+## Week 10: Advanced Patterns
+
+### Learning Goals
+- Implement histogram efficiently
+- Understand sorting on GPU
+- Use CUB and Thrust effectively
+- Cooperative groups basics
+
+### Topics
+1. Histogram (atomic, privatization)
+2. Sorting (odd-even, bitonic, radix)
+3. CUB library for common operations
+4. Thrust for STL-like GPU operations
+5. Cooperative groups for flexible sync
+
+### Project
+**Data Analysis Pipeline:** Load data, histogram, sort, statistics
+
+---
+
+## Week 11: Multi-GPU & Scaling
+
+### Learning Goals
+- Work with multiple GPUs
+- Understand peer-to-peer access
+- Distribute work across devices
+- Intro to NCCL
+
+### Topics
+1. Device enumeration and management
+2. P2P memory access
+3. Work distribution strategies
+4. Multi-GPU reduction
+5. NCCL collective operations
+
+### Project
+**Multi-GPU Matrix Multiply:** Distribute large matrix across 2+ GPUs
+
+---
+
+## Week 12: Capstone Project
+
+### Goals
+Apply everything learned to a complete, optimized application.
+
+### Choose One:
+1. **ML Inference Engine**
+   - Custom matrix multiply
+   - Fused activation functions
+   - Batch processing with streams
+
+2. **Image Processing Pipeline**
+   - Multi-filter convolution
+   - Edge detection
+   - Color space conversions
+   - Batch processing
+
+3. **Scientific Simulation**
+   - N-body simulation
+   - Particle system
+   - Heat diffusion
+
+### Requirements
+- Optimized (profile-guided)
+- Multi-GPU or heavily streamed
+- Well-documented
+- Performance comparison with CPU/library
+
+---
+
+## üìö Resources
+
+### Primary Materials
+- Interactive notebooks in `learning-path/week-XX/`
+- Practice exercises in `practice/`
+- Reference docs in `cuda-programming-guide/`
+
+### Supplementary
+- [NVIDIA Developer Blog](https://developer.nvidia.com/blog/)
+- [CUDA Samples](https://github.com/NVIDIA/cuda-samples)
+- [CUB Documentation](https://nvlabs.github.io/cub/)
+
+---
+
+## üéØ Success Criteria
+
+By completing this curriculum, you should be able to:
+
+- [ ] Write CUDA kernels from scratch without reference
+- [ ] Identify memory vs compute bottlenecks
+- [ ] Optimize kernels using profiling data
+- [ ] Implement common parallel patterns
+- [ ] Work with multi-GPU systems
+- [ ] Achieve 80%+ theoretical peak on bandwidth-bound kernels
+- [ ] Achieve 50%+ theoretical peak on compute-bound kernels
+
+---
+
+*This curriculum replaces the original 26-week plan with a more focused, achievable path.*
diff --git a/learning-path/README.md b/learning-path/README.md
new file mode 100644
index 0000000..edaedee
--- /dev/null
+++ b/learning-path/README.md
@@ -0,0 +1,96 @@
+# CUDA Learning Path
+
+> üéØ **Interactive, hands-on CUDA learning through Jupyter notebooks**
+
+## üöÄ Getting Started
+
+**First time?** See **[SETUP-GPU.md](SETUP-GPU.md)** for how to access a T4 GPU via:
+- Google Colab (free, easiest)
+- ODU HPC cluster
+- Cloud providers (AWS, Lambda Labs)
+
+This is your primary learning resource - structured week-by-week with interactive notebooks that combine theory, code examples, and exercises in one place.
+
+## üìö Structure
+
+Each week contains:
+- **Daily notebooks** - Theory + guided coding + exercises
+- **Exercise folders** - Independent practice problems
+- **Checkpoint quiz** - Self-assessment before moving on
+
+## üóìÔ∏è 12-Week MVP Curriculum
+
+| Week | Focus | Key Skills |
+|------|-------|------------|
+| **1** | GPU Fundamentals | Device query, first kernel, thread indexing |
+| **2** | Memory Basics | cudaMalloc, cudaMemcpy, error handling |
+| **3** | Parallel Patterns I | Vector operations, grid-stride loops |
+| **4** | Reduction | Sum, min/max, warp-level primitives |
+| **5** | Scan | Prefix sum, stream compaction |
+| **6** | Matrix Operations | GEMM naive ‚Üí tiled ‚Üí optimized |
+| **7** | Memory Optimization | Coalescing, shared memory, bank conflicts |
+| **8** | Profiling | Nsight Compute, roofline analysis |
+| **9** | Streams & Concurrency | Async execution, overlap |
+| **10** | Advanced Patterns | Histograms, sorting, atomics |
+| **11** | Multi-GPU & Dynamic Parallelism | Scaling up |
+| **12** | Capstone Project | End-to-end optimized application |
+
+## üöÄ Getting Started
+
+### Prerequisites
+- CUDA Toolkit installed (`nvcc --version`)
+- Jupyter with CUDA support
+- Basic C/C++ knowledge
+
+### Start Learning
+
+```bash
+cd learning-path/week-01
+jupyter notebook day-1-gpu-basics.ipynb
+```
+
+## üìä Progress Tracking
+
+| Week | Status | Completed Date | Notes |
+|------|--------|----------------|-------|
+| Week 1 | ‚¨ú Not Started | | |
+| Week 2 | ‚¨ú Not Started | | |
+| Week 3 | ‚¨ú Not Started | | |
+| Week 4 | ‚¨ú Not Started | | |
+| Week 5 | ‚¨ú Not Started | | |
+| Week 6 | ‚¨ú Not Started | | |
+| Week 7 | ‚¨ú Not Started | | |
+| Week 8 | ‚¨ú Not Started | | |
+| Week 9 | ‚¨ú Not Started | | |
+| Week 10 | ‚¨ú Not Started | | |
+| Week 11 | ‚¨ú Not Started | | |
+| Week 12 | ‚¨ú Not Started | | |
+
+## üìÅ Directory Layout
+
+```
+learning-path/
+‚îú‚îÄ‚îÄ README.md                 # This file
+‚îú‚îÄ‚îÄ week-01/
+‚îÇ   ‚îú‚îÄ‚îÄ day-1-gpu-basics.ipynb
+‚îÇ   ‚îú‚îÄ‚îÄ day-2-first-kernel.ipynb
+‚îÇ   ‚îú‚îÄ‚îÄ day-3-thread-indexing.ipynb
+‚îÇ   ‚îú‚îÄ‚îÄ day-4-memory-basics.ipynb
+‚îÇ   ‚îú‚îÄ‚îÄ exercises/
+‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ex-device-query/
+‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ex-vector-add/
+‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-quiz.md
+‚îú‚îÄ‚îÄ week-02/
+‚îÇ   ‚îî‚îÄ‚îÄ ...
+‚îî‚îÄ‚îÄ ...
+```
+
+## üîó Related Resources
+
+- [CUDA Programming Guide](../cuda-programming-guide/) - Reference documentation
+- [Quick Reference](../notes/cuda-quick-reference.md) - Cheatsheet
+- [Practice Exercises](../practice/) - Additional exercises
+
+---
+
+*Start with Week 1 and progress sequentially. Each week builds on previous knowledge.*
diff --git a/learning-path/SETUP-GPU.md b/learning-path/SETUP-GPU.md
new file mode 100644
index 0000000..0a6e9b7
--- /dev/null
+++ b/learning-path/SETUP-GPU.md
@@ -0,0 +1,291 @@
+# Using T4 GPU for Hands-On CUDA Learning
+
+This guide explains how to run the CUDA notebooks we created using your T4 GPU.
+
+---
+
+## üéØ Option 1: Google Colab (Easiest - Free T4 GPU) ‚≠ê RECOMMENDED
+
+**Start learning in 60 seconds - no setup required!**
+
+### One-Click Launch Links
+
+Click these links to open notebooks directly in Colab:
+
+| Day | Notebook | Open in Colab |
+|-----|----------|---------------|
+| 1 | GPU Basics | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-1-gpu-basics.ipynb) |
+| 2 | Thread Indexing | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-2-thread-indexing.ipynb) |
+| 3 | Memory Basics | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-3-memory-basics.ipynb) |
+| 4 | Error Handling | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-4-error-handling.ipynb) |
+
+### Quick Start (3 Steps)
+
+**Step 1:** Click any Colab link above
+
+**Step 2:** Enable GPU: **Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save**
+
+**Step 3:** Run this setup cell first:
+```python
+# Run this cell first in any notebook!
+!pip install numba -q
+import numpy as np
+from numba import cuda
+
+# Verify GPU
+print("‚úÖ GPU:", cuda.get_current_device().name.decode())
+!nvidia-smi --query-gpu=name,memory.total --format=csv
+```
+
+### Colab Pro Tips
+- **Save work**: File ‚Üí Save a copy in Drive
+- **Sessions timeout** after ~90 min idle (free tier)
+- **GPU limits**: ~12 hrs/day free, then may get CPU-only
+- **Reconnect**: If disconnected, Runtime ‚Üí Reconnect
+- **Keep alive**: Keep browser tab active
+
+---
+
+## üéØ Option 2: Your ODU HPC System
+
+If you have access to GPU nodes on ODU's HPC:
+
+### Step 1: Request a GPU Node
+
+```bash
+# Interactive session with T4 GPU
+srun --partition=gpu --gres=gpu:t4:1 --time=04:00:00 --pty bash
+
+# Or submit a job
+sbatch --partition=gpu --gres=gpu:1 your_script.sh
+```
+
+### Step 2: Load CUDA Module
+
+```bash
+module load cuda/12.0
+module load anaconda3/2023.09  # or your Python module
+```
+
+### Step 3: Create Conda Environment
+
+```bash
+conda create -n cuda-learning python=3.10 -y
+conda activate cuda-learning
+conda install numba cudatoolkit numpy jupyter -c conda-forge -y
+```
+
+### Step 4: Run Jupyter on GPU Node
+
+```bash
+# On the GPU node
+jupyter notebook --no-browser --port=8888
+
+# Then tunnel from your local machine:
+ssh -L 8888:localhost:8888 your_username@turing.hpc.odu.edu
+```
+
+### Step 5: Open Notebooks
+
+Navigate to `~/cuda-lab/learning-path/week-01/` and open notebooks.
+
+---
+
+## üéØ Option 3: Local Machine with NVIDIA GPU
+
+If you have a local NVIDIA GPU:
+
+### Step 1: Install CUDA Toolkit
+
+```bash
+# Ubuntu/Debian
+sudo apt update
+sudo apt install nvidia-driver-535 nvidia-cuda-toolkit
+
+# Verify
+nvidia-smi
+nvcc --version
+```
+
+### Step 2: Create Python Environment
+
+```bash
+python3 -m venv cuda-env
+source cuda-env/bin/activate
+pip install numba numpy jupyter matplotlib
+```
+
+### Step 3: Run Notebooks
+
+```bash
+cd ~/cuda-lab/learning-path/week-01
+jupyter notebook
+```
+
+---
+
+## üéØ Option 4: Cloud GPU (AWS, GCP, Lambda Labs)
+
+### AWS EC2 with T4
+
+1. Launch `g4dn.xlarge` instance (1√ó T4 GPU, ~$0.50/hr)
+2. Use Deep Learning AMI (CUDA pre-installed)
+3. SSH and clone your repo
+
+```bash
+git clone https://github.com/sdodlapa/cuda-lab.git
+cd cuda-lab
+pip install numba numpy jupyter
+jupyter notebook --no-browser --port=8888
+```
+
+### Lambda Labs (Recommended for simplicity)
+
+1. Sign up at [lambdalabs.com](https://lambdalabs.com/)
+2. Launch T4 instance (~$0.50/hr)
+3. CUDA and Python pre-installed!
+
+---
+
+## üìã Quick Verification Script
+
+Run this to verify your CUDA setup works:
+
+```python
+#!/usr/bin/env python3
+"""Verify CUDA setup is working"""
+
+import numpy as np
+from numba import cuda
+import math
+
+print("=" * 50)
+print("CUDA SETUP VERIFICATION")
+print("=" * 50)
+
+# Check CUDA
+if not cuda.is_available():
+    print("‚ùå CUDA is NOT available!")
+    print("   Make sure you have an NVIDIA GPU and drivers installed")
+    exit(1)
+
+print("‚úÖ CUDA is available")
+
+# Get device info
+device = cuda.get_current_device()
+print(f"‚úÖ GPU: {device.name.decode()}")
+print(f"   Compute Capability: {device.compute_capability}")
+print(f"   Max threads/block: {device.MAX_THREADS_PER_BLOCK}")
+
+# Test a simple kernel
+@cuda.jit
+def test_kernel(arr):
+    idx = cuda.grid(1)
+    if idx < arr.size:
+        arr[idx] = idx * 2
+
+# Run test
+n = 1000
+arr = cuda.device_array(n, dtype=np.float32)
+threads = 256
+blocks = math.ceil(n / threads)
+
+test_kernel[blocks, threads](arr)
+result = arr.copy_to_host()
+
+if result[10] == 20.0:
+    print("‚úÖ Kernel execution successful")
+else:
+    print("‚ùå Kernel execution failed")
+
+# Memory test
+ctx = cuda.current_context()
+free, total = ctx.get_memory_info()
+print(f"‚úÖ GPU Memory: {free/1e9:.1f} GB free / {total/1e9:.1f} GB total")
+
+print("=" * 50)
+print("üöÄ Ready for CUDA learning!")
+print("=" * 50)
+```
+
+Save as `verify_cuda.py` and run:
+```bash
+python verify_cuda.py
+```
+
+---
+
+## üéì Recommended Learning Workflow
+
+### Daily Session (4-6 hours)
+
+1. **Start GPU session** (Colab/HPC/Cloud)
+
+2. **Open today's notebook**
+   ```
+   learning-path/week-01/day-X-topic.ipynb
+   ```
+
+3. **Work through sections:**
+   - Read markdown explanations
+   - Run code cells (Shift+Enter)
+   - Complete TODO exercises
+   - Experiment with modifications
+
+4. **Save your work**
+   - In Colab: Save to Drive
+   - On HPC: Files persist in home directory
+   - Cloud: Git commit regularly
+
+5. **End of week: Take the quiz**
+   ```
+   learning-path/week-01/checkpoint-quiz.md
+   ```
+
+---
+
+## üí° Pro Tips
+
+### For Colab Users
+- Bookmark the notebooks in Drive for quick access
+- Use `%%time` magic to time cells
+- Mount Drive to save large datasets: 
+  ```python
+  from google.colab import drive
+  drive.mount('/content/drive')
+  ```
+
+### For HPC Users
+- Request longer sessions for project work: `--time=08:00:00`
+- Use screen/tmux to keep sessions alive
+- Store large data in scratch, not home
+
+### For Everyone
+- **Commit progress daily** to GitHub
+- Take notes on tricky concepts
+- If kernel crashes, restart and run all cells
+- GPU memory doesn't auto-clear - use `cuda.close()` or restart kernel
+
+---
+
+## üÜò Troubleshooting
+
+| Issue | Solution |
+|-------|----------|
+| "No CUDA GPU" | Check GPU runtime (Colab) or `nvidia-smi` |
+| "Out of memory" | Restart kernel, use smaller arrays |
+| Numba import error | `pip install numba` |
+| Kernel hangs | Add `cuda.synchronize()` calls |
+| Slow first run | Normal - JIT compilation. Second run is fast |
+
+---
+
+## üìö Next Steps
+
+1. **Pick your platform** (Colab recommended to start)
+2. **Run verification script** 
+3. **Start Day 1 notebook**
+4. **Complete all 4 days + quiz**
+5. **Move to Week 2!**
+
+Good luck! üöÄ
diff --git a/learning-path/week-01/README.md b/learning-path/week-01/README.md
new file mode 100644
index 0000000..ecb26d7
--- /dev/null
+++ b/learning-path/week-01/README.md
@@ -0,0 +1,65 @@
+# Week 1: GPU Fundamentals
+
+Welcome to Week 1 of your CUDA learning journey!
+
+## üìã Overview
+
+This week you'll build a solid foundation in GPU computing:
+- Why GPUs exist and when to use them
+- CUDA programming model basics
+- Memory management patterns
+- Debugging and error handling
+
+## üìÖ Daily Schedule
+
+| Day | Topic | Time | Materials |
+|-----|-------|------|-----------|
+| **Day 1** | GPU Basics | 4-5 hrs | [day-1-gpu-basics.ipynb](day-1-gpu-basics.ipynb) |
+| **Day 2** | Thread Indexing | 4-5 hrs | [day-2-thread-indexing.ipynb](day-2-thread-indexing.ipynb) |
+| **Day 3** | Memory Management | 4-5 hrs | [day-3-memory-basics.ipynb](day-3-memory-basics.ipynb) |
+| **Day 4** | Error Handling | 3-4 hrs | [day-4-error-handling.ipynb](day-4-error-handling.ipynb) |
+| **Day 5** | Review & Quiz | 3-4 hrs | All exercises + [checkpoint-quiz.md](checkpoint-quiz.md) |
+
+## ‚úÖ Checklist
+
+### By end of Day 1:
+- [ ] Can explain CPU vs GPU difference
+- [ ] Can query GPU properties
+- [ ] Can run a simple kernel
+
+### By end of Day 2:
+- [ ] Understand thread/block/grid hierarchy
+- [ ] Can calculate 1D and 2D indices
+- [ ] Can write grid-stride loops
+
+### By end of Day 3:
+- [ ] Understand explicit memory management
+- [ ] Know when to use pinned memory
+- [ ] Understand transfer overhead
+
+### By end of Day 4:
+- [ ] Can handle CUDA errors properly
+- [ ] Know common pitfalls
+- [ ] Can debug basic issues
+
+### By end of Day 5:
+- [ ] All exercises completed
+- [ ] Quiz score ‚â• 25/30
+- [ ] Ready for Week 2!
+
+## üîó Quick Links
+
+- [CUDA Programming Guide](../../cuda-programming-guide/index.md)
+- [Quick Reference](../../notes/cuda-quick-reference.md)
+- [Practice Exercises](../../practice/01-foundations/)
+
+## üìù Notes
+
+Take notes as you learn! Write down:
+- Key formulas (thread indexing, launch config)
+- Common patterns you discover
+- Questions to research later
+
+---
+
+[‚Üê Back to Learning Path](../README.md) | [12-Week Curriculum](../12-week-curriculum.md) | [Week 2 ‚Üí](../week-02/README.md)
diff --git a/learning-path/week-01/checkpoint-quiz.md b/learning-path/week-01/checkpoint-quiz.md
new file mode 100644
index 0000000..08c0611
--- /dev/null
+++ b/learning-path/week-01/checkpoint-quiz.md
@@ -0,0 +1,317 @@
+# Week 1 Checkpoint Quiz
+
+Test your understanding before moving to Week 2. Answer without looking at notes first!
+
+---
+
+## Section A: Conceptual Understanding (10 points)
+
+### Q1: CPU vs GPU (2 points)
+Which statement is TRUE about GPUs compared to CPUs?
+
+- [ ] A) GPUs have fewer cores but each core is much faster
+- [ ] B) GPUs have thousands of simpler cores optimized for throughput
+- [ ] C) GPUs are always faster than CPUs for any task
+- [ ] D) GPUs have larger per-core caches than CPUs
+
+<details>
+<summary>Click for answer</summary>
+
+**B) GPUs have thousands of simpler cores optimized for throughput**
+
+GPUs trade single-thread performance for massive parallelism. They're optimized for throughput (doing many things at once), while CPUs are optimized for latency (doing one thing as fast as possible).
+</details>
+
+---
+
+### Q2: When to Use GPU (2 points)
+Which task is BEST suited for GPU acceleration?
+
+- [ ] A) Parsing a text file line by line
+- [ ] B) Running a decision tree with many branches
+- [ ] C) Applying the same filter to every pixel in an image
+- [ ] D) Accessing a database with complex queries
+
+<details>
+<summary>Click for answer</summary>
+
+**C) Applying the same filter to every pixel in an image**
+
+Image processing is ideal for GPU because:
+- Same operation on millions of pixels (SIMD pattern)
+- Minimal branching
+- Data parallel, independent operations
+</details>
+
+---
+
+### Q3: Thread Hierarchy (2 points)
+In CUDA, threads are organized into _____, which are organized into _____.
+
+- [ ] A) Grids, Blocks
+- [ ] B) Blocks, Grids
+- [ ] C) Warps, Threads
+- [ ] D) Cores, SMs
+
+<details>
+<summary>Click for answer</summary>
+
+**B) Blocks, Grids**
+
+Thread hierarchy (smallest to largest):
+- Thread ‚Üí Block ‚Üí Grid
+- Threads within a block can cooperate (shared memory, sync)
+- Blocks within a grid are independent
+</details>
+
+---
+
+### Q4: Warp Size (2 points)
+What is the warp size in NVIDIA GPUs, and why does it matter?
+
+<details>
+<summary>Click for answer</summary>
+
+**Warp size is 32 threads.**
+
+It matters because:
+- Threads in a warp execute in lockstep (SIMT)
+- Branch divergence within a warp hurts performance
+- Memory access patterns should be aligned to warp boundaries
+- Warp-level primitives (shuffle, vote) operate on 32 threads
+</details>
+
+---
+
+### Q5: Memory Transfer (2 points)
+Why is memory transfer between CPU and GPU often the bottleneck?
+
+<details>
+<summary>Click for answer</summary>
+
+**PCIe bus bandwidth is much lower than GPU memory bandwidth.**
+
+- PCIe 4.0 x16: ~32 GB/s
+- GPU HBM2/GDDR6: 200-900 GB/s
+
+This means:
+- Transfer time can exceed compute time for simple operations
+- Keep data on GPU as long as possible
+- Batch operations before copying back
+</details>
+
+---
+
+## Section B: Practical Knowledge (10 points)
+
+### Q6: Global Index Formula (2 points)
+Write the formula to calculate a 1D global thread index.
+
+```
+global_idx = _______________________________________
+```
+
+<details>
+<summary>Click for answer</summary>
+
+```
+global_idx = blockIdx.x * blockDim.x + threadIdx.x
+```
+
+Or in Numba: `cuda.grid(1)`
+</details>
+
+---
+
+### Q7: Launch Configuration (2 points)
+You have an array of 1,000,000 elements and want 256 threads per block.
+How many blocks do you need?
+
+```
+blocks = _______________________________________
+```
+
+<details>
+<summary>Click for answer</summary>
+
+```
+blocks = ceil(1,000,000 / 256) = 3,907
+```
+
+Always round UP to ensure all elements are covered.
+In Python: `math.ceil(1_000_000 / 256)`
+</details>
+
+---
+
+### Q8: Boundary Check (2 points)
+What's wrong with this kernel? How would you fix it?
+
+```python
+@cuda.jit
+def broken_kernel(arr):
+    idx = cuda.grid(1)
+    arr[idx] = idx * 2  # üí• Bug here
+```
+
+<details>
+<summary>Click for answer</summary>
+
+**Missing boundary check!** When grid size > array size, threads will access out-of-bounds memory.
+
+**Fixed:**
+```python
+@cuda.jit
+def fixed_kernel(arr, n):
+    idx = cuda.grid(1)
+    if idx < n:  # Boundary check
+        arr[idx] = idx * 2
+```
+</details>
+
+---
+
+### Q9: Memory Functions (2 points)
+Match the Numba function to its purpose:
+
+| Function | Purpose |
+|----------|---------|
+| `cuda.to_device(arr)` | ___ |
+| `cuda.device_array(n, dtype)` | ___ |
+| `device_arr.copy_to_host()` | ___ |
+| `cuda.synchronize()` | ___ |
+
+A) Allocate on GPU without copying
+B) Copy host array to device
+C) Wait for all GPU operations to complete
+D) Copy device array back to host
+
+<details>
+<summary>Click for answer</summary>
+
+| Function | Purpose |
+|----------|---------|
+| `cuda.to_device(arr)` | **B) Copy host array to device** |
+| `cuda.device_array(n, dtype)` | **A) Allocate on GPU without copying** |
+| `device_arr.copy_to_host()` | **D) Copy device array back to host** |
+| `cuda.synchronize()` | **C) Wait for all GPU operations to complete** |
+</details>
+
+---
+
+### Q10: Data Types (2 points)
+Why should you prefer `np.float32` over `np.float64` for GPU computing?
+
+<details>
+<summary>Click for answer</summary>
+
+**float32 is 2x faster than float64 on most GPUs because:**
+
+1. Half the memory bandwidth required
+2. More float32 cores than float64 cores on most GPUs
+3. Twice as many elements fit in cache/registers
+4. Sufficient precision for most applications
+
+Use float64 only when you specifically need the extra precision.
+</details>
+
+---
+
+## Section C: Code Challenge (10 points)
+
+### Q11: Complete the Kernel (5 points)
+
+Complete this 2D image brightness adjustment kernel:
+
+```python
+@cuda.jit
+def adjust_brightness(image, output, height, width, factor):
+    """
+    Multiply each pixel by factor to adjust brightness.
+    Clamp values to 0-255 range.
+    """
+    # TODO: Get 2D coordinates
+    col, row = ___________________
+    
+    # TODO: Check boundaries
+    if ___________________:
+        # TODO: Adjust brightness and clamp
+        val = ___________________
+        output[row, col] = ___________________
+```
+
+<details>
+<summary>Click for answer</summary>
+
+```python
+@cuda.jit
+def adjust_brightness(image, output, height, width, factor):
+    col, row = cuda.grid(2)
+    
+    if row < height and col < width:
+        val = image[row, col] * factor
+        output[row, col] = min(255.0, max(0.0, val))
+```
+</details>
+
+---
+
+### Q12: Grid-Stride Loop (5 points)
+
+Rewrite this kernel to use a grid-stride loop so it can handle arrays of ANY size:
+
+```python
+@cuda.jit
+def double_values(arr, n):
+    idx = cuda.grid(1)
+    if idx < n:
+        arr[idx] *= 2
+```
+
+<details>
+<summary>Click for answer</summary>
+
+```python
+@cuda.jit
+def double_values_strided(arr, n):
+    idx = cuda.grid(1)
+    stride = cuda.gridsize(1)
+    
+    while idx < n:
+        arr[idx] *= 2
+        idx += stride
+```
+
+This allows handling arrays larger than the grid size!
+</details>
+
+---
+
+## Scoring
+
+| Section | Points | Your Score |
+|---------|--------|------------|
+| A: Conceptual | 10 | ___ |
+| B: Practical | 10 | ___ |
+| C: Code | 10 | ___ |
+| **Total** | **30** | ___ |
+
+### Interpretation
+
+- **25-30**: Excellent! Ready for Week 2
+- **20-24**: Good, but review weak areas
+- **15-19**: Need more practice - redo notebooks
+- **< 15**: Not ready - restart Week 1
+
+---
+
+## Next Steps
+
+- [ ] Score yourself honestly
+- [ ] Review any topics where you struggled
+- [ ] Complete any remaining exercises
+- [ ] Move on to Week 2 when ready!
+
+---
+
+[‚Üê Back to Week 1](./README.md) | [Week 2 ‚Üí](../week-02/README.md)
diff --git a/learning-path/week-01/day-1-gpu-basics.ipynb b/learning-path/week-01/day-1-gpu-basics.ipynb
new file mode 100644
index 0000000..b582c88
--- /dev/null
+++ b/learning-path/week-01/day-1-gpu-basics.ipynb
@@ -0,0 +1,599 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "id": "c1470948",
+   "metadata": {},
+   "source": [
+    "# üöÄ Day 1: GPU Fundamentals & Your First CUDA Program\n",
+    "\n",
+    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-1-gpu-basics.ipynb)\n",
+    "\n",
+    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "11304f41",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# ‚öôÔ∏è Colab Setup Cell - Run this first!\n",
+    "# This cell installs CUDA support for Numba on Google Colab\n",
+    "\n",
+    "import subprocess\n",
+    "import sys\n",
+    "\n",
+    "# Check if we're running on Google Colab\n",
+    "try:\n",
+    "    import google.colab\n",
+    "    IN_COLAB = True\n",
+    "except ImportError:\n",
+    "    IN_COLAB = False\n",
+    "\n",
+    "if IN_COLAB:\n",
+    "    print(\"üîß Running on Google Colab - Installing CUDA dependencies...\")\n",
+    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
+    "    print(\"‚úÖ Setup complete! You can now run the rest of the notebook.\")\n",
+    "else:\n",
+    "    print(\"üíª Running locally - no additional setup needed.\")\n",
+    "    print(\"   Make sure you have: pip install numba numpy\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "1f9f5e0c",
+   "metadata": {},
+   "source": [
+    "# Day 1: GPU Fundamentals & Your First CUDA Program\n",
+    "\n",
+    "Welcome to your CUDA learning journey! Today we'll understand:\n",
+    "- Why GPUs exist and when to use them\n",
+    "- GPU architecture basics\n",
+    "- How to query GPU properties\n",
+    "- Your first interaction with CUDA\n",
+    "\n",
+    "**Prerequisites:** Python basics, NumPy familiarity\n",
+    "\n",
+    "---\n",
+    "\n",
+    "## 1. Why GPUs? The Parallel Computing Revolution\n",
+    "\n",
+    "### The Fundamental Problem\n",
+    "\n",
+    "Modern applications process **massive amounts of data**:\n",
+    "- Neural networks: billions of matrix operations\n",
+    "- Video processing: millions of pixels per frame\n",
+    "- Scientific simulations: countless particles/cells\n",
+    "\n",
+    "CPUs are optimized for **speed on single tasks** (latency).  \n",
+    "GPUs are optimized for **throughput on many tasks** (parallelism).\n",
+    "\n",
+    "### CPU vs GPU: An Analogy\n",
+    "\n",
+    "```\n",
+    "üöó CPU (Sports Car)          üöõ GPU (Fleet of Trucks)\n",
+    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
+    "‚Ä¢ 4-16 very fast cores       ‚Ä¢ 1000s of simpler cores\n",
+    "‚Ä¢ Complex control logic      ‚Ä¢ Simple control logic  \n",
+    "‚Ä¢ Large caches per core      ‚Ä¢ Smaller shared caches\n",
+    "‚Ä¢ Great for: 1 task FAST     ‚Ä¢ Great for: MANY tasks\n",
+    "```\n",
+    "\n",
+    "**Delivering 10,000 packages:**\n",
+    "- CPU: 4 sports cars √ó 2,500 trips = slow\n",
+    "- GPU: 1,000 trucks √ó 10 trips = FAST!"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "d3895a31",
+   "metadata": {},
+   "source": [
+    "## 2. Setting Up: Import Libraries & Check CUDA Availability\n",
+    "\n",
+    "We'll use **Numba** for Python-based CUDA programming. It lets us write CUDA kernels in Python syntax!\n",
+    "\n",
+    "First, let's check if CUDA is available on your system."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "6d8c95ef",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Import required libraries\n",
+    "import numpy as np\n",
+    "from numba import cuda\n",
+    "import math\n",
+    "\n",
+    "# Check CUDA availability\n",
+    "print(\"=\" * 50)\n",
+    "print(\"CUDA AVAILABILITY CHECK\")\n",
+    "print(\"=\" * 50)\n",
+    "\n",
+    "if cuda.is_available():\n",
+    "    print(\"‚úÖ CUDA is available!\")\n",
+    "    print(f\"   CUDA GPUs detected: {len(cuda.gpus)}\")\n",
+    "else:\n",
+    "    print(\"‚ùå CUDA is NOT available!\")\n",
+    "    print(\"   Make sure you have:\")\n",
+    "    print(\"   1. NVIDIA GPU installed\")\n",
+    "    print(\"   2. CUDA Toolkit installed\")\n",
+    "    print(\"   3. numba[cuda] installed: pip install numba[cuda]\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "1d4a9819",
+   "metadata": {},
+   "source": [
+    "## 3. Understanding CUDA Device Properties\n",
+    "\n",
+    "Before writing CUDA code, we need to understand our GPU's capabilities. Key properties include:\n",
+    "\n",
+    "| Property | What It Means |\n",
+    "|----------|---------------|\n",
+    "| **Compute Capability** | GPU architecture version (e.g., 8.6 = Ampere) |\n",
+    "| **Streaming Multiprocessors (SMs)** | Independent processing units |\n",
+    "| **Max Threads per Block** | How many threads can cooperate |\n",
+    "| **Warp Size** | Threads executed in lockstep (always 32) |\n",
+    "| **Global Memory** | Total GPU memory (VRAM) |\n",
+    "| **Shared Memory per Block** | Fast on-chip memory for cooperation |\n",
+    "\n",
+    "Let's query our GPU:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "72ee2e8d",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Query GPU properties\n",
+    "device = cuda.get_current_device()\n",
+    "\n",
+    "print(\"=\" * 60)\n",
+    "print(f\"GPU: {device.name.decode('utf-8')}\")\n",
+    "print(\"=\" * 60)\n",
+    "\n",
+    "# Compute capability\n",
+    "cc = device.compute_capability\n",
+    "print(f\"\\nüìä Compute Capability: {cc[0]}.{cc[1]}\")\n",
+    "\n",
+    "# Architecture mapping\n",
+    "arch_names = {\n",
+    "    (7, 0): \"Volta\", (7, 5): \"Turing\",\n",
+    "    (8, 0): \"Ampere\", (8, 6): \"Ampere\", (8, 9): \"Ada Lovelace\",\n",
+    "    (9, 0): \"Hopper\"\n",
+    "}\n",
+    "arch = arch_names.get(cc, \"Unknown\")\n",
+    "print(f\"   Architecture: {arch}\")\n",
+    "\n",
+    "# Processor info\n",
+    "print(f\"\\nüîß Processor Info:\")\n",
+    "print(f\"   Multiprocessors (SMs): {device.MULTIPROCESSOR_COUNT}\")\n",
+    "print(f\"   Max Threads per Block: {device.MAX_THREADS_PER_BLOCK}\")\n",
+    "print(f\"   Max Block Dimensions: {device.MAX_BLOCK_DIM_X} x {device.MAX_BLOCK_DIM_Y} x {device.MAX_BLOCK_DIM_Z}\")\n",
+    "print(f\"   Max Grid Dimensions: {device.MAX_GRID_DIM_X} x {device.MAX_GRID_DIM_Y} x {device.MAX_GRID_DIM_Z}\")\n",
+    "print(f\"   Warp Size: {device.WARP_SIZE}\")\n",
+    "\n",
+    "# Memory info\n",
+    "print(f\"\\nüíæ Memory Info:\")\n",
+    "print(f\"   Shared Memory per Block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.1f} KB\")\n",
+    "\n",
+    "# Get total memory using context\n",
+    "context = cuda.current_context()\n",
+    "free_mem, total_mem = context.get_memory_info()\n",
+    "print(f\"   Total Global Memory: {total_mem / (1024**3):.2f} GB\")\n",
+    "print(f\"   Free Memory: {free_mem / (1024**3):.2f} GB\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "af50bb02",
+   "metadata": {},
+   "source": [
+    "## 4. Your First CUDA Kernel: Vector Addition\n",
+    "\n",
+    "A **kernel** is a function that runs on the GPU. Let's start with the \"Hello World\" of GPU programming: adding two vectors.\n",
+    "\n",
+    "### Key Concepts:\n",
+    "- `@cuda.jit` decorator marks a function as a GPU kernel\n",
+    "- Kernels run on **many threads simultaneously**\n",
+    "- Each thread processes a different element\n",
+    "\n",
+    "```\n",
+    "CPU View:           GPU View (1000 threads):\n",
+    "                    \n",
+    "for i in range(N):  Thread 0: c[0] = a[0] + b[0]\n",
+    "    c[i] = a[i]+b[i] Thread 1: c[1] = a[1] + b[1]\n",
+    "                    Thread 2: c[2] = a[2] + b[2]\n",
+    "(sequential)        ...\n",
+    "                    Thread 999: c[999] = a[999] + b[999]\n",
+    "                    (ALL AT ONCE!)\n",
+    "```"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "67832fc2",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Define a CUDA kernel for vector addition\n",
+    "@cuda.jit\n",
+    "def vector_add_kernel(a, b, c):\n",
+    "    \"\"\"\n",
+    "    Each thread computes one element of c = a + b\n",
+    "    \"\"\"\n",
+    "    # Get the global thread index\n",
+    "    idx = cuda.grid(1)  # 1D grid\n",
+    "    \n",
+    "    # Boundary check (important when array size != thread count)\n",
+    "    if idx < c.size:\n",
+    "        c[idx] = a[idx] + b[idx]\n",
+    "\n",
+    "# Create test data on CPU (host)\n",
+    "N = 1_000_000  # 1 million elements\n",
+    "a_host = np.random.randn(N).astype(np.float32)\n",
+    "b_host = np.random.randn(N).astype(np.float32)\n",
+    "c_host = np.zeros(N, dtype=np.float32)\n",
+    "\n",
+    "print(f\"Vector size: {N:,} elements\")\n",
+    "print(f\"Memory per vector: {a_host.nbytes / 1024 / 1024:.2f} MB\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "e67c2cc8",
+   "metadata": {},
+   "source": [
+    "## 5. Memory Management: Host ‚Üî Device Transfers\n",
+    "\n",
+    "Data must be **explicitly copied** between CPU (host) and GPU (device):\n",
+    "\n",
+    "```\n",
+    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
+    "‚îÇ   CPU       ‚îÇ  cudaMemcpy H‚ÜíD   ‚îÇ   GPU       ‚îÇ\n",
+    "‚îÇ   (Host)    ‚îÇ ================‚ñ∫ ‚îÇ  (Device)   ‚îÇ\n",
+    "‚îÇ             ‚îÇ                    ‚îÇ             ‚îÇ\n",
+    "‚îÇ  a_host[]   ‚îÇ                    ‚îÇ  a_device[] ‚îÇ\n",
+    "‚îÇ  b_host[]   ‚îÇ                    ‚îÇ  b_device[] ‚îÇ\n",
+    "‚îÇ  c_host[]   ‚îÇ ‚óÑ================ ‚îÇ  c_device[] ‚îÇ\n",
+    "‚îÇ             ‚îÇ  cudaMemcpy D‚ÜíH   ‚îÇ             ‚îÇ\n",
+    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
+    "        PCIe Bus (bottleneck!)\n",
+    "```\n",
+    "\n",
+    "**Key Functions:**\n",
+    "- `cuda.to_device(array)` - Copy host ‚Üí device\n",
+    "- `cuda.device_array(shape)` - Allocate on device (no copy)\n",
+    "- `device_array.copy_to_host()` - Copy device ‚Üí host"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "dfd0c9bc",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Transfer data to GPU\n",
+    "a_device = cuda.to_device(a_host)  # Copy a to GPU\n",
+    "b_device = cuda.to_device(b_host)  # Copy b to GPU\n",
+    "c_device = cuda.device_array(N, dtype=np.float32)  # Allocate c on GPU (no copy needed)\n",
+    "\n",
+    "print(\"‚úÖ Data transferred to GPU\")\n",
+    "print(f\"   a_device type: {type(a_device)}\")\n",
+    "print(f\"   Shape: {a_device.shape}, Dtype: {a_device.dtype}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "287eef1d",
+   "metadata": {},
+   "source": [
+    "## 6. Thread and Block Configuration\n",
+    "\n",
+    "CUDA organizes threads in a hierarchy:\n",
+    "\n",
+    "```\n",
+    "                    Grid (all threads)\n",
+    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
+    "                    ‚îÇ  Block 0    Block 1    Block 2  ...‚îÇ\n",
+    "                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
+    "                    ‚îÇ  ‚îÇThread‚îÇ   ‚îÇThread‚îÇ   ‚îÇThread‚îÇ    ‚îÇ\n",
+    "                    ‚îÇ  ‚îÇ  0-N ‚îÇ   ‚îÇ  0-N ‚îÇ   ‚îÇ  0-N ‚îÇ    ‚îÇ\n",
+    "                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
+    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
+    "```\n",
+    "\n",
+    "### Kernel Launch Syntax: `kernel[blocks_per_grid, threads_per_block](...)`\n",
+    "\n",
+    "**Rules of thumb:**\n",
+    "- `threads_per_block`: Usually 128, 256, or 512 (must be ‚â§ 1024)\n",
+    "- `blocks_per_grid`: Calculated to cover all elements\n",
+    "- Total threads = blocks √ó threads_per_block"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "350dd1b5",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Configure kernel launch parameters\n",
+    "threads_per_block = 256  # Common choice\n",
+    "\n",
+    "# Calculate blocks needed to cover all elements\n",
+    "# Formula: ceil(N / threads_per_block)\n",
+    "blocks_per_grid = math.ceil(N / threads_per_block)\n",
+    "\n",
+    "print(f\"üìê Launch Configuration:\")\n",
+    "print(f\"   Array size: {N:,}\")\n",
+    "print(f\"   Threads per block: {threads_per_block}\")\n",
+    "print(f\"   Blocks per grid: {blocks_per_grid:,}\")\n",
+    "print(f\"   Total threads: {blocks_per_grid * threads_per_block:,}\")\n",
+    "print(f\"   Extra threads (boundary check needed): {blocks_per_grid * threads_per_block - N:,}\")\n",
+    "\n",
+    "# Launch the kernel!\n",
+    "vector_add_kernel[blocks_per_grid, threads_per_block](a_device, b_device, c_device)\n",
+    "\n",
+    "# Wait for GPU to finish\n",
+    "cuda.synchronize()\n",
+    "print(\"\\n‚úÖ Kernel execution complete!\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "dbbeef96",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Copy result back to CPU and verify\n",
+    "c_host = c_device.copy_to_host()\n",
+    "\n",
+    "# Verify correctness\n",
+    "expected = a_host + b_host\n",
+    "if np.allclose(c_host, expected):\n",
+    "    print(\"‚úÖ VERIFICATION PASSED!\")\n",
+    "    print(f\"   First 5 elements: {c_host[:5]}\")\n",
+    "    print(f\"   Expected:         {expected[:5]}\")\n",
+    "else:\n",
+    "    print(\"‚ùå VERIFICATION FAILED!\")\n",
+    "    diff = np.abs(c_host - expected).max()\n",
+    "    print(f\"   Max difference: {diff}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "43906758",
+   "metadata": {},
+   "source": [
+    "## 7. Performance Comparison: CPU vs GPU\n",
+    "\n",
+    "Now let's see the real benefit of GPU computing - speed!"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "3ccefc34",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import time\n",
+    "\n",
+    "def benchmark_cpu_gpu(sizes):\n",
+    "    \"\"\"Compare CPU and GPU performance across different array sizes\"\"\"\n",
+    "    results = []\n",
+    "    \n",
+    "    for N in sizes:\n",
+    "        # Create data\n",
+    "        a = np.random.randn(N).astype(np.float32)\n",
+    "        b = np.random.randn(N).astype(np.float32)\n",
+    "        \n",
+    "        # CPU timing\n",
+    "        start = time.perf_counter()\n",
+    "        c_cpu = a + b\n",
+    "        cpu_time = time.perf_counter() - start\n",
+    "        \n",
+    "        # GPU timing (including transfers)\n",
+    "        start = time.perf_counter()\n",
+    "        a_d = cuda.to_device(a)\n",
+    "        b_d = cuda.to_device(b)\n",
+    "        c_d = cuda.device_array(N, dtype=np.float32)\n",
+    "        \n",
+    "        tpb = 256\n",
+    "        bpg = math.ceil(N / tpb)\n",
+    "        vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
+    "        cuda.synchronize()\n",
+    "        c_gpu = c_d.copy_to_host()\n",
+    "        gpu_time = time.perf_counter() - start\n",
+    "        \n",
+    "        speedup = cpu_time / gpu_time if gpu_time > 0 else 0\n",
+    "        results.append((N, cpu_time*1000, gpu_time*1000, speedup))\n",
+    "        \n",
+    "    return results\n",
+    "\n",
+    "# Run benchmarks\n",
+    "sizes = [1_000, 10_000, 100_000, 1_000_000, 10_000_000, 50_000_000]\n",
+    "print(\"üèÅ Benchmarking CPU vs GPU...\")\n",
+    "print(\"-\" * 65)\n",
+    "print(f\"{'Array Size':>12} | {'CPU (ms)':>10} | {'GPU (ms)':>10} | {'Speedup':>10}\")\n",
+    "print(\"-\" * 65)\n",
+    "\n",
+    "results = benchmark_cpu_gpu(sizes)\n",
+    "for N, cpu_ms, gpu_ms, speedup in results:\n",
+    "    indicator = \"üöÄ\" if speedup > 1 else \"üê¢\"\n",
+    "    print(f\"{N:>12,} | {cpu_ms:>10.3f} | {gpu_ms:>10.3f} | {speedup:>9.2f}x {indicator}\")\n",
+    "\n",
+    "print(\"-\" * 65)\n",
+    "print(\"\\nüí° Note: GPU shines with larger arrays (overhead amortized)\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "3fa7cd85",
+   "metadata": {},
+   "source": [
+    "## üéØ Exercises\n",
+    "\n",
+    "Now it's your turn! Complete these exercises to solidify your understanding.\n",
+    "\n",
+    "### Exercise 1: Vector Subtraction\n",
+    "Modify the vector addition kernel to perform subtraction (c = a - b).\n",
+    "\n",
+    "### Exercise 2: Element-wise Multiplication  \n",
+    "Create a new kernel for element-wise multiplication (c = a * b).\n",
+    "\n",
+    "### Exercise 3: Different Block Sizes\n",
+    "Experiment with different `threads_per_block` values (64, 128, 256, 512, 1024).\n",
+    "Which performs best? Why might that be?"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "9b94c0ee",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 1: Vector Subtraction\n",
+    "# Create a kernel that computes c = a - b\n",
+    "\n",
+    "@cuda.jit\n",
+    "def vector_sub_kernel(a, b, c):\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < c.size:\n",
+    "        # TODO: Replace pass with subtraction\n",
+    "        pass\n",
+    "\n",
+    "# Test your kernel here:\n",
+    "# ..."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "13fec6b4",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 2: Element-wise Multiplication\n",
+    "# Create a kernel that computes c = a * b\n",
+    "\n",
+    "@cuda.jit\n",
+    "def vector_mul_kernel(a, b, c):\n",
+    "    # TODO: Implement this kernel\n",
+    "    pass\n",
+    "\n",
+    "# Test your kernel here:\n",
+    "# ..."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "7aea6db1",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 3: Block Size Experiment\n",
+    "# Try different threads_per_block values and compare performance\n",
+    "\n",
+    "def benchmark_block_sizes(N=10_000_000):\n",
+    "    \"\"\"Test different block sizes\"\"\"\n",
+    "    a = np.random.randn(N).astype(np.float32)\n",
+    "    b = np.random.randn(N).astype(np.float32)\n",
+    "    a_d = cuda.to_device(a)\n",
+    "    b_d = cuda.to_device(b)\n",
+    "    c_d = cuda.device_array(N, dtype=np.float32)\n",
+    "    \n",
+    "    block_sizes = [32, 64, 128, 256, 512, 1024]\n",
+    "    \n",
+    "    print(f\"Testing with N = {N:,}\")\n",
+    "    print(\"-\" * 40)\n",
+    "    \n",
+    "    for tpb in block_sizes:\n",
+    "        bpg = math.ceil(N / tpb)\n",
+    "        \n",
+    "        # Warmup\n",
+    "        vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
+    "        cuda.synchronize()\n",
+    "        \n",
+    "        # Benchmark\n",
+    "        start = time.perf_counter()\n",
+    "        for _ in range(100):\n",
+    "            vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
+    "        cuda.synchronize()\n",
+    "        elapsed = (time.perf_counter() - start) / 100 * 1000\n",
+    "        \n",
+    "        print(f\"Block size {tpb:4d}: {elapsed:.3f} ms\")\n",
+    "\n",
+    "# TODO: Run the benchmark and analyze results\n",
+    "# benchmark_block_sizes()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "93678fce",
+   "metadata": {},
+   "source": [
+    "## üìù Key Takeaways\n",
+    "\n",
+    "### Today You Learned:\n",
+    "\n",
+    "1. **GPUs vs CPUs**: GPUs excel at throughput (many simple operations), CPUs at latency (complex single operations)\n",
+    "\n",
+    "2. **GPU Architecture**:\n",
+    "   - Streaming Multiprocessors (SMs) contain many CUDA cores\n",
+    "   - Warps are groups of 32 threads executing together\n",
+    "   - Compute capability indicates architecture generation\n",
+    "\n",
+    "3. **CUDA Programming Model**:\n",
+    "   - Kernels run on GPU, host code runs on CPU\n",
+    "   - Threads organized into blocks, blocks into grids\n",
+    "   - Each thread gets a unique index via `cuda.grid()`\n",
+    "\n",
+    "4. **Memory Management**:\n",
+    "   - Data must be explicitly transferred between host and device\n",
+    "   - `cuda.to_device()` copies to GPU\n",
+    "   - `array.copy_to_host()` copies back\n",
+    "\n",
+    "5. **Performance Considerations**:\n",
+    "   - GPU overhead matters for small arrays\n",
+    "   - GPU wins big for large parallel workloads\n",
+    "   - Block size affects performance (experiment!)\n",
+    "\n",
+    "---\n",
+    "\n",
+    "### üìö Next Up: Day 2 - Thread Indexing Deep Dive\n",
+    "- 1D, 2D, and 3D thread indexing\n",
+    "- Grid-stride loops for arbitrary sizes\n",
+    "- Handling edge cases\n",
+    "\n",
+    "---\n",
+    "\n",
+    "### üîó Additional Resources\n",
+    "- [CUDA Programming Guide - Introduction](../../cuda-programming-guide/01-introduction/programming-model.md)\n",
+    "- [Quick Reference](../../notes/cuda-quick-reference.md)"
+   ]
+  }
+ ],
+ "metadata": {
+  "language_info": {
+   "name": "python"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/learning-path/week-01/day-2-thread-indexing.ipynb b/learning-path/week-01/day-2-thread-indexing.ipynb
new file mode 100644
index 0000000..803af36
--- /dev/null
+++ b/learning-path/week-01/day-2-thread-indexing.ipynb
@@ -0,0 +1,614 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "id": "92ddfa01",
+   "metadata": {},
+   "source": [
+    "# üöÄ Day 2: Thread Indexing Mastery\n",
+    "\n",
+    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-2-thread-indexing.ipynb)\n",
+    "\n",
+    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "0ce586ba",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# ‚öôÔ∏è Colab Setup Cell - Run this first!\n",
+    "import subprocess, sys\n",
+    "try:\n",
+    "    import google.colab\n",
+    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
+    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
+    "    print(\"‚úÖ Setup complete!\")\n",
+    "except ImportError:\n",
+    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "54e64bf6",
+   "metadata": {},
+   "source": [
+    "# Day 2: Thread Indexing Mastery\n",
+    "\n",
+    "Yesterday you launched your first CUDA kernel. Today we'll master the art of **thread indexing** - how each thread knows which data element to process.\n",
+    "\n",
+    "## Learning Objectives\n",
+    "- Understand 1D, 2D, and 3D thread indexing\n",
+    "- Master the relationship between blocks, threads, and global indices\n",
+    "- Implement grid-stride loops for handling any array size\n",
+    "- Apply indexing to real 2D problems (matrices, images)\n",
+    "\n",
+    "---\n",
+    "\n",
+    "## 1. Thread Hierarchy Recap\n",
+    "\n",
+    "```\n",
+    "                           GRID\n",
+    "                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
+    "                ‚îÇ Block   Block   Block   ‚îÇ\n",
+    "                ‚îÇ (0,0)   (1,0)   (2,0)   ‚îÇ\n",
+    "                ‚îÇ                         ‚îÇ\n",
+    "                ‚îÇ Block   Block   Block   ‚îÇ\n",
+    "                ‚îÇ (0,1)   (1,1)   (2,1)   ‚îÇ\n",
+    "                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
+    "                        \n",
+    "                      Each Block:\n",
+    "                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
+    "                ‚îÇ Thread Thread Thread    ‚îÇ\n",
+    "                ‚îÇ (0,0)  (1,0)  (2,0)     ‚îÇ\n",
+    "                ‚îÇ                         ‚îÇ\n",
+    "                ‚îÇ Thread Thread Thread    ‚îÇ\n",
+    "                ‚îÇ (0,1)  (1,1)  (2,1)     ‚îÇ\n",
+    "                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
+    "```\n",
+    "\n",
+    "**Built-in Variables (in Numba):**\n",
+    "- `cuda.threadIdx.x/y/z` - Thread index within block\n",
+    "- `cuda.blockIdx.x/y/z` - Block index within grid\n",
+    "- `cuda.blockDim.x/y/z` - Threads per block\n",
+    "- `cuda.gridDim.x/y/z` - Blocks in grid"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "8b8f1276",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Setup\n",
+    "import numpy as np\n",
+    "from numba import cuda\n",
+    "import math\n",
+    "\n",
+    "print(\"CUDA available:\", cuda.is_available())\n",
+    "print(\"Current device:\", cuda.get_current_device().name.decode())"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "40ed9210",
+   "metadata": {},
+   "source": [
+    "## 2. 1D Indexing: The Foundation\n",
+    "\n",
+    "For 1D arrays, each thread needs a unique **global index**:\n",
+    "\n",
+    "```\n",
+    "Global Index = blockIdx.x * blockDim.x + threadIdx.x\n",
+    "\n",
+    "Example: 3 blocks √ó 4 threads/block = 12 threads\n",
+    "\n",
+    "Block 0:  Thread 0  Thread 1  Thread 2  Thread 3\n",
+    "          idx=0     idx=1     idx=2     idx=3\n",
+    "          \n",
+    "Block 1:  Thread 0  Thread 1  Thread 2  Thread 3  \n",
+    "          idx=4     idx=5     idx=6     idx=7\n",
+    "          \n",
+    "Block 2:  Thread 0  Thread 1  Thread 2  Thread 3\n",
+    "          idx=8     idx=9     idx=10    idx=11\n",
+    "```\n",
+    "\n",
+    "Let's visualize this:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "9540d064",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "@cuda.jit\n",
+    "def print_1d_indices(output):\n",
+    "    \"\"\"Each thread writes its indices to the output array\"\"\"\n",
+    "    # Get indices manually\n",
+    "    block_id = cuda.blockIdx.x\n",
+    "    thread_id = cuda.threadIdx.x\n",
+    "    block_size = cuda.blockDim.x\n",
+    "    \n",
+    "    # Calculate global index\n",
+    "    global_idx = block_id * block_size + thread_id\n",
+    "    \n",
+    "    # Or use the convenient helper:\n",
+    "    # global_idx = cuda.grid(1)\n",
+    "    \n",
+    "    if global_idx < output.shape[0]:\n",
+    "        # Store: [global_idx, block_id, thread_id]\n",
+    "        output[global_idx, 0] = global_idx\n",
+    "        output[global_idx, 1] = block_id\n",
+    "        output[global_idx, 2] = thread_id\n",
+    "\n",
+    "# Launch with 3 blocks √ó 4 threads\n",
+    "threads_per_block = 4\n",
+    "blocks = 3\n",
+    "total_threads = blocks * threads_per_block\n",
+    "\n",
+    "output = np.zeros((total_threads, 3), dtype=np.int32)\n",
+    "output_d = cuda.to_device(output)\n",
+    "\n",
+    "print_1d_indices[blocks, threads_per_block](output_d)\n",
+    "result = output_d.copy_to_host()\n",
+    "\n",
+    "print(\"1D Thread Indexing Visualization\")\n",
+    "print(\"=\" * 50)\n",
+    "print(f\"Configuration: {blocks} blocks √ó {threads_per_block} threads/block\")\n",
+    "print(\"-\" * 50)\n",
+    "print(f\"{'Global Idx':^12} | {'Block ID':^10} | {'Thread ID':^10}\")\n",
+    "print(\"-\" * 50)\n",
+    "for row in result:\n",
+    "    print(f\"{row[0]:^12} | {row[1]:^10} | {row[2]:^10}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "6b7e9050",
+   "metadata": {},
+   "source": [
+    "## 3. Boundary Checking: When Threads > Elements\n",
+    "\n",
+    "What happens when array size isn't a perfect multiple of block size?\n",
+    "\n",
+    "```\n",
+    "Array size: 10 elements\n",
+    "Block size: 4 threads\n",
+    "Blocks needed: ceil(10/4) = 3\n",
+    "Total threads: 3 √ó 4 = 12\n",
+    "\n",
+    "Thread indices: 0  1  2  3  4  5  6  7  8  9  10  11\n",
+    "Array elements: ‚úì  ‚úì  ‚úì  ‚úì  ‚úì  ‚úì  ‚úì  ‚úì  ‚úì  ‚úì   ‚úó   ‚úó\n",
+    "                                               ‚Üë    ‚Üë\n",
+    "                                          Out of bounds!\n",
+    "```\n",
+    "\n",
+    "**Always add boundary checks!**"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "7c6141a5",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "@cuda.jit\n",
+    "def safe_square(input_arr, output_arr, n):\n",
+    "    \"\"\"Square each element with proper boundary check\"\"\"\n",
+    "    idx = cuda.grid(1)\n",
+    "    \n",
+    "    # CRITICAL: Boundary check\n",
+    "    if idx < n:\n",
+    "        output_arr[idx] = input_arr[idx] ** 2\n",
+    "\n",
+    "# Array that's NOT a multiple of block size\n",
+    "N = 1000\n",
+    "a = np.arange(N, dtype=np.float32)\n",
+    "b = np.zeros(N, dtype=np.float32)\n",
+    "\n",
+    "threads_per_block = 256\n",
+    "blocks = math.ceil(N / threads_per_block)\n",
+    "\n",
+    "print(f\"Array size: {N}\")\n",
+    "print(f\"Threads per block: {threads_per_block}\")\n",
+    "print(f\"Blocks needed: {blocks}\")\n",
+    "print(f\"Total threads: {blocks * threads_per_block}\")\n",
+    "print(f\"Extra threads (idle): {blocks * threads_per_block - N}\")\n",
+    "\n",
+    "a_d = cuda.to_device(a)\n",
+    "b_d = cuda.to_device(b)\n",
+    "\n",
+    "safe_square[blocks, threads_per_block](a_d, b_d, N)\n",
+    "result = b_d.copy_to_host()\n",
+    "\n",
+    "# Verify\n",
+    "expected = a ** 2\n",
+    "print(f\"\\n‚úÖ Correct: {np.allclose(result, expected)}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "08549608",
+   "metadata": {},
+   "source": [
+    "## 4. Grid-Stride Loops: The Professional Pattern\n",
+    "\n",
+    "What if your array has **billions** of elements but you can only launch millions of threads?\n",
+    "\n",
+    "**Grid-stride loop**: Each thread processes multiple elements, striding by the total grid size.\n",
+    "\n",
+    "```\n",
+    "Array: [0][1][2][3][4][5][6][7][8][9][10][11]...\n",
+    "\n",
+    "Grid size: 4 threads\n",
+    "\n",
+    "Thread 0: processes indices 0, 4, 8, 12, ...\n",
+    "Thread 1: processes indices 1, 5, 9, 13, ...\n",
+    "Thread 2: processes indices 2, 6, 10, 14, ...\n",
+    "Thread 3: processes indices 3, 7, 11, 15, ...\n",
+    "```\n",
+    "\n",
+    "This pattern is **essential** for production code!"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "c37c94ce",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "@cuda.jit\n",
+    "def grid_stride_square(input_arr, output_arr, n):\n",
+    "    \"\"\"Process arbitrary-sized arrays with grid-stride loop\"\"\"\n",
+    "    # Starting index for this thread\n",
+    "    idx = cuda.grid(1)\n",
+    "    \n",
+    "    # Total number of threads in the grid\n",
+    "    stride = cuda.gridsize(1)  # = blockDim.x * gridDim.x\n",
+    "    \n",
+    "    # Grid-stride loop: each thread handles multiple elements\n",
+    "    while idx < n:\n",
+    "        output_arr[idx] = input_arr[idx] ** 2\n",
+    "        idx += stride  # Jump to next element this thread handles\n",
+    "\n",
+    "# Huge array with limited grid\n",
+    "N = 100_000_000  # 100 million elements!\n",
+    "a = np.random.randn(N).astype(np.float32)\n",
+    "b = np.zeros(N, dtype=np.float32)\n",
+    "\n",
+    "# Fixed, reasonable grid size\n",
+    "threads_per_block = 256\n",
+    "blocks = 256  # Only 256 blocks, but handles 100M elements!\n",
+    "\n",
+    "print(f\"Array size: {N:,}\")\n",
+    "print(f\"Grid size: {blocks} blocks √ó {threads_per_block} threads = {blocks * threads_per_block:,} threads\")\n",
+    "print(f\"Elements per thread (average): {N / (blocks * threads_per_block):.1f}\")\n",
+    "\n",
+    "a_d = cuda.to_device(a)\n",
+    "b_d = cuda.to_device(b)\n",
+    "\n",
+    "import time\n",
+    "start = time.perf_counter()\n",
+    "grid_stride_square[blocks, threads_per_block](a_d, b_d, N)\n",
+    "cuda.synchronize()\n",
+    "elapsed = time.perf_counter() - start\n",
+    "\n",
+    "result = b_d.copy_to_host()\n",
+    "print(f\"\\n‚è±Ô∏è  Time: {elapsed*1000:.2f} ms\")\n",
+    "print(f\"‚úÖ Correct: {np.allclose(result, a**2)}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "8aaca503",
+   "metadata": {},
+   "source": [
+    "## 5. 2D Indexing: Matrices and Images\n",
+    "\n",
+    "For 2D data (matrices, images), we use 2D thread blocks and grids:\n",
+    "\n",
+    "```\n",
+    "Image (Height √ó Width):\n",
+    "‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê\n",
+    "‚îÇ0,0‚îÇ0,1‚îÇ0,2‚îÇ0,3‚îÇ0,4‚îÇ  Row 0\n",
+    "‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§\n",
+    "‚îÇ1,0‚îÇ1,1‚îÇ1,2‚îÇ1,3‚îÇ1,4‚îÇ  Row 1\n",
+    "‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§\n",
+    "‚îÇ2,0‚îÇ2,1‚îÇ2,2‚îÇ2,3‚îÇ2,4‚îÇ  Row 2\n",
+    "‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò\n",
+    "\n",
+    "Thread coordinates:\n",
+    "  row = blockIdx.y * blockDim.y + threadIdx.y\n",
+    "  col = blockIdx.x * blockDim.x + threadIdx.x\n",
+    "```"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "bef3ac11",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "@cuda.jit\n",
+    "def matrix_double_2d(matrix, rows, cols):\n",
+    "    \"\"\"Double each element using 2D indexing\"\"\"\n",
+    "    # Get 2D thread coordinates\n",
+    "    col, row = cuda.grid(2)  # Returns (x, y) = (col, row)\n",
+    "    \n",
+    "    # Boundary check for both dimensions\n",
+    "    if row < rows and col < cols:\n",
+    "        matrix[row, col] *= 2\n",
+    "\n",
+    "# Create a small matrix to visualize\n",
+    "rows, cols = 6, 8\n",
+    "matrix = np.arange(rows * cols, dtype=np.float32).reshape(rows, cols)\n",
+    "\n",
+    "print(\"Original matrix:\")\n",
+    "print(matrix)\n",
+    "print()\n",
+    "\n",
+    "# 2D block configuration\n",
+    "threads_per_block_2d = (4, 4)  # 4√ó4 = 16 threads per block\n",
+    "blocks_per_grid_x = math.ceil(cols / threads_per_block_2d[0])\n",
+    "blocks_per_grid_y = math.ceil(rows / threads_per_block_2d[1])\n",
+    "blocks_per_grid_2d = (blocks_per_grid_x, blocks_per_grid_y)\n",
+    "\n",
+    "print(f\"Block size: {threads_per_block_2d}\")\n",
+    "print(f\"Grid size: {blocks_per_grid_2d}\")\n",
+    "\n",
+    "matrix_d = cuda.to_device(matrix)\n",
+    "matrix_double_2d[blocks_per_grid_2d, threads_per_block_2d](matrix_d, rows, cols)\n",
+    "result = matrix_d.copy_to_host()\n",
+    "\n",
+    "print(\"\\nDoubled matrix:\")\n",
+    "print(result)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "cc9f2cc0",
+   "metadata": {},
+   "source": [
+    "## 6. Practical Example: Image Processing (Grayscale)\n",
+    "\n",
+    "Let's apply 2D indexing to convert an RGB image to grayscale.\n",
+    "\n",
+    "Grayscale formula: `Y = 0.299*R + 0.587*G + 0.114*B`"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "ecff7807",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "@cuda.jit\n",
+    "def rgb_to_grayscale(rgb_image, gray_image, height, width):\n",
+    "    \"\"\"Convert RGB image to grayscale using 2D indexing\"\"\"\n",
+    "    col, row = cuda.grid(2)\n",
+    "    \n",
+    "    if row < height and col < width:\n",
+    "        # RGB channels\n",
+    "        r = rgb_image[row, col, 0]\n",
+    "        g = rgb_image[row, col, 1]\n",
+    "        b = rgb_image[row, col, 2]\n",
+    "        \n",
+    "        # Weighted sum (human perception weighting)\n",
+    "        gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
+    "        \n",
+    "        gray_image[row, col] = gray\n",
+    "\n",
+    "# Create a synthetic RGB image (like a gradient)\n",
+    "height, width = 1080, 1920  # Full HD\n",
+    "rgb_image = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8).astype(np.float32)\n",
+    "\n",
+    "gray_image = np.zeros((height, width), dtype=np.float32)\n",
+    "\n",
+    "# Configure 2D grid\n",
+    "threads_per_block = (16, 16)  # 256 threads per block\n",
+    "blocks_x = math.ceil(width / threads_per_block[0])\n",
+    "blocks_y = math.ceil(height / threads_per_block[1])\n",
+    "blocks_per_grid = (blocks_x, blocks_y)\n",
+    "\n",
+    "print(f\"Image size: {height} √ó {width} ({height * width:,} pixels)\")\n",
+    "print(f\"Thread block: {threads_per_block}\")\n",
+    "print(f\"Grid: {blocks_per_grid}\")\n",
+    "print(f\"Total threads: {blocks_x * threads_per_block[0]} √ó {blocks_y * threads_per_block[1]}\")\n",
+    "\n",
+    "# Run on GPU\n",
+    "rgb_d = cuda.to_device(rgb_image)\n",
+    "gray_d = cuda.to_device(gray_image)\n",
+    "\n",
+    "import time\n",
+    "start = time.perf_counter()\n",
+    "rgb_to_grayscale[blocks_per_grid, threads_per_block](rgb_d, gray_d, height, width)\n",
+    "cuda.synchronize()\n",
+    "gpu_time = time.perf_counter() - start\n",
+    "\n",
+    "gray_result = gray_d.copy_to_host()\n",
+    "\n",
+    "# Compare with NumPy (CPU)\n",
+    "start = time.perf_counter()\n",
+    "gray_cpu = 0.299 * rgb_image[:,:,0] + 0.587 * rgb_image[:,:,1] + 0.114 * rgb_image[:,:,2]\n",
+    "cpu_time = time.perf_counter() - start\n",
+    "\n",
+    "print(f\"\\n‚è±Ô∏è  GPU time: {gpu_time*1000:.3f} ms\")\n",
+    "print(f\"‚è±Ô∏è  CPU time: {cpu_time*1000:.3f} ms\")\n",
+    "print(f\"üöÄ Speedup: {cpu_time/gpu_time:.2f}x\")\n",
+    "print(f\"‚úÖ Correct: {np.allclose(gray_result, gray_cpu, atol=1e-5)}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "80f05a7c",
+   "metadata": {},
+   "source": [
+    "## üéØ Exercises\n",
+    "\n",
+    "### Exercise 1: Manual Index Calculation\n",
+    "Implement 1D indexing WITHOUT using `cuda.grid()` - calculate manually using `blockIdx`, `blockDim`, `threadIdx`."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "f724daed",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 1: Manual indexing\n",
+    "@cuda.jit\n",
+    "def manual_index_add(a, b, c):\n",
+    "    \"\"\"Add arrays using MANUAL index calculation\"\"\"\n",
+    "    # TODO: Calculate global index manually\n",
+    "    # idx = blockIdx.x * blockDim.x + threadIdx.x\n",
+    "    idx = 0  # FIX THIS\n",
+    "    \n",
+    "    if idx < c.size:\n",
+    "        c[idx] = a[idx] + b[idx]\n",
+    "\n",
+    "# Test your implementation\n",
+    "# N = 1000\n",
+    "# a = np.random.randn(N).astype(np.float32)\n",
+    "# b = np.random.randn(N).astype(np.float32)\n",
+    "# ..."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "cefcfb60",
+   "metadata": {},
+   "source": [
+    "### Exercise 2: 2D Grid-Stride Loop\n",
+    "Implement a 2D grid-stride loop for processing very large images."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "b29dc6e8",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 2: 2D Grid-Stride Loop\n",
+    "@cuda.jit\n",
+    "def grid_stride_2d(matrix, height, width):\n",
+    "    \"\"\"Process large matrix with 2D grid-stride loop\"\"\"\n",
+    "    # Starting position\n",
+    "    start_col, start_row = cuda.grid(2)\n",
+    "    \n",
+    "    # Stride (grid size in each dimension)\n",
+    "    stride_col, stride_row = cuda.gridsize(2)\n",
+    "    \n",
+    "    # TODO: Implement 2D grid-stride loop\n",
+    "    # Hint: Use nested while loops\n",
+    "    # row = start_row\n",
+    "    # while row < height:\n",
+    "    #     col = start_col\n",
+    "    #     while col < width:\n",
+    "    #         # process matrix[row, col]\n",
+    "    #         col += stride_col\n",
+    "    #     row += stride_row\n",
+    "    pass\n",
+    "\n",
+    "# Test with a huge matrix\n",
+    "# ..."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "ca86ad64",
+   "metadata": {},
+   "source": [
+    "### Exercise 3: Image Negative\n",
+    "Create a kernel that inverts an image (negative): `output[i,j] = 255 - input[i,j]`"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "64ac798d",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 3: Image Negative\n",
+    "@cuda.jit\n",
+    "def image_negative(input_img, output_img, height, width):\n",
+    "    \"\"\"Invert image: output = 255 - input\"\"\"\n",
+    "    # TODO: Implement using 2D indexing\n",
+    "    pass\n",
+    "\n",
+    "# Test your implementation\n",
+    "# ..."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "a806a8ff",
+   "metadata": {},
+   "source": [
+    "## üìù Key Takeaways\n",
+    "\n",
+    "### Today You Learned:\n",
+    "\n",
+    "1. **1D Indexing Formula**:\n",
+    "   ```\n",
+    "   global_idx = blockIdx.x * blockDim.x + threadIdx.x\n",
+    "   # Or simply: cuda.grid(1)\n",
+    "   ```\n",
+    "\n",
+    "2. **2D Indexing Formula**:\n",
+    "   ```\n",
+    "   col, row = cuda.grid(2)\n",
+    "   # Manually: \n",
+    "   # row = blockIdx.y * blockDim.y + threadIdx.y\n",
+    "   # col = blockIdx.x * blockDim.x + threadIdx.x\n",
+    "   ```\n",
+    "\n",
+    "3. **Always check boundaries**: `if idx < n:`\n",
+    "\n",
+    "4. **Grid-stride loops** handle any array size with fixed grid:\n",
+    "   ```python\n",
+    "   idx = cuda.grid(1)\n",
+    "   stride = cuda.gridsize(1)\n",
+    "   while idx < n:\n",
+    "       # process element\n",
+    "       idx += stride\n",
+    "   ```\n",
+    "\n",
+    "5. **Common block sizes**:\n",
+    "   - 1D: 256 or 512 threads\n",
+    "   - 2D: (16, 16) or (32, 32) threads\n",
+    "\n",
+    "---\n",
+    "\n",
+    "### üìö Next Up: Day 3 - Memory Fundamentals\n",
+    "- cudaMalloc vs cudaMallocManaged\n",
+    "- Pinned vs pageable memory\n",
+    "- Memory transfer optimization\n",
+    "\n",
+    "---\n",
+    "\n",
+    "### üîó Resources\n",
+    "- [Thread Hierarchy - Programming Guide](../../cuda-programming-guide/01-introduction/programming-model.md)\n",
+    "- [Quick Reference](../../notes/cuda-quick-reference.md)"
+   ]
+  }
+ ],
+ "metadata": {
+  "language_info": {
+   "name": "python"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/learning-path/week-01/day-3-memory-basics.ipynb b/learning-path/week-01/day-3-memory-basics.ipynb
new file mode 100644
index 0000000..1351c1d
--- /dev/null
+++ b/learning-path/week-01/day-3-memory-basics.ipynb
@@ -0,0 +1,555 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "id": "ea28ffed",
+   "metadata": {},
+   "source": [
+    "# üöÄ Day 3: GPU Memory Fundamentals\n",
+    "\n",
+    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-3-memory-basics.ipynb)\n",
+    "\n",
+    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "164740f9",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# ‚öôÔ∏è Colab Setup Cell - Run this first!\n",
+    "import subprocess, sys\n",
+    "try:\n",
+    "    import google.colab\n",
+    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
+    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
+    "    print(\"‚úÖ Setup complete!\")\n",
+    "except ImportError:\n",
+    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "b5957e72",
+   "metadata": {},
+   "source": [
+    "# Day 3: GPU Memory Fundamentals\n",
+    "\n",
+    "Memory management is **the most important skill** in CUDA programming. Today you'll learn:\n",
+    "- GPU memory hierarchy\n",
+    "- Explicit memory management (cudaMalloc, cudaMemcpy)\n",
+    "- Unified/Managed memory (cudaMallocManaged)\n",
+    "- Pinned vs pageable memory for transfers\n",
+    "- Performance implications of each approach\n",
+    "\n",
+    "---\n",
+    "\n",
+    "## 1. The Memory Hierarchy\n",
+    "\n",
+    "```\n",
+    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
+    "‚îÇ                        GPU MEMORY HIERARCHY                      ‚îÇ\n",
+    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
+    "‚îÇ                                                                  ‚îÇ\n",
+    "‚îÇ   Registers (per thread)     ‚Üê Fastest (~0 latency)             ‚îÇ\n",
+    "‚îÇ   ‚îú‚îÄ‚îÄ ~255 32-bit registers per thread                          ‚îÇ\n",
+    "‚îÇ   ‚îî‚îÄ‚îÄ Private to each thread                                    ‚îÇ\n",
+    "‚îÇ                                                                  ‚îÇ\n",
+    "‚îÇ   Shared Memory (per block)  ‚Üê Very fast (~5 cycles)            ‚îÇ\n",
+    "‚îÇ   ‚îú‚îÄ‚îÄ 48-164 KB per SM                                          ‚îÇ\n",
+    "‚îÇ   ‚îî‚îÄ‚îÄ Shared between threads in same block                      ‚îÇ\n",
+    "‚îÇ                                                                  ‚îÇ\n",
+    "‚îÇ   L1 Cache / Texture Cache   ‚Üê Fast (~30 cycles)                ‚îÇ\n",
+    "‚îÇ   ‚îî‚îÄ‚îÄ Automatic caching                                         ‚îÇ\n",
+    "‚îÇ                                                                  ‚îÇ\n",
+    "‚îÇ   L2 Cache                   ‚Üê Medium (~200 cycles)             ‚îÇ\n",
+    "‚îÇ   ‚îî‚îÄ‚îÄ Shared across all SMs                                     ‚îÇ\n",
+    "‚îÇ                                                                  ‚îÇ\n",
+    "‚îÇ   Global Memory (VRAM)       ‚Üê Slow (~400+ cycles)              ‚îÇ\n",
+    "‚îÇ   ‚îú‚îÄ‚îÄ 4-80 GB (main GPU memory)                                 ‚îÇ\n",
+    "‚îÇ   ‚îî‚îÄ‚îÄ Accessible by all threads                                 ‚îÇ\n",
+    "‚îÇ                                                                  ‚îÇ\n",
+    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
+    "‚îÇ   Host Memory (System RAM)   ‚Üê Very slow (PCIe transfer)        ‚îÇ\n",
+    "‚îÇ   ‚îî‚îÄ‚îÄ Must be copied to GPU before use                          ‚îÇ\n",
+    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
+    "```"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "347ffdf9",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Setup\n",
+    "import numpy as np\n",
+    "from numba import cuda\n",
+    "import math\n",
+    "import time\n",
+    "\n",
+    "print(\"CUDA device:\", cuda.get_current_device().name.decode())\n",
+    "\n",
+    "# Get memory info\n",
+    "ctx = cuda.current_context()\n",
+    "free_mem, total_mem = ctx.get_memory_info()\n",
+    "print(f\"GPU Memory: {free_mem/1e9:.2f} GB free / {total_mem/1e9:.2f} GB total\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "ef3276b7",
+   "metadata": {},
+   "source": [
+    "## 2. Explicit Memory Management\n",
+    "\n",
+    "The traditional CUDA workflow requires **explicit** memory transfers:\n",
+    "\n",
+    "```\n",
+    "1. Allocate memory on GPU (cudaMalloc)\n",
+    "2. Copy data from CPU to GPU (cudaMemcpy H2D)\n",
+    "3. Launch kernel\n",
+    "4. Copy results from GPU to CPU (cudaMemcpy D2H)\n",
+    "5. Free GPU memory (cudaFree)\n",
+    "```\n",
+    "\n",
+    "In Numba:\n",
+    "- `cuda.to_device(array)` - Allocates and copies to GPU\n",
+    "- `cuda.device_array(shape, dtype)` - Allocates on GPU only\n",
+    "- `device_array.copy_to_host()` - Copies back to CPU"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "eb3c31bf",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "@cuda.jit\n",
+    "def saxpy_kernel(a, x, y, out, n):\n",
+    "    \"\"\"SAXPY: out = a*x + y (Single-precision A*X Plus Y)\"\"\"\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < n:\n",
+    "        out[idx] = a * x[idx] + y[idx]\n",
+    "\n",
+    "def explicit_memory_workflow(n, a=2.0):\n",
+    "    \"\"\"Demonstrate explicit memory management\"\"\"\n",
+    "    print(f\"\\nüìä Explicit Memory Workflow (N = {n:,})\")\n",
+    "    print(\"-\" * 50)\n",
+    "    \n",
+    "    # Step 1: Create data on CPU\n",
+    "    x_host = np.random.randn(n).astype(np.float32)\n",
+    "    y_host = np.random.randn(n).astype(np.float32)\n",
+    "    out_host = np.zeros(n, dtype=np.float32)\n",
+    "    \n",
+    "    # Step 2: Allocate and copy to GPU\n",
+    "    start = time.perf_counter()\n",
+    "    x_device = cuda.to_device(x_host)        # Allocate + copy\n",
+    "    y_device = cuda.to_device(y_host)        # Allocate + copy\n",
+    "    out_device = cuda.device_array(n, dtype=np.float32)  # Allocate only\n",
+    "    h2d_time = time.perf_counter() - start\n",
+    "    print(f\"  Host‚ÜíDevice transfer: {h2d_time*1000:.3f} ms\")\n",
+    "    \n",
+    "    # Step 3: Launch kernel\n",
+    "    threads = 256\n",
+    "    blocks = math.ceil(n / threads)\n",
+    "    \n",
+    "    start = time.perf_counter()\n",
+    "    saxpy_kernel[blocks, threads](a, x_device, y_device, out_device, n)\n",
+    "    cuda.synchronize()\n",
+    "    kernel_time = time.perf_counter() - start\n",
+    "    print(f\"  Kernel execution:      {kernel_time*1000:.3f} ms\")\n",
+    "    \n",
+    "    # Step 4: Copy result back\n",
+    "    start = time.perf_counter()\n",
+    "    out_host = out_device.copy_to_host()\n",
+    "    d2h_time = time.perf_counter() - start\n",
+    "    print(f\"  Device‚ÜíHost transfer:  {d2h_time*1000:.3f} ms\")\n",
+    "    \n",
+    "    print(f\"  Total time:            {(h2d_time + kernel_time + d2h_time)*1000:.3f} ms\")\n",
+    "    \n",
+    "    # Verify\n",
+    "    expected = a * x_host + y_host\n",
+    "    print(f\"  ‚úÖ Correct: {np.allclose(out_host, expected)}\")\n",
+    "    \n",
+    "    return h2d_time, kernel_time, d2h_time\n",
+    "\n",
+    "# Run with different sizes\n",
+    "for size in [100_000, 1_000_000, 10_000_000]:\n",
+    "    explicit_memory_workflow(size)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "1a7e73a4",
+   "metadata": {},
+   "source": [
+    "## 3. Unified Memory (cudaMallocManaged)\n",
+    "\n",
+    "**Unified Memory** simplifies programming - memory is automatically migrated between CPU and GPU as needed.\n",
+    "\n",
+    "```\n",
+    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
+    "‚îÇ     CPU      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ Automatic ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ     GPU      ‚îÇ\n",
+    "‚îÇ              ‚îÇ      Migration      ‚îÇ              ‚îÇ\n",
+    "‚îÇ  ptr[i] = x  ‚îÇ                    ‚îÇ  ptr[i] = y  ‚îÇ\n",
+    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
+    "        Same pointer works on both!\n",
+    "```\n",
+    "\n",
+    "**Pros:**\n",
+    "- Simpler code\n",
+    "- No explicit transfers needed\n",
+    "- Can access more data than GPU memory (oversubscription)\n",
+    "\n",
+    "**Cons:**\n",
+    "- May have higher latency (page faults)\n",
+    "- Less control over when transfers happen\n",
+    "- Can be slower if not used carefully"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "2d01d685",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def unified_memory_workflow(n, a=2.0):\n",
+    "    \"\"\"Demonstrate unified memory (managed memory)\"\"\"\n",
+    "    print(f\"\\nüìä Unified Memory Workflow (N = {n:,})\")\n",
+    "    print(\"-\" * 50)\n",
+    "    \n",
+    "    # Allocate managed memory - accessible from both CPU and GPU\n",
+    "    x = cuda.managed_array(n, dtype=np.float32)\n",
+    "    y = cuda.managed_array(n, dtype=np.float32)\n",
+    "    out = cuda.managed_array(n, dtype=np.float32)\n",
+    "    \n",
+    "    # Initialize on CPU (memory will migrate to GPU when needed)\n",
+    "    x[:] = np.random.randn(n).astype(np.float32)\n",
+    "    y[:] = np.random.randn(n).astype(np.float32)\n",
+    "    \n",
+    "    # Launch kernel (data migrates automatically)\n",
+    "    threads = 256\n",
+    "    blocks = math.ceil(n / threads)\n",
+    "    \n",
+    "    start = time.perf_counter()\n",
+    "    saxpy_kernel[blocks, threads](a, x, y, out, n)\n",
+    "    cuda.synchronize()\n",
+    "    total_time = time.perf_counter() - start\n",
+    "    print(f\"  Kernel + migration time: {total_time*1000:.3f} ms\")\n",
+    "    \n",
+    "    # Access result on CPU (data migrates back automatically)\n",
+    "    expected = a * x + y\n",
+    "    print(f\"  ‚úÖ Correct: {np.allclose(out, expected)}\")\n",
+    "    \n",
+    "    return total_time\n",
+    "\n",
+    "# Compare with explicit management\n",
+    "for size in [100_000, 1_000_000, 10_000_000]:\n",
+    "    unified_memory_workflow(size)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "153f7062",
+   "metadata": {},
+   "source": [
+    "## 4. Pinned (Page-Locked) Memory\n",
+    "\n",
+    "Normal CPU memory can be **paged out** to disk by the OS. This causes problems for DMA transfers.\n",
+    "\n",
+    "**Pinned memory** is locked in physical RAM - no paging allowed.\n",
+    "\n",
+    "```\n",
+    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
+    "‚îÇ  Pageable Memory (default)     ‚îÇ  Pinned Memory             ‚îÇ\n",
+    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
+    "‚îÇ  CPU Memory                    ‚îÇ  CPU Memory (locked)       ‚îÇ\n",
+    "‚îÇ      ‚Üì (copy to staging)       ‚îÇ      ‚Üì (direct DMA)        ‚îÇ\n",
+    "‚îÇ  Pinned Buffer                 ‚îÇ                            ‚îÇ\n",
+    "‚îÇ      ‚Üì (DMA to GPU)            ‚îÇ      ‚Üì (DMA to GPU)        ‚îÇ\n",
+    "‚îÇ  GPU Memory                    ‚îÇ  GPU Memory                ‚îÇ\n",
+    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
+    "‚îÇ  TWO copies!                   ‚îÇ  ONE copy! (faster)        ‚îÇ\n",
+    "‚îÇ  Can be swapped to disk        ‚îÇ  Always in RAM             ‚îÇ\n",
+    "‚îÇ  Unlimited size                ‚îÇ  Limited by system RAM     ‚îÇ\n",
+    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
+    "```"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "31f57309",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def compare_pinned_vs_pageable(n):\n",
+    "    \"\"\"Compare transfer speeds with pinned vs pageable memory\"\"\"\n",
+    "    print(f\"\\nüìä Pinned vs Pageable Memory (N = {n:,})\")\n",
+    "    print(\"-\" * 50)\n",
+    "    \n",
+    "    # Pageable memory (default NumPy allocation)\n",
+    "    pageable = np.random.randn(n).astype(np.float32)\n",
+    "    \n",
+    "    # Pinned memory\n",
+    "    pinned = cuda.pinned_array(n, dtype=np.float32)\n",
+    "    pinned[:] = pageable  # Copy data to pinned\n",
+    "    \n",
+    "    # Measure pageable transfer\n",
+    "    start = time.perf_counter()\n",
+    "    for _ in range(10):\n",
+    "        d_pageable = cuda.to_device(pageable)\n",
+    "        cuda.synchronize()\n",
+    "    pageable_time = (time.perf_counter() - start) / 10\n",
+    "    \n",
+    "    # Measure pinned transfer\n",
+    "    start = time.perf_counter()\n",
+    "    for _ in range(10):\n",
+    "        d_pinned = cuda.to_device(pinned)\n",
+    "        cuda.synchronize()\n",
+    "    pinned_time = (time.perf_counter() - start) / 10\n",
+    "    \n",
+    "    # Calculate bandwidth\n",
+    "    data_size_gb = n * 4 / 1e9  # float32 = 4 bytes\n",
+    "    pageable_bw = data_size_gb / pageable_time\n",
+    "    pinned_bw = data_size_gb / pinned_time\n",
+    "    \n",
+    "    print(f\"  Pageable: {pageable_time*1000:.3f} ms ({pageable_bw:.2f} GB/s)\")\n",
+    "    print(f\"  Pinned:   {pinned_time*1000:.3f} ms ({pinned_bw:.2f} GB/s)\")\n",
+    "    print(f\"  Speedup:  {pageable_time/pinned_time:.2f}x\")\n",
+    "\n",
+    "# Compare for different sizes\n",
+    "for size in [1_000_000, 10_000_000, 50_000_000]:\n",
+    "    compare_pinned_vs_pageable(size)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "d3578ad8",
+   "metadata": {},
+   "source": [
+    "## 5. When Transfer Time Dominates\n",
+    "\n",
+    "For simple operations like vector addition, **memory transfers dominate execution time**.\n",
+    "\n",
+    "This is why:\n",
+    "1. GPU is best for **compute-intensive** operations\n",
+    "2. You should **minimize transfers** (keep data on GPU)\n",
+    "3. **Batch operations** together before copying back"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "5e57c30c",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def analyze_transfer_overhead(n):\n",
+    "    \"\"\"Show how transfer time compares to compute time\"\"\"\n",
+    "    print(f\"\\nüìä Transfer vs Compute Analysis (N = {n:,})\")\n",
+    "    print(\"-\" * 60)\n",
+    "    \n",
+    "    a = np.random.randn(n).astype(np.float32)\n",
+    "    b = np.random.randn(n).astype(np.float32)\n",
+    "    \n",
+    "    # Measure H2D transfer\n",
+    "    start = time.perf_counter()\n",
+    "    a_d = cuda.to_device(a)\n",
+    "    b_d = cuda.to_device(b)\n",
+    "    c_d = cuda.device_array(n, dtype=np.float32)\n",
+    "    cuda.synchronize()\n",
+    "    h2d_time = time.perf_counter() - start\n",
+    "    \n",
+    "    # Measure kernel (data already on GPU)\n",
+    "    threads = 256\n",
+    "    blocks = math.ceil(n / threads)\n",
+    "    \n",
+    "    @cuda.jit\n",
+    "    def add_kernel(a, b, c):\n",
+    "        idx = cuda.grid(1)\n",
+    "        if idx < c.size:\n",
+    "            c[idx] = a[idx] + b[idx]\n",
+    "    \n",
+    "    # Warmup\n",
+    "    add_kernel[blocks, threads](a_d, b_d, c_d)\n",
+    "    cuda.synchronize()\n",
+    "    \n",
+    "    start = time.perf_counter()\n",
+    "    for _ in range(100):\n",
+    "        add_kernel[blocks, threads](a_d, b_d, c_d)\n",
+    "    cuda.synchronize()\n",
+    "    kernel_time = (time.perf_counter() - start) / 100\n",
+    "    \n",
+    "    # Measure D2H transfer\n",
+    "    start = time.perf_counter()\n",
+    "    c = c_d.copy_to_host()\n",
+    "    cuda.synchronize()\n",
+    "    d2h_time = time.perf_counter() - start\n",
+    "    \n",
+    "    total = h2d_time + kernel_time + d2h_time\n",
+    "    \n",
+    "    print(f\"  H2D Transfer:  {h2d_time*1000:>8.3f} ms ({h2d_time/total*100:>5.1f}%)\")\n",
+    "    print(f\"  Kernel:        {kernel_time*1000:>8.3f} ms ({kernel_time/total*100:>5.1f}%)\")\n",
+    "    print(f\"  D2H Transfer:  {d2h_time*1000:>8.3f} ms ({d2h_time/total*100:>5.1f}%)\")\n",
+    "    print(f\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
+    "    print(f\"  Total:         {total*1000:>8.3f} ms\")\n",
+    "    \n",
+    "    if (h2d_time + d2h_time) > kernel_time:\n",
+    "        print(f\"\\n  ‚ö†Ô∏è  Transfer time ({(h2d_time + d2h_time)*1000:.2f} ms) > Kernel time ({kernel_time*1000:.2f} ms)\")\n",
+    "        print(f\"     Consider: Keep data on GPU, batch operations!\")\n",
+    "\n",
+    "analyze_transfer_overhead(10_000_000)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "3a87ad6d",
+   "metadata": {},
+   "source": [
+    "## üéØ Exercises\n",
+    "\n",
+    "### Exercise 1: Memory Reuse Pattern\n",
+    "Implement a pipeline that reuses GPU memory for multiple operations without copying back to CPU between each step."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "f62579f0",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 1: Memory Reuse Pattern\n",
+    "# Compute: result = ((a + b) * c) - d\n",
+    "# Do this with ONE H2D transfer and ONE D2H transfer\n",
+    "\n",
+    "@cuda.jit\n",
+    "def add_kernel(a, b, out):\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < out.size:\n",
+    "        out[idx] = a[idx] + b[idx]\n",
+    "\n",
+    "@cuda.jit\n",
+    "def mul_kernel(a, b, out):\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < out.size:\n",
+    "        out[idx] = a[idx] * b[idx]\n",
+    "\n",
+    "@cuda.jit  \n",
+    "def sub_kernel(a, b, out):\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < out.size:\n",
+    "        out[idx] = a[idx] - b[idx]\n",
+    "\n",
+    "def pipeline_inefficient(a, b, c, d):\n",
+    "    \"\"\"BAD: Transfers after each operation\"\"\"\n",
+    "    # TODO: This is the slow way - see how many transfers happen\n",
+    "    pass\n",
+    "\n",
+    "def pipeline_efficient(a, b, c, d):\n",
+    "    \"\"\"GOOD: All operations on GPU, one final transfer\"\"\"\n",
+    "    # TODO: Implement efficient version\n",
+    "    # 1. Copy all inputs to GPU\n",
+    "    # 2. Perform all operations\n",
+    "    # 3. Copy only result back\n",
+    "    pass\n",
+    "\n",
+    "# Test your implementations\n",
+    "# N = 10_000_000\n",
+    "# a = np.random.randn(N).astype(np.float32)\n",
+    "# ..."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "418bb032",
+   "metadata": {},
+   "source": [
+    "### Exercise 2: Memory Bandwidth Calculation\n",
+    "Calculate the theoretical vs achieved memory bandwidth for a copy operation."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "93de29ad",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 2: Bandwidth Measurement\n",
+    "@cuda.jit\n",
+    "def copy_kernel(src, dst):\n",
+    "    \"\"\"Simple copy kernel - measures GPU memory bandwidth\"\"\"\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < dst.size:\n",
+    "        dst[idx] = src[idx]\n",
+    "\n",
+    "def measure_bandwidth(n):\n",
+    "    \"\"\"Measure achieved memory bandwidth\"\"\"\n",
+    "    # TODO: \n",
+    "    # 1. Create two device arrays\n",
+    "    # 2. Time the copy kernel\n",
+    "    # 3. Calculate bandwidth: (bytes_read + bytes_written) / time\n",
+    "    # 4. Compare to theoretical peak (from device properties)\n",
+    "    pass\n",
+    "\n",
+    "# measure_bandwidth(100_000_000)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "49ae5362",
+   "metadata": {},
+   "source": [
+    "## üìù Key Takeaways\n",
+    "\n",
+    "### Memory Management Summary:\n",
+    "\n",
+    "| Method | When to Use | Pros | Cons |\n",
+    "|--------|-------------|------|------|\n",
+    "| **Explicit** (`to_device`, `copy_to_host`) | Production code, max control | Fastest, predictable | More code |\n",
+    "| **Unified** (`managed_array`) | Prototyping, complex access patterns | Simple code | Page fault overhead |\n",
+    "| **Pinned** (`pinned_array`) | High-bandwidth transfers | ~2x transfer speed | Uses system RAM |\n",
+    "\n",
+    "### Best Practices:\n",
+    "\n",
+    "1. **Minimize transfers**: Keep data on GPU as long as possible\n",
+    "2. **Use pinned memory**: For frequent large transfers\n",
+    "3. **Batch operations**: Don't copy back between every operation\n",
+    "4. **Profile first**: Measure before optimizing\n",
+    "\n",
+    "### Memory Bandwidth Rule of Thumb:\n",
+    "- PCIe 3.0 x16: ~16 GB/s\n",
+    "- PCIe 4.0 x16: ~32 GB/s  \n",
+    "- GPU Memory: 200-900 GB/s (much faster!)\n",
+    "\n",
+    "---\n",
+    "\n",
+    "### üìö Next Up: Day 4 - Error Handling & Debugging\n",
+    "- CUDA error codes\n",
+    "- Debugging techniques\n",
+    "- Common pitfalls\n",
+    "\n",
+    "---\n",
+    "\n",
+    "### üîó Resources\n",
+    "- [CUDA Memory Guide](../../cuda-programming-guide/02-basics/cuda-memory.md)\n",
+    "- [Understanding Memory](../../cuda-programming-guide/02-basics/understanding-memory.md)"
+   ]
+  }
+ ],
+ "metadata": {
+  "language_info": {
+   "name": "python"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/learning-path/week-01/day-4-error-handling.ipynb b/learning-path/week-01/day-4-error-handling.ipynb
new file mode 100644
index 0000000..f64156f
--- /dev/null
+++ b/learning-path/week-01/day-4-error-handling.ipynb
@@ -0,0 +1,537 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "id": "d5f6a276",
+   "metadata": {},
+   "source": [
+    "# üöÄ Day 4: Error Handling & Debugging\n",
+    "\n",
+    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-4-error-handling.ipynb)\n",
+    "\n",
+    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "4c4f5cb0",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# ‚öôÔ∏è Colab Setup Cell - Run this first!\n",
+    "import subprocess, sys\n",
+    "try:\n",
+    "    import google.colab\n",
+    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
+    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
+    "    print(\"‚úÖ Setup complete!\")\n",
+    "except ImportError:\n",
+    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "b5b16164",
+   "metadata": {},
+   "source": [
+    "# Day 4: Error Handling & Debugging\n",
+    "\n",
+    "Bugs in CUDA code can be subtle and hard to find. Today you'll learn:\n",
+    "- How CUDA errors work\n",
+    "- Proper error checking patterns\n",
+    "- Common pitfalls and how to avoid them\n",
+    "- Debugging techniques\n",
+    "- Using CUDA-MEMCHECK and compute-sanitizer\n",
+    "\n",
+    "---\n",
+    "\n",
+    "## 1. Understanding CUDA Errors\n",
+    "\n",
+    "CUDA operations can fail for many reasons:\n",
+    "- Invalid kernel launch configuration\n",
+    "- Out of memory\n",
+    "- Device not available\n",
+    "- Invalid memory access\n",
+    "- Race conditions\n",
+    "\n",
+    "**Key concept:** CUDA operations are often **asynchronous**. Errors may not be reported until later!\n",
+    "\n",
+    "```\n",
+    "kernel<<<grid, block>>>(...);  // Launches, returns immediately\n",
+    "// ... other code ...\n",
+    "cudaDeviceSynchronize();       // Error might appear HERE!\n",
+    "```"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "77a6ea57",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Setup\n",
+    "import numpy as np\n",
+    "from numba import cuda\n",
+    "from numba.cuda.cudadrv.driver import CudaAPIError\n",
+    "import math\n",
+    "import traceback\n",
+    "\n",
+    "print(\"CUDA device:\", cuda.get_current_device().name.decode())"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "51604cf3",
+   "metadata": {},
+   "source": [
+    "## 2. Common CUDA Errors & How to Trigger Them\n",
+    "\n",
+    "Let's intentionally cause errors to understand how they appear."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "faf918e2",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Error 1: Invalid Launch Configuration\n",
+    "# Max threads per block is 1024, what happens if we exceed it?\n",
+    "\n",
+    "@cuda.jit\n",
+    "def simple_kernel(arr):\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < arr.size:\n",
+    "        arr[idx] = idx\n",
+    "\n",
+    "arr = np.zeros(100, dtype=np.float32)\n",
+    "arr_d = cuda.to_device(arr)\n",
+    "\n",
+    "print(\"Attempting to launch with 2048 threads per block...\")\n",
+    "print(\"(Max allowed is 1024)\")\n",
+    "print(\"-\" * 50)\n",
+    "\n",
+    "try:\n",
+    "    # This will fail - too many threads per block!\n",
+    "    simple_kernel[1, 2048](arr_d)\n",
+    "    cuda.synchronize()\n",
+    "except Exception as e:\n",
+    "    print(f\"‚ùå Error caught: {type(e).__name__}\")\n",
+    "    print(f\"   Message: {e}\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "5c1aeb03",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Error 2: Out of Memory\n",
+    "# Trying to allocate more than available GPU memory\n",
+    "\n",
+    "ctx = cuda.current_context()\n",
+    "free_mem, total_mem = ctx.get_memory_info()\n",
+    "print(f\"Free GPU memory: {free_mem / 1e9:.2f} GB\")\n",
+    "print(f\"Attempting to allocate: {free_mem * 2 / 1e9:.2f} GB (2x available)\")\n",
+    "print(\"-\" * 50)\n",
+    "\n",
+    "try:\n",
+    "    # Try to allocate more than available\n",
+    "    huge_array = cuda.device_array(int(free_mem * 2), dtype=np.uint8)\n",
+    "except Exception as e:\n",
+    "    print(f\"‚ùå Error caught: {type(e).__name__}\")\n",
+    "    print(f\"   Message: {e}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "e4df5a20",
+   "metadata": {},
+   "source": [
+    "## 3. The Debugging Checklist\n",
+    "\n",
+    "When your CUDA code doesn't work, check these in order:\n",
+    "\n",
+    "### üîç Checklist\n",
+    "\n",
+    "1. **Is CUDA available?**\n",
+    "   ```python\n",
+    "   cuda.is_available()\n",
+    "   ```\n",
+    "\n",
+    "2. **Are launch parameters valid?**\n",
+    "   - `threads_per_block` ‚â§ 1024\n",
+    "   - `blocks` > 0\n",
+    "   - Grid dimensions within limits\n",
+    "\n",
+    "3. **Is there enough memory?**\n",
+    "   - Check `cuda.current_context().get_memory_info()`\n",
+    "\n",
+    "4. **Are array sizes correct?**\n",
+    "   - Boundary checks in kernel: `if idx < n:`\n",
+    "\n",
+    "5. **Are data types matching?**\n",
+    "   - GPU prefers float32, not float64\n",
+    "\n",
+    "6. **Did you synchronize?**\n",
+    "   - `cuda.synchronize()` before reading results"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "e726d69b",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Helper function: Safe kernel launch wrapper\n",
+    "def safe_launch(kernel, grid, block, *args, **kwargs):\n",
+    "    \"\"\"Launch kernel with error checking\"\"\"\n",
+    "    device = cuda.get_current_device()\n",
+    "    \n",
+    "    # Validate block size\n",
+    "    if isinstance(block, int):\n",
+    "        block = (block,)\n",
+    "    total_threads = 1\n",
+    "    for dim in block:\n",
+    "        total_threads *= dim\n",
+    "    if total_threads > device.MAX_THREADS_PER_BLOCK:\n",
+    "        raise ValueError(f\"Block size {block} = {total_threads} threads exceeds max {device.MAX_THREADS_PER_BLOCK}\")\n",
+    "    \n",
+    "    # Validate grid size\n",
+    "    if isinstance(grid, int):\n",
+    "        grid = (grid,)\n",
+    "    for i, dim in enumerate(grid):\n",
+    "        max_dim = [device.MAX_GRID_DIM_X, device.MAX_GRID_DIM_Y, device.MAX_GRID_DIM_Z][i]\n",
+    "        if dim > max_dim:\n",
+    "            raise ValueError(f\"Grid dimension {i} = {dim} exceeds max {max_dim}\")\n",
+    "    \n",
+    "    # Launch\n",
+    "    kernel[grid, block](*args, **kwargs)\n",
+    "    cuda.synchronize()\n",
+    "\n",
+    "# Test safe launch\n",
+    "print(\"Testing safe_launch helper:\")\n",
+    "arr = cuda.device_array(100, dtype=np.float32)\n",
+    "\n",
+    "try:\n",
+    "    safe_launch(simple_kernel, 1, 2048, arr)  # Should fail validation\n",
+    "except ValueError as e:\n",
+    "    print(f\"‚úÖ Caught before launch: {e}\")\n",
+    "\n",
+    "safe_launch(simple_kernel, 1, 256, arr)  # Should work\n",
+    "print(\"‚úÖ Valid launch succeeded\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "d441f135",
+   "metadata": {},
+   "source": [
+    "## 4. Common Pitfalls & Bug Patterns\n",
+    "\n",
+    "### Pitfall 1: Missing Boundary Check"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "e73ab03e",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# BAD: No boundary check\n",
+    "@cuda.jit\n",
+    "def bad_kernel_no_bounds(arr):\n",
+    "    idx = cuda.grid(1)\n",
+    "    arr[idx] = idx  # üí• Will access out-of-bounds memory!\n",
+    "\n",
+    "# GOOD: With boundary check\n",
+    "@cuda.jit  \n",
+    "def good_kernel_with_bounds(arr, n):\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < n:  # ‚úÖ Always check!\n",
+    "        arr[idx] = idx\n",
+    "\n",
+    "# Demonstrate the difference\n",
+    "n = 100\n",
+    "arr = cuda.device_array(n, dtype=np.float32)\n",
+    "threads = 256  # More threads than elements!\n",
+    "blocks = 1\n",
+    "\n",
+    "print(\"With proper bounds checking:\")\n",
+    "good_kernel_with_bounds[blocks, threads](arr, n)\n",
+    "cuda.synchronize()\n",
+    "print(\"‚úÖ Completed safely\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "0461cc81",
+   "metadata": {},
+   "source": [
+    "### Pitfall 2: Wrong Data Type"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "babf7720",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# NumPy defaults to float64, but CUDA prefers float32\n",
+    "import time\n",
+    "\n",
+    "@cuda.jit\n",
+    "def add_arrays(a, b, c):\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < c.size:\n",
+    "        c[idx] = a[idx] + b[idx]\n",
+    "\n",
+    "n = 10_000_000\n",
+    "\n",
+    "# float64 (default) - slower on most GPUs\n",
+    "a64 = np.random.randn(n)  # Default is float64!\n",
+    "b64 = np.random.randn(n)\n",
+    "c64 = np.zeros(n)\n",
+    "\n",
+    "# float32 - preferred\n",
+    "a32 = np.random.randn(n).astype(np.float32)\n",
+    "b32 = np.random.randn(n).astype(np.float32)\n",
+    "c32 = np.zeros(n, dtype=np.float32)\n",
+    "\n",
+    "threads, blocks = 256, math.ceil(n / 256)\n",
+    "\n",
+    "# Benchmark float64\n",
+    "a64_d, b64_d = cuda.to_device(a64), cuda.to_device(b64)\n",
+    "c64_d = cuda.device_array(n, dtype=np.float64)\n",
+    "add_arrays[blocks, threads](a64_d, b64_d, c64_d)\n",
+    "cuda.synchronize()\n",
+    "\n",
+    "start = time.perf_counter()\n",
+    "for _ in range(10):\n",
+    "    add_arrays[blocks, threads](a64_d, b64_d, c64_d)\n",
+    "cuda.synchronize()\n",
+    "time64 = (time.perf_counter() - start) / 10\n",
+    "\n",
+    "# Benchmark float32\n",
+    "a32_d, b32_d = cuda.to_device(a32), cuda.to_device(b32)\n",
+    "c32_d = cuda.device_array(n, dtype=np.float32)\n",
+    "\n",
+    "start = time.perf_counter()\n",
+    "for _ in range(10):\n",
+    "    add_arrays[blocks, threads](a32_d, b32_d, c32_d)\n",
+    "cuda.synchronize()\n",
+    "time32 = (time.perf_counter() - start) / 10\n",
+    "\n",
+    "print(f\"float64: {time64*1000:.3f} ms\")\n",
+    "print(f\"float32: {time32*1000:.3f} ms\")\n",
+    "print(f\"Speedup: {time64/time32:.2f}x\")\n",
+    "print(\"\\nüí° Tip: Always use .astype(np.float32) unless you need float64 precision!\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "c32f0cfe",
+   "metadata": {},
+   "source": [
+    "### Pitfall 3: Forgetting to Synchronize"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "aabe6910",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Kernel execution is ASYNCHRONOUS\n",
+    "@cuda.jit\n",
+    "def slow_kernel(arr):\n",
+    "    \"\"\"Simulate slow computation\"\"\"\n",
+    "    idx = cuda.grid(1)\n",
+    "    if idx < arr.size:\n",
+    "        # Busy work\n",
+    "        val = 0.0\n",
+    "        for i in range(1000):\n",
+    "            val += idx * 0.001\n",
+    "        arr[idx] = val\n",
+    "\n",
+    "arr = cuda.device_array(10000, dtype=np.float32)\n",
+    "threads, blocks = 256, math.ceil(10000 / 256)\n",
+    "\n",
+    "# BAD: Timing without synchronization\n",
+    "start = time.perf_counter()\n",
+    "slow_kernel[blocks, threads](arr)\n",
+    "# Missing: cuda.synchronize()\n",
+    "bad_time = time.perf_counter() - start\n",
+    "print(f\"Without sync: {bad_time*1000:.3f} ms (WRONG! Kernel still running)\")\n",
+    "\n",
+    "# GOOD: Proper timing with synchronization  \n",
+    "start = time.perf_counter()\n",
+    "slow_kernel[blocks, threads](arr)\n",
+    "cuda.synchronize()  # Wait for kernel to complete\n",
+    "good_time = time.perf_counter() - start\n",
+    "print(f\"With sync:    {good_time*1000:.3f} ms (Correct)\")\n",
+    "\n",
+    "print(f\"\\n‚ö†Ô∏è The unsynchronized time is {good_time/bad_time:.0f}x too fast!\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "64d257e0",
+   "metadata": {},
+   "source": [
+    "## 5. Debugging with Print Statements\n",
+    "\n",
+    "In Numba CUDA, you can use `print()` inside kernels for debugging (but use sparingly - it's slow!)."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "af72390c",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "@cuda.jit\n",
+    "def debug_kernel(arr, n):\n",
+    "    idx = cuda.grid(1)\n",
+    "    \n",
+    "    # Only print from first few threads to avoid output flood\n",
+    "    if idx < 3:\n",
+    "        print(\"Thread\", idx, \"starting\")\n",
+    "    \n",
+    "    if idx < n:\n",
+    "        arr[idx] = idx * 2\n",
+    "        \n",
+    "        # Debug: Print values for first few elements\n",
+    "        if idx < 3:\n",
+    "            print(\"Thread\", idx, \"wrote value\", arr[idx])\n",
+    "\n",
+    "# Run with small array\n",
+    "arr = cuda.device_array(10, dtype=np.float32)\n",
+    "debug_kernel[1, 10](arr, 10)\n",
+    "cuda.synchronize()\n",
+    "\n",
+    "print(\"\\nFinal array:\", arr.copy_to_host())"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "20e3183d",
+   "metadata": {},
+   "source": [
+    "## üéØ Exercises\n",
+    "\n",
+    "### Exercise 1: Error-Proof Kernel Wrapper\n",
+    "Create a robust wrapper function that validates all inputs before launching a kernel."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "96ccfd93",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# TODO Exercise 1: Complete this error-proof wrapper\n",
+    "\n",
+    "def launch_kernel_safe(kernel, data, threads_per_block=256):\n",
+    "    \"\"\"\n",
+    "    Safely launch a kernel with automatic configuration and error checking.\n",
+    "    \n",
+    "    Args:\n",
+    "        kernel: The CUDA kernel function\n",
+    "        data: Input array (numpy or device array)\n",
+    "        threads_per_block: Threads per block (default 256)\n",
+    "    \n",
+    "    Returns:\n",
+    "        Device array with results\n",
+    "        \n",
+    "    Raises:\n",
+    "        ValueError: If inputs are invalid\n",
+    "        MemoryError: If not enough GPU memory\n",
+    "    \"\"\"\n",
+    "    # TODO: Implement the following checks:\n",
+    "    # 1. Verify CUDA is available\n",
+    "    # 2. Check data is not empty\n",
+    "    # 3. Validate threads_per_block (1-1024)\n",
+    "    # 4. Check sufficient GPU memory\n",
+    "    # 5. Launch kernel with proper grid configuration\n",
+    "    # 6. Synchronize and check for errors\n",
+    "    \n",
+    "    pass\n",
+    "\n",
+    "# Test your implementation\n",
+    "# ..."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "6762d8c3",
+   "metadata": {},
+   "source": [
+    "## üìù Key Takeaways\n",
+    "\n",
+    "### Error Handling Best Practices:\n",
+    "\n",
+    "1. **Always synchronize** before reading results or timing\n",
+    "   ```python\n",
+    "   kernel[grid, block](...)\n",
+    "   cuda.synchronize()  # Wait for completion\n",
+    "   result = output.copy_to_host()\n",
+    "   ```\n",
+    "\n",
+    "2. **Validate launch configuration**\n",
+    "   - threads_per_block ‚â§ 1024\n",
+    "   - Check grid dimensions against device limits\n",
+    "\n",
+    "3. **Always include boundary checks**\n",
+    "   ```python\n",
+    "   if idx < n:\n",
+    "       arr[idx] = ...\n",
+    "   ```\n",
+    "\n",
+    "4. **Use try/except for error handling**\n",
+    "   ```python\n",
+    "   try:\n",
+    "       kernel[grid, block](...)\n",
+    "   except CudaAPIError as e:\n",
+    "       print(f\"CUDA Error: {e}\")\n",
+    "   ```\n",
+    "\n",
+    "5. **Prefer float32** unless you need float64 precision\n",
+    "\n",
+    "6. **Debug strategically**\n",
+    "   - Use print() sparingly (only first few threads)\n",
+    "   - Use small test cases first\n",
+    "   - Verify CPU results before GPU\n",
+    "\n",
+    "---\n",
+    "\n",
+    "### üìö Next Steps\n",
+    "You've completed Week 1! Before moving on:\n",
+    "1. Complete the checkpoint quiz\n",
+    "2. Finish all exercises in each notebook\n",
+    "3. Make sure you can run all code without errors\n",
+    "\n",
+    "### üîó Resources\n",
+    "- [Error Handling Guide](../../cuda-programming-guide/02-basics/nvcc.md)\n",
+    "- [Debugging Documentation](../../cuda-programming-guide/04-special-topics/error-log-management.md)"
+   ]
+  }
+ ],
+ "metadata": {
+  "language_info": {
+   "name": "python"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/practice/01-foundations/ex01-device-query/Makefile b/practice/01-foundations/ex01-device-query/Makefile
new file mode 100644
index 0000000..bd69fb9
--- /dev/null
+++ b/practice/01-foundations/ex01-device-query/Makefile
@@ -0,0 +1,31 @@
+# Makefile for CUDA exercises
+
+NVCC = nvcc
+NVCC_FLAGS = -O2
+
+# Default target
+all: device_query solution
+
+device_query: device_query.cu
+	$(NVCC) $(NVCC_FLAGS) $< -o $@
+
+solution: solution.cu
+	$(NVCC) $(NVCC_FLAGS) $< -o $@
+
+# Run your implementation
+run: device_query
+	./device_query
+
+# Run solution
+run-solution: solution
+	./solution
+
+# Test against expected output
+test:
+	bash test.sh
+
+# Clean build artifacts
+clean:
+	rm -f device_query solution *.o output.txt
+
+.PHONY: all run run-solution test clean
diff --git a/practice/01-foundations/ex01-device-query/test.sh b/practice/01-foundations/ex01-device-query/test.sh
new file mode 100644
index 0000000..8dbf595
--- /dev/null
+++ b/practice/01-foundations/ex01-device-query/test.sh
@@ -0,0 +1,62 @@
+#!/bin/bash
+# Test script for ex01-device-query
+
+echo "========================================"
+echo "Testing Device Query Exercise"
+echo "========================================"
+
+# Compile
+echo "Compiling solution.cu..."
+nvcc solution.cu -o solution 2>&1
+if [ $? -ne 0 ]; then
+    echo "‚ùå COMPILATION FAILED"
+    exit 1
+fi
+echo "‚úÖ Compilation successful"
+
+# Run and capture output
+echo ""
+echo "Running device query..."
+./solution > output.txt 2>&1
+
+# Check for expected output patterns
+echo ""
+echo "Checking output..."
+
+# Must contain these key patterns
+patterns=(
+    "Device"
+    "Compute Capability"
+    "Multiprocessors"
+    "Global Memory"
+    "Shared Memory"
+    "Max Threads"
+    "Warp Size"
+)
+
+all_passed=true
+for pattern in "${patterns[@]}"; do
+    if grep -qi "$pattern" output.txt; then
+        echo "‚úÖ Found: $pattern"
+    else
+        echo "‚ùå Missing: $pattern"
+        all_passed=false
+    fi
+done
+
+echo ""
+if [ "$all_passed" = true ]; then
+    echo "========================================"
+    echo "‚úÖ ALL TESTS PASSED!"
+    echo "========================================"
+    rm -f solution output.txt
+    exit 0
+else
+    echo "========================================"
+    echo "‚ùå SOME TESTS FAILED"
+    echo "========================================"
+    echo "Your output:"
+    cat output.txt
+    rm -f solution output.txt
+    exit 1
+fi
diff --git a/practice/01-foundations/ex02-hello-gpu/Makefile b/practice/01-foundations/ex02-hello-gpu/Makefile
new file mode 100644
index 0000000..9f45629
--- /dev/null
+++ b/practice/01-foundations/ex02-hello-gpu/Makefile
@@ -0,0 +1,31 @@
+# Makefile for CUDA exercises
+
+NVCC = nvcc
+NVCC_FLAGS = -O2
+
+# Default target
+all: hello_gpu solution
+
+hello_gpu: hello_gpu.cu
+	$(NVCC) $(NVCC_FLAGS) $< -o $@
+
+solution: solution.cu
+	$(NVCC) $(NVCC_FLAGS) $< -o $@
+
+# Run your implementation
+run: hello_gpu
+	./hello_gpu
+
+# Run solution
+run-solution: solution
+	./solution
+
+# Test against expected output
+test:
+	bash test.sh
+
+# Clean build artifacts
+clean:
+	rm -f hello_gpu solution *.o output.txt
+
+.PHONY: all run run-solution test clean
diff --git a/practice/01-foundations/ex02-hello-gpu/test.sh b/practice/01-foundations/ex02-hello-gpu/test.sh
new file mode 100644
index 0000000..251c388
--- /dev/null
+++ b/practice/01-foundations/ex02-hello-gpu/test.sh
@@ -0,0 +1,90 @@
+#!/bin/bash
+# Test script for ex02-hello-gpu
+
+echo "========================================"
+echo "Testing Hello GPU Exercise"
+echo "========================================"
+
+# Compile
+echo "Compiling solution.cu..."
+nvcc solution.cu -o solution 2>&1
+if [ $? -ne 0 ]; then
+    echo "‚ùå COMPILATION FAILED"
+    exit 1
+fi
+echo "‚úÖ Compilation successful"
+
+# Run and capture output
+echo ""
+echo "Running hello GPU..."
+./solution > output.txt 2>&1
+
+# Check for expected output patterns
+echo ""
+echo "Checking output..."
+
+# Should have output from multiple thread configurations
+tests_passed=0
+tests_total=5
+
+# Test 1: Output exists
+if [ -s output.txt ]; then
+    echo "‚úÖ Output generated"
+    ((tests_passed++))
+else
+    echo "‚ùå No output generated"
+fi
+
+# Test 2: Contains "Hello"
+if grep -qi "hello" output.txt; then
+    echo "‚úÖ Contains greeting"
+    ((tests_passed++))
+else
+    echo "‚ùå Missing greeting output"
+fi
+
+# Test 3: Contains block info
+if grep -qi "block" output.txt; then
+    echo "‚úÖ Contains block information"
+    ((tests_passed++))
+else
+    echo "‚ùå Missing block information"
+fi
+
+# Test 4: Contains thread info  
+if grep -qi "thread" output.txt; then
+    echo "‚úÖ Contains thread information"
+    ((tests_passed++))
+else
+    echo "‚ùå Missing thread information"
+fi
+
+# Test 5: Multiple configurations run (check for different block counts)
+config_count=$(grep -c "===" output.txt)
+if [ "$config_count" -ge 3 ]; then
+    echo "‚úÖ Multiple configurations tested"
+    ((tests_passed++))
+else
+    echo "‚ùå Expected 3+ configurations, found $config_count"
+fi
+
+echo ""
+echo "Score: $tests_passed / $tests_total"
+echo ""
+
+if [ "$tests_passed" -eq "$tests_total" ]; then
+    echo "========================================"
+    echo "‚úÖ ALL TESTS PASSED!"
+    echo "========================================"
+    rm -f solution output.txt
+    exit 0
+else
+    echo "========================================"
+    echo "‚ùå SOME TESTS FAILED"
+    echo "========================================"
+    echo ""
+    echo "Your output:"
+    cat output.txt
+    rm -f solution output.txt
+    exit 1
+fi
diff --git a/scripts/gpu-session.sh b/scripts/gpu-session.sh
new file mode 100644
index 0000000..6f4b248
--- /dev/null
+++ b/scripts/gpu-session.sh
@@ -0,0 +1,32 @@
+#!/bin/bash
+# Quick interactive GPU session for CUDA learning
+# Usage: ./gpu-session.sh
+
+echo "üöÄ Requesting T4 GPU node..."
+echo "   Partition: t4flex"
+echo "   GPU: 1x T4"
+echo "   Time: 4 hours"
+echo ""
+
+srun --partition=t4flex \
+     --gres=gpu:1 \
+     --cpus-per-task=4 \
+     --mem=16G \
+     --time=04:00:00 \
+     --pty bash -c '
+echo "=========================================="
+echo "‚úÖ GPU Node: $(hostname)"
+nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv
+echo "=========================================="
+echo ""
+echo "Activating CUDA environment..."
+source ~/miniconda3/bin/activate cuda-learning 2>/dev/null || conda activate cuda-learning
+echo ""
+echo "üéì Ready! You can now:"
+echo "   1. Run: python verify_cuda.py"
+echo "   2. Run: jupyter notebook --no-browser --port=8888"
+echo "   3. Run: python -c \"from numba import cuda; print(cuda.get_current_device().name.decode())\""
+echo ""
+cd ~/cuda-lab
+exec bash
+'
diff --git a/scripts/setup-environment.sh b/scripts/setup-environment.sh
new file mode 100644
index 0000000..03f8dda
--- /dev/null
+++ b/scripts/setup-environment.sh
@@ -0,0 +1,51 @@
+#!/bin/bash
+# Setup CUDA learning environment (run once)
+
+echo "=========================================="
+echo "Setting up CUDA Learning Environment"
+echo "=========================================="
+
+# Check if conda is available
+if ! command -v conda &> /dev/null; then
+    echo "Installing Miniconda..."
+    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh
+    bash /tmp/miniconda.sh -b -p $HOME/miniconda3
+    eval "$($HOME/miniconda3/bin/conda shell.bash hook)"
+    conda init bash
+    echo "‚úÖ Miniconda installed"
+else
+    echo "‚úÖ Conda already available"
+fi
+
+# Create environment
+echo ""
+echo "Creating cuda-learning environment..."
+conda create -n cuda-learning python=3.10 -y
+
+echo ""
+echo "Installing packages..."
+conda activate cuda-learning
+pip install numba numpy jupyter jupyterlab matplotlib
+
+# Verify
+echo ""
+echo "=========================================="
+echo "Verifying installation..."
+echo "=========================================="
+python -c "
+from numba import cuda
+import numpy as np
+print('‚úÖ NumPy:', np.__version__)
+print('‚úÖ Numba installed')
+print('‚úÖ CUDA module available:', hasattr(cuda, 'jit'))
+"
+
+echo ""
+echo "=========================================="
+echo "‚úÖ Setup complete!"
+echo "=========================================="
+echo ""
+echo "To start learning:"
+echo "  1. Run: ./scripts/gpu-session.sh"
+echo "  2. Or submit: sbatch scripts/start-jupyter-gpu.sh"
+echo ""
diff --git a/scripts/start-jupyter-gpu.sh b/scripts/start-jupyter-gpu.sh
new file mode 100644
index 0000000..7f3cb27
--- /dev/null
+++ b/scripts/start-jupyter-gpu.sh
@@ -0,0 +1,45 @@
+#!/bin/bash
+#SBATCH --job-name=cuda-jupyter
+#SBATCH --partition=t4flex
+#SBATCH --gres=gpu:1
+#SBATCH --cpus-per-task=4
+#SBATCH --mem=16G
+#SBATCH --time=04:00:00
+#SBATCH --output=jupyter-%j.log
+
+echo "=========================================="
+echo "CUDA Learning Session"
+echo "=========================================="
+echo "Node: $(hostname)"
+echo "GPU:"
+nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
+echo "=========================================="
+
+# Load modules (adjust based on your cluster)
+module load cuda 2>/dev/null || echo "No cuda module needed"
+
+# Activate conda environment
+source ~/miniconda3/bin/activate cuda-learning 2>/dev/null || \
+    source ~/.conda/envs/cuda-learning/bin/activate 2>/dev/null || \
+    echo "Activating environment..."
+
+# Get a free port
+PORT=$(shuf -i 8000-9000 -n 1)
+
+# Print connection instructions
+echo ""
+echo "=========================================="
+echo "üöÄ Jupyter Lab starting on port $PORT"
+echo "=========================================="
+echo ""
+echo "To connect, run this on your LOCAL machine:"
+echo ""
+echo "  ssh -L $PORT:$(hostname):$PORT $(whoami)@$(hostname -f | sed 's/hpcslurm-nst4flex-[0-9]/LOGIN_NODE/')"
+echo ""
+echo "Then open in browser: http://localhost:$PORT"
+echo ""
+echo "=========================================="
+
+# Start Jupyter
+cd ~/cuda-lab/learning-path/week-01
+jupyter lab --no-browser --ip=0.0.0.0 --port=$PORT
diff --git a/verify_cuda.py b/verify_cuda.py
new file mode 100644
index 0000000..bcbbb51
--- /dev/null
+++ b/verify_cuda.py
@@ -0,0 +1,90 @@
+#!/usr/bin/env python3
+"""Verify CUDA setup is working on T4 GPU"""
+
+import numpy as np
+from numba import cuda
+import math
+
+print("=" * 50)
+print("CUDA SETUP VERIFICATION")
+print("=" * 50)
+
+# Check CUDA
+if not cuda.is_available():
+    print("‚ùå CUDA is NOT available!")
+    print("   Make sure you're on a GPU node:")
+    print("   srun --partition=t4flex --gres=gpu:1 --pty bash")
+    exit(1)
+
+print("‚úÖ CUDA is available")
+
+# Get device info
+device = cuda.get_current_device()
+print(f"‚úÖ GPU: {device.name.decode()}")
+cc = device.compute_capability
+print(f"   Compute Capability: {cc[0]}.{cc[1]}")
+print(f"   SMs: {device.MULTIPROCESSOR_COUNT}")
+print(f"   Max threads/block: {device.MAX_THREADS_PER_BLOCK}")
+
+# Test a simple kernel
+@cuda.jit
+def test_kernel(arr):
+    idx = cuda.grid(1)
+    if idx < arr.size:
+        arr[idx] = idx * 2
+
+# Run test
+n = 1000
+arr = cuda.device_array(n, dtype=np.float32)
+threads = 256
+blocks = math.ceil(n / threads)
+
+test_kernel[blocks, threads](arr)
+result = arr.copy_to_host()
+
+if result[10] == 20.0 and result[100] == 200.0:
+    print("‚úÖ Kernel execution successful")
+else:
+    print("‚ùå Kernel execution failed")
+    exit(1)
+
+# Memory test
+ctx = cuda.current_context()
+free, total = ctx.get_memory_info()
+print(f"‚úÖ GPU Memory: {free/1e9:.1f} GB free / {total/1e9:.1f} GB total")
+
+# Performance test
+import time
+
+@cuda.jit
+def vector_add(a, b, c):
+    idx = cuda.grid(1)
+    if idx < c.size:
+        c[idx] = a[idx] + b[idx]
+
+n = 10_000_000
+a = cuda.to_device(np.random.randn(n).astype(np.float32))
+b = cuda.to_device(np.random.randn(n).astype(np.float32))
+c = cuda.device_array(n, dtype=np.float32)
+
+# Warmup
+vector_add[math.ceil(n/256), 256](a, b, c)
+cuda.synchronize()
+
+# Benchmark
+start = time.perf_counter()
+for _ in range(100):
+    vector_add[math.ceil(n/256), 256](a, b, c)
+cuda.synchronize()
+elapsed = (time.perf_counter() - start) / 100
+
+bandwidth = (3 * n * 4) / elapsed / 1e9  # Read a, b, write c
+print(f"‚úÖ Vector add bandwidth: {bandwidth:.1f} GB/s")
+
+print("=" * 50)
+print("üöÄ T4 GPU ready for CUDA learning!")
+print("=" * 50)
+print()
+print("Next steps:")
+print("  cd ~/cuda-lab/learning-path/week-01")
+print("  jupyter notebook day-1-gpu-basics.ipynb")
-- 
2.43.7


From 856f503acb4daba7a04d5d7244afb7301d2c8ff6 Mon Sep 17 00:00:00 2001
From: sdodlapa <your-email@example.com>
Date: Wed, 17 Dec 2025 21:38:41 +0000
Subject: [PATCH 2/3] Update scripts to use crun with Python 3.12 module

- Replace conda with crun container system for Python 3.12.8
- Update gpu-session.sh for crun workflow
- Update start-jupyter-gpu.sh for crun workflow
- Update setup-environment.sh with crun environment creation
- Update SETUP-GPU.md with cluster-specific instructions
---
 learning-path/SETUP-GPU.md   | 65 ++++++++++++++++++++++++------------
 scripts/gpu-session.sh       | 34 +++++++++++--------
 scripts/setup-environment.sh | 64 ++++++++++++++++++++---------------
 scripts/start-jupyter-gpu.sh | 19 +++++------
 4 files changed, 111 insertions(+), 71 deletions(-)
 mode change 100644 => 100755 scripts/gpu-session.sh
 mode change 100644 => 100755 scripts/setup-environment.sh
 mode change 100644 => 100755 scripts/start-jupyter-gpu.sh

diff --git a/learning-path/SETUP-GPU.md b/learning-path/SETUP-GPU.md
index 0a6e9b7..fb153a8 100644
--- a/learning-path/SETUP-GPU.md
+++ b/learning-path/SETUP-GPU.md
@@ -46,48 +46,71 @@ print("‚úÖ GPU:", cuda.get_current_device().name.decode())
 
 ---
 
-## üéØ Option 2: Your ODU HPC System
+## üéØ Option 2: ODU HPC t4flex Cluster ‚≠ê YOUR SETUP
 
-If you have access to GPU nodes on ODU's HPC:
+You have access to 10 T4 GPU nodes on the `t4flex` partition!
 
-### Step 1: Request a GPU Node
+### One-Time Setup (already done!)
 
 ```bash
-# Interactive session with T4 GPU
-srun --partition=gpu --gres=gpu:t4:1 --time=04:00:00 --pty bash
+# Load Python 3.12 module and create environment
+module load python3
+crun -c -p ~/envs/cuda_lab
 
-# Or submit a job
-sbatch --partition=gpu --gres=gpu:1 your_script.sh
+# Install packages
+crun -p ~/envs/cuda_lab pip install numba numpy jupyter jupyterlab matplotlib
 ```
 
-### Step 2: Load CUDA Module
+### Daily Workflow
+
+#### Option A: Interactive GPU Session
 
 ```bash
-module load cuda/12.0
-module load anaconda3/2023.09  # or your Python module
+# Request a T4 GPU node (4 hours)
+./scripts/gpu-session.sh
+
+# Once on the GPU node, run commands with:
+crun -p ~/envs/cuda_lab python verify_cuda.py
+crun -p ~/envs/cuda_lab jupyter lab --no-browser --port=8888
 ```
 
-### Step 3: Create Conda Environment
+#### Option B: Submit Jupyter Job
 
 ```bash
-conda create -n cuda-learning python=3.10 -y
-conda activate cuda-learning
-conda install numba cudatoolkit numpy jupyter -c conda-forge -y
+# Submit job to run Jupyter on GPU
+sbatch scripts/start-jupyter-gpu.sh
+
+# Check the log for connection instructions
+cat jupyter-*.log
 ```
 
-### Step 4: Run Jupyter on GPU Node
+### Quick Reference
 
 ```bash
-# On the GPU node
-jupyter notebook --no-browser --port=8888
+# Load environment (do this first!)
+module load python3
+
+# Run Python script
+crun -p ~/envs/cuda_lab python my_script.py
 
-# Then tunnel from your local machine:
-ssh -L 8888:localhost:8888 your_username@turing.hpc.odu.edu
+# Run Jupyter
+crun -p ~/envs/cuda_lab jupyter lab
+
+# Interactive Python
+crun -p ~/envs/cuda_lab python
+
+# Install packages
+crun -p ~/envs/cuda_lab pip install package_name
 ```
 
-### Step 5: Open Notebooks
+### Connecting to Jupyter from Local Machine
 
-Navigate to `~/cuda-lab/learning-path/week-01/` and open notebooks.
+```bash
+# In a new local terminal, create SSH tunnel:
+ssh -L 8888:GPUNODE:8888 sdodl001_odu_edu@hpcslurm-slurm-login-001
+
+# Then open in browser: http://localhost:8888
+```
 
 ---
 
diff --git a/scripts/gpu-session.sh b/scripts/gpu-session.sh
old mode 100644
new mode 100755
index 6f4b248..28e9390
--- a/scripts/gpu-session.sh
+++ b/scripts/gpu-session.sh
@@ -2,10 +2,13 @@
 # Quick interactive GPU session for CUDA learning
 # Usage: ./gpu-session.sh
 
+ENV_PATH="${1:-$HOME/envs/cuda_lab}"
+
 echo "üöÄ Requesting T4 GPU node..."
 echo "   Partition: t4flex"
 echo "   GPU: 1x T4"
 echo "   Time: 4 hours"
+echo "   Environment: $ENV_PATH"
 echo ""
 
 srun --partition=t4flex \
@@ -13,20 +16,23 @@ srun --partition=t4flex \
      --cpus-per-task=4 \
      --mem=16G \
      --time=04:00:00 \
-     --pty bash -c '
-echo "=========================================="
-echo "‚úÖ GPU Node: $(hostname)"
+     --pty bash -c "
+echo '=========================================='
+echo '‚úÖ GPU Node:' \$(hostname)
 nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv
-echo "=========================================="
-echo ""
-echo "Activating CUDA environment..."
-source ~/miniconda3/bin/activate cuda-learning 2>/dev/null || conda activate cuda-learning
-echo ""
-echo "üéì Ready! You can now:"
-echo "   1. Run: python verify_cuda.py"
-echo "   2. Run: jupyter notebook --no-browser --port=8888"
-echo "   3. Run: python -c \"from numba import cuda; print(cuda.get_current_device().name.decode())\""
-echo ""
+echo '=========================================='
+echo ''
+echo 'Loading Python 3.12 environment...'
+module load python3
+echo ''
+echo 'üéì Ready! Run commands with crun:'
+echo '   crun -p $ENV_PATH python ~/cuda-lab/verify_cuda.py'
+echo '   crun -p $ENV_PATH jupyter lab --no-browser --port=8888'
+echo '   crun -p $ENV_PATH python -c \"from numba import cuda; print(cuda.get_current_device().name.decode())\"'
+echo ''
+echo 'Or start an interactive Python shell:'
+echo '   crun -p $ENV_PATH python'
+echo ''
 cd ~/cuda-lab
 exec bash
-'
+"
diff --git a/scripts/setup-environment.sh b/scripts/setup-environment.sh
old mode 100644
new mode 100755
index 03f8dda..4001c64
--- a/scripts/setup-environment.sh
+++ b/scripts/setup-environment.sh
@@ -1,43 +1,50 @@
 #!/bin/bash
-# Setup CUDA learning environment (run once)
+# Setup CUDA learning environment for ODU HPC cluster
+# Uses Python 3.12 module with crun container system
+
+set -e
+
+ENV_PATH="${1:-$HOME/envs/cuda_lab}"
 
 echo "=========================================="
 echo "Setting up CUDA Learning Environment"
 echo "=========================================="
+echo "Environment path: $ENV_PATH"
+echo ""
 
-# Check if conda is available
-if ! command -v conda &> /dev/null; then
-    echo "Installing Miniconda..."
-    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh
-    bash /tmp/miniconda.sh -b -p $HOME/miniconda3
-    eval "$($HOME/miniconda3/bin/conda shell.bash hook)"
-    conda init bash
-    echo "‚úÖ Miniconda installed"
+# Load Python module
+echo "Loading Python 3.12 module..."
+module load python3
+
+# Create environment if it doesn't exist
+if [ -d "$ENV_PATH" ]; then
+    echo "‚úÖ Environment already exists at $ENV_PATH"
 else
-    echo "‚úÖ Conda already available"
+    echo "Creating new Python 3.12 environment..."
+    crun -c -p "$ENV_PATH"
+    echo "‚úÖ Environment created"
 fi
 
-# Create environment
+# Install packages
 echo ""
-echo "Creating cuda-learning environment..."
-conda create -n cuda-learning python=3.10 -y
+echo "Installing/verifying CUDA learning packages..."
+crun -p "$ENV_PATH" pip install --quiet numba numpy jupyter jupyterlab matplotlib ipywidgets
 
-echo ""
-echo "Installing packages..."
-conda activate cuda-learning
-pip install numba numpy jupyter jupyterlab matplotlib
-
-# Verify
+# Verify installation
 echo ""
 echo "=========================================="
 echo "Verifying installation..."
 echo "=========================================="
-python -c "
-from numba import cuda
+crun -p "$ENV_PATH" python -c "
 import numpy as np
+from numba import cuda
+print('‚úÖ Python:', __import__('sys').version.split()[0])
 print('‚úÖ NumPy:', np.__version__)
-print('‚úÖ Numba installed')
-print('‚úÖ CUDA module available:', hasattr(cuda, 'jit'))
+print('‚úÖ Numba:', __import__('numba').__version__)
+print('‚úÖ CUDA module:', 'available' if hasattr(cuda, 'jit') else 'missing')
+print('')
+print('Note: CUDA will show as unavailable on login node.')
+print('      It will work on GPU nodes (t4flex partition).')
 "
 
 echo ""
@@ -45,7 +52,12 @@ echo "=========================================="
 echo "‚úÖ Setup complete!"
 echo "=========================================="
 echo ""
-echo "To start learning:"
-echo "  1. Run: ./scripts/gpu-session.sh"
-echo "  2. Or submit: sbatch scripts/start-jupyter-gpu.sh"
+echo "Quick reference:"
+echo "  Load env:    module load python3"
+echo "  Run Python:  crun -p $ENV_PATH python script.py"
+echo "  Run Jupyter: crun -p $ENV_PATH jupyter lab"
+echo ""
+echo "To start learning on GPU:"
+echo "  1. Interactive: ./scripts/gpu-session.sh"
+echo "  2. Jupyter job: sbatch scripts/start-jupyter-gpu.sh"
 echo ""
diff --git a/scripts/start-jupyter-gpu.sh b/scripts/start-jupyter-gpu.sh
old mode 100644
new mode 100755
index 7f3cb27..d296814
--- a/scripts/start-jupyter-gpu.sh
+++ b/scripts/start-jupyter-gpu.sh
@@ -7,21 +7,19 @@
 #SBATCH --time=04:00:00
 #SBATCH --output=jupyter-%j.log
 
+ENV_PATH="${ENV_PATH:-$HOME/envs/cuda_lab}"
+
 echo "=========================================="
 echo "CUDA Learning Session"
 echo "=========================================="
 echo "Node: $(hostname)"
+echo "Environment: $ENV_PATH"
 echo "GPU:"
 nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
 echo "=========================================="
 
-# Load modules (adjust based on your cluster)
-module load cuda 2>/dev/null || echo "No cuda module needed"
-
-# Activate conda environment
-source ~/miniconda3/bin/activate cuda-learning 2>/dev/null || \
-    source ~/.conda/envs/cuda-learning/bin/activate 2>/dev/null || \
-    echo "Activating environment..."
+# Load Python 3.12 module
+module load python3
 
 # Get a free port
 PORT=$(shuf -i 8000-9000 -n 1)
@@ -34,12 +32,13 @@ echo "=========================================="
 echo ""
 echo "To connect, run this on your LOCAL machine:"
 echo ""
-echo "  ssh -L $PORT:$(hostname):$PORT $(whoami)@$(hostname -f | sed 's/hpcslurm-nst4flex-[0-9]/LOGIN_NODE/')"
+echo "  ssh -L $PORT:$(hostname):$PORT $(whoami)@hpcslurm-slurm-login-001"
 echo ""
 echo "Then open in browser: http://localhost:$PORT"
 echo ""
+echo "Job log: jupyter-$SLURM_JOB_ID.log"
 echo "=========================================="
 
-# Start Jupyter
+# Start Jupyter using crun
 cd ~/cuda-lab/learning-path/week-01
-jupyter lab --no-browser --ip=0.0.0.0 --port=$PORT
+crun -p "$ENV_PATH" jupyter lab --no-browser --ip=0.0.0.0 --port=$PORT
-- 
2.43.7


From c09507258deb0f7611b972320152a8ede6bac0a4 Mon Sep 17 00:00:00 2001
From: sdodlapa <your-email@example.com>
Date: Wed, 17 Dec 2025 21:49:14 +0000
Subject: [PATCH 3/3] Add CUDA 13.1 support for B200/H100/A100 GPUs

- Add setup-cuda13.sh for CUDA 13.1 environment with numba-cuda
- Add gpu-advanced.sh for B200/H100/A100 GPU sessions
- Update SETUP-GPU.md with advanced GPU options and CUDA 13.1 info
- Document new numba-cuda package (separate from numba for CUDA 13)
---
 learning-path/SETUP-GPU.md |  57 +++++++++++++++++++-
 scripts/gpu-advanced.sh    |  70 ++++++++++++++++++++++++
 scripts/setup-cuda13.sh    | 107 +++++++++++++++++++++++++++++++++++++
 3 files changed, 233 insertions(+), 1 deletion(-)
 create mode 100755 scripts/gpu-advanced.sh
 create mode 100755 scripts/setup-cuda13.sh

diff --git a/learning-path/SETUP-GPU.md b/learning-path/SETUP-GPU.md
index fb153a8..ffa8216 100644
--- a/learning-path/SETUP-GPU.md
+++ b/learning-path/SETUP-GPU.md
@@ -114,7 +114,62 @@ ssh -L 8888:GPUNODE:8888 sdodl001_odu_edu@hpcslurm-slurm-login-001
 
 ---
 
-## üéØ Option 3: Local Machine with NVIDIA GPU
+## üéØ Option 3: Advanced GPUs (B200, H100, A100) + CUDA 13.1 üî•
+
+Your cluster has cutting-edge GPUs! Here's how to use them:
+
+### Available Advanced GPU Partitions
+
+| Partition | GPU | GPUs/Node | Memory | Use Case |
+|-----------|-----|-----------|--------|----------|
+| `b200flex` | B200 (Blackwell) | 8 | 192GB HBM3e | Latest architecture! |
+| `h100octflex` | H100 | 8 | 80GB HBM3 | Large models, multi-GPU |
+| `h100quadflex` | H100 | 4 | 80GB HBM3 | Medium workloads |
+| `h100flex` | H100 | 1 | 80GB HBM3 | Single-GPU learning |
+| `a100flex` | A100 | 1 | 40-80GB | Great for AI/ML |
+| `t4flex` | T4 | 1 | 16GB | Entry-level learning |
+
+### Setup CUDA 13.1 Environment
+
+```bash
+# One-time setup for CUDA 13.1
+./scripts/setup-cuda13.sh
+
+# This creates ~/envs/cuda13 with:
+# - numba + numba-cuda (new separate package!)
+# - cuda-python (NVIDIA bindings)
+# - cupy (GPU NumPy)
+```
+
+### Request Advanced GPU
+
+```bash
+# B200 (Blackwell) - Latest GPU!
+./scripts/gpu-advanced.sh b200flex 1
+
+# H100 (Hopper) - Great for learning
+./scripts/gpu-advanced.sh h100flex 1
+
+# A100 (Ampere) - Solid all-rounder
+./scripts/gpu-advanced.sh a100flex 1
+
+# Or manually:
+srun --partition=h100flex --gres=gpu:1 -c 8 --mem=64G --time=04:00:00 --pty bash
+```
+
+### CUDA 13.1 Features (B200/H100)
+
+| Feature | T4 | H100/B200 |
+|---------|-------|-----------|
+| FP8 Tensor Cores | ‚ùå | ‚úÖ |
+| Thread Block Clusters | ‚ùå | ‚úÖ |
+| Tensor Memory Accelerator | ‚ùå | ‚úÖ |
+| Async Copy | Basic | Advanced |
+| Memory Bandwidth | 320 GB/s | 3+ TB/s |
+
+---
+
+## üéØ Option 4: Local Machine with NVIDIA GPU
 
 If you have a local NVIDIA GPU:
 
diff --git a/scripts/gpu-advanced.sh b/scripts/gpu-advanced.sh
new file mode 100755
index 0000000..3d5a299
--- /dev/null
+++ b/scripts/gpu-advanced.sh
@@ -0,0 +1,70 @@
+#!/bin/bash
+# Interactive GPU session for advanced GPUs (B200, H100, A100)
+# Usage: ./gpu-advanced.sh [partition] [gpus]
+#
+# Examples:
+#   ./gpu-advanced.sh              # Default: h100flex, 1 GPU
+#   ./gpu-advanced.sh b200flex 2   # B200 with 2 GPUs
+#   ./gpu-advanced.sh a100flex 1   # A100 with 1 GPU
+
+PARTITION="${1:-h100flex}"
+NUM_GPUS="${2:-1}"
+ENV_PATH="${CUDA13_ENV:-$HOME/envs/cuda13}"
+
+echo "üöÄ Requesting GPU node..."
+echo "   Partition: $PARTITION"
+echo "   GPUs: $NUM_GPUS"
+echo "   Environment: $ENV_PATH"
+echo "   Time: 4 hours"
+echo ""
+
+# Validate partition
+case $PARTITION in
+    b200flex)
+        echo "   GPU Type: NVIDIA B200 (Blackwell) - 192GB HBM3e"
+        ;;
+    h100flex|h100dualflex|h100quadflex|h100octflex|h100spot)
+        echo "   GPU Type: NVIDIA H100 (Hopper) - 80GB HBM3"
+        ;;
+    a100flex)
+        echo "   GPU Type: NVIDIA A100 (Ampere) - 40/80GB HBM2e"
+        ;;
+    t4flex)
+        echo "   GPU Type: NVIDIA T4 (Turing) - 16GB GDDR6"
+        echo "   Note: For T4, use ./gpu-session.sh instead"
+        ;;
+    *)
+        echo "   Unknown partition: $PARTITION"
+        echo "   Available: b200flex, h100flex, h100octflex, a100flex, t4flex"
+        ;;
+esac
+echo ""
+
+srun --partition=$PARTITION \
+     --gres=gpu:$NUM_GPUS \
+     --cpus-per-task=8 \
+     --mem=64G \
+     --time=04:00:00 \
+     --pty bash -c "
+echo '=========================================='
+echo '‚úÖ GPU Node:' \$(hostname)
+echo '=========================================='
+nvidia-smi --query-gpu=index,name,memory.total,driver_version,compute_cap --format=csv
+echo ''
+echo 'CUDA Version:'
+nvidia-smi | grep 'CUDA Version' | head -1
+echo '=========================================='
+echo ''
+echo 'Loading Python 3.12 environment...'
+module load python3
+echo ''
+echo 'üéì Ready! Run commands with crun:'
+echo '   crun -p $ENV_PATH python ~/cuda-lab/verify_cuda.py'
+echo '   crun -p $ENV_PATH jupyter lab --no-browser --port=8888'
+echo ''
+echo 'Multi-GPU example:'
+echo '   crun -p $ENV_PATH python -c \"from numba import cuda; print([cuda.gpus[i].name.decode() for i in range(len(cuda.gpus))])\"'
+echo ''
+cd ~/cuda-lab
+exec bash
+"
diff --git a/scripts/setup-cuda13.sh b/scripts/setup-cuda13.sh
new file mode 100755
index 0000000..adf72a1
--- /dev/null
+++ b/scripts/setup-cuda13.sh
@@ -0,0 +1,107 @@
+#!/bin/bash
+# Setup CUDA 13.1 environment for L40/B200/H100 GPUs
+# This uses the new numba-cuda package (separate from numba)
+
+set -e
+
+ENV_PATH="${1:-$HOME/envs/cuda13}"
+
+echo "=========================================="
+echo "Setting up CUDA 13.1 Environment"
+echo "=========================================="
+echo "Environment path: $ENV_PATH"
+echo ""
+echo "Target GPUs: B200, H100, A100, L40"
+echo ""
+
+# Load Python module
+echo "Loading Python 3.12 module..."
+module load python3
+
+# Create environment if it doesn't exist
+if [ -d "$ENV_PATH" ]; then
+    echo "‚úÖ Environment already exists at $ENV_PATH"
+    echo "   To recreate, run: rm -rf $ENV_PATH"
+else
+    echo "Creating new Python 3.12 environment..."
+    crun -c -p "$ENV_PATH"
+    echo "‚úÖ Environment created"
+fi
+
+# Install packages
+echo ""
+echo "Installing CUDA 13.1 compatible packages..."
+crun -p "$ENV_PATH" pip install --quiet --upgrade pip
+
+# Core packages
+crun -p "$ENV_PATH" pip install --quiet numpy matplotlib jupyter jupyterlab ipywidgets
+
+# CUDA packages - NEW numba-cuda is separate!
+echo "Installing numba + numba-cuda (CUDA 13 support)..."
+crun -p "$ENV_PATH" pip install --quiet numba numba-cuda
+
+# NVIDIA CUDA Python bindings (recommended for CUDA 13)
+echo "Installing NVIDIA CUDA Python bindings..."
+crun -p "$ENV_PATH" pip install --quiet cuda-python
+
+# Optional: cupy for GPU arrays (alternative to numba for some use cases)
+echo "Installing CuPy (GPU NumPy)..."
+crun -p "$ENV_PATH" pip install --quiet cupy-cuda12x || echo "CuPy install skipped (may need CUDA 13 version)"
+
+# Verify installation
+echo ""
+echo "=========================================="
+echo "Verifying installation..."
+echo "=========================================="
+crun -p "$ENV_PATH" python -c "
+import sys
+print('Python:', sys.version.split()[0])
+
+import numpy as np
+print('NumPy:', np.__version__)
+
+import numba
+print('Numba:', numba.__version__)
+
+try:
+    import numba_cuda
+    print('numba-cuda: installed ‚úÖ')
+except ImportError:
+    print('numba-cuda: not found (using built-in)')
+
+try:
+    from cuda import cuda as cuda_driver
+    print('cuda-python: installed ‚úÖ')
+except ImportError:
+    print('cuda-python: not found')
+
+from numba import cuda
+print('CUDA available:', cuda.is_available())
+if cuda.is_available():
+    dev = cuda.get_current_device()
+    print(f'  GPU: {dev.name.decode()}')
+    print(f'  Compute Capability: {dev.compute_capability}')
+"
+
+echo ""
+echo "=========================================="
+echo "‚úÖ CUDA 13.1 Environment Ready!"
+echo "=========================================="
+echo ""
+echo "Available GPU partitions on this cluster:"
+echo "  ‚Ä¢ b200flex    - 1 node √ó 8 B200 GPUs (Blackwell)"
+echo "  ‚Ä¢ h100octflex - 2 nodes √ó 8 H100 GPUs"
+echo "  ‚Ä¢ h100quadflex - 4 nodes √ó 4 H100 GPUs"
+echo "  ‚Ä¢ h100flex    - 15 nodes √ó 1 H100 GPU"
+echo "  ‚Ä¢ a100flex    - 10 nodes √ó 1 A100 GPU"
+echo ""
+echo "Quick start commands:"
+echo "  # Request B200 GPU (8 GPUs per node!)"
+echo "  srun --partition=b200flex --gres=gpu:1 -c 8 --time=01:00:00 --pty bash"
+echo ""
+echo "  # Request H100 GPU"
+echo "  srun --partition=h100flex --gres=gpu:1 -c 4 --time=04:00:00 --pty bash"
+echo ""
+echo "  # Then run Python:"
+echo "  module load python3"
+echo "  crun -p $ENV_PATH python your_script.py"
-- 
2.43.7

