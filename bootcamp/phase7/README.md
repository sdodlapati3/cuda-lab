# Phase 7: DL Kernels & Attention (Weeks 33-40)

Deep learning kernel optimization and attention mechanisms.

## Overview

| Week | Topic | Focus |
|------|-------|-------|
| 33-34 | Softmax & LayerNorm | Numerically stable implementations |
| 35-36 | Attention Building Blocks | QKV computation, masking |
| 37-38 | FlashAttention Study | IO-aware algorithm design |
| 39-40 | Kernel Fusion Strategies | Memory traffic optimization |

## Key Technologies
- Online softmax (single pass)
- Welford's algorithm for variance
- Tiled attention patterns
- FlashAttention concepts

## Learning Objectives
- Implement numerically stable softmax
- Optimize LayerNorm forward/backward
- Build attention from primitives
- Understand IO-aware algorithms

## Prerequisites
- Phase 1-6 completion
- GEMM optimization experience
- cuDNN familiarity
