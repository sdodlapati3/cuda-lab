# CUDA-Lab Enhancement Plan: NESAP/HPC Career Alignment

> **Created:** January 24, 2026  
> **Goal:** Transform cuda-lab into a comprehensive portfolio that demonstrates NESAP-ready skills  
> **Target Role:** NERSC/NESAP ML Postdoc (Systems-aware ML at HPC scale)

---

## ðŸ“‹ Executive Summary

This plan addresses three objectives:
1. **Reorganize** - Eliminate redundant folders, consolidate learning paths
2. **Expand** - Add missing HPC/scientific computing content
3. **Align** - Map all content to NESAP skill requirements

### Current State Assessment

| Folder | Purpose | Redundancy Issue | Action |
|--------|---------|------------------|--------|
| `learning-path/` | 18-week interactive notebooks | âœ… Core curriculum | **KEEP - Primary** |
| `bootcamp/` | 52-week intensive curriculum | âœ… Advanced track | **KEEP - Advanced** |
| `tutorials/` | Markdown tutorials | âŒ Duplicates learning-path, 1 file exists | **DEPRECATE** |
| `notes/` | Personal notes, curriculum drafts | âš ï¸ `cuda-learning-curriculum.md` duplicates tutorials | **CONSOLIDATE** |
| `cuda-programming-guide/` | Reference documentation | âœ… Unique reference | **KEEP** |
| `practice/` | Hands-on exercises | âœ… Complements learning-path | **KEEP + EXPAND** |
| `blog-templates/` | Blog conversion templates | âš ï¸ Empty infrastructure | **DEFER** |

---

## ðŸ—‚ï¸ Part 1: Organizational Cleanup

### 1.1 Deprecate `tutorials/` Folder

**Problem:** The `tutorials/` folder promises 40+ tutorials but contains only 1 file. All this content already exists in better form in `learning-path/` notebooks.

**Action:**
```bash
# Move the single existing tutorial to notes/ as reference
mv tutorials/01-foundations/01-cpu-vs-gpu.md notes/reference/
# Remove empty tutorials structure
rm -rf tutorials/
```

**Update `mkdocs.yml`** to remove tutorials from navigation.

---

### 1.2 Consolidate `notes/` Folder

**Problem:** `notes/cuda-learning-curriculum.md` (566 lines) duplicates tutorial planning that's already implemented in `learning-path/`.

**Action:**
```
notes/                          # BEFORE
â”œâ”€â”€ cuda-learning-curriculum.md   # Redundant planning doc
â””â”€â”€ cuda-quick-reference.md       # Useful cheatsheet

notes/                          # AFTER
â”œâ”€â”€ cuda-quick-reference.md       # Keep
â”œâ”€â”€ reference/                    # New
â”‚   â””â”€â”€ cpu-vs-gpu.md             # From tutorials/
â””â”€â”€ archive/                      # Archive old planning
    â””â”€â”€ cuda-learning-curriculum.md
```

---

### 1.3 Clarify Learning Path vs Bootcamp Relationship

**Problem:** Unclear when to use 18-week learning-path vs 52-week bootcamp.

**Solution:** Add a top-level `LEARNING-TRACKS.md`:

```markdown
# cuda-lab Learning Tracks

## Track 1: Foundation (learning-path/)
- **Duration:** 18 weeks part-time
- **Audience:** Anyone learning CUDA
- **Outcome:** Working CUDA proficiency

## Track 2: Mastery (bootcamp/)  
- **Duration:** 52 weeks full-time
- **Audience:** ML engineers targeting performance roles
- **Prerequisites:** Complete Track 1 or equivalent
- **Outcome:** Expert-level GPU performance engineering

## Track 3: NESAP Preparation (NEW)
- **Duration:** 12 weeks intensive
- **Audience:** Targeting HPC/scientific ML roles
- **Focus:** Distributed training, profiling, HPC workflows
```

---

## ðŸŽ¯ Part 2: NESAP Skill Gap Analysis

### Mapping NESAP Requirements to Current Content

| NESAP Skill | Current Coverage | Gap Level | Priority |
|-------------|------------------|-----------|----------|
| **Deep Learning Fundamentals** | âŒ Assumed prerequisite | HIGH | Add ML foundations module |
| **Scientific ML (PINNs, UQ)** | âŒ Not covered | HIGH | New module needed |
| **Distributed Training (DDP, FSDP)** | âœ… bootcamp/phase8 | LOW | Expand with benchmarks |
| **GPU Architecture Awareness** | âœ… Excellent coverage | NONE | Complete |
| **Performance Profiling** | âš ï¸ Partial (theory, light practice) | MEDIUM | Add profiling lab |
| **Python at Scale** | âŒ CUDA C++ focus | MEDIUM | Add PyTorch optimization |
| **CUDA/Triton/C++ Extensions** | âœ… bootcamp/phase8 | LOW | Complete |
| **Large-Scale Data Pipelines** | âŒ Not covered | HIGH | New module needed |
| **HPC Workflows (Slurm, containers)** | âš ï¸ Mentioned, not practiced | HIGH | Add HPC lab |
| **Linux & HPC Environments** | âŒ Assumed | MEDIUM | Add quick reference |
| **Benchmarking & Scaling Metrics** | âš ï¸ Concepts only | HIGH | Add benchmark suite |
| **Scientific Domains** | âŒ Not covered | MEDIUM | Add case studies |

---

## ðŸ”§ Part 3: New Content Modules

### 3.1 Performance Profiling Lab (HIGH PRIORITY)

**Location:** `profiling-lab/` (new top-level directory)

**Why NESAP cares:** "Performance bugs often come from hardware misunderstandings... NESAP success = measured improvements, not guesses."

```
profiling-lab/
â”œâ”€â”€ README.md                         # Profiling philosophy & tools overview
â”œâ”€â”€ 01-nsight-systems/
â”‚   â”œâ”€â”€ README.md                     # Timeline analysis, CPU-GPU overlap
â”‚   â”œâ”€â”€ exercises/
â”‚   â”‚   â”œâ”€â”€ ex01-timeline-basics/     # Read timeline, identify stalls
â”‚   â”‚   â”œâ”€â”€ ex02-kernel-overlap/      # Streams and concurrency
â”‚   â”‚   â”œâ”€â”€ ex03-memory-timeline/     # H2D/D2H transfer analysis
â”‚   â”‚   â””â”€â”€ ex04-multi-gpu-timeline/  # NCCL communication profiling
â”‚   â””â”€â”€ case-studies/
â”‚       â”œâ”€â”€ case01-transformer-training.md
â”‚       â””â”€â”€ case02-inference-latency.md
â”œâ”€â”€ 02-nsight-compute/
â”‚   â”œâ”€â”€ README.md                     # Kernel-level profiling
â”‚   â”œâ”€â”€ exercises/
â”‚   â”‚   â”œâ”€â”€ ex01-memory-metrics/      # Bandwidth, cache hit rates
â”‚   â”‚   â”œâ”€â”€ ex02-compute-metrics/     # Occupancy, warp efficiency
â”‚   â”‚   â”œâ”€â”€ ex03-roofline-practice/   # Plot kernels, identify bottlenecks
â”‚   â”‚   â””â”€â”€ ex04-optimization-loop/   # Profile â†’ optimize â†’ reprofile
â”‚   â””â”€â”€ reference/
â”‚       â”œâ”€â”€ key-metrics-cheatsheet.md # Most important metrics explained
â”‚       â””â”€â”€ common-bottlenecks.md     # Memory-bound vs compute-bound
â”œâ”€â”€ 03-pytorch-profiler/
â”‚   â”œâ”€â”€ README.md                     # PyTorch Profiler + TensorBoard
â”‚   â”œâ”€â”€ exercises/
â”‚   â”‚   â”œâ”€â”€ ex01-basic-profiling/     # torch.profiler basics
â”‚   â”‚   â”œâ”€â”€ ex02-memory-profiling/    # Memory snapshots
â”‚   â”‚   â””â”€â”€ ex03-distributed-profiling/ # DDP profiling
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ profiling-harness.py      # Reusable profiling wrapper
â”œâ”€â”€ 04-energy-profiling/              # NESAP: "science per joule"
â”‚   â”œâ”€â”€ README.md                     # nvidia-smi, NVML, power monitoring
â”‚   â”œâ”€â”€ exercises/
â”‚   â”‚   â”œâ”€â”€ ex01-power-measurement/
â”‚   â”‚   â””â”€â”€ ex02-energy-efficiency/
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ energy-benchmark.py
â””â”€â”€ 05-scaling-benchmarks/
    â”œâ”€â”€ README.md                     # Strong vs weak scaling
    â”œâ”€â”€ exercises/
    â”‚   â”œâ”€â”€ ex01-single-gpu-baseline/
    â”‚   â”œâ”€â”€ ex02-multi-gpu-scaling/
    â”‚   â””â”€â”€ ex03-communication-overhead/
    â””â”€â”€ templates/
        â”œâ”€â”€ scaling-benchmark-template.py
        â””â”€â”€ scaling-report-template.md
```

**Key Deliverables:**
- [ ] Nsight Systems timeline analysis for DDP training
- [ ] Nsight Compute optimization loop (3 iterations minimum)
- [ ] Roofline plot of reduction/matmul kernels with annotations
- [ ] Energy efficiency benchmark comparing implementations
- [ ] Scaling efficiency report (90%+ efficiency at 4 GPUs)

---

### 3.2 HPC Workflows Lab (HIGH PRIORITY)

**Location:** `hpc-lab/` (new top-level directory)

**Why NESAP cares:** "NERSC â‰  local workstation... Can you design a fault-tolerant training workflow?"

```
hpc-lab/
â”œâ”€â”€ README.md                         # HPC mindset for ML practitioners
â”œâ”€â”€ 01-slurm-basics/
â”‚   â”œâ”€â”€ README.md                     # Job submission, arrays, dependencies
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ single-gpu-job.sbatch
â”‚   â”‚   â”œâ”€â”€ multi-gpu-job.sbatch
â”‚   â”‚   â”œâ”€â”€ multi-node-job.sbatch
â”‚   â”‚   â””â”€â”€ job-array.sbatch
â”‚   â””â”€â”€ exercises/
â”‚       â”œâ”€â”€ ex01-submit-monitor/      # sbatch, squeue, scancel
â”‚       â”œâ”€â”€ ex02-resource-requests/   # GPU allocation, memory
â”‚       â””â”€â”€ ex03-job-dependencies/    # Workflow orchestration
â”œâ”€â”€ 02-checkpointing/
â”‚   â”œâ”€â”€ README.md                     # Fault-tolerant training
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”œâ”€â”€ pytorch-checkpoint.py
â”‚   â”‚   â”œâ”€â”€ distributed-checkpoint.py
â”‚   â”‚   â””â”€â”€ auto-resume.sbatch
â”‚   â””â”€â”€ exercises/
â”‚       â”œâ”€â”€ ex01-basic-checkpoint/
â”‚       â”œâ”€â”€ ex02-distributed-checkpoint/
â”‚       â””â”€â”€ ex03-preemption-handling/
â”œâ”€â”€ 03-containers/
â”‚   â”œâ”€â”€ README.md                     # Singularity/Apptainer for HPC
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ cuda-pytorch.def          # Container definition
â”‚   â”‚   â””â”€â”€ build-container.sh
â”‚   â””â”€â”€ exercises/
â”‚       â”œâ”€â”€ ex01-build-container/
â”‚       â”œâ”€â”€ ex02-gpu-in-container/
â”‚       â””â”€â”€ ex03-mpi-container/
â”œâ”€â”€ 04-filesystems/
â”‚   â”œâ”€â”€ README.md                     # Lustre, GPFS, scratch vs home
â”‚   â”œâ”€â”€ best-practices.md             # I/O patterns for HPC
â”‚   â””â”€â”€ exercises/
â”‚       â”œâ”€â”€ ex01-io-benchmarking/
â”‚       â””â”€â”€ ex02-data-staging/
â”œâ”€â”€ 05-environment-management/
â”‚   â”œâ”€â”€ README.md                     # Modules, conda, pip
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ environment.yml
â”‚   â”‚   â””â”€â”€ setup-env.sh
â”‚   â””â”€â”€ nersc-specific.md             # NERSC module system
â””â”€â”€ 06-debugging-hpc/
    â”œâ”€â”€ README.md                     # Debugging multi-node jobs
    â”œâ”€â”€ common-failures.md            # Timeout, OOM, NCCL errors
    â””â”€â”€ exercises/
        â”œâ”€â”€ ex01-log-analysis/
        â””â”€â”€ ex02-distributed-debugging/
```

**Key Deliverables:**
- [ ] Fault-tolerant training script with auto-resume
- [ ] Multi-node job script with proper resource allocation
- [ ] Singularity container for reproducible ML environment
- [ ] I/O benchmark showing optimal data loading patterns

---

### 3.3 Scientific ML Module (HIGH PRIORITY)

**Location:** `learning-path/scientific-ml/` OR `bootcamp/scientific-ml/`

**Why NESAP cares:** "NESAP projects almost always couple ML to simulations or scientific pipelines."

```
scientific-ml/
â”œâ”€â”€ README.md                         # Scientific ML overview
â”œâ”€â”€ 01-pinns/
â”‚   â”œâ”€â”€ README.md                     # Physics-Informed Neural Networks
â”‚   â”œâ”€â”€ theory/
â”‚   â”‚   â”œâ”€â”€ pinn-formulation.md
â”‚   â”‚   â””â”€â”€ loss-function-design.md
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”œâ”€â”€ 1d-heat-equation.ipynb
â”‚   â”‚   â”œâ”€â”€ burgers-equation.ipynb
â”‚   â”‚   â””â”€â”€ navier-stokes-2d.ipynb
â”‚   â””â”€â”€ exercises/
â”‚       â””â”€â”€ ex01-custom-pinn/
â”œâ”€â”€ 02-surrogate-models/
â”‚   â”œâ”€â”€ README.md                     # Replacing expensive simulations
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”œâ”€â”€ neural-operator.ipynb
â”‚   â”‚   â””â”€â”€ autoencoder-dynamics.ipynb
â”‚   â””â”€â”€ exercises/
â”‚       â””â”€â”€ ex01-simulation-surrogate/
â”œâ”€â”€ 03-hybrid-solvers/
â”‚   â”œâ”€â”€ README.md                     # ML + numerical methods
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”œâ”€â”€ ml-preconditioner.ipynb
â”‚   â”‚   â””â”€â”€ learned-correction.ipynb
â”‚   â””â”€â”€ exercises/
â”‚       â””â”€â”€ ex01-hybrid-system/
â”œâ”€â”€ 04-uncertainty-quantification/
â”‚   â”œâ”€â”€ README.md                     # UQ methods
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”œâ”€â”€ mc-dropout.ipynb
â”‚   â”‚   â”œâ”€â”€ deep-ensembles.ipynb
â”‚   â”‚   â””â”€â”€ bayesian-nn.ipynb
â”‚   â””â”€â”€ exercises/
â”‚       â””â”€â”€ ex01-uq-pipeline/
â””â”€â”€ 05-case-studies/
    â”œâ”€â”€ climate-emulator.md
    â”œâ”€â”€ materials-property-prediction.md
    â””â”€â”€ particle-physics-reconstruction.md
```

---

### 3.4 Data Pipeline Module (HIGH PRIORITY)

**Location:** `learning-path/data-pipelines/` or integrate into existing weeks

**Why NESAP cares:** "I/O often dominates ML at scale... Can you explain how to avoid GPU starvation due to I/O?"

```
data-pipelines/
â”œâ”€â”€ README.md
â”œâ”€â”€ 01-parallel-loading/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ pytorch-dataloader.ipynb      # num_workers, pin_memory
â”‚   â”œâ”€â”€ prefetching-patterns.ipynb
â”‚   â””â”€â”€ exercises/
â”œâ”€â”€ 02-large-datasets/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ sharding-strategies.ipynb
â”‚   â”œâ”€â”€ memory-mapped-datasets.ipynb
â”‚   â”œâ”€â”€ streaming-datasets.ipynb
â”‚   â””â”€â”€ exercises/
â”œâ”€â”€ 03-io-optimization/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ hdf5-patterns.ipynb
â”‚   â”œâ”€â”€ webdataset.ipynb
â”‚   â””â”€â”€ exercises/
â””â”€â”€ 04-distributed-data/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ distributed-sampler.ipynb
    â””â”€â”€ exercises/
```

---

### 3.5 Benchmarking Suite (HIGH PRIORITY)

**Location:** `benchmarks/` (new top-level directory)

**Why NESAP cares:** "Scaling efficiency, time-to-solution, energy efficiency... Can you design a fair scaling benchmark?"

```
benchmarks/
â”œâ”€â”€ README.md                         # Benchmarking philosophy
â”œâ”€â”€ kernels/
â”‚   â”œâ”€â”€ reduction/
â”‚   â”‚   â”œâ”€â”€ benchmark.py
â”‚   â”‚   â”œâ”€â”€ baselines/                # cuBLAS, CUB reference
â”‚   â”‚   â””â”€â”€ results/                  # CSV/JSON results
â”‚   â”œâ”€â”€ matmul/
â”‚   â”œâ”€â”€ softmax/
â”‚   â””â”€â”€ attention/
â”œâ”€â”€ scaling/
â”‚   â”œâ”€â”€ strong-scaling/
â”‚   â”‚   â”œâ”€â”€ benchmark.py
â”‚   â”‚   â””â”€â”€ plot-scaling.py
â”‚   â”œâ”€â”€ weak-scaling/
â”‚   â””â”€â”€ communication-overhead/
â”œâ”€â”€ hardware-baselines/
â”‚   â”œâ”€â”€ T4.json                       # Baseline numbers for comparison
â”‚   â”œâ”€â”€ A100-40GB.json
â”‚   â”œâ”€â”€ A100-80GB.json
â”‚   â””â”€â”€ H100.json
â”œâ”€â”€ roofline/
â”‚   â”œâ”€â”€ generate-roofline.py
â”‚   â”œâ”€â”€ plot-roofline.py
â”‚   â””â”€â”€ reference-plots/
â”œâ”€â”€ energy/
â”‚   â”œâ”€â”€ power-benchmark.py
â”‚   â””â”€â”€ efficiency-report.py
â””â”€â”€ templates/
    â”œâ”€â”€ benchmark-template.py
    â”œâ”€â”€ scaling-report-template.md
    â””â”€â”€ regression-test.py
```

---

## ðŸ”„ Part 4: Content Updates to Existing Modules

### 4.1 Enhance `practice/06-systems/` 

Add missing exercises:

```
practice/06-systems/                  # CURRENT
â”œâ”€â”€ ex01-ipc-producer-consumer/
â”œâ”€â”€ ex02-texture-image-processing/

practice/06-systems/                  # ENHANCED
â”œâ”€â”€ ex01-ipc-producer-consumer/
â”œâ”€â”€ ex02-texture-image-processing/
â”œâ”€â”€ ex03-production-error-handling/   # NEW: Async errors, watchdogs
â”œâ”€â”€ ex04-gpu-health-monitoring/       # NEW: NVML, health checks
â”œâ”€â”€ ex05-multi-process-inference/     # NEW: Triton-style serving
â””â”€â”€ ex06-mig-partitioning/            # NEW: A100/H100 MIG
```

---

### 4.2 Add PyTorch Optimization Track to Bootcamp

**Location:** `bootcamp/pytorch-optimization/` (supplement to phase8)

```
pytorch-optimization/
â”œâ”€â”€ README.md
â”œâ”€â”€ 01-torch-compile/
â”‚   â”œâ”€â”€ basics.ipynb
â”‚   â”œâ”€â”€ debugging-failures.ipynb
â”‚   â””â”€â”€ custom-backends.ipynb
â”œâ”€â”€ 02-memory-optimization/
â”‚   â”œâ”€â”€ gradient-checkpointing.ipynb
â”‚   â”œâ”€â”€ activation-checkpointing.ipynb
â”‚   â””â”€â”€ memory-efficient-attention.ipynb
â”œâ”€â”€ 03-distributed-optimization/
â”‚   â”œâ”€â”€ ddp-tuning.ipynb
â”‚   â”œâ”€â”€ fsdp-sharding.ipynb
â”‚   â””â”€â”€ pipeline-parallelism.ipynb
â””â”€â”€ 04-inference-optimization/
    â”œâ”€â”€ export-optimization.ipynb
    â”œâ”€â”€ quantization.ipynb
    â””â”€â”€ batching-strategies.ipynb
```

---

### 4.3 Add Quick Reference for HPC/Linux

**Location:** `notes/hpc-quick-reference.md`

```markdown
# HPC Quick Reference

## Slurm Commands
- sbatch, squeue, scancel, sinfo, sacct

## Environment Modules  
- module load/unload/list/avail

## Common NERSC Modules
- python, pytorch, cuda, cudnn, nccl

## Filesystem Layout
- $HOME (small, backed up)
- $SCRATCH (large, not backed up)
- $CFS (community shared)

## Debugging Distributed Jobs
- NCCL_DEBUG=INFO
- CUDA_LAUNCH_BLOCKING=1
- torch.distributed.breakpoint()
```

---

## ðŸ“… Part 5: Implementation Timeline

### Phase 1: Cleanup (Week 1)
- [ ] Deprecate `tutorials/` folder
- [ ] Consolidate `notes/` folder
- [ ] Create `LEARNING-TRACKS.md`
- [ ] Update main `README.md` navigation

### Phase 2: Profiling Lab (Weeks 2-3)
- [ ] Create `profiling-lab/` structure
- [ ] Nsight Systems exercises (4)
- [ ] Nsight Compute exercises (4)
- [ ] PyTorch profiler exercises (3)
- [ ] Energy profiling module
- [ ] Scaling benchmarks template

### Phase 3: HPC Lab (Weeks 4-5)
- [ ] Create `hpc-lab/` structure
- [ ] Slurm templates and exercises
- [ ] Checkpointing module
- [ ] Container templates
- [ ] Filesystem best practices

### Phase 4: Scientific ML (Weeks 6-8)
- [ ] PINN examples and exercises
- [ ] Surrogate model module
- [ ] UQ methods module
- [ ] Case studies (3)

### Phase 5: Benchmarks & Polish (Weeks 9-10)
- [ ] Benchmark suite setup
- [ ] Hardware baselines
- [ ] Roofline generation tools
- [ ] Scaling benchmark templates

### Phase 6: Integration (Weeks 11-12)
- [ ] Data pipelines module
- [ ] PyTorch optimization track
- [ ] Cross-link all modules
- [ ] Final documentation pass

---

## ðŸŽ¯ Part 6: NESAP Readiness Checklist

After completing this enhancement plan, verify:

### Systems-Aware ML Thinking
- [ ] Can explain memory-bound vs compute-bound with profiler evidence
- [ ] Can diagnose scaling efficiency drops
- [ ] Can identify when Python abstraction is the bottleneck

### Performance Modeling
- [ ] Can plot kernels on roofline and explain position
- [ ] Can predict performance improvement from optimization
- [ ] Can design fair scaling benchmarks

### ML + Simulation Integration
- [ ] Have implemented PINN or surrogate model
- [ ] Understand hybrid solver patterns
- [ ] Can apply UQ methods

### Efficiency Narratives
- [ ] Can write optimization report with before/after metrics
- [ ] Can present scaling results clearly
- [ ] Have documented case studies in portfolio

---

## ðŸ“Š Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Profiling exercises completed | 15+ | Checklist |
| HPC workflow exercises | 10+ | Checklist |
| Scientific ML examples | 5+ | Notebook count |
| Benchmark baselines | 4 GPUs | JSON files |
| Scaling efficiency demonstrated | >90% at 4 GPU | Benchmark results |
| Portfolio case studies | 3+ | Markdown docs |
| NESAP skill coverage | >90% | Gap analysis |

---

## ðŸ”— Related Documents

- [CURRICULUM-ENHANCEMENT-PLAN.md](CURRICULUM-ENHANCEMENT-PLAN.md) - Previous enhancement (completed)
- [ADVANCED-TOPICS-ENHANCEMENT.md](ADVANCED-TOPICS-ENHANCEMENT.md) - Advanced CUDA features
- [modern-gpu-ecosystem.md](modern-gpu-ecosystem.md) - Tool selection guide
- [notebook-quality-guide.md](notebook-quality-guide.md) - Notebook standards

---

## Appendix A: Proposed Final Directory Structure

```
cuda-lab/
â”œâ”€â”€ README.md                         # Main entry point
â”œâ”€â”€ LEARNING-TRACKS.md                # NEW: Track overview
â”œâ”€â”€ mkdocs.yml
â”‚
â”œâ”€â”€ learning-path/                    # 18-week foundation curriculum
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ week-01/ ... week-18/
â”‚   â”œâ”€â”€ scientific-ml/                # NEW: PINN, surrogate, UQ
â”‚   â””â”€â”€ data-pipelines/               # NEW: I/O optimization
â”‚
â”œâ”€â”€ bootcamp/                         # 52-week mastery curriculum
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ phase0/ ... phase8/
â”‚   â”œâ”€â”€ pytorch-optimization/         # NEW: torch.compile, DDP tuning
â”‚   â”œâ”€â”€ starters/
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ capstones/
â”‚
â”œâ”€â”€ profiling-lab/                    # NEW: Performance analysis
â”‚   â”œâ”€â”€ 01-nsight-systems/
â”‚   â”œâ”€â”€ 02-nsight-compute/
â”‚   â”œâ”€â”€ 03-pytorch-profiler/
â”‚   â”œâ”€â”€ 04-energy-profiling/
â”‚   â””â”€â”€ 05-scaling-benchmarks/
â”‚
â”œâ”€â”€ hpc-lab/                          # NEW: HPC workflows
â”‚   â”œâ”€â”€ 01-slurm-basics/
â”‚   â”œâ”€â”€ 02-checkpointing/
â”‚   â”œâ”€â”€ 03-containers/
â”‚   â”œâ”€â”€ 04-filesystems/
â”‚   â””â”€â”€ 05-debugging-hpc/
â”‚
â”œâ”€â”€ benchmarks/                       # NEW: Benchmark suite
â”‚   â”œâ”€â”€ kernels/
â”‚   â”œâ”€â”€ scaling/
â”‚   â”œâ”€â”€ hardware-baselines/
â”‚   â””â”€â”€ roofline/
â”‚
â”œâ”€â”€ cuda-programming-guide/           # Reference documentation
â”‚   â””â”€â”€ (unchanged)
â”‚
â”œâ”€â”€ practice/                         # Hands-on exercises
â”‚   â”œâ”€â”€ 01-foundations/
â”‚   â”œâ”€â”€ 02-memory/
â”‚   â”œâ”€â”€ 03-parallel/
â”‚   â”œâ”€â”€ 04-optimization/
â”‚   â”œâ”€â”€ 05-advanced/
â”‚   â””â”€â”€ 06-systems/                   # ENHANCED
â”‚
â”œâ”€â”€ notes/                            # CONSOLIDATED
â”‚   â”œâ”€â”€ cuda-quick-reference.md
â”‚   â”œâ”€â”€ hpc-quick-reference.md        # NEW
â”‚   â”œâ”€â”€ reference/
â”‚   â””â”€â”€ archive/
â”‚
â”œâ”€â”€ docs/                             # Planning & guides
â”‚   â”œâ”€â”€ NESAP-ALIGNED-ENHANCEMENT-PLAN.md  # THIS DOCUMENT
â”‚   â”œâ”€â”€ CURRICULUM-ENHANCEMENT-PLAN.md
â”‚   â”œâ”€â”€ ADVANCED-TOPICS-ENHANCEMENT.md
â”‚   â””â”€â”€ modern-gpu-ecosystem.md
â”‚
â”œâ”€â”€ blog-templates/                   # DEFERRED
â”‚
â””â”€â”€ scripts/                          # Utility scripts
    â””â”€â”€ (unchanged)
```

---

*Last updated: January 24, 2026*
