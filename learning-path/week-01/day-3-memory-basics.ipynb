{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea28ffed",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 3: GPU Memory Fundamentals\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-01/day-3-memory-basics.ipynb)\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "> **Note:** If running on Google Colab, go to `Runtime â†’ Change runtime type â†’ T4 GPU` before starting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164740f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"\\nâš ï¸  Remember: CUDA C++ code is the PRIMARY learning material!\")\n",
    "print(\"   Python/Numba is provided for quick interactive testing only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5957e72",
   "metadata": {},
   "source": [
    "# Day 3: GPU Memory Fundamentals\n",
    "\n",
    "Memory management is **the most important skill** in CUDA programming. Today you'll learn:\n",
    "- GPU memory hierarchy\n",
    "- Explicit memory management (`cudaMalloc`, `cudaMemcpy`, `cudaFree`)\n",
    "- Unified/Managed memory (`cudaMallocManaged`)\n",
    "- Pinned vs pageable memory for transfers\n",
    "- Performance implications of each approach\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Memory Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        GPU MEMORY HIERARCHY                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Registers (per thread)     â† Fastest (~0 latency)             â”‚\n",
    "â”‚   â”œâ”€â”€ ~255 32-bit registers per thread                          â”‚\n",
    "â”‚   â””â”€â”€ Private to each thread                                    â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Shared Memory (per block)  â† Very fast (~5 cycles)            â”‚\n",
    "â”‚   â”œâ”€â”€ 48-164 KB per SM                                          â”‚\n",
    "â”‚   â””â”€â”€ Shared between threads in same block                      â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   L1 Cache / Texture Cache   â† Fast (~30 cycles)                â”‚\n",
    "â”‚   â””â”€â”€ Automatic caching                                         â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   L2 Cache                   â† Medium (~200 cycles)             â”‚\n",
    "â”‚   â””â”€â”€ Shared across all SMs                                     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Global Memory (VRAM)       â† Slow (~400+ cycles)              â”‚\n",
    "â”‚   â”œâ”€â”€ 4-80 GB (main GPU memory)                                 â”‚\n",
    "â”‚   â””â”€â”€ Accessible by all threads                                 â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Host Memory (System RAM)   â† Very slow (PCIe transfer)        â”‚\n",
    "â”‚   â””â”€â”€ Must be copied to GPU before use                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ffdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"CUDA device:\", cuda.get_current_device().name.decode())\n",
    "\n",
    "# Get memory info\n",
    "ctx = cuda.current_context()\n",
    "free_mem, total_mem = ctx.get_memory_info()\n",
    "print(f\"GPU Memory: {free_mem/1e9:.2f} GB free / {total_mem/1e9:.2f} GB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3276b7",
   "metadata": {},
   "source": [
    "## 2. Explicit Memory Management\n",
    "\n",
    "The traditional CUDA workflow requires **explicit** memory transfers:\n",
    "\n",
    "```\n",
    "1. Allocate memory on GPU (cudaMalloc)\n",
    "2. Copy data from CPU to GPU (cudaMemcpy H2D)\n",
    "3. Launch kernel\n",
    "4. Copy results from GPU to CPU (cudaMemcpy D2H)\n",
    "5. Free GPU memory (cudaFree)\n",
    "```\n",
    "\n",
    "In Numba:\n",
    "- `cuda.to_device(array)` - Allocates and copies to GPU\n",
    "- `cuda.device_array(shape, dtype)` - Allocates on GPU only\n",
    "- `device_array.copy_to_host()` - Copies back to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def saxpy_kernel(a, x, y, out, n):\n",
    "    \"\"\"SAXPY: out = a*x + y (Single-precision A*X Plus Y)\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n:\n",
    "        out[idx] = a * x[idx] + y[idx]\n",
    "\n",
    "def explicit_memory_workflow(n, a=2.0):\n",
    "    \"\"\"Demonstrate explicit memory management\"\"\"\n",
    "    print(f\"\\nğŸ“Š Explicit Memory Workflow (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Step 1: Create data on CPU\n",
    "    x_host = np.random.randn(n).astype(np.float32)\n",
    "    y_host = np.random.randn(n).astype(np.float32)\n",
    "    out_host = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    # Step 2: Allocate and copy to GPU\n",
    "    start = time.perf_counter()\n",
    "    x_device = cuda.to_device(x_host)        # Allocate + copy\n",
    "    y_device = cuda.to_device(y_host)        # Allocate + copy\n",
    "    out_device = cuda.device_array(n, dtype=np.float32)  # Allocate only\n",
    "    h2d_time = time.perf_counter() - start\n",
    "    print(f\"  Hostâ†’Device transfer: {h2d_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Step 3: Launch kernel\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    saxpy_kernel[blocks, threads](a, x_device, y_device, out_device, n)\n",
    "    cuda.synchronize()\n",
    "    kernel_time = time.perf_counter() - start\n",
    "    print(f\"  Kernel execution:      {kernel_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Step 4: Copy result back\n",
    "    start = time.perf_counter()\n",
    "    out_host = out_device.copy_to_host()\n",
    "    d2h_time = time.perf_counter() - start\n",
    "    print(f\"  Deviceâ†’Host transfer:  {d2h_time*1000:.3f} ms\")\n",
    "    \n",
    "    print(f\"  Total time:            {(h2d_time + kernel_time + d2h_time)*1000:.3f} ms\")\n",
    "    \n",
    "    # Verify\n",
    "    expected = a * x_host + y_host\n",
    "    print(f\"  âœ… Correct: {np.allclose(out_host, expected)}\")\n",
    "    \n",
    "    return h2d_time, kernel_time, d2h_time\n",
    "\n",
    "# Run with different sizes\n",
    "for size in [100_000, 1_000_000, 10_000_000]:\n",
    "    explicit_memory_workflow(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e73a4",
   "metadata": {},
   "source": [
    "## 3. Unified Memory (cudaMallocManaged)\n",
    "\n",
    "**Unified Memory** simplifies programming - memory is automatically migrated between CPU and GPU as needed.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     CPU      â”‚â—„â”€â”€â”€â”€ Automatic â”€â”€â”€â”€â–ºâ”‚     GPU      â”‚\n",
    "â”‚              â”‚      Migration      â”‚              â”‚\n",
    "â”‚  ptr[i] = x  â”‚                    â”‚  ptr[i] = y  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        Same pointer works on both!\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simpler code\n",
    "- No explicit transfers needed\n",
    "- Can access more data than GPU memory (oversubscription)\n",
    "\n",
    "**Cons:**\n",
    "- May have higher latency (page faults)\n",
    "- Less control over when transfers happen\n",
    "- Can be slower if not used carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_memory_workflow(n, a=2.0):\n",
    "    \"\"\"Demonstrate unified memory (managed memory)\"\"\"\n",
    "    print(f\"\\nğŸ“Š Unified Memory Workflow (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Allocate managed memory - accessible from both CPU and GPU\n",
    "    x = cuda.managed_array(n, dtype=np.float32)\n",
    "    y = cuda.managed_array(n, dtype=np.float32)\n",
    "    out = cuda.managed_array(n, dtype=np.float32)\n",
    "    \n",
    "    # Initialize on CPU (memory will migrate to GPU when needed)\n",
    "    x[:] = np.random.randn(n).astype(np.float32)\n",
    "    y[:] = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Launch kernel (data migrates automatically)\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    saxpy_kernel[blocks, threads](a, x, y, out, n)\n",
    "    cuda.synchronize()\n",
    "    total_time = time.perf_counter() - start\n",
    "    print(f\"  Kernel + migration time: {total_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Access result on CPU (data migrates back automatically)\n",
    "    expected = a * x + y\n",
    "    print(f\"  âœ… Correct: {np.allclose(out, expected)}\")\n",
    "    \n",
    "    return total_time\n",
    "\n",
    "# Compare with explicit management\n",
    "for size in [100_000, 1_000_000, 10_000_000]:\n",
    "    unified_memory_workflow(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f7062",
   "metadata": {},
   "source": [
    "## 4. Pinned (Page-Locked) Memory\n",
    "\n",
    "Normal CPU memory can be **paged out** to disk by the OS. This causes problems for DMA transfers.\n",
    "\n",
    "**Pinned memory** is locked in physical RAM - no paging allowed.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Pageable Memory (default)     â”‚  Pinned Memory             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  CPU Memory                    â”‚  CPU Memory (locked)       â”‚\n",
    "â”‚      â†“ (copy to staging)       â”‚      â†“ (direct DMA)        â”‚\n",
    "â”‚  Pinned Buffer                 â”‚                            â”‚\n",
    "â”‚      â†“ (DMA to GPU)            â”‚      â†“ (DMA to GPU)        â”‚\n",
    "â”‚  GPU Memory                    â”‚  GPU Memory                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  TWO copies!                   â”‚  ONE copy! (faster)        â”‚\n",
    "â”‚  Can be swapped to disk        â”‚  Always in RAM             â”‚\n",
    "â”‚  Unlimited size                â”‚  Limited by system RAM     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f57309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pinned_vs_pageable(n):\n",
    "    \"\"\"Compare transfer speeds with pinned vs pageable memory\"\"\"\n",
    "    print(f\"\\nğŸ“Š Pinned vs Pageable Memory (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Pageable memory (default NumPy allocation)\n",
    "    pageable = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Pinned memory\n",
    "    pinned = cuda.pinned_array(n, dtype=np.float32)\n",
    "    pinned[:] = pageable  # Copy data to pinned\n",
    "    \n",
    "    # Measure pageable transfer\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        d_pageable = cuda.to_device(pageable)\n",
    "        cuda.synchronize()\n",
    "    pageable_time = (time.perf_counter() - start) / 10\n",
    "    \n",
    "    # Measure pinned transfer\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        d_pinned = cuda.to_device(pinned)\n",
    "        cuda.synchronize()\n",
    "    pinned_time = (time.perf_counter() - start) / 10\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    data_size_gb = n * 4 / 1e9  # float32 = 4 bytes\n",
    "    pageable_bw = data_size_gb / pageable_time\n",
    "    pinned_bw = data_size_gb / pinned_time\n",
    "    \n",
    "    print(f\"  Pageable: {pageable_time*1000:.3f} ms ({pageable_bw:.2f} GB/s)\")\n",
    "    print(f\"  Pinned:   {pinned_time*1000:.3f} ms ({pinned_bw:.2f} GB/s)\")\n",
    "    print(f\"  Speedup:  {pageable_time/pinned_time:.2f}x\")\n",
    "\n",
    "# Compare for different sizes\n",
    "for size in [1_000_000, 10_000_000, 50_000_000]:\n",
    "    compare_pinned_vs_pageable(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3578ad8",
   "metadata": {},
   "source": [
    "## 5. When Transfer Time Dominates\n",
    "\n",
    "For simple operations like vector addition, **memory transfers dominate execution time**.\n",
    "\n",
    "This is why:\n",
    "1. GPU is best for **compute-intensive** operations\n",
    "2. You should **minimize transfers** (keep data on GPU)\n",
    "3. **Batch operations** together before copying back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transfer_overhead(n):\n",
    "    \"\"\"Show how transfer time compares to compute time\"\"\"\n",
    "    print(f\"\\nğŸ“Š Transfer vs Compute Analysis (N = {n:,})\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    a = np.random.randn(n).astype(np.float32)\n",
    "    b = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Measure H2D transfer\n",
    "    start = time.perf_counter()\n",
    "    a_d = cuda.to_device(a)\n",
    "    b_d = cuda.to_device(b)\n",
    "    c_d = cuda.device_array(n, dtype=np.float32)\n",
    "    cuda.synchronize()\n",
    "    h2d_time = time.perf_counter() - start\n",
    "    \n",
    "    # Measure kernel (data already on GPU)\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    @cuda.jit\n",
    "    def add_kernel(a, b, c):\n",
    "        idx = cuda.grid(1)\n",
    "        if idx < c.size:\n",
    "            c[idx] = a[idx] + b[idx]\n",
    "    \n",
    "    # Warmup\n",
    "    add_kernel[blocks, threads](a_d, b_d, c_d)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        add_kernel[blocks, threads](a_d, b_d, c_d)\n",
    "    cuda.synchronize()\n",
    "    kernel_time = (time.perf_counter() - start) / 100\n",
    "    \n",
    "    # Measure D2H transfer\n",
    "    start = time.perf_counter()\n",
    "    c = c_d.copy_to_host()\n",
    "    cuda.synchronize()\n",
    "    d2h_time = time.perf_counter() - start\n",
    "    \n",
    "    total = h2d_time + kernel_time + d2h_time\n",
    "    \n",
    "    print(f\"  H2D Transfer:  {h2d_time*1000:>8.3f} ms ({h2d_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  Kernel:        {kernel_time*1000:>8.3f} ms ({kernel_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  D2H Transfer:  {d2h_time*1000:>8.3f} ms ({d2h_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(f\"  Total:         {total*1000:>8.3f} ms\")\n",
    "    \n",
    "    if (h2d_time + d2h_time) > kernel_time:\n",
    "        print(f\"\\n  âš ï¸  Transfer time ({(h2d_time + d2h_time)*1000:.2f} ms) > Kernel time ({kernel_time*1000:.2f} ms)\")\n",
    "        print(f\"     Consider: Keep data on GPU, batch operations!\")\n",
    "\n",
    "analyze_transfer_overhead(10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87ad6d",
   "metadata": {},
   "source": [
    "## ğŸ¯ Exercises\n",
    "\n",
    "### Exercise 1: Memory Reuse Pattern\n",
    "Implement a pipeline that reuses GPU memory for multiple operations without copying back to CPU between each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62579f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 1: Memory Reuse Pattern\n",
    "# Compute: result = ((a + b) * c) - d\n",
    "# Do this with ONE H2D transfer and ONE D2H transfer\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] + b[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def mul_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] * b[idx]\n",
    "\n",
    "@cuda.jit  \n",
    "def sub_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] - b[idx]\n",
    "\n",
    "def pipeline_inefficient(a, b, c, d):\n",
    "    \"\"\"BAD: Transfers after each operation\"\"\"\n",
    "    # TODO: This is the slow way - see how many transfers happen\n",
    "    pass\n",
    "\n",
    "def pipeline_efficient(a, b, c, d):\n",
    "    \"\"\"GOOD: All operations on GPU, one final transfer\"\"\"\n",
    "    # TODO: Implement efficient version\n",
    "    # 1. Copy all inputs to GPU\n",
    "    # 2. Perform all operations\n",
    "    # 3. Copy only result back\n",
    "    pass\n",
    "\n",
    "# Test your implementations\n",
    "# N = 10_000_000\n",
    "# a = np.random.randn(N).astype(np.float32)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418bb032",
   "metadata": {},
   "source": [
    "### Exercise 2: Memory Bandwidth Calculation\n",
    "Calculate the theoretical vs achieved memory bandwidth for a copy operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 2: Bandwidth Measurement\n",
    "@cuda.jit\n",
    "def copy_kernel(src, dst):\n",
    "    \"\"\"Simple copy kernel - measures GPU memory bandwidth\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < dst.size:\n",
    "        dst[idx] = src[idx]\n",
    "\n",
    "def measure_bandwidth(n):\n",
    "    \"\"\"Measure achieved memory bandwidth\"\"\"\n",
    "    # TODO: \n",
    "    # 1. Create two device arrays\n",
    "    # 2. Time the copy kernel\n",
    "    # 3. Calculate bandwidth: (bytes_read + bytes_written) / time\n",
    "    # 4. Compare to theoretical peak (from device properties)\n",
    "    pass\n",
    "\n",
    "# measure_bandwidth(100_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae5362",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### Memory Management Summary:\n",
    "\n",
    "| Method | When to Use | Pros | Cons |\n",
    "|--------|-------------|------|------|\n",
    "| **Explicit** (`to_device`, `copy_to_host`) | Production code, max control | Fastest, predictable | More code |\n",
    "| **Unified** (`managed_array`) | Prototyping, complex access patterns | Simple code | Page fault overhead |\n",
    "| **Pinned** (`pinned_array`) | High-bandwidth transfers | ~2x transfer speed | Uses system RAM |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Minimize transfers**: Keep data on GPU as long as possible\n",
    "2. **Use pinned memory**: For frequent large transfers\n",
    "3. **Batch operations**: Don't copy back between every operation\n",
    "4. **Profile first**: Measure before optimizing\n",
    "\n",
    "### Memory Bandwidth Rule of Thumb:\n",
    "- PCIe 3.0 x16: ~16 GB/s\n",
    "- PCIe 4.0 x16: ~32 GB/s  \n",
    "- GPU Memory: 200-900 GB/s (much faster!)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Next Up: Day 4 - Error Handling & Debugging\n",
    "- CUDA error codes\n",
    "- Debugging techniques\n",
    "- Common pitfalls\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”— Resources\n",
    "- [CUDA Memory Guide](../../cuda-programming-guide/02-basics/cuda-memory.md)\n",
    "- [Understanding Memory](../../cuda-programming-guide/02-basics/understanding-memory.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
