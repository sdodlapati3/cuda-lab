{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea28ffed",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 3: GPU Memory Fundamentals\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-01/day-3-memory-basics.ipynb)\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "> **Note:** If running on Google Colab, go to `Runtime â†’ Change runtime type â†’ T4 GPU` before starting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CUDA is available\n",
    "!nvcc --version\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d65d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 3: GPU Memory Fundamentals\n",
    "\n",
    "Memory management is **the most important skill** in CUDA programming. Today you'll learn:\n",
    "- GPU memory hierarchy\n",
    "- Explicit memory management (`cudaMalloc`, `cudaMemcpy`, `cudaFree`)\n",
    "- Unified/Managed memory (`cudaMallocManaged`)\n",
    "- Pinned vs pageable memory for transfers\n",
    "- Performance implications of each approach\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Memory Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        GPU MEMORY HIERARCHY                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Registers (per thread)     â† Fastest (~0 latency)             â”‚\n",
    "â”‚   â”œâ”€â”€ ~255 32-bit registers per thread                          â”‚\n",
    "â”‚   â””â”€â”€ Private to each thread                                    â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Shared Memory (per block)  â† Very fast (~5 cycles)            â”‚\n",
    "â”‚   â”œâ”€â”€ 48-164 KB per SM                                          â”‚\n",
    "â”‚   â””â”€â”€ Shared between threads in same block                      â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   L1 Cache / Texture Cache   â† Fast (~30 cycles)                â”‚\n",
    "â”‚   â””â”€â”€ Automatic caching                                         â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   L2 Cache                   â† Medium (~200 cycles)             â”‚\n",
    "â”‚   â””â”€â”€ Shared across all SMs                                     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Global Memory (VRAM)       â† Slow (~400+ cycles)              â”‚\n",
    "â”‚   â”œâ”€â”€ 4-80 GB (main GPU memory)                                 â”‚\n",
    "â”‚   â””â”€â”€ Accessible by all threads                                 â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Host Memory (System RAM)   â† Very slow (PCIe transfer)        â”‚\n",
    "â”‚   â””â”€â”€ Must be copied to GPU before use                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752b43d",
   "metadata": {},
   "source": [
    "## 2. Explicit Memory Management\n",
    "\n",
    "The traditional CUDA workflow requires **explicit** memory transfers:\n",
    "\n",
    "```\n",
    "1. Allocate memory on GPU     (cudaMalloc)\n",
    "2. Copy data CPU â†’ GPU        (cudaMemcpy H2D)\n",
    "3. Launch kernel\n",
    "4. Copy results GPU â†’ CPU     (cudaMemcpy D2H)\n",
    "5. Free GPU memory            (cudaFree)\n",
    "```\n",
    "\n",
    "Let's implement the classic **SAXPY** operation: `y = a*x + y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile explicit_memory.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <time.h>\n",
    "\n",
    "// SAXPY kernel: y = a*x + y\n",
    "__global__ void saxpy(float a, float* x, float* y, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        y[idx] = a * x[idx] + y[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void explicitMemoryWorkflow(int n, float a) {\n",
    "    printf(\"\\n=== Explicit Memory Workflow (N = %d) ===\\n\", n);\n",
    "    printf(\"---------------------------------------------------\\n\");\n",
    "    \n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Step 1: Allocate host memory\n",
    "    float *h_x = (float*)malloc(size);\n",
    "    float *h_y = (float*)malloc(size);\n",
    "    float *h_result = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize host data\n",
    "    srand(42);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_x[i] = (float)rand() / RAND_MAX;\n",
    "        h_y[i] = (float)rand() / RAND_MAX;\n",
    "        h_result[i] = h_y[i];  // Copy for verification\n",
    "    }\n",
    "    \n",
    "    // Step 2: Allocate device memory\n",
    "    float *d_x, *d_y;\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMalloc(&d_y, size);\n",
    "    \n",
    "    // Create CUDA events for timing\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Step 3: Copy Host â†’ Device (H2D)\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_y, h_result, size, cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float h2dTime;\n",
    "    cudaEventElapsedTime(&h2dTime, start, stop);\n",
    "    printf(\"  Hostâ†’Device transfer: %.3f ms\\n\", h2dTime);\n",
    "    \n",
    "    // Step 4: Launch kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    saxpy<<<blocksPerGrid, threadsPerBlock>>>(a, d_x, d_y, n);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float kernelTime;\n",
    "    cudaEventElapsedTime(&kernelTime, start, stop);\n",
    "    printf(\"  Kernel execution:      %.3f ms\\n\", kernelTime);\n",
    "    \n",
    "    // Step 5: Copy Device â†’ Host (D2H)\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(h_result, d_y, size, cudaMemcpyDeviceToHost);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float d2hTime;\n",
    "    cudaEventElapsedTime(&d2hTime, start, stop);\n",
    "    printf(\"  Deviceâ†’Host transfer:  %.3f ms\\n\", d2hTime);\n",
    "    \n",
    "    printf(\"  -----------------------------------\\n\");\n",
    "    printf(\"  Total time:            %.3f ms\\n\", h2dTime + kernelTime + d2hTime);\n",
    "    \n",
    "    // Verify results\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n && correct; i++) {\n",
    "        float expected = a * h_x[i] + h_y[i];\n",
    "        if (fabsf(h_result[i] - expected) > 1e-5) {\n",
    "            printf(\"  Mismatch at %d: %f vs %f\\n\", i, h_result[i], expected);\n",
    "            correct = false;\n",
    "        }\n",
    "    }\n",
    "    printf(\"  %s Results correct!\\n\", correct ? \"âœ…\" : \"âŒ\");\n",
    "    \n",
    "    // Step 6: Cleanup\n",
    "    cudaFree(d_x);\n",
    "    cudaFree(d_y);\n",
    "    free(h_x);\n",
    "    free(h_y);\n",
    "    free(h_result);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== CUDA Explicit Memory Management ===\\n\");\n",
    "    \n",
    "    // Get device info\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Global Memory: %.1f GB\\n\", prop.totalGlobalMem / 1e9);\n",
    "    \n",
    "    // Test with different sizes\n",
    "    explicitMemoryWorkflow(100000, 2.0f);      // 100K elements\n",
    "    explicitMemoryWorkflow(1000000, 2.0f);     // 1M elements\n",
    "    explicitMemoryWorkflow(10000000, 2.0f);    // 10M elements\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8faaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -o explicit_memory explicit_memory.cu && ./explicit_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94678372",
   "metadata": {},
   "source": [
    "## 3. Unified Memory (cudaMallocManaged)\n",
    "\n",
    "**Unified Memory** simplifies programming - memory is automatically migrated between CPU and GPU as needed.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     CPU      â”‚â—„â”€â”€â”€â”€ Automatic â”€â”€â”€â”€â–ºâ”‚     GPU      â”‚\n",
    "â”‚              â”‚      Migration      â”‚              â”‚\n",
    "â”‚  ptr[i] = x  â”‚                    â”‚  ptr[i] = y  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        Same pointer works on both!\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simpler code - no explicit transfers\n",
    "- Can access more data than GPU memory (oversubscription)\n",
    "\n",
    "**Cons:**\n",
    "- May have higher latency (page faults)\n",
    "- Less control over when transfers happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f23cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile unified_memory.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Same SAXPY kernel works with unified memory!\n",
    "__global__ void saxpy(float a, float* x, float* y, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        y[idx] = a * x[idx] + y[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void unifiedMemoryWorkflow(int n, float a) {\n",
    "    printf(\"\\n=== Unified Memory Workflow (N = %d) ===\\n\", n);\n",
    "    printf(\"---------------------------------------------------\\n\");\n",
    "    \n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate UNIFIED memory - accessible from CPU and GPU!\n",
    "    float *x, *y;\n",
    "    cudaMallocManaged(&x, size);\n",
    "    cudaMallocManaged(&y, size);\n",
    "    \n",
    "    // Initialize on CPU (data will migrate to GPU when needed)\n",
    "    srand(42);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        x[i] = (float)rand() / RAND_MAX;\n",
    "        y[i] = (float)rand() / RAND_MAX;\n",
    "    }\n",
    "    \n",
    "    // Save original y for verification\n",
    "    float *y_orig = (float*)malloc(size);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        y_orig[i] = y[i];\n",
    "    }\n",
    "    \n",
    "    // Launch kernel - data migrates automatically!\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    saxpy<<<blocksPerGrid, threadsPerBlock>>>(a, x, y, n);\n",
    "    cudaDeviceSynchronize();  // Wait for kernel AND migration\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float totalTime;\n",
    "    cudaEventElapsedTime(&totalTime, start, stop);\n",
    "    printf(\"  Kernel + migration time: %.3f ms\\n\", totalTime);\n",
    "    \n",
    "    // Access result on CPU (data migrates back automatically)\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n && correct; i++) {\n",
    "        float expected = a * x[i] + y_orig[i];\n",
    "        if (fabsf(y[i] - expected) > 1e-5) {\n",
    "            printf(\"  Mismatch at %d: %f vs %f\\n\", i, y[i], expected);\n",
    "            correct = false;\n",
    "        }\n",
    "    }\n",
    "    printf(\"  %s Results correct!\\n\", correct ? \"âœ…\" : \"âŒ\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    free(y_orig);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== CUDA Unified Memory ===\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Managed Memory: %s\\n\", prop.managedMemory ? \"Yes\" : \"No\");\n",
    "    \n",
    "    // Test with different sizes\n",
    "    unifiedMemoryWorkflow(100000, 2.0f);\n",
    "    unifiedMemoryWorkflow(1000000, 2.0f);\n",
    "    unifiedMemoryWorkflow(10000000, 2.0f);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a944c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -o unified_memory unified_memory.cu && ./unified_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd5b0",
   "metadata": {},
   "source": [
    "## 4. Pinned (Page-Locked) Memory\n",
    "\n",
    "Normal CPU memory can be **paged out** to disk by the OS. This causes problems for DMA transfers.\n",
    "\n",
    "**Pinned memory** is locked in physical RAM - no paging allowed.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Pageable Memory (default)     â”‚  Pinned Memory             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  CPU Memory                    â”‚  CPU Memory (locked)       â”‚\n",
    "â”‚      â†“ (copy to staging)       â”‚      â†“ (direct DMA)        â”‚\n",
    "â”‚  Pinned Buffer                 â”‚                            â”‚\n",
    "â”‚      â†“ (DMA to GPU)            â”‚      â†“ (DMA to GPU)        â”‚\n",
    "â”‚  GPU Memory                    â”‚  GPU Memory                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  TWO copies!                   â”‚  ONE copy! (faster)        â”‚\n",
    "â”‚  Can be swapped to disk        â”‚  Always in RAM             â”‚\n",
    "â”‚  Unlimited size                â”‚  Limited by system RAM     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pinned_memory.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "void comparePinnedVsPageable(int n) {\n",
    "    printf(\"\\n=== Pinned vs Pageable Memory (N = %d) ===\\n\", n);\n",
    "    printf(\"---------------------------------------------------\\n\");\n",
    "    \n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate pageable memory (default malloc)\n",
    "    float* h_pageable = (float*)malloc(size);\n",
    "    \n",
    "    // Allocate pinned (page-locked) memory\n",
    "    float* h_pinned;\n",
    "    cudaMallocHost(&h_pinned, size);  // or cudaHostAlloc\n",
    "    \n",
    "    // Initialize both with same data\n",
    "    srand(42);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        float val = (float)rand() / RAND_MAX;\n",
    "        h_pageable[i] = val;\n",
    "        h_pinned[i] = val;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, size);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 10;\n",
    "    \n",
    "    // Benchmark pageable transfer\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaMemcpy(d_data, h_pageable, size, cudaMemcpyHostToDevice);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float pageableTime;\n",
    "    cudaEventElapsedTime(&pageableTime, start, stop);\n",
    "    pageableTime /= ITERATIONS;\n",
    "    \n",
    "    // Benchmark pinned transfer\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaMemcpy(d_data, h_pinned, size, cudaMemcpyHostToDevice);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float pinnedTime;\n",
    "    cudaEventElapsedTime(&pinnedTime, start, stop);\n",
    "    pinnedTime /= ITERATIONS;\n",
    "    \n",
    "    // Calculate bandwidth (GB/s)\n",
    "    float dataSizeGB = size / 1e9;\n",
    "    float pageableBW = dataSizeGB / (pageableTime / 1000);\n",
    "    float pinnedBW = dataSizeGB / (pinnedTime / 1000);\n",
    "    \n",
    "    printf(\"  Pageable: %.3f ms (%.2f GB/s)\\n\", pageableTime, pageableBW);\n",
    "    printf(\"  Pinned:   %.3f ms (%.2f GB/s)\\n\", pinnedTime, pinnedBW);\n",
    "    printf(\"  Speedup:  %.2fx\\n\", pageableTime / pinnedTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_data);\n",
    "    cudaFreeHost(h_pinned);\n",
    "    free(h_pageable);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== CUDA Pinned Memory ===\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    \n",
    "    // Test with different sizes\n",
    "    comparePinnedVsPageable(1000000);     // 1M elements (4 MB)\n",
    "    comparePinnedVsPageable(10000000);    // 10M elements (40 MB)\n",
    "    comparePinnedVsPageable(50000000);    // 50M elements (200 MB)\n",
    "    \n",
    "    printf(\"\\nğŸ’¡ Tip: Use pinned memory for frequent CPU-GPU transfers!\\n\");\n",
    "    printf(\"   But don't over-allocate - it reduces available RAM.\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47385aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -o pinned_memory pinned_memory.cu && ./pinned_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551501e7",
   "metadata": {},
   "source": [
    "## 5. When Transfer Time Dominates\n",
    "\n",
    "For simple operations like vector addition, **memory transfers dominate execution time**.\n",
    "\n",
    "This is why:\n",
    "1. GPU is best for **compute-intensive** operations\n",
    "2. You should **minimize transfers** (keep data on GPU)\n",
    "3. **Batch operations** together before copying back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transfer_analysis.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void addKernel(float* a, float* b, float* c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void analyzeTransferOverhead(int n) {\n",
    "    printf(\"\\n=== Transfer vs Compute Analysis (N = %d) ===\\n\", n);\n",
    "    printf(\"------------------------------------------------------------\\n\");\n",
    "    \n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Measure H2D transfer\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float h2dTime;\n",
    "    cudaEventElapsedTime(&h2dTime, start, stop);\n",
    "    \n",
    "    // Measure kernel (warmup first)\n",
    "    int threads = 256;\n",
    "    int blocks = (n + threads - 1) / threads;\n",
    "    addKernel<<<blocks, threads>>>(d_a, d_b, d_c, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        addKernel<<<blocks, threads>>>(d_a, d_b, d_c, n);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float kernelTime;\n",
    "    cudaEventElapsedTime(&kernelTime, start, stop);\n",
    "    kernelTime /= 100;\n",
    "    \n",
    "    // Measure D2H transfer\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float d2hTime;\n",
    "    cudaEventElapsedTime(&d2hTime, start, stop);\n",
    "    \n",
    "    float total = h2dTime + kernelTime + d2hTime;\n",
    "    \n",
    "    printf(\"  H2D Transfer:  %8.3f ms (%5.1f%%)\\n\", h2dTime, h2dTime/total*100);\n",
    "    printf(\"  Kernel:        %8.3f ms (%5.1f%%)\\n\", kernelTime, kernelTime/total*100);\n",
    "    printf(\"  D2H Transfer:  %8.3f ms (%5.1f%%)\\n\", d2hTime, d2hTime/total*100);\n",
    "    printf(\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\");\n",
    "    printf(\"  Total:         %8.3f ms\\n\", total);\n",
    "    \n",
    "    if ((h2dTime + d2hTime) > kernelTime * 10) {\n",
    "        printf(\"\\n  âš ï¸  Transfer time dominates!\\n\");\n",
    "        printf(\"     Consider: Keep data on GPU, batch operations!\\n\");\n",
    "    }\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    free(h_a);\n",
    "    free(h_b);\n",
    "    free(h_c);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== CUDA Transfer Overhead Analysis ===\\n\");\n",
    "    \n",
    "    analyzeTransferOverhead(1000000);    // 1M elements\n",
    "    analyzeTransferOverhead(10000000);   // 10M elements\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -o transfer_analysis transfer_analysis.cu && ./transfer_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c5fa8",
   "metadata": {},
   "source": [
    "## 6. Memory Reuse Pattern\n",
    "\n",
    "**Best Practice:** Keep data on GPU for multiple operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile memory_reuse.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Multiple operations - all stay on GPU\n",
    "__global__ void addKernel(float* a, float* b, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = a[idx] + b[idx];\n",
    "}\n",
    "\n",
    "__global__ void mulKernel(float* a, float* b, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = a[idx] * b[idx];\n",
    "}\n",
    "\n",
    "__global__ void subKernel(float* a, float* b, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = a[idx] - b[idx];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Memory Reuse Pattern ===\\n\\n\");\n",
    "    printf(\"Computing: result = ((a + b) * c) - d\\n\");\n",
    "    printf(\"Using memory reuse to minimize transfers!\\n\\n\");\n",
    "    \n",
    "    const int N = 10000000;  // 10M elements\n",
    "    size_t size = N * sizeof(float);\n",
    "    \n",
    "    // Allocate and initialize host data\n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    float *h_d = (float*)malloc(size);\n",
    "    float *h_result = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "        h_c[i] = 3.0f;\n",
    "        h_d[i] = 4.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_a, *d_b, *d_c, *d_d, *d_temp1, *d_temp2, *d_result;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    cudaMalloc(&d_d, size);\n",
    "    cudaMalloc(&d_temp1, size);\n",
    "    cudaMalloc(&d_temp2, size);\n",
    "    cudaMalloc(&d_result, size);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Single H2D transfer - copy all inputs ONCE\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_c, h_c, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_d, h_d, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // All computation stays on GPU!\n",
    "    addKernel<<<blocks, threads>>>(d_a, d_b, d_temp1, N);    // temp1 = a + b\n",
    "    mulKernel<<<blocks, threads>>>(d_temp1, d_c, d_temp2, N); // temp2 = temp1 * c\n",
    "    subKernel<<<blocks, threads>>>(d_temp2, d_d, d_result, N);// result = temp2 - d\n",
    "    \n",
    "    // Single D2H transfer - copy only result\n",
    "    cudaMemcpy(h_result, d_result, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float totalTime;\n",
    "    cudaEventElapsedTime(&totalTime, start, stop);\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < N && correct; i++) {\n",
    "        float expected = ((h_a[i] + h_b[i]) * h_c[i]) - h_d[i];\n",
    "        if (fabsf(h_result[i] - expected) > 1e-5) {\n",
    "            printf(\"Mismatch at %d: %f vs %f\\n\", i, h_result[i], expected);\n",
    "            correct = false;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"Pipeline executed in: %.3f ms\\n\", totalTime);\n",
    "    printf(\"%s Results correct!\\n\", correct ? \"âœ…\" : \"âŒ\");\n",
    "    printf(\"\\nğŸ’¡ Only 5 transfers (4 in + 1 out) instead of 6+!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c); cudaFree(d_d);\n",
    "    cudaFree(d_temp1); cudaFree(d_temp2); cudaFree(d_result);\n",
    "    free(h_a); free(h_b); free(h_c); free(h_d); free(h_result);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbafdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -o memory_reuse memory_reuse.cu && ./memory_reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b9c40",
   "metadata": {},
   "source": [
    "## ğŸ¯ Exercises\n",
    "\n",
    "### Exercise 1: Compare All Memory Types\n",
    "Complete this program to compare explicit, unified, and pinned memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68525860",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile exercise1_compare_memory.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void scaleKernel(float* data, float scale, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= scale;\n",
    "    }\n",
    "}\n",
    "\n",
    "// TODO: Implement these three functions\n",
    "float testExplicitMemory(int n) {\n",
    "    // 1. Allocate with malloc and cudaMalloc\n",
    "    // 2. cudaMemcpy H2D\n",
    "    // 3. Launch kernel\n",
    "    // 4. cudaMemcpy D2H\n",
    "    // 5. Return total time\n",
    "    return 0.0f;\n",
    "}\n",
    "\n",
    "float testUnifiedMemory(int n) {\n",
    "    // 1. Allocate with cudaMallocManaged\n",
    "    // 2. Initialize on CPU\n",
    "    // 3. Launch kernel + synchronize\n",
    "    // 4. Return total time\n",
    "    return 0.0f;\n",
    "}\n",
    "\n",
    "float testPinnedMemory(int n) {\n",
    "    // 1. Allocate with cudaMallocHost\n",
    "    // 2. cudaMemcpy H2D\n",
    "    // 3. Launch kernel\n",
    "    // 4. cudaMemcpy D2H\n",
    "    // 5. Return total time\n",
    "    return 0.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Exercise 1: Compare Memory Types!\\n\");\n",
    "    printf(\"Implement the three test functions above.\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ffc77",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### Memory Management Summary:\n",
    "\n",
    "| Approach | API | Pros | Cons | Use When |\n",
    "|----------|-----|------|------|----------|\n",
    "| **Explicit** | `cudaMalloc` / `cudaMemcpy` | Full control, predictable | Verbose, error-prone | Performance critical |\n",
    "| **Unified** | `cudaMallocManaged` | Simple, automatic | Less control, page faults | Prototyping, complex access |\n",
    "| **Pinned** | `cudaMallocHost` | Faster transfers | Uses locked RAM | Frequent CPU-GPU transfers |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Minimize transfers** - Keep data on GPU for multiple operations\n",
    "2. **Use pinned memory** for frequently transferred data\n",
    "3. **Overlap transfers and compute** using streams (Week 5)\n",
    "4. **Profile first** - Measure where time is actually spent\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Next Up: Day 4 - Error Handling & Debugging\n",
    "- The CUDA_CHECK macro\n",
    "- Error codes and their meanings\n",
    "- Debugging with compute-sanitizer\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”— Resources\n",
    "- [Memory Management - Programming Guide](../../cuda-programming-guide/02-basics/cuda-memory.md)\n",
    "- [Quick Reference](../../notes/cuda-quick-reference.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup generated files\n",
    "!rm -f explicit_memory unified_memory pinned_memory transfer_analysis memory_reuse\n",
    "!rm -f *.cu\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164740f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"\\nâš ï¸  Remember: CUDA C++ code is the PRIMARY learning material!\")\n",
    "print(\"   Python/Numba is provided for quick interactive testing only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5957e72",
   "metadata": {},
   "source": [
    "# Day 3: GPU Memory Fundamentals\n",
    "\n",
    "Memory management is **the most important skill** in CUDA programming. Today you'll learn:\n",
    "- GPU memory hierarchy\n",
    "- Explicit memory management (`cudaMalloc`, `cudaMemcpy`, `cudaFree`)\n",
    "- Unified/Managed memory (`cudaMallocManaged`)\n",
    "- Pinned vs pageable memory for transfers\n",
    "- Performance implications of each approach\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Memory Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        GPU MEMORY HIERARCHY                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Registers (per thread)     â† Fastest (~0 latency)             â”‚\n",
    "â”‚   â”œâ”€â”€ ~255 32-bit registers per thread                          â”‚\n",
    "â”‚   â””â”€â”€ Private to each thread                                    â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Shared Memory (per block)  â† Very fast (~5 cycles)            â”‚\n",
    "â”‚   â”œâ”€â”€ 48-164 KB per SM                                          â”‚\n",
    "â”‚   â””â”€â”€ Shared between threads in same block                      â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   L1 Cache / Texture Cache   â† Fast (~30 cycles)                â”‚\n",
    "â”‚   â””â”€â”€ Automatic caching                                         â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   L2 Cache                   â† Medium (~200 cycles)             â”‚\n",
    "â”‚   â””â”€â”€ Shared across all SMs                                     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Global Memory (VRAM)       â† Slow (~400+ cycles)              â”‚\n",
    "â”‚   â”œâ”€â”€ 4-80 GB (main GPU memory)                                 â”‚\n",
    "â”‚   â””â”€â”€ Accessible by all threads                                 â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Host Memory (System RAM)   â† Very slow (PCIe transfer)        â”‚\n",
    "â”‚   â””â”€â”€ Must be copied to GPU before use                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ffdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"CUDA device:\", cuda.get_current_device().name.decode())\n",
    "\n",
    "# Get memory info\n",
    "ctx = cuda.current_context()\n",
    "free_mem, total_mem = ctx.get_memory_info()\n",
    "print(f\"GPU Memory: {free_mem/1e9:.2f} GB free / {total_mem/1e9:.2f} GB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3276b7",
   "metadata": {},
   "source": [
    "## 2. Explicit Memory Management\n",
    "\n",
    "The traditional CUDA workflow requires **explicit** memory transfers:\n",
    "\n",
    "```\n",
    "1. Allocate memory on GPU (cudaMalloc)\n",
    "2. Copy data from CPU to GPU (cudaMemcpy H2D)\n",
    "3. Launch kernel\n",
    "4. Copy results from GPU to CPU (cudaMemcpy D2H)\n",
    "5. Free GPU memory (cudaFree)\n",
    "```\n",
    "\n",
    "In Numba:\n",
    "- `cuda.to_device(array)` - Allocates and copies to GPU\n",
    "- `cuda.device_array(shape, dtype)` - Allocates on GPU only\n",
    "- `device_array.copy_to_host()` - Copies back to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def saxpy_kernel(a, x, y, out, n):\n",
    "    \"\"\"SAXPY: out = a*x + y (Single-precision A*X Plus Y)\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n:\n",
    "        out[idx] = a * x[idx] + y[idx]\n",
    "\n",
    "def explicit_memory_workflow(n, a=2.0):\n",
    "    \"\"\"Demonstrate explicit memory management\"\"\"\n",
    "    print(f\"\\nğŸ“Š Explicit Memory Workflow (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Step 1: Create data on CPU\n",
    "    x_host = np.random.randn(n).astype(np.float32)\n",
    "    y_host = np.random.randn(n).astype(np.float32)\n",
    "    out_host = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    # Step 2: Allocate and copy to GPU\n",
    "    start = time.perf_counter()\n",
    "    x_device = cuda.to_device(x_host)        # Allocate + copy\n",
    "    y_device = cuda.to_device(y_host)        # Allocate + copy\n",
    "    out_device = cuda.device_array(n, dtype=np.float32)  # Allocate only\n",
    "    h2d_time = time.perf_counter() - start\n",
    "    print(f\"  Hostâ†’Device transfer: {h2d_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Step 3: Launch kernel\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    saxpy_kernel[blocks, threads](a, x_device, y_device, out_device, n)\n",
    "    cuda.synchronize()\n",
    "    kernel_time = time.perf_counter() - start\n",
    "    print(f\"  Kernel execution:      {kernel_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Step 4: Copy result back\n",
    "    start = time.perf_counter()\n",
    "    out_host = out_device.copy_to_host()\n",
    "    d2h_time = time.perf_counter() - start\n",
    "    print(f\"  Deviceâ†’Host transfer:  {d2h_time*1000:.3f} ms\")\n",
    "    \n",
    "    print(f\"  Total time:            {(h2d_time + kernel_time + d2h_time)*1000:.3f} ms\")\n",
    "    \n",
    "    # Verify\n",
    "    expected = a * x_host + y_host\n",
    "    print(f\"  âœ… Correct: {np.allclose(out_host, expected)}\")\n",
    "    \n",
    "    return h2d_time, kernel_time, d2h_time\n",
    "\n",
    "# Run with different sizes\n",
    "for size in [100_000, 1_000_000, 10_000_000]:\n",
    "    explicit_memory_workflow(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e73a4",
   "metadata": {},
   "source": [
    "## 3. Unified Memory (cudaMallocManaged)\n",
    "\n",
    "**Unified Memory** simplifies programming - memory is automatically migrated between CPU and GPU as needed.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     CPU      â”‚â—„â”€â”€â”€â”€ Automatic â”€â”€â”€â”€â–ºâ”‚     GPU      â”‚\n",
    "â”‚              â”‚      Migration      â”‚              â”‚\n",
    "â”‚  ptr[i] = x  â”‚                    â”‚  ptr[i] = y  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        Same pointer works on both!\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simpler code\n",
    "- No explicit transfers needed\n",
    "- Can access more data than GPU memory (oversubscription)\n",
    "\n",
    "**Cons:**\n",
    "- May have higher latency (page faults)\n",
    "- Less control over when transfers happen\n",
    "- Can be slower if not used carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_memory_workflow(n, a=2.0):\n",
    "    \"\"\"Demonstrate unified memory (managed memory)\"\"\"\n",
    "    print(f\"\\nğŸ“Š Unified Memory Workflow (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Allocate managed memory - accessible from both CPU and GPU\n",
    "    x = cuda.managed_array(n, dtype=np.float32)\n",
    "    y = cuda.managed_array(n, dtype=np.float32)\n",
    "    out = cuda.managed_array(n, dtype=np.float32)\n",
    "    \n",
    "    # Initialize on CPU (memory will migrate to GPU when needed)\n",
    "    x[:] = np.random.randn(n).astype(np.float32)\n",
    "    y[:] = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Launch kernel (data migrates automatically)\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    saxpy_kernel[blocks, threads](a, x, y, out, n)\n",
    "    cuda.synchronize()\n",
    "    total_time = time.perf_counter() - start\n",
    "    print(f\"  Kernel + migration time: {total_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Access result on CPU (data migrates back automatically)\n",
    "    expected = a * x + y\n",
    "    print(f\"  âœ… Correct: {np.allclose(out, expected)}\")\n",
    "    \n",
    "    return total_time\n",
    "\n",
    "# Compare with explicit management\n",
    "for size in [100_000, 1_000_000, 10_000_000]:\n",
    "    unified_memory_workflow(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f7062",
   "metadata": {},
   "source": [
    "## 4. Pinned (Page-Locked) Memory\n",
    "\n",
    "Normal CPU memory can be **paged out** to disk by the OS. This causes problems for DMA transfers.\n",
    "\n",
    "**Pinned memory** is locked in physical RAM - no paging allowed.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Pageable Memory (default)     â”‚  Pinned Memory             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  CPU Memory                    â”‚  CPU Memory (locked)       â”‚\n",
    "â”‚      â†“ (copy to staging)       â”‚      â†“ (direct DMA)        â”‚\n",
    "â”‚  Pinned Buffer                 â”‚                            â”‚\n",
    "â”‚      â†“ (DMA to GPU)            â”‚      â†“ (DMA to GPU)        â”‚\n",
    "â”‚  GPU Memory                    â”‚  GPU Memory                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  TWO copies!                   â”‚  ONE copy! (faster)        â”‚\n",
    "â”‚  Can be swapped to disk        â”‚  Always in RAM             â”‚\n",
    "â”‚  Unlimited size                â”‚  Limited by system RAM     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f57309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pinned_vs_pageable(n):\n",
    "    \"\"\"Compare transfer speeds with pinned vs pageable memory\"\"\"\n",
    "    print(f\"\\nğŸ“Š Pinned vs Pageable Memory (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Pageable memory (default NumPy allocation)\n",
    "    pageable = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Pinned memory\n",
    "    pinned = cuda.pinned_array(n, dtype=np.float32)\n",
    "    pinned[:] = pageable  # Copy data to pinned\n",
    "    \n",
    "    # Measure pageable transfer\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        d_pageable = cuda.to_device(pageable)\n",
    "        cuda.synchronize()\n",
    "    pageable_time = (time.perf_counter() - start) / 10\n",
    "    \n",
    "    # Measure pinned transfer\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        d_pinned = cuda.to_device(pinned)\n",
    "        cuda.synchronize()\n",
    "    pinned_time = (time.perf_counter() - start) / 10\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    data_size_gb = n * 4 / 1e9  # float32 = 4 bytes\n",
    "    pageable_bw = data_size_gb / pageable_time\n",
    "    pinned_bw = data_size_gb / pinned_time\n",
    "    \n",
    "    print(f\"  Pageable: {pageable_time*1000:.3f} ms ({pageable_bw:.2f} GB/s)\")\n",
    "    print(f\"  Pinned:   {pinned_time*1000:.3f} ms ({pinned_bw:.2f} GB/s)\")\n",
    "    print(f\"  Speedup:  {pageable_time/pinned_time:.2f}x\")\n",
    "\n",
    "# Compare for different sizes\n",
    "for size in [1_000_000, 10_000_000, 50_000_000]:\n",
    "    compare_pinned_vs_pageable(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3578ad8",
   "metadata": {},
   "source": [
    "## 5. When Transfer Time Dominates\n",
    "\n",
    "For simple operations like vector addition, **memory transfers dominate execution time**.\n",
    "\n",
    "This is why:\n",
    "1. GPU is best for **compute-intensive** operations\n",
    "2. You should **minimize transfers** (keep data on GPU)\n",
    "3. **Batch operations** together before copying back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transfer_overhead(n):\n",
    "    \"\"\"Show how transfer time compares to compute time\"\"\"\n",
    "    print(f\"\\nğŸ“Š Transfer vs Compute Analysis (N = {n:,})\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    a = np.random.randn(n).astype(np.float32)\n",
    "    b = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Measure H2D transfer\n",
    "    start = time.perf_counter()\n",
    "    a_d = cuda.to_device(a)\n",
    "    b_d = cuda.to_device(b)\n",
    "    c_d = cuda.device_array(n, dtype=np.float32)\n",
    "    cuda.synchronize()\n",
    "    h2d_time = time.perf_counter() - start\n",
    "    \n",
    "    # Measure kernel (data already on GPU)\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    @cuda.jit\n",
    "    def add_kernel(a, b, c):\n",
    "        idx = cuda.grid(1)\n",
    "        if idx < c.size:\n",
    "            c[idx] = a[idx] + b[idx]\n",
    "    \n",
    "    # Warmup\n",
    "    add_kernel[blocks, threads](a_d, b_d, c_d)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        add_kernel[blocks, threads](a_d, b_d, c_d)\n",
    "    cuda.synchronize()\n",
    "    kernel_time = (time.perf_counter() - start) / 100\n",
    "    \n",
    "    # Measure D2H transfer\n",
    "    start = time.perf_counter()\n",
    "    c = c_d.copy_to_host()\n",
    "    cuda.synchronize()\n",
    "    d2h_time = time.perf_counter() - start\n",
    "    \n",
    "    total = h2d_time + kernel_time + d2h_time\n",
    "    \n",
    "    print(f\"  H2D Transfer:  {h2d_time*1000:>8.3f} ms ({h2d_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  Kernel:        {kernel_time*1000:>8.3f} ms ({kernel_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  D2H Transfer:  {d2h_time*1000:>8.3f} ms ({d2h_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(f\"  Total:         {total*1000:>8.3f} ms\")\n",
    "    \n",
    "    if (h2d_time + d2h_time) > kernel_time:\n",
    "        print(f\"\\n  âš ï¸  Transfer time ({(h2d_time + d2h_time)*1000:.2f} ms) > Kernel time ({kernel_time*1000:.2f} ms)\")\n",
    "        print(f\"     Consider: Keep data on GPU, batch operations!\")\n",
    "\n",
    "analyze_transfer_overhead(10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87ad6d",
   "metadata": {},
   "source": [
    "## ğŸ¯ Exercises\n",
    "\n",
    "### Exercise 1: Memory Reuse Pattern\n",
    "Implement a pipeline that reuses GPU memory for multiple operations without copying back to CPU between each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62579f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 1: Memory Reuse Pattern\n",
    "# Compute: result = ((a + b) * c) - d\n",
    "# Do this with ONE H2D transfer and ONE D2H transfer\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] + b[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def mul_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] * b[idx]\n",
    "\n",
    "@cuda.jit  \n",
    "def sub_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] - b[idx]\n",
    "\n",
    "def pipeline_inefficient(a, b, c, d):\n",
    "    \"\"\"BAD: Transfers after each operation\"\"\"\n",
    "    # TODO: This is the slow way - see how many transfers happen\n",
    "    pass\n",
    "\n",
    "def pipeline_efficient(a, b, c, d):\n",
    "    \"\"\"GOOD: All operations on GPU, one final transfer\"\"\"\n",
    "    # TODO: Implement efficient version\n",
    "    # 1. Copy all inputs to GPU\n",
    "    # 2. Perform all operations\n",
    "    # 3. Copy only result back\n",
    "    pass\n",
    "\n",
    "# Test your implementations\n",
    "# N = 10_000_000\n",
    "# a = np.random.randn(N).astype(np.float32)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418bb032",
   "metadata": {},
   "source": [
    "### Exercise 2: Memory Bandwidth Calculation\n",
    "Calculate the theoretical vs achieved memory bandwidth for a copy operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 2: Bandwidth Measurement\n",
    "@cuda.jit\n",
    "def copy_kernel(src, dst):\n",
    "    \"\"\"Simple copy kernel - measures GPU memory bandwidth\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < dst.size:\n",
    "        dst[idx] = src[idx]\n",
    "\n",
    "def measure_bandwidth(n):\n",
    "    \"\"\"Measure achieved memory bandwidth\"\"\"\n",
    "    # TODO: \n",
    "    # 1. Create two device arrays\n",
    "    # 2. Time the copy kernel\n",
    "    # 3. Calculate bandwidth: (bytes_read + bytes_written) / time\n",
    "    # 4. Compare to theoretical peak (from device properties)\n",
    "    pass\n",
    "\n",
    "# measure_bandwidth(100_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae5362",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### Memory Management Summary:\n",
    "\n",
    "| Method | When to Use | Pros | Cons |\n",
    "|--------|-------------|------|------|\n",
    "| **Explicit** (`to_device`, `copy_to_host`) | Production code, max control | Fastest, predictable | More code |\n",
    "| **Unified** (`managed_array`) | Prototyping, complex access patterns | Simple code | Page fault overhead |\n",
    "| **Pinned** (`pinned_array`) | High-bandwidth transfers | ~2x transfer speed | Uses system RAM |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Minimize transfers**: Keep data on GPU as long as possible\n",
    "2. **Use pinned memory**: For frequent large transfers\n",
    "3. **Batch operations**: Don't copy back between every operation\n",
    "4. **Profile first**: Measure before optimizing\n",
    "\n",
    "### Memory Bandwidth Rule of Thumb:\n",
    "- PCIe 3.0 x16: ~16 GB/s\n",
    "- PCIe 4.0 x16: ~32 GB/s  \n",
    "- GPU Memory: 200-900 GB/s (much faster!)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Next Up: Day 4 - Error Handling & Debugging\n",
    "- CUDA error codes\n",
    "- Debugging techniques\n",
    "- Common pitfalls\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”— Resources\n",
    "- [CUDA Memory Guide](../../cuda-programming-guide/02-basics/cuda-memory.md)\n",
    "- [Understanding Memory](../../cuda-programming-guide/02-basics/understanding-memory.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
