{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f6a276",
   "metadata": {},
   "source": [
    "# üöÄ Day 4: Error Handling & Debugging\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-01/day-4-error-handling.ipynb)\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba.cuda.cudadrv.driver import CudaAPIError\n",
    "import math\n",
    "import traceback\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Remember: CUDA C++ code is the PRIMARY learning material!\")\n",
    "print(\"   Python/Numba is provided for quick interactive testing only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b16164",
   "metadata": {},
   "source": [
    "# Day 4: Error Handling & Debugging\n",
    "\n",
    "Bugs in CUDA code can be subtle and hard to find. Today you'll learn:\n",
    "- How CUDA errors work (and the `CUDA_CHECK` macro)\n",
    "- Proper error checking patterns in CUDA C++\n",
    "- Common pitfalls and how to avoid them\n",
    "- Debugging with `cuda-memcheck` and `compute-sanitizer`\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Understanding CUDA Errors\n",
    "\n",
    "CUDA operations can fail for many reasons:\n",
    "- Invalid kernel launch configuration\n",
    "- Out of memory\n",
    "- Device not available\n",
    "- Invalid memory access\n",
    "- Race conditions\n",
    "\n",
    "**Key concept:** CUDA operations are often **asynchronous**. Errors may not be reported until later!\n",
    "\n",
    "```\n",
    "kernel<<<grid, block>>>(...);  // Launches, returns immediately\n",
    "// ... other code ...\n",
    "cudaDeviceSynchronize();       // Error might appear HERE!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba.cuda.cudadrv.driver import CudaAPIError\n",
    "import math\n",
    "import traceback\n",
    "\n",
    "print(\"CUDA device:\", cuda.get_current_device().name.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51604cf3",
   "metadata": {},
   "source": [
    "## 2. Common CUDA Errors & How to Trigger Them\n",
    "\n",
    "Let's intentionally cause errors to understand how they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf918e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 1: Invalid Launch Configuration\n",
    "# Max threads per block is 1024, what happens if we exceed it?\n",
    "\n",
    "@cuda.jit\n",
    "def simple_kernel(arr):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < arr.size:\n",
    "        arr[idx] = idx\n",
    "\n",
    "arr = np.zeros(100, dtype=np.float32)\n",
    "arr_d = cuda.to_device(arr)\n",
    "\n",
    "print(\"Attempting to launch with 2048 threads per block...\")\n",
    "print(\"(Max allowed is 1024)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # This will fail - too many threads per block!\n",
    "    simple_kernel[1, 2048](arr_d)\n",
    "    cuda.synchronize()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error caught: {type(e).__name__}\")\n",
    "    print(f\"   Message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1aeb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 2: Out of Memory\n",
    "# Trying to allocate more than available GPU memory\n",
    "\n",
    "ctx = cuda.current_context()\n",
    "free_mem, total_mem = ctx.get_memory_info()\n",
    "print(f\"Free GPU memory: {free_mem / 1e9:.2f} GB\")\n",
    "print(f\"Attempting to allocate: {free_mem * 2 / 1e9:.2f} GB (2x available)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Try to allocate more than available\n",
    "    huge_array = cuda.device_array(int(free_mem * 2), dtype=np.uint8)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error caught: {type(e).__name__}\")\n",
    "    print(f\"   Message: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df5a20",
   "metadata": {},
   "source": [
    "## 3. The Debugging Checklist\n",
    "\n",
    "When your CUDA code doesn't work, check these in order:\n",
    "\n",
    "### üîç Checklist\n",
    "\n",
    "1. **Is CUDA available?**\n",
    "   ```python\n",
    "   cuda.is_available()\n",
    "   ```\n",
    "\n",
    "2. **Are launch parameters valid?**\n",
    "   - `threads_per_block` ‚â§ 1024\n",
    "   - `blocks` > 0\n",
    "   - Grid dimensions within limits\n",
    "\n",
    "3. **Is there enough memory?**\n",
    "   - Check `cuda.current_context().get_memory_info()`\n",
    "\n",
    "4. **Are array sizes correct?**\n",
    "   - Boundary checks in kernel: `if idx < n:`\n",
    "\n",
    "5. **Are data types matching?**\n",
    "   - GPU prefers float32, not float64\n",
    "\n",
    "6. **Did you synchronize?**\n",
    "   - `cuda.synchronize()` before reading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Safe kernel launch wrapper\n",
    "def safe_launch(kernel, grid, block, *args, **kwargs):\n",
    "    \"\"\"Launch kernel with error checking\"\"\"\n",
    "    device = cuda.get_current_device()\n",
    "    \n",
    "    # Validate block size\n",
    "    if isinstance(block, int):\n",
    "        block = (block,)\n",
    "    total_threads = 1\n",
    "    for dim in block:\n",
    "        total_threads *= dim\n",
    "    if total_threads > device.MAX_THREADS_PER_BLOCK:\n",
    "        raise ValueError(f\"Block size {block} = {total_threads} threads exceeds max {device.MAX_THREADS_PER_BLOCK}\")\n",
    "    \n",
    "    # Validate grid size\n",
    "    if isinstance(grid, int):\n",
    "        grid = (grid,)\n",
    "    for i, dim in enumerate(grid):\n",
    "        max_dim = [device.MAX_GRID_DIM_X, device.MAX_GRID_DIM_Y, device.MAX_GRID_DIM_Z][i]\n",
    "        if dim > max_dim:\n",
    "            raise ValueError(f\"Grid dimension {i} = {dim} exceeds max {max_dim}\")\n",
    "    \n",
    "    # Launch\n",
    "    kernel[grid, block](*args, **kwargs)\n",
    "    cuda.synchronize()\n",
    "\n",
    "# Test safe launch\n",
    "print(\"Testing safe_launch helper:\")\n",
    "arr = cuda.device_array(100, dtype=np.float32)\n",
    "\n",
    "try:\n",
    "    safe_launch(simple_kernel, 1, 2048, arr)  # Should fail validation\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Caught before launch: {e}\")\n",
    "\n",
    "safe_launch(simple_kernel, 1, 256, arr)  # Should work\n",
    "print(\"‚úÖ Valid launch succeeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441f135",
   "metadata": {},
   "source": [
    "## 4. Common Pitfalls & Bug Patterns\n",
    "\n",
    "### Pitfall 1: Missing Boundary Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ab03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: No boundary check\n",
    "@cuda.jit\n",
    "def bad_kernel_no_bounds(arr):\n",
    "    idx = cuda.grid(1)\n",
    "    arr[idx] = idx  # üí• Will access out-of-bounds memory!\n",
    "\n",
    "# GOOD: With boundary check\n",
    "@cuda.jit  \n",
    "def good_kernel_with_bounds(arr, n):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n:  # ‚úÖ Always check!\n",
    "        arr[idx] = idx\n",
    "\n",
    "# Demonstrate the difference\n",
    "n = 100\n",
    "arr = cuda.device_array(n, dtype=np.float32)\n",
    "threads = 256  # More threads than elements!\n",
    "blocks = 1\n",
    "\n",
    "print(\"With proper bounds checking:\")\n",
    "good_kernel_with_bounds[blocks, threads](arr, n)\n",
    "cuda.synchronize()\n",
    "print(\"‚úÖ Completed safely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0461cc81",
   "metadata": {},
   "source": [
    "### Pitfall 2: Wrong Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy defaults to float64, but CUDA prefers float32\n",
    "import time\n",
    "\n",
    "@cuda.jit\n",
    "def add_arrays(a, b, c):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < c.size:\n",
    "        c[idx] = a[idx] + b[idx]\n",
    "\n",
    "n = 10_000_000\n",
    "\n",
    "# float64 (default) - slower on most GPUs\n",
    "a64 = np.random.randn(n)  # Default is float64!\n",
    "b64 = np.random.randn(n)\n",
    "c64 = np.zeros(n)\n",
    "\n",
    "# float32 - preferred\n",
    "a32 = np.random.randn(n).astype(np.float32)\n",
    "b32 = np.random.randn(n).astype(np.float32)\n",
    "c32 = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "threads, blocks = 256, math.ceil(n / 256)\n",
    "\n",
    "# Benchmark float64\n",
    "a64_d, b64_d = cuda.to_device(a64), cuda.to_device(b64)\n",
    "c64_d = cuda.device_array(n, dtype=np.float64)\n",
    "add_arrays[blocks, threads](a64_d, b64_d, c64_d)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    add_arrays[blocks, threads](a64_d, b64_d, c64_d)\n",
    "cuda.synchronize()\n",
    "time64 = (time.perf_counter() - start) / 10\n",
    "\n",
    "# Benchmark float32\n",
    "a32_d, b32_d = cuda.to_device(a32), cuda.to_device(b32)\n",
    "c32_d = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    add_arrays[blocks, threads](a32_d, b32_d, c32_d)\n",
    "cuda.synchronize()\n",
    "time32 = (time.perf_counter() - start) / 10\n",
    "\n",
    "print(f\"float64: {time64*1000:.3f} ms\")\n",
    "print(f\"float32: {time32*1000:.3f} ms\")\n",
    "print(f\"Speedup: {time64/time32:.2f}x\")\n",
    "print(\"\\nüí° Tip: Always use .astype(np.float32) unless you need float64 precision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f0cfe",
   "metadata": {},
   "source": [
    "### Pitfall 3: Forgetting to Synchronize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel execution is ASYNCHRONOUS\n",
    "@cuda.jit\n",
    "def slow_kernel(arr):\n",
    "    \"\"\"Simulate slow computation\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < arr.size:\n",
    "        # Busy work\n",
    "        val = 0.0\n",
    "        for i in range(1000):\n",
    "            val += idx * 0.001\n",
    "        arr[idx] = val\n",
    "\n",
    "arr = cuda.device_array(10000, dtype=np.float32)\n",
    "threads, blocks = 256, math.ceil(10000 / 256)\n",
    "\n",
    "# BAD: Timing without synchronization\n",
    "start = time.perf_counter()\n",
    "slow_kernel[blocks, threads](arr)\n",
    "# Missing: cuda.synchronize()\n",
    "bad_time = time.perf_counter() - start\n",
    "print(f\"Without sync: {bad_time*1000:.3f} ms (WRONG! Kernel still running)\")\n",
    "\n",
    "# GOOD: Proper timing with synchronization  \n",
    "start = time.perf_counter()\n",
    "slow_kernel[blocks, threads](arr)\n",
    "cuda.synchronize()  # Wait for kernel to complete\n",
    "good_time = time.perf_counter() - start\n",
    "print(f\"With sync:    {good_time*1000:.3f} ms (Correct)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è The unsynchronized time is {good_time/bad_time:.0f}x too fast!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d257e0",
   "metadata": {},
   "source": [
    "## 5. Debugging with Print Statements\n",
    "\n",
    "In Numba CUDA, you can use `print()` inside kernels for debugging (but use sparingly - it's slow!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def debug_kernel(arr, n):\n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    # Only print from first few threads to avoid output flood\n",
    "    if idx < 3:\n",
    "        print(\"Thread\", idx, \"starting\")\n",
    "    \n",
    "    if idx < n:\n",
    "        arr[idx] = idx * 2\n",
    "        \n",
    "        # Debug: Print values for first few elements\n",
    "        if idx < 3:\n",
    "            print(\"Thread\", idx, \"wrote value\", arr[idx])\n",
    "\n",
    "# Run with small array\n",
    "arr = cuda.device_array(10, dtype=np.float32)\n",
    "debug_kernel[1, 10](arr, 10)\n",
    "cuda.synchronize()\n",
    "\n",
    "print(\"\\nFinal array:\", arr.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3183d",
   "metadata": {},
   "source": [
    "## üéØ Exercises\n",
    "\n",
    "### Exercise 1: Error-Proof Kernel Wrapper\n",
    "Create a robust wrapper function that validates all inputs before launching a kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 1: Complete this error-proof wrapper\n",
    "\n",
    "def launch_kernel_safe(kernel, data, threads_per_block=256):\n",
    "    \"\"\"\n",
    "    Safely launch a kernel with automatic configuration and error checking.\n",
    "    \n",
    "    Args:\n",
    "        kernel: The CUDA kernel function\n",
    "        data: Input array (numpy or device array)\n",
    "        threads_per_block: Threads per block (default 256)\n",
    "    \n",
    "    Returns:\n",
    "        Device array with results\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If inputs are invalid\n",
    "        MemoryError: If not enough GPU memory\n",
    "    \"\"\"\n",
    "    # TODO: Implement the following checks:\n",
    "    # 1. Verify CUDA is available\n",
    "    # 2. Check data is not empty\n",
    "    # 3. Validate threads_per_block (1-1024)\n",
    "    # 4. Check sufficient GPU memory\n",
    "    # 5. Launch kernel with proper grid configuration\n",
    "    # 6. Synchronize and check for errors\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762d8c3",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "### Error Handling Best Practices:\n",
    "\n",
    "1. **Always synchronize** before reading results or timing\n",
    "   ```python\n",
    "   kernel[grid, block](...)\n",
    "   cuda.synchronize()  # Wait for completion\n",
    "   result = output.copy_to_host()\n",
    "   ```\n",
    "\n",
    "2. **Validate launch configuration**\n",
    "   - threads_per_block ‚â§ 1024\n",
    "   - Check grid dimensions against device limits\n",
    "\n",
    "3. **Always include boundary checks**\n",
    "   ```python\n",
    "   if idx < n:\n",
    "       arr[idx] = ...\n",
    "   ```\n",
    "\n",
    "4. **Use try/except for error handling**\n",
    "   ```python\n",
    "   try:\n",
    "       kernel[grid, block](...)\n",
    "   except CudaAPIError as e:\n",
    "       print(f\"CUDA Error: {e}\")\n",
    "   ```\n",
    "\n",
    "5. **Prefer float32** unless you need float64 precision\n",
    "\n",
    "6. **Debug strategically**\n",
    "   - Use print() sparingly (only first few threads)\n",
    "   - Use small test cases first\n",
    "   - Verify CPU results before GPU\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Steps\n",
    "You've completed Week 1! Before moving on:\n",
    "1. Complete the checkpoint quiz\n",
    "2. Finish all exercises in each notebook\n",
    "3. Make sure you can run all code without errors\n",
    "\n",
    "### üîó Resources\n",
    "- [Error Handling Guide](../../cuda-programming-guide/02-basics/nvcc.md)\n",
    "- [Debugging Documentation](../../cuda-programming-guide/04-special-topics/error-log-management.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
