{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f6a276",
   "metadata": {},
   "source": [
    "# üöÄ Day 4: Error Handling & Debugging\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-01/day-4-error-handling.ipynb)\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CUDA is available\n",
    "!nvcc --version\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f83b0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 4: Error Handling & Debugging\n",
    "\n",
    "Bugs in CUDA code can be subtle and hard to find. Today you'll learn:\n",
    "- The essential `CUDA_CHECK` macro\n",
    "- How CUDA errors work (synchronous vs asynchronous)\n",
    "- Common pitfalls and how to avoid them\n",
    "- Debugging with `compute-sanitizer`\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Understanding CUDA Errors\n",
    "\n",
    "CUDA operations can fail for many reasons:\n",
    "- Invalid kernel launch configuration\n",
    "- Out of memory\n",
    "- Invalid memory access\n",
    "- Device not available\n",
    "\n",
    "**Key concept:** Many CUDA operations are **asynchronous**. Errors may not appear until later!\n",
    "\n",
    "```cpp\n",
    "kernel<<<grid, block>>>(...)  // Launches, returns immediately\n",
    "// ... other code ...\n",
    "cudaDeviceSynchronize();       // Error might appear HERE!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af43c99",
   "metadata": {},
   "source": [
    "## 2. The Essential CUDA_CHECK Macro\n",
    "\n",
    "**Every CUDA call should be wrapped in error checking!**\n",
    "\n",
    "This macro is used throughout production CUDA code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_check.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// ============================================================\n",
    "// THE ESSENTIAL CUDA_CHECK MACRO - Use this in EVERY project!\n",
    "// ============================================================\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t error = call; \\\n",
    "        if (error != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s:%d, \", __FILE__, __LINE__); \\\n",
    "            fprintf(stderr, \"code: %d, reason: %s\\n\", error, \\\n",
    "                    cudaGetErrorString(error)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Additional macro to check kernel launch errors\n",
    "#define CUDA_CHECK_KERNEL() \\\n",
    "    do { \\\n",
    "        cudaError_t error = cudaGetLastError(); \\\n",
    "        if (error != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Kernel Error: %s:%d, \", __FILE__, __LINE__); \\\n",
    "            fprintf(stderr, \"code: %d, reason: %s\\n\", error, \\\n",
    "                    cudaGetErrorString(error)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "        error = cudaDeviceSynchronize(); \\\n",
    "        if (error != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Sync Error: %s:%d, \", __FILE__, __LINE__); \\\n",
    "            fprintf(stderr, \"code: %d, reason: %s\\n\", error, \\\n",
    "                    cudaGetErrorString(error)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// ============================================================\n",
    "\n",
    "__global__ void simpleKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== CUDA Error Checking Demo ===\\n\\n\");\n",
    "    \n",
    "    const int N = 1000;\n",
    "    size_t size = N * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float* h_data = (float*)malloc(size);\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_data[i] = i;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory WITH error checking\n",
    "    float* d_data;\n",
    "    CUDA_CHECK(cudaMalloc(&d_data, size));\n",
    "    printf(\"‚úÖ cudaMalloc succeeded\\n\");\n",
    "    \n",
    "    // Copy to device WITH error checking\n",
    "    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n",
    "    printf(\"‚úÖ cudaMemcpy H2D succeeded\\n\");\n",
    "    \n",
    "    // Launch kernel\n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    simpleKernel<<<blocks, threads>>>(d_data, N);\n",
    "    CUDA_CHECK_KERNEL();\n",
    "    printf(\"‚úÖ Kernel execution succeeded\\n\");\n",
    "    \n",
    "    // Copy back WITH error checking\n",
    "    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n",
    "    printf(\"‚úÖ cudaMemcpy D2H succeeded\\n\");\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        if (h_data[i] != i * 2.0f) {\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    printf(\"\\n%s Results correct!\\n\", correct ? \"‚úÖ\" : \"‚ùå\");\n",
    "    \n",
    "    // Cleanup WITH error checking\n",
    "    CUDA_CHECK(cudaFree(d_data));\n",
    "    free(h_data);\n",
    "    printf(\"‚úÖ Cleanup succeeded\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o cuda_check cuda_check.cu && ./cuda_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdedb34",
   "metadata": {},
   "source": [
    "## 3. Common CUDA Errors & How to Trigger Them\n",
    "\n",
    "Let's intentionally cause errors to understand how they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile common_errors.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t error = call; \\\n",
    "        if (error != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"‚ùå CUDA Error: %s\\n\", cudaGetErrorString(error)); \\\n",
    "            fprintf(stderr, \"   at %s:%d\\n\", __FILE__, __LINE__); \\\n",
    "            return; \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void simpleKernel(float* arr, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) arr[idx] = idx;\n",
    "}\n",
    "\n",
    "// Error 1: Invalid launch configuration (too many threads)\n",
    "void testInvalidLaunchConfig() {\n",
    "    printf(\"\\n=== Test 1: Invalid Launch Configuration ===\\n\");\n",
    "    printf(\"Attempting to launch with 2048 threads per block...\\n\");\n",
    "    printf(\"(Max allowed is 1024)\\n\\n\");\n",
    "    \n",
    "    float* d_arr;\n",
    "    CUDA_CHECK(cudaMalloc(&d_arr, 100 * sizeof(float)));\n",
    "    \n",
    "    // This will fail - too many threads per block!\n",
    "    simpleKernel<<<1, 2048>>>(d_arr, 100);\n",
    "    \n",
    "    cudaError_t error = cudaGetLastError();\n",
    "    if (error != cudaSuccess) {\n",
    "        printf(\"‚ùå Launch failed: %s\\n\", cudaGetErrorString(error));\n",
    "    } else {\n",
    "        printf(\"‚úÖ Launch succeeded (unexpected!)\\n\");\n",
    "    }\n",
    "    \n",
    "    // Reset error state for next test\n",
    "    cudaGetLastError();\n",
    "    cudaFree(d_arr);\n",
    "}\n",
    "\n",
    "// Error 2: Out of memory\n",
    "void testOutOfMemory() {\n",
    "    printf(\"\\n=== Test 2: Out of Memory ===\\n\");\n",
    "    \n",
    "    size_t freeBytes, totalBytes;\n",
    "    cudaMemGetInfo(&freeBytes, &totalBytes);\n",
    "    printf(\"Free GPU memory: %.1f GB\\n\", freeBytes / 1e9);\n",
    "    printf(\"Attempting to allocate: %.1f GB (2x available)\\n\\n\", freeBytes * 2 / 1e9);\n",
    "    \n",
    "    float* hugePtr;\n",
    "    cudaError_t error = cudaMalloc(&hugePtr, freeBytes * 2);\n",
    "    \n",
    "    if (error != cudaSuccess) {\n",
    "        printf(\"‚ùå Allocation failed: %s\\n\", cudaGetErrorString(error));\n",
    "    } else {\n",
    "        printf(\"‚úÖ Allocation succeeded (unexpected!)\\n\");\n",
    "        cudaFree(hugePtr);\n",
    "    }\n",
    "    \n",
    "    // Reset error state\n",
    "    cudaGetLastError();\n",
    "}\n",
    "\n",
    "// Error 3: Invalid memory access\n",
    "__global__ void outOfBoundsKernel(float* arr, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // Intentionally access out of bounds!\n",
    "    arr[idx + 1000000] = 42.0f;  // BAD!\n",
    "}\n",
    "\n",
    "void testInvalidMemoryAccess() {\n",
    "    printf(\"\\n=== Test 3: Invalid Memory Access ===\\n\");\n",
    "    printf(\"Launching kernel that accesses out-of-bounds memory...\\n\\n\");\n",
    "    \n",
    "    float* d_arr;\n",
    "    CUDA_CHECK(cudaMalloc(&d_arr, 100 * sizeof(float)));\n",
    "    \n",
    "    outOfBoundsKernel<<<1, 32>>>(d_arr, 100);\n",
    "    \n",
    "    // Must synchronize to catch the error!\n",
    "    cudaError_t error = cudaDeviceSynchronize();\n",
    "    \n",
    "    if (error != cudaSuccess) {\n",
    "        printf(\"‚ùå Execution failed: %s\\n\", cudaGetErrorString(error));\n",
    "    } else {\n",
    "        printf(\"‚úÖ Execution succeeded (error not detected without sanitizer)\\n\");\n",
    "    }\n",
    "    \n",
    "    // Reset error state\n",
    "    cudaDeviceReset();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Common CUDA Errors Demo ===\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Max threads per block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "    \n",
    "    testInvalidLaunchConfig();\n",
    "    testOutOfMemory();\n",
    "    testInvalidMemoryAccess();\n",
    "    \n",
    "    printf(\"\\n=== Summary ===\\n\");\n",
    "    printf(\"1. Always check cudaGetLastError() after kernel launches\\n\");\n",
    "    printf(\"2. Use cudaDeviceSynchronize() to catch async errors\\n\");\n",
    "    printf(\"3. Use compute-sanitizer for memory errors\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4454ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o common_errors common_errors.cu && ./common_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bd80a",
   "metadata": {},
   "source": [
    "## 4. The Debugging Checklist\n",
    "\n",
    "When your CUDA code doesn't work, check these in order:\n",
    "\n",
    "### üîç Checklist\n",
    "\n",
    "1. **Is CUDA available?**\n",
    "   ```cpp\n",
    "   int deviceCount;\n",
    "   cudaGetDeviceCount(&deviceCount);\n",
    "   ```\n",
    "\n",
    "2. **Are launch parameters valid?**\n",
    "   - `threads_per_block` ‚â§ 1024\n",
    "   - `blocks` > 0\n",
    "   - Grid dimensions within limits\n",
    "\n",
    "3. **Is there enough memory?**\n",
    "   ```cpp\n",
    "   size_t freeBytes, totalBytes;\n",
    "   cudaMemGetInfo(&freeBytes, &totalBytes);\n",
    "   ```\n",
    "\n",
    "4. **Are array sizes correct?**\n",
    "   - Boundary checks in kernel: `if (idx < n)`\n",
    "\n",
    "5. **Are data types matching?**\n",
    "   - GPU prefers float32 over float64\n",
    "\n",
    "6. **Did you synchronize?**\n",
    "   - `cudaDeviceSynchronize()` before reading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile debug_checklist.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t error = call; \\\n",
    "        if (error != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"‚ùå %s\\n\", cudaGetErrorString(error)); \\\n",
    "            return false; \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Validation function - run this before any CUDA code\n",
    "bool validateCudaSetup() {\n",
    "    printf(\"=== CUDA Setup Validation ===\\n\\n\");\n",
    "    \n",
    "    // Check 1: Device availability\n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    if (deviceCount == 0) {\n",
    "        printf(\"‚ùå No CUDA devices found!\\n\");\n",
    "        return false;\n",
    "    }\n",
    "    printf(\"‚úÖ Found %d CUDA device(s)\\n\", deviceCount);\n",
    "    \n",
    "    // Check 2: Device properties\n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
    "    printf(\"‚úÖ Device: %s\\n\", prop.name);\n",
    "    printf(\"   - Compute capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "    printf(\"   - Max threads/block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "    printf(\"   - Max grid dims: [%d, %d, %d]\\n\",\n",
    "           prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]);\n",
    "    printf(\"   - Total memory: %.1f GB\\n\", prop.totalGlobalMem / 1e9);\n",
    "    \n",
    "    // Check 3: Available memory\n",
    "    size_t freeBytes, totalBytes;\n",
    "    CUDA_CHECK(cudaMemGetInfo(&freeBytes, &totalBytes));\n",
    "    printf(\"‚úÖ Memory: %.1f GB free / %.1f GB total\\n\",\n",
    "           freeBytes / 1e9, totalBytes / 1e9);\n",
    "    \n",
    "    // Check 4: Test allocation\n",
    "    float* testPtr;\n",
    "    CUDA_CHECK(cudaMalloc(&testPtr, 1024));\n",
    "    CUDA_CHECK(cudaFree(testPtr));\n",
    "    printf(\"‚úÖ Test allocation succeeded\\n\");\n",
    "    \n",
    "    printf(\"\\n=== All checks passed! ===\\n\");\n",
    "    return true;\n",
    "}\n",
    "\n",
    "// Safe kernel launcher with validation\n",
    "template<typename KernelFunc>\n",
    "bool safeLaunch(KernelFunc kernel, dim3 grid, dim3 block,\n",
    "                const char* kernelName) {\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    \n",
    "    // Validate block size\n",
    "    int totalThreads = block.x * block.y * block.z;\n",
    "    if (totalThreads > prop.maxThreadsPerBlock) {\n",
    "        printf(\"‚ùå Block size %d exceeds max %d\\n\",\n",
    "               totalThreads, prop.maxThreadsPerBlock);\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // Validate grid size\n",
    "    if (grid.x > (unsigned)prop.maxGridSize[0] ||\n",
    "        grid.y > (unsigned)prop.maxGridSize[1] ||\n",
    "        grid.z > (unsigned)prop.maxGridSize[2]) {\n",
    "        printf(\"‚ùå Grid size exceeds device limits\\n\");\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    printf(\"‚úÖ %s: grid(%d,%d,%d) block(%d,%d,%d) validated\\n\",\n",
    "           kernelName, grid.x, grid.y, grid.z, block.x, block.y, block.z);\n",
    "    return true;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    if (!validateCudaSetup()) {\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n=== Launch Validation Examples ===\\n\\n\");\n",
    "    \n",
    "    // Valid configuration\n",
    "    safeLaunch(nullptr, dim3(256), dim3(256), \"validKernel\");\n",
    "    \n",
    "    // Invalid: too many threads\n",
    "    safeLaunch(nullptr, dim3(1), dim3(2048), \"invalidKernel\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf85733",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o debug_checklist debug_checklist.cu && ./debug_checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd7019",
   "metadata": {},
   "source": [
    "## 5. Common Pitfalls & Bug Patterns\n",
    "\n",
    "### Pitfall 1: Missing Boundary Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d77955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pitfall_bounds.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// BAD: No boundary check - will access invalid memory!\n",
    "__global__ void badKernelNoBounds(float* arr) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    arr[idx] = idx;  // üí• May access out-of-bounds!\n",
    "}\n",
    "\n",
    "// GOOD: With boundary check - safe!\n",
    "__global__ void goodKernelWithBounds(float* arr, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {  // ‚úÖ Always check!\n",
    "        arr[idx] = idx;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Boundary Check Demo ===\\n\\n\");\n",
    "    \n",
    "    const int N = 100;\n",
    "    const int THREADS = 256;  // More threads than elements!\n",
    "    const int BLOCKS = 1;\n",
    "    \n",
    "    printf(\"Array size: %d elements\\n\", N);\n",
    "    printf(\"Threads launched: %d (more than elements!)\\n\\n\", THREADS * BLOCKS);\n",
    "    \n",
    "    float* d_arr;\n",
    "    cudaMalloc(&d_arr, N * sizeof(float));\n",
    "    \n",
    "    // Safe version with bounds check\n",
    "    goodKernelWithBounds<<<BLOCKS, THREADS>>>(d_arr, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaError_t error = cudaGetLastError();\n",
    "    printf(\"%s Kernel with bounds check\\n\", \n",
    "           error == cudaSuccess ? \"‚úÖ\" : \"‚ùå\");\n",
    "    \n",
    "    cudaFree(d_arr);\n",
    "    \n",
    "    printf(\"\\nüí° Always use: if (idx < n) before accessing array[idx]\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c29a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o pitfall_bounds pitfall_bounds.cu && ./pitfall_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a70dd",
   "metadata": {},
   "source": [
    "### Pitfall 2: Forgetting to Synchronize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pitfall_sync.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <time.h>\n",
    "\n",
    "__global__ void slowKernel(float* arr, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Simulate slow computation\n",
    "        float val = 0.0f;\n",
    "        for (int i = 0; i < 10000; i++) {\n",
    "            val += idx * 0.0001f;\n",
    "        }\n",
    "        arr[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Synchronization Demo ===\\n\\n\");\n",
    "    \n",
    "    const int N = 10000;\n",
    "    float* d_arr;\n",
    "    cudaMalloc(&d_arr, N * sizeof(float));\n",
    "    \n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    \n",
    "    // BAD: Timing without synchronization\n",
    "    clock_t start1 = clock();\n",
    "    slowKernel<<<blocks, threads>>>(d_arr, N);\n",
    "    // Missing: cudaDeviceSynchronize();\n",
    "    clock_t end1 = clock();\n",
    "    float badTime = (float)(end1 - start1) / CLOCKS_PER_SEC * 1000;\n",
    "    \n",
    "    // Wait for kernel to complete before next measurement\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // GOOD: Timing with synchronization\n",
    "    clock_t start2 = clock();\n",
    "    slowKernel<<<blocks, threads>>>(d_arr, N);\n",
    "    cudaDeviceSynchronize();  // Wait for completion!\n",
    "    clock_t end2 = clock();\n",
    "    float goodTime = (float)(end2 - start2) / CLOCKS_PER_SEC * 1000;\n",
    "    \n",
    "    printf(\"Without sync: %.3f ms (WRONG - kernel still running!)\\n\", badTime);\n",
    "    printf(\"With sync:    %.3f ms (Correct)\\n\", goodTime);\n",
    "    printf(\"\\n‚ö†Ô∏è  Unsynchronized time is %.0fx too fast!\\n\", goodTime / badTime);\n",
    "    \n",
    "    printf(\"\\nüí° Always call cudaDeviceSynchronize() before:\\n\");\n",
    "    printf(\"   - Reading results from device memory\\n\");\n",
    "    printf(\"   - Timing kernel execution\\n\");\n",
    "    printf(\"   - Error checking for kernel issues\\n\");\n",
    "    \n",
    "    cudaFree(d_arr);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -o pitfall_sync pitfall_sync.cu && ./pitfall_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3f2c8",
   "metadata": {},
   "source": [
    "### Pitfall 3: Wrong Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1550d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pitfall_dtype.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Template kernel works with any floating point type\n",
    "template<typename T>\n",
    "__global__ void addArrays(T* a, T* b, T* c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "template<typename T>\n",
    "float benchmarkAddition(int n, const char* typeName) {\n",
    "    size_t size = n * sizeof(T);\n",
    "    \n",
    "    // Allocate and initialize\n",
    "    T *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    int threads = 256;\n",
    "    int blocks = (n + threads - 1) / threads;\n",
    "    \n",
    "    // Warmup\n",
    "    addArrays<<<blocks, threads>>>(d_a, d_b, d_c, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Time multiple runs\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        addArrays<<<blocks, threads>>>(d_a, d_b, d_c, n);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    ms /= 100;\n",
    "    \n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    printf(\"%s: %.3f ms\\n\", typeName, ms);\n",
    "    return ms;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Data Type Performance ===\\n\\n\");\n",
    "    \n",
    "    const int N = 10000000;\n",
    "    printf(\"Array size: %d elements\\n\\n\", N);\n",
    "    \n",
    "    float timeF32 = benchmarkAddition<float>(N, \"float32\");\n",
    "    float timeF64 = benchmarkAddition<double>(N, \"float64\");\n",
    "    \n",
    "    printf(\"\\nSpeedup (float32 vs float64): %.2fx\\n\", timeF64 / timeF32);\n",
    "    \n",
    "    printf(\"\\nüí° Use float32 unless you need float64 precision!\\n\");\n",
    "    printf(\"   Most consumer GPUs have limited float64 performance.\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -o pitfall_dtype pitfall_dtype.cu && ./pitfall_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747a8cb",
   "metadata": {},
   "source": [
    "## 6. Debugging with compute-sanitizer\n",
    "\n",
    "NVIDIA's `compute-sanitizer` is like Valgrind for CUDA. It catches:\n",
    "- Out-of-bounds memory access\n",
    "- Race conditions\n",
    "- Memory leaks\n",
    "- Uninitialized memory access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sanitizer_test.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// This kernel has a bug - out of bounds access\n",
    "__global__ void buggyKernel(float* arr, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // BUG: Missing bounds check!\n",
    "    arr[idx] = 42.0f;  // Will access beyond allocated memory\n",
    "}\n",
    "\n",
    "// This kernel is correct\n",
    "__global__ void correctKernel(float* arr, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {  // Proper bounds check\n",
    "        arr[idx] = 42.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    printf(\"=== compute-sanitizer Test ===\\n\\n\");\n",
    "    \n",
    "    const int N = 100;\n",
    "    float* d_arr;\n",
    "    cudaMalloc(&d_arr, N * sizeof(float));\n",
    "    \n",
    "    // Run with more threads than elements to trigger the bug\n",
    "    int threads = 256;\n",
    "    int blocks = 1;\n",
    "    \n",
    "    if (argc > 1 && strcmp(argv[1], \"--buggy\") == 0) {\n",
    "        printf(\"Running BUGGY kernel (will be caught by sanitizer)...\\n\");\n",
    "        buggyKernel<<<blocks, threads>>>(d_arr, N);\n",
    "    } else {\n",
    "        printf(\"Running CORRECT kernel...\\n\");\n",
    "        correctKernel<<<blocks, threads>>>(d_arr, N);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaError_t error = cudaGetLastError();\n",
    "    if (error != cudaSuccess) {\n",
    "        printf(\"‚ùå CUDA Error: %s\\n\", cudaGetErrorString(error));\n",
    "    } else {\n",
    "        printf(\"‚úÖ No CUDA errors detected by runtime\\n\");\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_arr);\n",
    "    \n",
    "    printf(\"\\nRun with compute-sanitizer to detect memory errors:\\n\");\n",
    "    printf(\"  compute-sanitizer --tool memcheck ./sanitizer_test --buggy\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -g -G -o sanitizer_test sanitizer_test.cu && ./sanitizer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to run with compute-sanitizer (may not be available on all systems)\n",
    "!which compute-sanitizer && compute-sanitizer --tool memcheck ./sanitizer_test --buggy || echo \"compute-sanitizer not available in this environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9fb21",
   "metadata": {},
   "source": [
    "## üéØ Exercises\n",
    "\n",
    "### Exercise 1: Create a Safe Kernel Launcher\n",
    "Complete this robust wrapper that validates all inputs before launching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f06f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile exercise1_safe_launch.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            return false; \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void scaleKernel(float* data, float scale, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] *= scale;\n",
    "}\n",
    "\n",
    "// TODO: Implement this safe launcher\n",
    "bool safeLaunchScaleKernel(float* d_data, float scale, int n, int threadsPerBlock) {\n",
    "    // TODO: Add these checks:\n",
    "    // 1. Verify d_data is not NULL\n",
    "    // 2. Verify n > 0\n",
    "    // 3. Verify threadsPerBlock is 1-1024\n",
    "    // 4. Calculate proper grid size\n",
    "    // 5. Check available memory\n",
    "    // 6. Launch kernel\n",
    "    // 7. Check for launch errors\n",
    "    // 8. Synchronize and check for execution errors\n",
    "    \n",
    "    return true;  // Return false on any error\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Exercise 1: Implement safeLaunchScaleKernel!\\n\");\n",
    "    printf(\"Add validation for all inputs and proper error checking.\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933134a",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "### Error Handling Best Practices:\n",
    "\n",
    "1. **Use CUDA_CHECK macro** for every API call\n",
    "   ```cpp\n",
    "   CUDA_CHECK(cudaMalloc(&ptr, size));\n",
    "   ```\n",
    "\n",
    "2. **Check kernel errors immediately**\n",
    "   ```cpp\n",
    "   kernel<<<grid, block>>>(...);\n",
    "   CUDA_CHECK(cudaGetLastError());  // Launch errors\n",
    "   CUDA_CHECK(cudaDeviceSynchronize());  // Execution errors\n",
    "   ```\n",
    "\n",
    "3. **Always include boundary checks**\n",
    "   ```cpp\n",
    "   if (idx < n) {\n",
    "       array[idx] = ...;\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **Validate launch configuration**\n",
    "   - threads_per_block ‚â§ 1024\n",
    "   - Check grid dimensions against device limits\n",
    "\n",
    "5. **Use compute-sanitizer for debugging**\n",
    "   ```bash\n",
    "   compute-sanitizer --tool memcheck ./myprogram\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Week 1 Complete!\n",
    "You've learned:\n",
    "- Day 1: GPU basics and your first kernel\n",
    "- Day 2: Thread indexing and grid-stride loops\n",
    "- Day 3: Memory management fundamentals\n",
    "- Day 4: Error handling and debugging\n",
    "\n",
    "**Next:** Week 2 - Shared Memory & Performance Basics\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Resources\n",
    "- [CUDA Error Handling](../../cuda-programming-guide/02-basics/intro-to-cuda-cpp.md)\n",
    "- [Quick Reference](../../notes/cuda-quick-reference.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup generated files\n",
    "!rm -f cuda_check common_errors debug_checklist pitfall_bounds pitfall_sync pitfall_dtype sanitizer_test\n",
    "!rm -f *.cu\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba.cuda.cudadrv.driver import CudaAPIError\n",
    "import math\n",
    "import traceback\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Remember: CUDA C++ code is the PRIMARY learning material!\")\n",
    "print(\"   Python/Numba is provided for quick interactive testing only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b16164",
   "metadata": {},
   "source": [
    "# Day 4: Error Handling & Debugging\n",
    "\n",
    "Bugs in CUDA code can be subtle and hard to find. Today you'll learn:\n",
    "- How CUDA errors work (and the `CUDA_CHECK` macro)\n",
    "- Proper error checking patterns in CUDA C++\n",
    "- Common pitfalls and how to avoid them\n",
    "- Debugging with `cuda-memcheck` and `compute-sanitizer`\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Understanding CUDA Errors\n",
    "\n",
    "CUDA operations can fail for many reasons:\n",
    "- Invalid kernel launch configuration\n",
    "- Out of memory\n",
    "- Device not available\n",
    "- Invalid memory access\n",
    "- Race conditions\n",
    "\n",
    "**Key concept:** CUDA operations are often **asynchronous**. Errors may not be reported until later!\n",
    "\n",
    "```\n",
    "kernel<<<grid, block>>>(...);  // Launches, returns immediately\n",
    "// ... other code ...\n",
    "cudaDeviceSynchronize();       // Error might appear HERE!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba.cuda.cudadrv.driver import CudaAPIError\n",
    "import math\n",
    "import traceback\n",
    "\n",
    "print(\"CUDA device:\", cuda.get_current_device().name.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51604cf3",
   "metadata": {},
   "source": [
    "## 2. Common CUDA Errors & How to Trigger Them\n",
    "\n",
    "Let's intentionally cause errors to understand how they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf918e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 1: Invalid Launch Configuration\n",
    "# Max threads per block is 1024, what happens if we exceed it?\n",
    "\n",
    "@cuda.jit\n",
    "def simple_kernel(arr):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < arr.size:\n",
    "        arr[idx] = idx\n",
    "\n",
    "arr = np.zeros(100, dtype=np.float32)\n",
    "arr_d = cuda.to_device(arr)\n",
    "\n",
    "print(\"Attempting to launch with 2048 threads per block...\")\n",
    "print(\"(Max allowed is 1024)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # This will fail - too many threads per block!\n",
    "    simple_kernel[1, 2048](arr_d)\n",
    "    cuda.synchronize()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error caught: {type(e).__name__}\")\n",
    "    print(f\"   Message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1aeb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 2: Out of Memory\n",
    "# Trying to allocate more than available GPU memory\n",
    "\n",
    "ctx = cuda.current_context()\n",
    "free_mem, total_mem = ctx.get_memory_info()\n",
    "print(f\"Free GPU memory: {free_mem / 1e9:.2f} GB\")\n",
    "print(f\"Attempting to allocate: {free_mem * 2 / 1e9:.2f} GB (2x available)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Try to allocate more than available\n",
    "    huge_array = cuda.device_array(int(free_mem * 2), dtype=np.uint8)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error caught: {type(e).__name__}\")\n",
    "    print(f\"   Message: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df5a20",
   "metadata": {},
   "source": [
    "## 3. The Debugging Checklist\n",
    "\n",
    "When your CUDA code doesn't work, check these in order:\n",
    "\n",
    "### üîç Checklist\n",
    "\n",
    "1. **Is CUDA available?**\n",
    "   ```python\n",
    "   cuda.is_available()\n",
    "   ```\n",
    "\n",
    "2. **Are launch parameters valid?**\n",
    "   - `threads_per_block` ‚â§ 1024\n",
    "   - `blocks` > 0\n",
    "   - Grid dimensions within limits\n",
    "\n",
    "3. **Is there enough memory?**\n",
    "   - Check `cuda.current_context().get_memory_info()`\n",
    "\n",
    "4. **Are array sizes correct?**\n",
    "   - Boundary checks in kernel: `if idx < n:`\n",
    "\n",
    "5. **Are data types matching?**\n",
    "   - GPU prefers float32, not float64\n",
    "\n",
    "6. **Did you synchronize?**\n",
    "   - `cuda.synchronize()` before reading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Safe kernel launch wrapper\n",
    "def safe_launch(kernel, grid, block, *args, **kwargs):\n",
    "    \"\"\"Launch kernel with error checking\"\"\"\n",
    "    device = cuda.get_current_device()\n",
    "    \n",
    "    # Validate block size\n",
    "    if isinstance(block, int):\n",
    "        block = (block,)\n",
    "    total_threads = 1\n",
    "    for dim in block:\n",
    "        total_threads *= dim\n",
    "    if total_threads > device.MAX_THREADS_PER_BLOCK:\n",
    "        raise ValueError(f\"Block size {block} = {total_threads} threads exceeds max {device.MAX_THREADS_PER_BLOCK}\")\n",
    "    \n",
    "    # Validate grid size\n",
    "    if isinstance(grid, int):\n",
    "        grid = (grid,)\n",
    "    for i, dim in enumerate(grid):\n",
    "        max_dim = [device.MAX_GRID_DIM_X, device.MAX_GRID_DIM_Y, device.MAX_GRID_DIM_Z][i]\n",
    "        if dim > max_dim:\n",
    "            raise ValueError(f\"Grid dimension {i} = {dim} exceeds max {max_dim}\")\n",
    "    \n",
    "    # Launch\n",
    "    kernel[grid, block](*args, **kwargs)\n",
    "    cuda.synchronize()\n",
    "\n",
    "# Test safe launch\n",
    "print(\"Testing safe_launch helper:\")\n",
    "arr = cuda.device_array(100, dtype=np.float32)\n",
    "\n",
    "try:\n",
    "    safe_launch(simple_kernel, 1, 2048, arr)  # Should fail validation\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Caught before launch: {e}\")\n",
    "\n",
    "safe_launch(simple_kernel, 1, 256, arr)  # Should work\n",
    "print(\"‚úÖ Valid launch succeeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441f135",
   "metadata": {},
   "source": [
    "## 4. Common Pitfalls & Bug Patterns\n",
    "\n",
    "### Pitfall 1: Missing Boundary Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ab03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: No boundary check\n",
    "@cuda.jit\n",
    "def bad_kernel_no_bounds(arr):\n",
    "    idx = cuda.grid(1)\n",
    "    arr[idx] = idx  # üí• Will access out-of-bounds memory!\n",
    "\n",
    "# GOOD: With boundary check\n",
    "@cuda.jit  \n",
    "def good_kernel_with_bounds(arr, n):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n:  # ‚úÖ Always check!\n",
    "        arr[idx] = idx\n",
    "\n",
    "# Demonstrate the difference\n",
    "n = 100\n",
    "arr = cuda.device_array(n, dtype=np.float32)\n",
    "threads = 256  # More threads than elements!\n",
    "blocks = 1\n",
    "\n",
    "print(\"With proper bounds checking:\")\n",
    "good_kernel_with_bounds[blocks, threads](arr, n)\n",
    "cuda.synchronize()\n",
    "print(\"‚úÖ Completed safely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0461cc81",
   "metadata": {},
   "source": [
    "### Pitfall 2: Wrong Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy defaults to float64, but CUDA prefers float32\n",
    "import time\n",
    "\n",
    "@cuda.jit\n",
    "def add_arrays(a, b, c):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < c.size:\n",
    "        c[idx] = a[idx] + b[idx]\n",
    "\n",
    "n = 10_000_000\n",
    "\n",
    "# float64 (default) - slower on most GPUs\n",
    "a64 = np.random.randn(n)  # Default is float64!\n",
    "b64 = np.random.randn(n)\n",
    "c64 = np.zeros(n)\n",
    "\n",
    "# float32 - preferred\n",
    "a32 = np.random.randn(n).astype(np.float32)\n",
    "b32 = np.random.randn(n).astype(np.float32)\n",
    "c32 = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "threads, blocks = 256, math.ceil(n / 256)\n",
    "\n",
    "# Benchmark float64\n",
    "a64_d, b64_d = cuda.to_device(a64), cuda.to_device(b64)\n",
    "c64_d = cuda.device_array(n, dtype=np.float64)\n",
    "add_arrays[blocks, threads](a64_d, b64_d, c64_d)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    add_arrays[blocks, threads](a64_d, b64_d, c64_d)\n",
    "cuda.synchronize()\n",
    "time64 = (time.perf_counter() - start) / 10\n",
    "\n",
    "# Benchmark float32\n",
    "a32_d, b32_d = cuda.to_device(a32), cuda.to_device(b32)\n",
    "c32_d = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    add_arrays[blocks, threads](a32_d, b32_d, c32_d)\n",
    "cuda.synchronize()\n",
    "time32 = (time.perf_counter() - start) / 10\n",
    "\n",
    "print(f\"float64: {time64*1000:.3f} ms\")\n",
    "print(f\"float32: {time32*1000:.3f} ms\")\n",
    "print(f\"Speedup: {time64/time32:.2f}x\")\n",
    "print(\"\\nüí° Tip: Always use .astype(np.float32) unless you need float64 precision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f0cfe",
   "metadata": {},
   "source": [
    "### Pitfall 3: Forgetting to Synchronize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel execution is ASYNCHRONOUS\n",
    "@cuda.jit\n",
    "def slow_kernel(arr):\n",
    "    \"\"\"Simulate slow computation\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < arr.size:\n",
    "        # Busy work\n",
    "        val = 0.0\n",
    "        for i in range(1000):\n",
    "            val += idx * 0.001\n",
    "        arr[idx] = val\n",
    "\n",
    "arr = cuda.device_array(10000, dtype=np.float32)\n",
    "threads, blocks = 256, math.ceil(10000 / 256)\n",
    "\n",
    "# BAD: Timing without synchronization\n",
    "start = time.perf_counter()\n",
    "slow_kernel[blocks, threads](arr)\n",
    "# Missing: cuda.synchronize()\n",
    "bad_time = time.perf_counter() - start\n",
    "print(f\"Without sync: {bad_time*1000:.3f} ms (WRONG! Kernel still running)\")\n",
    "\n",
    "# GOOD: Proper timing with synchronization  \n",
    "start = time.perf_counter()\n",
    "slow_kernel[blocks, threads](arr)\n",
    "cuda.synchronize()  # Wait for kernel to complete\n",
    "good_time = time.perf_counter() - start\n",
    "print(f\"With sync:    {good_time*1000:.3f} ms (Correct)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è The unsynchronized time is {good_time/bad_time:.0f}x too fast!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d257e0",
   "metadata": {},
   "source": [
    "## 5. Debugging with Print Statements\n",
    "\n",
    "In Numba CUDA, you can use `print()` inside kernels for debugging (but use sparingly - it's slow!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def debug_kernel(arr, n):\n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    # Only print from first few threads to avoid output flood\n",
    "    if idx < 3:\n",
    "        print(\"Thread\", idx, \"starting\")\n",
    "    \n",
    "    if idx < n:\n",
    "        arr[idx] = idx * 2\n",
    "        \n",
    "        # Debug: Print values for first few elements\n",
    "        if idx < 3:\n",
    "            print(\"Thread\", idx, \"wrote value\", arr[idx])\n",
    "\n",
    "# Run with small array\n",
    "arr = cuda.device_array(10, dtype=np.float32)\n",
    "debug_kernel[1, 10](arr, 10)\n",
    "cuda.synchronize()\n",
    "\n",
    "print(\"\\nFinal array:\", arr.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3183d",
   "metadata": {},
   "source": [
    "## üéØ Additional Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete this error handling exercise in CUDA C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile error_handling_exercises.cu\n",
    "// error_handling_exercises.cu - Safe kernel launcher exercise\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "// =============================================================================\n",
    "// CUDA Error Checking Macro\n",
    "// =============================================================================\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                    cudaGetErrorString(err)); \\\n",
    "            return false; \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Error-Proof Kernel Wrapper\n",
    "// Create a robust wrapper that validates all inputs before launching\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void scaleKernel(float* data, float scale, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= scale;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Safe launcher that validates inputs\n",
    "bool safeLaunchScaleKernel(float* d_data, float scale, int n, int threadsPerBlock) {\n",
    "    // 1. Check device is available\n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    if (deviceCount == 0) {\n",
    "        fprintf(stderr, \"Error: No CUDA devices found\\n\");\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // 2. Validate pointer (can't be NULL for device pointer)\n",
    "    if (d_data == NULL) {\n",
    "        fprintf(stderr, \"Error: Device pointer is NULL\\n\");\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // 3. Validate array size\n",
    "    if (n <= 0) {\n",
    "        fprintf(stderr, \"Error: Array size must be positive (got %d)\\n\", n);\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // 4. Validate threads per block\n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
    "    \n",
    "    if (threadsPerBlock <= 0 || threadsPerBlock > prop.maxThreadsPerBlock) {\n",
    "        fprintf(stderr, \"Error: Invalid threads per block: %d (max: %d)\\n\", \n",
    "                threadsPerBlock, prop.maxThreadsPerBlock);\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // 5. Calculate grid size and validate\n",
    "    int blocks = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    if (blocks > prop.maxGridSize[0]) {\n",
    "        fprintf(stderr, \"Error: Grid too large: %d blocks (max: %d)\\n\",\n",
    "                blocks, prop.maxGridSize[0]);\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // 6. Check for enough memory\n",
    "    size_t freeMem, totalMem;\n",
    "    CUDA_CHECK(cudaMemGetInfo(&freeMem, &totalMem));\n",
    "    size_t required = n * sizeof(float);\n",
    "    if (required > freeMem) {\n",
    "        fprintf(stderr, \"Error: Not enough GPU memory. Need %zu, have %zu\\n\",\n",
    "                required, freeMem);\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // 7. Launch kernel\n",
    "    printf(\"  Launching with %d blocks √ó %d threads\\n\", blocks, threadsPerBlock);\n",
    "    scaleKernel<<<blocks, threadsPerBlock>>>(d_data, scale, n);\n",
    "    \n",
    "    // 8. Check for launch errors\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        fprintf(stderr, \"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // 9. Check for execution errors\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    \n",
    "    return true;\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Test harness\n",
    "// =============================================================================\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Error Handling Exercise ===\\n\\n\");\n",
    "    \n",
    "    // Test valid launch\n",
    "    printf(\"Test 1: Valid launch\\n\");\n",
    "    {\n",
    "        const int N = 1000;\n",
    "        float *d_data;\n",
    "        cudaMalloc(&d_data, N * sizeof(float));\n",
    "        \n",
    "        // Initialize\n",
    "        float *h_data = (float*)malloc(N * sizeof(float));\n",
    "        for (int i = 0; i < N; i++) h_data[i] = (float)i;\n",
    "        cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "        \n",
    "        bool result = safeLaunchScaleKernel(d_data, 2.0f, N, 256);\n",
    "        printf(\"  Result: %s\\n\\n\", result ? \"‚úì SUCCESS\" : \"‚úó FAILED\");\n",
    "        \n",
    "        cudaFree(d_data);\n",
    "        free(h_data);\n",
    "    }\n",
    "    \n",
    "    // Test invalid threads per block\n",
    "    printf(\"Test 2: Invalid threads per block (2048)\\n\");\n",
    "    {\n",
    "        const int N = 1000;\n",
    "        float *d_data;\n",
    "        cudaMalloc(&d_data, N * sizeof(float));\n",
    "        \n",
    "        bool result = safeLaunchScaleKernel(d_data, 2.0f, N, 2048);  // Too many!\n",
    "        printf(\"  Result: %s (expected: caught error)\\n\\n\", \n",
    "               result ? \"‚úó SHOULD HAVE FAILED\" : \"‚úì Correctly caught\");\n",
    "        \n",
    "        cudaFree(d_data);\n",
    "    }\n",
    "    \n",
    "    // Test NULL pointer\n",
    "    printf(\"Test 3: NULL pointer\\n\");\n",
    "    {\n",
    "        bool result = safeLaunchScaleKernel(NULL, 2.0f, 1000, 256);\n",
    "        printf(\"  Result: %s (expected: caught error)\\n\\n\",\n",
    "               result ? \"‚úó SHOULD HAVE FAILED\" : \"‚úì Correctly caught\");\n",
    "    }\n",
    "    \n",
    "    // Test invalid array size\n",
    "    printf(\"Test 4: Invalid array size (0)\\n\");\n",
    "    {\n",
    "        float *d_data;\n",
    "        cudaMalloc(&d_data, 1000 * sizeof(float));\n",
    "        \n",
    "        bool result = safeLaunchScaleKernel(d_data, 2.0f, 0, 256);\n",
    "        printf(\"  Result: %s (expected: caught error)\\n\\n\",\n",
    "               result ? \"‚úó SHOULD HAVE FAILED\" : \"‚úì Correctly caught\");\n",
    "        \n",
    "        cudaFree(d_data);\n",
    "    }\n",
    "    \n",
    "    printf(\"=== All tests complete! ===\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f820f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o error_handling_exercises error_handling_exercises.cu && ./error_handling_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622bdff",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Error-Proof Kernel Wrapper\n",
    "Create a robust wrapper function that validates all inputs before launching a kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 1: Complete this error-proof wrapper\n",
    "\n",
    "def launch_kernel_safe(kernel, data, threads_per_block=256):\n",
    "    \"\"\"\n",
    "    Safely launch a kernel with automatic configuration and error checking.\n",
    "    \n",
    "    Args:\n",
    "        kernel: The CUDA kernel function\n",
    "        data: Input array (numpy or device array)\n",
    "        threads_per_block: Threads per block (default 256)\n",
    "    \n",
    "    Returns:\n",
    "        Device array with results\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If inputs are invalid\n",
    "        MemoryError: If not enough GPU memory\n",
    "    \"\"\"\n",
    "    # TODO: Implement the following checks:\n",
    "    # 1. Verify CUDA is available\n",
    "    # 2. Check data is not empty\n",
    "    # 3. Validate threads_per_block (1-1024)\n",
    "    # 4. Check sufficient GPU memory\n",
    "    # 5. Launch kernel with proper grid configuration\n",
    "    # 6. Synchronize and check for errors\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762d8c3",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "### Error Handling Best Practices:\n",
    "\n",
    "1. **Always synchronize** before reading results or timing\n",
    "   ```python\n",
    "   kernel[grid, block](...)\n",
    "   cuda.synchronize()  # Wait for completion\n",
    "   result = output.copy_to_host()\n",
    "   ```\n",
    "\n",
    "2. **Validate launch configuration**\n",
    "   - threads_per_block ‚â§ 1024\n",
    "   - Check grid dimensions against device limits\n",
    "\n",
    "3. **Always include boundary checks**\n",
    "   ```python\n",
    "   if idx < n:\n",
    "       arr[idx] = ...\n",
    "   ```\n",
    "\n",
    "4. **Use try/except for error handling**\n",
    "   ```python\n",
    "   try:\n",
    "       kernel[grid, block](...)\n",
    "   except CudaAPIError as e:\n",
    "       print(f\"CUDA Error: {e}\")\n",
    "   ```\n",
    "\n",
    "5. **Prefer float32** unless you need float64 precision\n",
    "\n",
    "6. **Debug strategically**\n",
    "   - Use print() sparingly (only first few threads)\n",
    "   - Use small test cases first\n",
    "   - Verify CPU results before GPU\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Steps\n",
    "You've completed Week 1! Before moving on:\n",
    "1. Complete the checkpoint quiz\n",
    "2. Finish all exercises in each notebook\n",
    "3. Make sure you can run all code without errors\n",
    "\n",
    "### üîó Resources\n",
    "- [Error Handling Guide](../../cuda-programming-guide/02-basics/nvcc.md)\n",
    "- [Debugging Documentation](../../cuda-programming-guide/04-special-topics/error-log-management.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
