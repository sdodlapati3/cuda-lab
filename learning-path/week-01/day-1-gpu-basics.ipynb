{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1470948",
   "metadata": {},
   "source": [
    "# üöÄ Day 1: GPU Fundamentals & Your First CUDA Program\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-01/day-1-gpu-basics.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## üé£ Welcome to CUDA: Why This Matters\n",
    "\n",
    "> *The fastest code in the world is useless if you can't harness the hardware.*\n",
    "\n",
    "You're about to learn the skill that powers:\n",
    "- ü§ñ **AI/ML**: Every ChatGPT response, every Stable Diffusion image\n",
    "- üéÆ **Gaming**: Real-time ray tracing, physics simulations\n",
    "- üî¨ **Science**: Protein folding, climate models, drug discovery\n",
    "- üí∞ **Finance**: Real-time risk analysis, algorithmic trading\n",
    "\n",
    "**The GPU is the most powerful computing tool available to programmers today.** This notebook is your first step to mastering it.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of today, you will:\n",
    "- üéØ Understand **why GPUs exist** and when to use them\n",
    "- üîß Know the **key GPU architecture concepts** (SMs, warps, threads)\n",
    "- üìä Query your GPU's capabilities programmatically\n",
    "- üöÄ Write and run your **first CUDA kernel**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11304f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "# Primary learning should be done with CUDA C++ code\n",
    "\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Remember: CUDA C++ code is the PRIMARY learning material!\")\n",
    "print(\"   Python/Numba is provided for quick interactive testing only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f5e0c",
   "metadata": {},
   "source": [
    "## 1. Why GPUs? The Parallel Computing Revolution\n",
    "\n",
    "<details open>\n",
    "<summary>üí° <b>Concept Card: The Sports Car vs. Truck Fleet</b></summary>\n",
    "\n",
    "### üéØ The Problem\n",
    "Modern applications process **massive amounts of data**:\n",
    "- Neural networks: billions of matrix multiplications\n",
    "- Video processing: millions of pixels per frame (30-60 times per second!)\n",
    "- Scientific simulations: millions of particles or grid cells\n",
    "\n",
    "A CPU is like a **sports car**: incredibly fast, but can only carry one package at a time.\n",
    "A GPU is like a **fleet of 1000 delivery trucks**: each truck is slower, but together they move mountains.\n",
    "\n",
    "### üöö The Delivery Truck Analogy\n",
    "\n",
    "**Task: Deliver 10,000 packages**\n",
    "\n",
    "| Approach | Vehicle | Trips | Result |\n",
    "|----------|---------|-------|--------|\n",
    "| CPU | 4 sports cars | 2,500 each | üê¢ Slow (sequential) |\n",
    "| GPU | 1,000 trucks | 10 each | üöÄ Fast (parallel) |\n",
    "\n",
    "### üîß Hardware Reality\n",
    "\n",
    "| Aspect | CPU | GPU |\n",
    "|--------|-----|-----|\n",
    "| Cores | 4-64 \"smart\" cores | 1000s of \"simple\" cores |\n",
    "| Clock Speed | 3-5 GHz | 1-2 GHz |\n",
    "| Cache per Core | Large (MB) | Small (KB) |\n",
    "| Control Logic | Complex (branch prediction, out-of-order) | Simple |\n",
    "| Best For | **Latency** (1 task FAST) | **Throughput** (MANY tasks) |\n",
    "\n",
    "### ‚úÖ When to Use a GPU\n",
    "- ‚úÖ Same operation on many data elements (SIMD)\n",
    "- ‚úÖ Computation >> Memory access\n",
    "- ‚úÖ Data can be organized for parallel access\n",
    "- ‚ùå Lots of branching/conditionals\n",
    "- ‚ùå Sequential dependencies between steps\n",
    "- ‚ùå Small datasets that don't justify transfer overhead\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3895a31",
   "metadata": {},
   "source": [
    "## 2. Your First CUDA Program: Device Query\n",
    "\n",
    "*Before driving a car, you check the dashboard. Before writing CUDA code, you query the GPU.*\n",
    "\n",
    "The first thing any CUDA program should do is understand what hardware it's running on. Different GPUs have different capabilities, and your code may need to adapt.\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile device_query.cu\n",
    "// device_query.cu - Query GPU properties\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    int deviceCount = 0;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount == 0) {\n",
    "        printf(\"No CUDA devices found!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    printf(\"Found %d CUDA device(s)\\n\\n\", deviceCount);\n",
    "    \n",
    "    for (int dev = 0; dev < deviceCount; dev++) {\n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, dev);\n",
    "        \n",
    "        printf(\"Device %d: %s\\n\", dev, prop.name);\n",
    "        printf(\"  Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"  Multiprocessors: %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"  Max Threads/Block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "        printf(\"  Warp Size: %d\\n\", prop.warpSize);\n",
    "        printf(\"  Global Memory: %.2f GB\\n\", prop.totalGlobalMem / 1e9);\n",
    "        printf(\"  Shared Memory/Block: %.1f KB\\n\", prop.sharedMemPerBlock / 1024.0);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbe477",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o device_query device_query.cu\n",
    "!./device_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a4ca6",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for quick testing (OPTIONAL)\n",
    "from numba import cuda\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CUDA AVAILABILITY CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(\"‚úÖ CUDA is available!\")\n",
    "    print(f\"   CUDA GPUs detected: {len(cuda.gpus)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is NOT available!\")\n",
    "    print(\"   Make sure you have:\")\n",
    "    print(\"   1. NVIDIA GPU installed\")\n",
    "    print(\"   2. CUDA Toolkit installed\")\n",
    "    print(\"   3. numba installed: pip install numba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a9819",
   "metadata": {},
   "source": [
    "## 3. Understanding GPU Architecture\n",
    "\n",
    "<details open>\n",
    "<summary>üí° <b>Concept Card: The Factory Floor Analogy</b></summary>\n",
    "\n",
    "### üéØ The Mental Model\n",
    "Think of a GPU as a **factory with many assembly lines**:\n",
    "\n",
    "```\n",
    "GPU (The Factory)\n",
    "‚îú‚îÄ‚îÄ SM 0 (Assembly Line)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Warp 0 (Team of 32 workers)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Warp 1 (Team of 32 workers)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ SM 1 (Assembly Line)\n",
    "‚îú‚îÄ‚îÄ SM 2 (Assembly Line)\n",
    "‚îî‚îÄ‚îÄ ... (more SMs)\n",
    "```\n",
    "\n",
    "### üîß Key Components\n",
    "\n",
    "| Component | Factory Analogy | What It Does |\n",
    "|-----------|-----------------|--------------|\n",
    "| **SM** (Streaming Multiprocessor) | Assembly line | Independent processing unit with its own resources |\n",
    "| **Warp** | Team of 32 workers | 32 threads that execute the SAME instruction together |\n",
    "| **Thread** | Individual worker | Smallest unit of execution |\n",
    "| **Shared Memory** | Team whiteboard | Fast memory shared within a block |\n",
    "| **Global Memory** | Warehouse | Large but slow storage accessible by all |\n",
    "\n",
    "### üìä Typical GPU Numbers (T4 Example)\n",
    "\n",
    "| Property | Value | Why It Matters |\n",
    "|----------|-------|----------------|\n",
    "| SMs | 40 | More SMs = more parallel work |\n",
    "| Max Threads/Block | 1024 | Limits how large your blocks can be |\n",
    "| Warp Size | 32 | Always 32! Threads execute in groups of 32 |\n",
    "| Shared Memory/Block | 48 KB | Fast on-chip memory for cooperation |\n",
    "| Global Memory | 16 GB | Total GPU memory |\n",
    "\n",
    "### ‚ö†Ô∏è Key Insight\n",
    "**Warps are everything.** 32 threads in a warp execute the **exact same instruction** at the **exact same time**. If they need to do different things (branch divergence), they take turns‚Äîkilling performance.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "Let's query our GPU to see these properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query GPU properties\n",
    "device = cuda.get_current_device()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"GPU: {device.name.decode('utf-8')}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute capability\n",
    "cc = device.compute_capability\n",
    "print(f\"\\nüìä Compute Capability: {cc[0]}.{cc[1]}\")\n",
    "\n",
    "# Architecture mapping\n",
    "arch_names = {\n",
    "    (7, 0): \"Volta\", (7, 5): \"Turing\",\n",
    "    (8, 0): \"Ampere\", (8, 6): \"Ampere\", (8, 9): \"Ada Lovelace\",\n",
    "    (9, 0): \"Hopper\"\n",
    "}\n",
    "arch = arch_names.get(cc, \"Unknown\")\n",
    "print(f\"   Architecture: {arch}\")\n",
    "\n",
    "# Processor info\n",
    "print(f\"\\nüîß Processor Info:\")\n",
    "print(f\"   Multiprocessors (SMs): {device.MULTIPROCESSOR_COUNT}\")\n",
    "print(f\"   Max Threads per Block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "print(f\"   Max Block Dimensions: {device.MAX_BLOCK_DIM_X} x {device.MAX_BLOCK_DIM_Y} x {device.MAX_BLOCK_DIM_Z}\")\n",
    "print(f\"   Max Grid Dimensions: {device.MAX_GRID_DIM_X} x {device.MAX_GRID_DIM_Y} x {device.MAX_GRID_DIM_Z}\")\n",
    "print(f\"   Warp Size: {device.WARP_SIZE}\")\n",
    "\n",
    "# Memory info\n",
    "print(f\"\\nüíæ Memory Info:\")\n",
    "print(f\"   Shared Memory per Block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.1f} KB\")\n",
    "\n",
    "# Get total memory using context\n",
    "context = cuda.current_context()\n",
    "free_mem, total_mem = context.get_memory_info()\n",
    "print(f\"   Total Global Memory: {total_mem / (1024**3):.2f} GB\")\n",
    "print(f\"   Free Memory: {free_mem / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50bb02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Your First CUDA Kernel: Vector Addition\n",
    "\n",
    "*This is the \"Hello World\" of GPU programming. Master this, and everything else builds on it.*\n",
    "\n",
    "<details open>\n",
    "<summary>üí° <b>Concept Card: From Sequential to Parallel Thinking</b></summary>\n",
    "\n",
    "### üéØ The Mental Shift\n",
    "CPU programming is **sequential**: you write a loop, and elements are processed one after another.\n",
    "GPU programming is **parallel**: you write what **one thread** does, and thousands run simultaneously.\n",
    "\n",
    "### üîÑ The Transformation\n",
    "\n",
    "```\n",
    "CPU (Sequential):                 GPU (Parallel):\n",
    "                                  \n",
    "for (int i = 0; i < N; i++) {     __global__ void add(...) {\n",
    "    c[i] = a[i] + b[i];               int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "}                                     c[i] = a[i] + b[i];\n",
    "                                  }\n",
    "// 1 million iterations           // 1 million threads (all at once!)\n",
    "// ~1 million cycles              // ~1000 cycles (1000x faster potential)\n",
    "```\n",
    "\n",
    "### üîß Key Concepts\n",
    "\n",
    "| Keyword | Meaning |\n",
    "|---------|---------|\n",
    "| `__global__` | This function runs on GPU, called from CPU |\n",
    "| `blockIdx.x` | Which block am I in? (0, 1, 2, ...) |\n",
    "| `threadIdx.x` | Which thread am I within my block? (0, 1, ... 255) |\n",
    "| `blockDim.x` | How many threads per block? (typically 256) |\n",
    "\n",
    "### üìê The Index Formula\n",
    "Every thread needs to know which element to process:\n",
    "```cuda\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "//        ‚îî‚îÄ‚îÄ block offset ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ thread offset ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "Example with 256 threads per block:\n",
    "- Block 0, Thread 0: idx = 0√ó256 + 0 = **0**\n",
    "- Block 0, Thread 255: idx = 0√ó256 + 255 = **255**\n",
    "- Block 1, Thread 0: idx = 1√ó256 + 0 = **256**\n",
    "- Block 1, Thread 1: idx = 1√ó256 + 1 = **257**\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector_add.cu\n",
    "// vector_add.cu - Your first CUDA kernel!\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA kernel - runs on GPU\n",
    "__global__ void vectorAdd(const float* a, const float* b, float* c, int n) {\n",
    "    // Calculate global thread ID\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Boundary check\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;  // 1 million elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize host arrays\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    // Copy data from host to device\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "    \n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify\n",
    "    printf(\"c[0] = %f (expected 3.0)\\n\", h_c[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o vector_add vector_add.cu\n",
    "!./vector_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084475e",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67832fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for quick testing (OPTIONAL)\n",
    "@cuda.jit\n",
    "def vector_add_kernel(a, b, c):\n",
    "    \"\"\"Each thread computes one element of c = a + b\"\"\"\n",
    "    idx = cuda.grid(1)  # Same as: blockIdx.x * blockDim.x + threadIdx.x\n",
    "    if idx < c.size:\n",
    "        c[idx] = a[idx] + b[idx]\n",
    "\n",
    "# Create test data\n",
    "N = 1_000_000\n",
    "a_host = np.random.randn(N).astype(np.float32)\n",
    "b_host = np.random.randn(N).astype(np.float32)\n",
    "c_host = np.zeros(N, dtype=np.float32)\n",
    "\n",
    "print(f\"Vector size: {N:,} elements\")\n",
    "print(f\"Memory per vector: {a_host.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c2cc8",
   "metadata": {},
   "source": [
    "## 5. Memory Management: Host ‚Üî Device Transfers\n",
    "\n",
    "### CUDA C++ Memory Functions\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `cudaMalloc(&ptr, size)` | Allocate GPU memory |\n",
    "| `cudaMemcpy(dst, src, size, kind)` | Copy between host/device |\n",
    "| `cudaFree(ptr)` | Free GPU memory |\n",
    "\n",
    "```cpp\n",
    "// Memory management in CUDA C++\n",
    "float *d_array;\n",
    "cudaMalloc(&d_array, n * sizeof(float));                    // Allocate on GPU\n",
    "cudaMemcpy(d_array, h_array, n * sizeof(float), cudaMemcpyHostToDevice);  // Copy to GPU\n",
    "cudaMemcpy(h_result, d_array, n * sizeof(float), cudaMemcpyDeviceToHost); // Copy from GPU\n",
    "cudaFree(d_array);                                           // Free GPU memory\n",
    "```\n",
    "\n",
    "Data must be **explicitly copied** between CPU (host) and GPU (device):\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   CPU       ‚îÇ  cudaMemcpy H‚ÜíD   ‚îÇ   GPU       ‚îÇ\n",
    "‚îÇ   (Host)    ‚îÇ ================‚ñ∫ ‚îÇ  (Device)   ‚îÇ\n",
    "‚îÇ             ‚îÇ                    ‚îÇ             ‚îÇ\n",
    "‚îÇ  a_host[]   ‚îÇ                    ‚îÇ  a_device[] ‚îÇ\n",
    "‚îÇ  b_host[]   ‚îÇ                    ‚îÇ  b_device[] ‚îÇ\n",
    "‚îÇ  c_host[]   ‚îÇ ‚óÑ================ ‚îÇ  c_device[] ‚îÇ\n",
    "‚îÇ             ‚îÇ  cudaMemcpy D‚ÜíH   ‚îÇ             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        PCIe Bus (bottleneck!)\n",
    "```\n",
    "\n",
    "**Key Functions:**\n",
    "- `cuda.to_device(array)` - Copy host ‚Üí device\n",
    "- `cuda.device_array(shape)` - Allocate on device (no copy)\n",
    "- `device_array.copy_to_host()` - Copy device ‚Üí host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer data to GPU\n",
    "a_device = cuda.to_device(a_host)  # Copy a to GPU\n",
    "b_device = cuda.to_device(b_host)  # Copy b to GPU\n",
    "c_device = cuda.device_array(N, dtype=np.float32)  # Allocate c on GPU (no copy needed)\n",
    "\n",
    "print(\"‚úÖ Data transferred to GPU\")\n",
    "print(f\"   a_device type: {type(a_device)}\")\n",
    "print(f\"   Shape: {a_device.shape}, Dtype: {a_device.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287eef1d",
   "metadata": {},
   "source": [
    "## 6. Thread and Block Configuration\n",
    "\n",
    "CUDA organizes threads in a hierarchy:\n",
    "\n",
    "```\n",
    "                    Grid (all threads)\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  Block 0    Block 1    Block 2  ...‚îÇ\n",
    "                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "                    ‚îÇ  ‚îÇThread‚îÇ   ‚îÇThread‚îÇ   ‚îÇThread‚îÇ    ‚îÇ\n",
    "                    ‚îÇ  ‚îÇ  0-N ‚îÇ   ‚îÇ  0-N ‚îÇ   ‚îÇ  0-N ‚îÇ    ‚îÇ\n",
    "                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Kernel Launch Syntax: `kernel[blocks_per_grid, threads_per_block](...)`\n",
    "\n",
    "**Rules of thumb:**\n",
    "- `threads_per_block`: Usually 128, 256, or 512 (must be ‚â§ 1024)\n",
    "- `blocks_per_grid`: Calculated to cover all elements\n",
    "- Total threads = blocks √ó threads_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure kernel launch parameters\n",
    "threads_per_block = 256  # Common choice\n",
    "\n",
    "# Calculate blocks needed to cover all elements\n",
    "# Formula: ceil(N / threads_per_block)\n",
    "blocks_per_grid = math.ceil(N / threads_per_block)\n",
    "\n",
    "print(f\"üìê Launch Configuration:\")\n",
    "print(f\"   Array size: {N:,}\")\n",
    "print(f\"   Threads per block: {threads_per_block}\")\n",
    "print(f\"   Blocks per grid: {blocks_per_grid:,}\")\n",
    "print(f\"   Total threads: {blocks_per_grid * threads_per_block:,}\")\n",
    "print(f\"   Extra threads (boundary check needed): {blocks_per_grid * threads_per_block - N:,}\")\n",
    "\n",
    "# Launch the kernel!\n",
    "vector_add_kernel[blocks_per_grid, threads_per_block](a_device, b_device, c_device)\n",
    "\n",
    "# Wait for GPU to finish\n",
    "cuda.synchronize()\n",
    "print(\"\\n‚úÖ Kernel execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbeef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy result back to CPU and verify\n",
    "c_host = c_device.copy_to_host()\n",
    "\n",
    "# Verify correctness\n",
    "expected = a_host + b_host\n",
    "if np.allclose(c_host, expected):\n",
    "    print(\"‚úÖ VERIFICATION PASSED!\")\n",
    "    print(f\"   First 5 elements: {c_host[:5]}\")\n",
    "    print(f\"   Expected:         {expected[:5]}\")\n",
    "else:\n",
    "    print(\"‚ùå VERIFICATION FAILED!\")\n",
    "    diff = np.abs(c_host - expected).max()\n",
    "    print(f\"   Max difference: {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43906758",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison: CPU vs GPU\n",
    "\n",
    "Now let's see the real benefit of GPU computing - speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccefc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_cpu_gpu(sizes):\n",
    "    \"\"\"Compare CPU and GPU performance across different array sizes\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for N in sizes:\n",
    "        # Create data\n",
    "        a = np.random.randn(N).astype(np.float32)\n",
    "        b = np.random.randn(N).astype(np.float32)\n",
    "        \n",
    "        # CPU timing\n",
    "        start = time.perf_counter()\n",
    "        c_cpu = a + b\n",
    "        cpu_time = time.perf_counter() - start\n",
    "        \n",
    "        # GPU timing (including transfers)\n",
    "        start = time.perf_counter()\n",
    "        a_d = cuda.to_device(a)\n",
    "        b_d = cuda.to_device(b)\n",
    "        c_d = cuda.device_array(N, dtype=np.float32)\n",
    "        \n",
    "        tpb = 256\n",
    "        bpg = math.ceil(N / tpb)\n",
    "        vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
    "        cuda.synchronize()\n",
    "        c_gpu = c_d.copy_to_host()\n",
    "        gpu_time = time.perf_counter() - start\n",
    "        \n",
    "        speedup = cpu_time / gpu_time if gpu_time > 0 else 0\n",
    "        results.append((N, cpu_time*1000, gpu_time*1000, speedup))\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "sizes = [1_000, 10_000, 100_000, 1_000_000, 10_000_000, 50_000_000]\n",
    "print(\"üèÅ Benchmarking CPU vs GPU...\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Array Size':>12} | {'CPU (ms)':>10} | {'GPU (ms)':>10} | {'Speedup':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "results = benchmark_cpu_gpu(sizes)\n",
    "for N, cpu_ms, gpu_ms, speedup in results:\n",
    "    indicator = \"üöÄ\" if speedup > 1 else \"üê¢\"\n",
    "    print(f\"{N:>12,} | {cpu_ms:>10.3f} | {gpu_ms:>10.3f} | {speedup:>9.2f}x {indicator}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(\"\\nüí° Note: GPU shines with larger arrays (overhead amortized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa7cd85",
   "metadata": {},
   "source": [
    "## üéØ Exercises\n",
    "\n",
    "Complete these exercises to solidify your understanding.\n",
    "\n",
    "### Exercise 1: Vector Subtraction\n",
    "Modify the vector addition kernel to perform subtraction (c = a - b).\n",
    "\n",
    "### Exercise 2: Element-wise Multiplication  \n",
    "Create a new kernel for element-wise multiplication (c = a * b).\n",
    "\n",
    "### Exercise 3: Different Block Sizes\n",
    "Experiment with different `threads_per_block` values (64, 128, 256, 512, 1024).\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile exercises_day1.cu\n",
    "// exercises_day1.cu - Vector operations exercises\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "// Exercise 1: Vector Subtraction\n",
    "// TODO: Modify to compute c = a - b\n",
    "__global__ void vectorSub(const float* a, const float* b, float* c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] - b[idx];  // Subtraction instead of addition\n",
    "    }\n",
    "}\n",
    "\n",
    "// Exercise 2: Element-wise Multiplication\n",
    "// TODO: Compute c = a * b\n",
    "__global__ void vectorMul(const float* a, const float* b, float* c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] * b[idx];  // Element-wise multiplication\n",
    "    }\n",
    "}\n",
    "\n",
    "// Exercise 3: Block size experiment kernel\n",
    "__global__ void vectorAdd(const float* a, const float* b, float* c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 10000000;\n",
    "    const size_t bytes = N * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_a = (float*)malloc(bytes);\n",
    "    float *h_b = (float*)malloc(bytes);\n",
    "    float *h_c = (float*)malloc(bytes);\n",
    "    \n",
    "    // Initialize with random data\n",
    "    srand(42);\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_a[i] = (float)rand() / RAND_MAX;\n",
    "        h_b[i] = (float)rand() / RAND_MAX;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, bytes);\n",
    "    cudaMalloc(&d_b, bytes);\n",
    "    cudaMalloc(&d_c, bytes);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    printf(\"=== Exercise 1: Vector Subtraction ===\\n\");\n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    vectorSub<<<blocks, threads>>>(d_a, d_b, d_c, N);\n",
    "    cudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost);\n",
    "    printf(\"a[0]=%.4f, b[0]=%.4f, c[0]=%.4f (expected %.4f)\\n\\n\", \n",
    "           h_a[0], h_b[0], h_c[0], h_a[0] - h_b[0]);\n",
    "    \n",
    "    printf(\"=== Exercise 2: Element-wise Multiplication ===\\n\");\n",
    "    vectorMul<<<blocks, threads>>>(d_a, d_b, d_c, N);\n",
    "    cudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost);\n",
    "    printf(\"a[0]=%.4f, b[0]=%.4f, c[0]=%.4f (expected %.4f)\\n\\n\", \n",
    "           h_a[0], h_b[0], h_c[0], h_a[0] * h_b[0]);\n",
    "    \n",
    "    printf(\"=== Exercise 3: Block Size Experiment ===\\n\");\n",
    "    int block_sizes[] = {32, 64, 128, 256, 512, 1024};\n",
    "    int num_sizes = 6;\n",
    "    \n",
    "    for (int i = 0; i < num_sizes; i++) {\n",
    "        int tpb = block_sizes[i];\n",
    "        int blks = (N + tpb - 1) / tpb;\n",
    "        \n",
    "        // Warmup\n",
    "        vectorAdd<<<blks, tpb>>>(d_a, d_b, d_c, N);\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        // Benchmark\n",
    "        cudaEventRecord(start);\n",
    "        for (int j = 0; j < 100; j++) {\n",
    "            vectorAdd<<<blks, tpb>>>(d_a, d_b, d_c, N);\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        printf(\"Block size %4d: %.3f ms avg\\n\", tpb, ms / 100);\n",
    "    }\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf144eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o exercises_day1 exercises_day1.cu && ./exercises_day1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec448462",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 1: Vector Subtraction\n",
    "# Create a kernel that computes c = a - b\n",
    "\n",
    "@cuda.jit\n",
    "def vector_sub_kernel(a, b, c):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < c.size:\n",
    "        # TODO: Replace pass with subtraction\n",
    "        pass\n",
    "\n",
    "# Test your kernel here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fec6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 2: Element-wise Multiplication\n",
    "# Create a kernel that computes c = a * b\n",
    "\n",
    "@cuda.jit\n",
    "def vector_mul_kernel(a, b, c):\n",
    "    # TODO: Implement this kernel\n",
    "    pass\n",
    "\n",
    "# Test your kernel here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 3: Block Size Experiment\n",
    "# Try different threads_per_block values and compare performance\n",
    "\n",
    "def benchmark_block_sizes(N=10_000_000):\n",
    "    \"\"\"Test different block sizes\"\"\"\n",
    "    a = np.random.randn(N).astype(np.float32)\n",
    "    b = np.random.randn(N).astype(np.float32)\n",
    "    a_d = cuda.to_device(a)\n",
    "    b_d = cuda.to_device(b)\n",
    "    c_d = cuda.device_array(N, dtype=np.float32)\n",
    "    \n",
    "    block_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "    \n",
    "    print(f\"Testing with N = {N:,}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for tpb in block_sizes:\n",
    "        bpg = math.ceil(N / tpb)\n",
    "        \n",
    "        # Warmup\n",
    "        vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(100):\n",
    "            vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
    "        cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / 100 * 1000\n",
    "        \n",
    "        print(f\"Block size {tpb:4d}: {elapsed:.3f} ms\")\n",
    "\n",
    "# TODO: Run the benchmark and analyze results\n",
    "# benchmark_block_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93678fce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary: What You Learned Today\n",
    "\n",
    "<details open>\n",
    "<summary><b>üìã Quick Reference Card</b></summary>\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "| Concept | CPU | GPU |\n",
    "|---------|-----|-----|\n",
    "| Philosophy | Fast on ONE thing | Fast on MANY things |\n",
    "| Cores | 4-64 complex | 1000s simple |\n",
    "| Best for | Latency-sensitive | Throughput-heavy |\n",
    "\n",
    "### GPU Architecture Hierarchy\n",
    "```\n",
    "GPU ‚Üí SMs (40) ‚Üí Warps (32 threads each) ‚Üí Threads\n",
    "       ‚Üì              ‚Üì\n",
    "   Assembly       Workers that\n",
    "    Lines        move in lockstep\n",
    "```\n",
    "\n",
    "### The Index Formula\n",
    "```cuda\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "//        ‚îî‚îÄ‚îÄ which block ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ which thread in block ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Memory Transfer Pattern\n",
    "```cuda\n",
    "cudaMalloc(&d_ptr, size);                    // 1. Allocate on GPU\n",
    "cudaMemcpy(d_ptr, h_ptr, size, H2D);         // 2. Copy to GPU\n",
    "kernel<<<blocks, threads>>>(d_ptr, ...);     // 3. Run kernel\n",
    "cudaMemcpy(h_ptr, d_ptr, size, D2H);         // 4. Copy back\n",
    "cudaFree(d_ptr);                              // 5. Free GPU memory\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Three Things to Remember\n",
    "\n",
    "1. **THE MENTAL SHIFT**: Stop thinking \"loop over elements.\" Start thinking \"what does ONE thread do?\"\n",
    "\n",
    "2. **THE WARP**: 32 threads execute together. They're a team‚Äîif one branches differently, ALL wait.\n",
    "\n",
    "3. **THE OVERHEAD**: GPU isn't always faster. Small data + transfer time can make GPU slower than CPU.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö What's Next?\n",
    "\n",
    "**Day 2: Thread Indexing Deep Dive** - You'll learn:\n",
    "- 1D, 2D, and 3D thread organizations\n",
    "- Grid-stride loops for processing any size data\n",
    "- How to avoid the \"boundary bug\" (threads past array end)\n",
    "- Practical patterns for images and matrices\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Deep Dive Resources\n",
    "- [CUDA Programming Guide - Introduction](../../cuda-programming-guide/01-introduction/programming-model.md)\n",
    "- [Quick Reference](../../notes/cuda-quick-reference.md)\n",
    "- [NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
