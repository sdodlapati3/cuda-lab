{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1470948",
   "metadata": {},
   "source": [
    "# üöÄ Day 1: GPU Fundamentals & Your First CUDA Program\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-01/day-1-gpu-basics.ipynb)\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11304f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "# Primary learning should be done with CUDA C++ code\n",
    "\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Remember: CUDA C++ code is the PRIMARY learning material!\")\n",
    "print(\"   Python/Numba is provided for quick interactive testing only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f5e0c",
   "metadata": {},
   "source": [
    "# Day 1: GPU Fundamentals & Your First CUDA Program\n",
    "\n",
    "Welcome to your CUDA learning journey! Today we'll understand:\n",
    "- Why GPUs exist and when to use them\n",
    "- GPU architecture basics\n",
    "- How to query GPU properties\n",
    "- Your first CUDA kernel\n",
    "\n",
    "**Prerequisites:** Basic C/C++ knowledge, understanding of pointers\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why GPUs? The Parallel Computing Revolution\n",
    "\n",
    "### The Fundamental Problem\n",
    "\n",
    "Modern applications process **massive amounts of data**:\n",
    "- Neural networks: billions of matrix operations\n",
    "- Video processing: millions of pixels per frame\n",
    "- Scientific simulations: countless particles/cells\n",
    "\n",
    "CPUs are optimized for **speed on single tasks** (latency).  \n",
    "GPUs are optimized for **throughput on many tasks** (parallelism).\n",
    "\n",
    "### CPU vs GPU: An Analogy\n",
    "\n",
    "```\n",
    "üöó CPU (Sports Car)          üöõ GPU (Fleet of Trucks)\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚Ä¢ 4-16 very fast cores       ‚Ä¢ 1000s of simpler cores\n",
    "‚Ä¢ Complex control logic      ‚Ä¢ Simple control logic  \n",
    "‚Ä¢ Large caches per core      ‚Ä¢ Smaller shared caches\n",
    "‚Ä¢ Great for: 1 task FAST     ‚Ä¢ Great for: MANY tasks\n",
    "```\n",
    "\n",
    "**Delivering 10,000 packages:**\n",
    "- CPU: 4 sports cars √ó 2,500 trips = slow\n",
    "- GPU: 1,000 trucks √ó 10 trips = FAST!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3895a31",
   "metadata": {},
   "source": [
    "## 2. Your First CUDA Program: Device Query\n",
    "\n",
    "### CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile device_query.cu\n",
    "// device_query.cu - Query GPU properties\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    int deviceCount = 0;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount == 0) {\n",
    "        printf(\"No CUDA devices found!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    printf(\"Found %d CUDA device(s)\\n\\n\", deviceCount);\n",
    "    \n",
    "    for (int dev = 0; dev < deviceCount; dev++) {\n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, dev);\n",
    "        \n",
    "        printf(\"Device %d: %s\\n\", dev, prop.name);\n",
    "        printf(\"  Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"  Multiprocessors: %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"  Max Threads/Block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "        printf(\"  Warp Size: %d\\n\", prop.warpSize);\n",
    "        printf(\"  Global Memory: %.2f GB\\n\", prop.totalGlobalMem / 1e9);\n",
    "        printf(\"  Shared Memory/Block: %.1f KB\\n\", prop.sharedMemPerBlock / 1024.0);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbe477",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o device_query device_query.cu\n",
    "!./device_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a4ca6",
   "metadata": {},
   "source": [
    "### Python/Numba (Optional - Interactive Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for quick testing (OPTIONAL)\n",
    "from numba import cuda\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CUDA AVAILABILITY CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(\"‚úÖ CUDA is available!\")\n",
    "    print(f\"   CUDA GPUs detected: {len(cuda.gpus)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is NOT available!\")\n",
    "    print(\"   Make sure you have:\")\n",
    "    print(\"   1. NVIDIA GPU installed\")\n",
    "    print(\"   2. CUDA Toolkit installed\")\n",
    "    print(\"   3. numba installed: pip install numba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a9819",
   "metadata": {},
   "source": [
    "## 3. Understanding CUDA Device Properties\n",
    "\n",
    "Before writing CUDA code, we need to understand our GPU's capabilities. Key properties include:\n",
    "\n",
    "| Property | What It Means |\n",
    "|----------|---------------|\n",
    "| **Compute Capability** | GPU architecture version (e.g., 8.6 = Ampere) |\n",
    "| **Streaming Multiprocessors (SMs)** | Independent processing units |\n",
    "| **Max Threads per Block** | How many threads can cooperate |\n",
    "| **Warp Size** | Threads executed in lockstep (always 32) |\n",
    "| **Global Memory** | Total GPU memory (VRAM) |\n",
    "| **Shared Memory per Block** | Fast on-chip memory for cooperation |\n",
    "\n",
    "Let's query our GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query GPU properties\n",
    "device = cuda.get_current_device()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"GPU: {device.name.decode('utf-8')}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute capability\n",
    "cc = device.compute_capability\n",
    "print(f\"\\nüìä Compute Capability: {cc[0]}.{cc[1]}\")\n",
    "\n",
    "# Architecture mapping\n",
    "arch_names = {\n",
    "    (7, 0): \"Volta\", (7, 5): \"Turing\",\n",
    "    (8, 0): \"Ampere\", (8, 6): \"Ampere\", (8, 9): \"Ada Lovelace\",\n",
    "    (9, 0): \"Hopper\"\n",
    "}\n",
    "arch = arch_names.get(cc, \"Unknown\")\n",
    "print(f\"   Architecture: {arch}\")\n",
    "\n",
    "# Processor info\n",
    "print(f\"\\nüîß Processor Info:\")\n",
    "print(f\"   Multiprocessors (SMs): {device.MULTIPROCESSOR_COUNT}\")\n",
    "print(f\"   Max Threads per Block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "print(f\"   Max Block Dimensions: {device.MAX_BLOCK_DIM_X} x {device.MAX_BLOCK_DIM_Y} x {device.MAX_BLOCK_DIM_Z}\")\n",
    "print(f\"   Max Grid Dimensions: {device.MAX_GRID_DIM_X} x {device.MAX_GRID_DIM_Y} x {device.MAX_GRID_DIM_Z}\")\n",
    "print(f\"   Warp Size: {device.WARP_SIZE}\")\n",
    "\n",
    "# Memory info\n",
    "print(f\"\\nüíæ Memory Info:\")\n",
    "print(f\"   Shared Memory per Block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.1f} KB\")\n",
    "\n",
    "# Get total memory using context\n",
    "context = cuda.current_context()\n",
    "free_mem, total_mem = context.get_memory_info()\n",
    "print(f\"   Total Global Memory: {total_mem / (1024**3):.2f} GB\")\n",
    "print(f\"   Free Memory: {free_mem / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50bb02",
   "metadata": {},
   "source": [
    "## 4. Your First CUDA Kernel: Vector Addition\n",
    "\n",
    "A **kernel** is a function that runs on the GPU. Let's start with the \"Hello World\" of GPU programming: adding two vectors.\n",
    "\n",
    "### Key Concepts:\n",
    "- `__global__` keyword marks a function as a GPU kernel\n",
    "- Kernels run on **many threads simultaneously**\n",
    "- Each thread processes a different element\n",
    "\n",
    "```\n",
    "CPU View:           GPU View (1000 threads):\n",
    "                    \n",
    "for i in range(N):  Thread 0: c[0] = a[0] + b[0]\n",
    "    c[i] = a[i]+b[i] Thread 1: c[1] = a[1] + b[1]\n",
    "                    Thread 2: c[2] = a[2] + b[2]\n",
    "(sequential)        ...\n",
    "                    Thread 999: c[999] = a[999] + b[999]\n",
    "                    (ALL AT ONCE!)\n",
    "```\n",
    "\n",
    "### CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector_add.cu\n",
    "// vector_add.cu - Your first CUDA kernel!\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA kernel - runs on GPU\n",
    "__global__ void vectorAdd(const float* a, const float* b, float* c, int n) {\n",
    "    // Calculate global thread ID\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Boundary check\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;  // 1 million elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize host arrays\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    // Copy data from host to device\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "    \n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify\n",
    "    printf(\"c[0] = %f (expected 3.0)\\n\", h_c[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o vector_add vector_add.cu\n",
    "!./vector_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084475e",
   "metadata": {},
   "source": [
    "### Python/Numba (Optional - Interactive Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67832fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for quick testing (OPTIONAL)\n",
    "@cuda.jit\n",
    "def vector_add_kernel(a, b, c):\n",
    "    \"\"\"Each thread computes one element of c = a + b\"\"\"\n",
    "    idx = cuda.grid(1)  # Same as: blockIdx.x * blockDim.x + threadIdx.x\n",
    "    if idx < c.size:\n",
    "        c[idx] = a[idx] + b[idx]\n",
    "\n",
    "# Create test data\n",
    "N = 1_000_000\n",
    "a_host = np.random.randn(N).astype(np.float32)\n",
    "b_host = np.random.randn(N).astype(np.float32)\n",
    "c_host = np.zeros(N, dtype=np.float32)\n",
    "\n",
    "print(f\"Vector size: {N:,} elements\")\n",
    "print(f\"Memory per vector: {a_host.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c2cc8",
   "metadata": {},
   "source": [
    "## 5. Memory Management: Host ‚Üî Device Transfers\n",
    "\n",
    "### CUDA C++ Memory Functions\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `cudaMalloc(&ptr, size)` | Allocate GPU memory |\n",
    "| `cudaMemcpy(dst, src, size, kind)` | Copy between host/device |\n",
    "| `cudaFree(ptr)` | Free GPU memory |\n",
    "\n",
    "```cpp\n",
    "// Memory management in CUDA C++\n",
    "float *d_array;\n",
    "cudaMalloc(&d_array, n * sizeof(float));                    // Allocate on GPU\n",
    "cudaMemcpy(d_array, h_array, n * sizeof(float), cudaMemcpyHostToDevice);  // Copy to GPU\n",
    "cudaMemcpy(h_result, d_array, n * sizeof(float), cudaMemcpyDeviceToHost); // Copy from GPU\n",
    "cudaFree(d_array);                                           // Free GPU memory\n",
    "```\n",
    "\n",
    "Data must be **explicitly copied** between CPU (host) and GPU (device):\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   CPU       ‚îÇ  cudaMemcpy H‚ÜíD   ‚îÇ   GPU       ‚îÇ\n",
    "‚îÇ   (Host)    ‚îÇ ================‚ñ∫ ‚îÇ  (Device)   ‚îÇ\n",
    "‚îÇ             ‚îÇ                    ‚îÇ             ‚îÇ\n",
    "‚îÇ  a_host[]   ‚îÇ                    ‚îÇ  a_device[] ‚îÇ\n",
    "‚îÇ  b_host[]   ‚îÇ                    ‚îÇ  b_device[] ‚îÇ\n",
    "‚îÇ  c_host[]   ‚îÇ ‚óÑ================ ‚îÇ  c_device[] ‚îÇ\n",
    "‚îÇ             ‚îÇ  cudaMemcpy D‚ÜíH   ‚îÇ             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        PCIe Bus (bottleneck!)\n",
    "```\n",
    "\n",
    "**Key Functions:**\n",
    "- `cuda.to_device(array)` - Copy host ‚Üí device\n",
    "- `cuda.device_array(shape)` - Allocate on device (no copy)\n",
    "- `device_array.copy_to_host()` - Copy device ‚Üí host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer data to GPU\n",
    "a_device = cuda.to_device(a_host)  # Copy a to GPU\n",
    "b_device = cuda.to_device(b_host)  # Copy b to GPU\n",
    "c_device = cuda.device_array(N, dtype=np.float32)  # Allocate c on GPU (no copy needed)\n",
    "\n",
    "print(\"‚úÖ Data transferred to GPU\")\n",
    "print(f\"   a_device type: {type(a_device)}\")\n",
    "print(f\"   Shape: {a_device.shape}, Dtype: {a_device.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287eef1d",
   "metadata": {},
   "source": [
    "## 6. Thread and Block Configuration\n",
    "\n",
    "CUDA organizes threads in a hierarchy:\n",
    "\n",
    "```\n",
    "                    Grid (all threads)\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  Block 0    Block 1    Block 2  ...‚îÇ\n",
    "                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "                    ‚îÇ  ‚îÇThread‚îÇ   ‚îÇThread‚îÇ   ‚îÇThread‚îÇ    ‚îÇ\n",
    "                    ‚îÇ  ‚îÇ  0-N ‚îÇ   ‚îÇ  0-N ‚îÇ   ‚îÇ  0-N ‚îÇ    ‚îÇ\n",
    "                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Kernel Launch Syntax: `kernel[blocks_per_grid, threads_per_block](...)`\n",
    "\n",
    "**Rules of thumb:**\n",
    "- `threads_per_block`: Usually 128, 256, or 512 (must be ‚â§ 1024)\n",
    "- `blocks_per_grid`: Calculated to cover all elements\n",
    "- Total threads = blocks √ó threads_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure kernel launch parameters\n",
    "threads_per_block = 256  # Common choice\n",
    "\n",
    "# Calculate blocks needed to cover all elements\n",
    "# Formula: ceil(N / threads_per_block)\n",
    "blocks_per_grid = math.ceil(N / threads_per_block)\n",
    "\n",
    "print(f\"üìê Launch Configuration:\")\n",
    "print(f\"   Array size: {N:,}\")\n",
    "print(f\"   Threads per block: {threads_per_block}\")\n",
    "print(f\"   Blocks per grid: {blocks_per_grid:,}\")\n",
    "print(f\"   Total threads: {blocks_per_grid * threads_per_block:,}\")\n",
    "print(f\"   Extra threads (boundary check needed): {blocks_per_grid * threads_per_block - N:,}\")\n",
    "\n",
    "# Launch the kernel!\n",
    "vector_add_kernel[blocks_per_grid, threads_per_block](a_device, b_device, c_device)\n",
    "\n",
    "# Wait for GPU to finish\n",
    "cuda.synchronize()\n",
    "print(\"\\n‚úÖ Kernel execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbeef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy result back to CPU and verify\n",
    "c_host = c_device.copy_to_host()\n",
    "\n",
    "# Verify correctness\n",
    "expected = a_host + b_host\n",
    "if np.allclose(c_host, expected):\n",
    "    print(\"‚úÖ VERIFICATION PASSED!\")\n",
    "    print(f\"   First 5 elements: {c_host[:5]}\")\n",
    "    print(f\"   Expected:         {expected[:5]}\")\n",
    "else:\n",
    "    print(\"‚ùå VERIFICATION FAILED!\")\n",
    "    diff = np.abs(c_host - expected).max()\n",
    "    print(f\"   Max difference: {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43906758",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison: CPU vs GPU\n",
    "\n",
    "Now let's see the real benefit of GPU computing - speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccefc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_cpu_gpu(sizes):\n",
    "    \"\"\"Compare CPU and GPU performance across different array sizes\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for N in sizes:\n",
    "        # Create data\n",
    "        a = np.random.randn(N).astype(np.float32)\n",
    "        b = np.random.randn(N).astype(np.float32)\n",
    "        \n",
    "        # CPU timing\n",
    "        start = time.perf_counter()\n",
    "        c_cpu = a + b\n",
    "        cpu_time = time.perf_counter() - start\n",
    "        \n",
    "        # GPU timing (including transfers)\n",
    "        start = time.perf_counter()\n",
    "        a_d = cuda.to_device(a)\n",
    "        b_d = cuda.to_device(b)\n",
    "        c_d = cuda.device_array(N, dtype=np.float32)\n",
    "        \n",
    "        tpb = 256\n",
    "        bpg = math.ceil(N / tpb)\n",
    "        vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
    "        cuda.synchronize()\n",
    "        c_gpu = c_d.copy_to_host()\n",
    "        gpu_time = time.perf_counter() - start\n",
    "        \n",
    "        speedup = cpu_time / gpu_time if gpu_time > 0 else 0\n",
    "        results.append((N, cpu_time*1000, gpu_time*1000, speedup))\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "sizes = [1_000, 10_000, 100_000, 1_000_000, 10_000_000, 50_000_000]\n",
    "print(\"üèÅ Benchmarking CPU vs GPU...\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Array Size':>12} | {'CPU (ms)':>10} | {'GPU (ms)':>10} | {'Speedup':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "results = benchmark_cpu_gpu(sizes)\n",
    "for N, cpu_ms, gpu_ms, speedup in results:\n",
    "    indicator = \"üöÄ\" if speedup > 1 else \"üê¢\"\n",
    "    print(f\"{N:>12,} | {cpu_ms:>10.3f} | {gpu_ms:>10.3f} | {speedup:>9.2f}x {indicator}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(\"\\nüí° Note: GPU shines with larger arrays (overhead amortized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa7cd85",
   "metadata": {},
   "source": [
    "## üéØ Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to solidify your understanding.\n",
    "\n",
    "### Exercise 1: Vector Subtraction\n",
    "Modify the vector addition kernel to perform subtraction (c = a - b).\n",
    "\n",
    "### Exercise 2: Element-wise Multiplication  \n",
    "Create a new kernel for element-wise multiplication (c = a * b).\n",
    "\n",
    "### Exercise 3: Different Block Sizes\n",
    "Experiment with different `threads_per_block` values (64, 128, 256, 512, 1024).\n",
    "Which performs best? Why might that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 1: Vector Subtraction\n",
    "# Create a kernel that computes c = a - b\n",
    "\n",
    "@cuda.jit\n",
    "def vector_sub_kernel(a, b, c):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < c.size:\n",
    "        # TODO: Replace pass with subtraction\n",
    "        pass\n",
    "\n",
    "# Test your kernel here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fec6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 2: Element-wise Multiplication\n",
    "# Create a kernel that computes c = a * b\n",
    "\n",
    "@cuda.jit\n",
    "def vector_mul_kernel(a, b, c):\n",
    "    # TODO: Implement this kernel\n",
    "    pass\n",
    "\n",
    "# Test your kernel here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 3: Block Size Experiment\n",
    "# Try different threads_per_block values and compare performance\n",
    "\n",
    "def benchmark_block_sizes(N=10_000_000):\n",
    "    \"\"\"Test different block sizes\"\"\"\n",
    "    a = np.random.randn(N).astype(np.float32)\n",
    "    b = np.random.randn(N).astype(np.float32)\n",
    "    a_d = cuda.to_device(a)\n",
    "    b_d = cuda.to_device(b)\n",
    "    c_d = cuda.device_array(N, dtype=np.float32)\n",
    "    \n",
    "    block_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "    \n",
    "    print(f\"Testing with N = {N:,}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for tpb in block_sizes:\n",
    "        bpg = math.ceil(N / tpb)\n",
    "        \n",
    "        # Warmup\n",
    "        vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(100):\n",
    "            vector_add_kernel[bpg, tpb](a_d, b_d, c_d)\n",
    "        cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / 100 * 1000\n",
    "        \n",
    "        print(f\"Block size {tpb:4d}: {elapsed:.3f} ms\")\n",
    "\n",
    "# TODO: Run the benchmark and analyze results\n",
    "# benchmark_block_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93678fce",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "### Today You Learned:\n",
    "\n",
    "1. **GPUs vs CPUs**: GPUs excel at throughput (many simple operations), CPUs at latency (complex single operations)\n",
    "\n",
    "2. **GPU Architecture**:\n",
    "   - Streaming Multiprocessors (SMs) contain many CUDA cores\n",
    "   - Warps are groups of 32 threads executing together\n",
    "   - Compute capability indicates architecture generation\n",
    "\n",
    "3. **CUDA Programming Model**:\n",
    "   - Kernels run on GPU, host code runs on CPU\n",
    "   - Threads organized into blocks, blocks into grids\n",
    "   - Each thread gets a unique index via `cuda.grid()`\n",
    "\n",
    "4. **Memory Management**:\n",
    "   - Data must be explicitly transferred between host and device\n",
    "   - `cuda.to_device()` copies to GPU\n",
    "   - `array.copy_to_host()` copies back\n",
    "\n",
    "5. **Performance Considerations**:\n",
    "   - GPU overhead matters for small arrays\n",
    "   - GPU wins big for large parallel workloads\n",
    "   - Block size affects performance (experiment!)\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Up: Day 2 - Thread Indexing Deep Dive\n",
    "- 1D, 2D, and 3D thread indexing\n",
    "- Grid-stride loops for arbitrary sizes\n",
    "- Handling edge cases\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Additional Resources\n",
    "- [CUDA Programming Guide - Introduction](../../cuda-programming-guide/01-introduction/programming-model.md)\n",
    "- [Quick Reference](../../notes/cuda-quick-reference.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
