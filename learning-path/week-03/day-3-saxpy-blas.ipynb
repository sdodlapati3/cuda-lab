{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c235653a",
   "metadata": {},
   "source": [
    "# üöÄ Day 3: SAXPY & BLAS Level-1 Operations\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-03/day-3-saxpy-blas.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand BLAS levels and operations\n",
    "- Implement SAXPY: y = Œ±x + y\n",
    "- Implement DOT product, SCAL, AXPY\n",
    "- Analyze memory bandwidth\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3064af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Introduction to BLAS\n",
    "\n",
    "### What is BLAS?\n",
    "\n",
    "**BLAS** (Basic Linear Algebra Subprograms) is a specification for low-level linear algebra operations.\n",
    "\n",
    "```\n",
    "BLAS Levels:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Level 1: Vector-Vector operations  O(n)\n",
    "  ‚Ä¢ AXPY:  y = Œ±x + y\n",
    "  ‚Ä¢ DOT:   Œ± = x¬∑y\n",
    "  ‚Ä¢ SCAL:  x = Œ±x\n",
    "  ‚Ä¢ NRM2:  Œ± = ||x||‚ÇÇ\n",
    "\n",
    "Level 2: Matrix-Vector operations  O(n¬≤)\n",
    "  ‚Ä¢ GEMV:  y = Œ±Ax + Œ≤y\n",
    "  ‚Ä¢ SYMV:  y = Œ±Ax + Œ≤y (A symmetric)\n",
    "\n",
    "Level 3: Matrix-Matrix operations  O(n¬≥)\n",
    "  ‚Ä¢ GEMM:  C = Œ±AB + Œ≤C\n",
    "  ‚Ä¢ SYMM:  C = Œ±AB + Œ≤C (A or B symmetric)\n",
    "```\n",
    "\n",
    "### Naming Convention\n",
    "\n",
    "```\n",
    "S = Single precision (float32)\n",
    "D = Double precision (float64)\n",
    "C = Complex single\n",
    "Z = Complex double\n",
    "\n",
    "Examples:\n",
    "  SAXPY = Single-precision A*X Plus Y\n",
    "  DGEMM = Double-precision GEneral Matrix Multiply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1468bb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: SAXPY - The Hello World of GPU Computing\n",
    "\n",
    "### The Operation\n",
    "\n",
    "$$y_i = \\alpha \\cdot x_i + y_i$$\n",
    "\n",
    "SAXPY is special because:\n",
    "1. **Simple**: One multiply, one add per element\n",
    "2. **Memory-bound**: More memory traffic than compute\n",
    "3. **Benchmark**: Used to measure memory bandwidth\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)\n",
    "\n",
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f023ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile saxpy.cu\n",
    "// saxpy.cu - The classic GPU benchmark\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// SAXPY: y = alpha * x + y\n",
    "__global__ void saxpy(float alpha, const float* x, float* y, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        y[i] = alpha * x[i] + y[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    float alpha = 2.0f;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_x = (float*)malloc(size);\n",
    "    float *h_y = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_x[i] = 1.0f;\n",
    "        h_y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_x, *d_y;\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMalloc(&d_y, size);\n",
    "    \n",
    "    // Copy to device\n",
    "    cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_y, h_y, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch SAXPY\n",
    "    int threads = 256;\n",
    "    int blocks = 256;\n",
    "    saxpy<<<blocks, threads>>>(alpha, d_x, d_y, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Copy back and verify\n",
    "    cudaMemcpy(h_y, d_y, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // y[0] should be: 2.0 * 1.0 + 2.0 = 4.0\n",
    "    printf(\"y[0] = %f (expected 4.0)\\n\", h_y[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_x); cudaFree(d_y);\n",
    "    free(h_x); free(h_y);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o saxpy saxpy.cu\n",
    "!./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile blas_level1.cu\n",
    "// blas_level1.cu - Other BLAS Level-1 Operations\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// SCAL: x = alpha * x\n",
    "__global__ void scal(float alpha, float* x, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        x[i] = alpha * x[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// AXPY: y = alpha * x + y (same as SAXPY)\n",
    "__global__ void axpy(float alpha, const float* x, float* y, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        y[i] = alpha * x[i] + y[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// DOT: result = sum(x[i] * y[i]) - requires reduction!\n",
    "// See Week 4 for proper implementation\n",
    "__global__ void dot_partial(const float* x, const float* y, float* partial, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        sum += x[i] * y[i];\n",
    "    }\n",
    "    partial[tid] = sum;  // Needs reduction to complete\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    float alpha = 2.0f;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_x = (float*)malloc(size);\n",
    "    for (int i = 0; i < n; i++) h_x[i] = 3.0f;\n",
    "    \n",
    "    float *d_x;\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threads = 256, blocks = 256;\n",
    "    \n",
    "    // Test SCAL\n",
    "    scal<<<blocks, threads>>>(alpha, d_x, n);\n",
    "    cudaMemcpy(h_x, d_x, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"SCAL: 3.0 * 2.0 = %f (expected 6.0)\\n\", h_x[0]);\n",
    "    \n",
    "    cudaFree(d_x);\n",
    "    free(h_x);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61776015",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o blas_level1 blas_level1.cu\n",
    "!./blas_level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da470e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for interactive testing\n",
    "@cuda.jit\n",
    "def saxpy(alpha, x, y, n):\n",
    "    \"\"\"\n",
    "    SAXPY: y = alpha * x + y\n",
    "    Modifies y in-place.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = alpha * x[i] + y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SAXPY\n",
    "n = 1_000_000\n",
    "alpha = 2.0\n",
    "\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "y_original = y.copy()\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "\n",
    "result = d_y.copy_to_host()\n",
    "expected = alpha * x + y_original\n",
    "\n",
    "print(f\"SAXPY: y = {alpha} * x + y\")\n",
    "print(f\"N = {n:,}\")\n",
    "print(f\"Correct: {np.allclose(result, expected)}\")\n",
    "print(f\"\\nSample results (first 5):\")\n",
    "print(f\"  x:        {x[:5]}\")\n",
    "print(f\"  y (orig): {y_original[:5]}\")\n",
    "print(f\"  y (new):  {result[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400494b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Bandwidth Analysis\n",
    "\n",
    "### SAXPY Memory Traffic\n",
    "\n",
    "```\n",
    "For each element:\n",
    "  Read:  x[i]   ‚Üí 4 bytes\n",
    "  Read:  y[i]   ‚Üí 4 bytes\n",
    "  Write: y[i]   ‚Üí 4 bytes\n",
    "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "  Total:          12 bytes\n",
    "\n",
    "For N elements: 12 * N bytes\n",
    "\n",
    "Arithmetic Intensity = FLOPs / Bytes\n",
    "                     = 2 / 12\n",
    "                     = 0.167 FLOPs/byte\n",
    "\n",
    "This is VERY low ‚Üí memory bandwidth bound!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_saxpy(n, iterations=100):\n",
    "    \"\"\"Benchmark SAXPY and calculate effective bandwidth.\"\"\"\n",
    "    alpha = 2.0\n",
    "    x = np.random.rand(n).astype(np.float32)\n",
    "    y = np.random.rand(n).astype(np.float32)\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # Warmup\n",
    "    saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "    cuda.synchronize()\n",
    "    elapsed = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    bytes_transferred = n * 12  # 3 arrays √ó 4 bytes (read x, read y, write y)\n",
    "    bandwidth_gb_s = (bytes_transferred / elapsed) / 1e9\n",
    "    \n",
    "    return elapsed * 1000, bandwidth_gb_s  # ms, GB/s\n",
    "\n",
    "print(f\"{'N':>12} | {'Time (ms)':>10} | {'Bandwidth (GB/s)':>16}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for n in [100_000, 1_000_000, 10_000_000, 100_000_000]:\n",
    "    time_ms, bw = benchmark_saxpy(n)\n",
    "    print(f\"{n:>12,} | {time_ms:>10.3f} | {bw:>16.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ab367",
   "metadata": {},
   "source": [
    "### Interpreting Bandwidth Results\n",
    "\n",
    "```\n",
    "GPU Memory Bandwidth (theoretical peak):\n",
    "  T4:    320 GB/s\n",
    "  V100:  900 GB/s\n",
    "  A100: 1555 GB/s (HBM2e)\n",
    "  H100: 3350 GB/s (HBM3)\n",
    "\n",
    "Achievable: Usually 70-85% of peak\n",
    "\n",
    "If your measured bandwidth is:\n",
    "  > 70% of peak ‚Üí Well optimized\n",
    "  50-70% of peak ‚Üí Room for improvement\n",
    "  < 50% of peak ‚Üí Check for issues\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143e858",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Other BLAS Level-1 Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCAL: x = alpha * x\n",
    "@cuda.jit\n",
    "def sscal(alpha, x, n):\n",
    "    \"\"\"Scale vector: x = alpha * x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        x[i] = alpha * x[i]\n",
    "\n",
    "# COPY: y = x\n",
    "@cuda.jit\n",
    "def scopy(x, y, n):\n",
    "    \"\"\"Copy vector: y = x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = x[i]\n",
    "\n",
    "# SWAP: swap x and y\n",
    "@cuda.jit\n",
    "def sswap(x, y, n):\n",
    "    \"\"\"Swap vectors: x, y = y, x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        temp = x[i]\n",
    "        x[i] = y[i]\n",
    "        y[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOT product (partial - needs reduction for full result)\n",
    "@cuda.jit\n",
    "def sdot_partial(x, y, partial_sums, n):\n",
    "    \"\"\"\n",
    "    Partial dot product: each thread computes partial sum.\n",
    "    Full reduction needed to get final result.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(tid, n, stride):\n",
    "        local_sum += x[i] * y[i]\n",
    "    \n",
    "    partial_sums[tid] = local_sum\n",
    "\n",
    "def sdot(x, y):\n",
    "    \"\"\"Complete dot product implementation.\"\"\"\n",
    "    n = len(x)\n",
    "    blocks, threads = 256, 256\n",
    "    total_threads = blocks * threads\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "    \n",
    "    sdot_partial[blocks, threads](d_x, d_y, d_partial, n)\n",
    "    \n",
    "    # Final reduction on CPU (we'll learn GPU reduction in Week 4)\n",
    "    partial = d_partial.copy_to_host()\n",
    "    return partial.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BLAS operations\n",
    "n = 100_000\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Test SCAL\n",
    "d_x = cuda.to_device(x.copy())\n",
    "sscal[blocks, threads](2.0, d_x, n)\n",
    "result = d_x.copy_to_host()\n",
    "print(f\"SCAL (x *= 2): {'‚úì' if np.allclose(result, x * 2) else '‚úó'}\")\n",
    "\n",
    "# Test COPY\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.device_array(n, dtype=np.float32)\n",
    "scopy[blocks, threads](d_x, d_y, n)\n",
    "result = d_y.copy_to_host()\n",
    "print(f\"COPY (y = x): {'‚úì' if np.allclose(result, x) else '‚úó'}\")\n",
    "\n",
    "# Test SWAP\n",
    "d_x = cuda.to_device(x.copy())\n",
    "d_y = cuda.to_device(y.copy())\n",
    "sswap[blocks, threads](d_x, d_y, n)\n",
    "result_x = d_x.copy_to_host()\n",
    "result_y = d_y.copy_to_host()\n",
    "print(f\"SWAP: {'‚úì' if np.allclose(result_x, y) and np.allclose(result_y, x) else '‚úó'}\")\n",
    "\n",
    "# Test DOT\n",
    "gpu_dot = sdot(x, y)\n",
    "cpu_dot = np.dot(x, y)\n",
    "print(f\"DOT: {'‚úì' if np.isclose(gpu_dot, cpu_dot, rtol=1e-4) else '‚úó'} (GPU: {gpu_dot:.4f}, CPU: {cpu_dot:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7449df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: NRM2 - Vector Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def snrm2_partial(x, partial_sums, n):\n",
    "    \"\"\"\n",
    "    Partial L2 norm squared: each thread computes sum of squares.\n",
    "    Need to sqrt the final sum.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(tid, n, stride):\n",
    "        local_sum += x[i] * x[i]\n",
    "    \n",
    "    partial_sums[tid] = local_sum\n",
    "\n",
    "def snrm2(x):\n",
    "    \"\"\"Complete L2 norm: ||x||_2 = sqrt(sum(x_i^2))\"\"\"\n",
    "    n = len(x)\n",
    "    blocks, threads = 256, 256\n",
    "    total_threads = blocks * threads\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "    \n",
    "    snrm2_partial[blocks, threads](d_x, d_partial, n)\n",
    "    \n",
    "    partial = d_partial.copy_to_host()\n",
    "    return np.sqrt(partial.sum())\n",
    "\n",
    "# Test\n",
    "x = np.random.rand(100_000).astype(np.float32)\n",
    "gpu_norm = snrm2(x)\n",
    "cpu_norm = np.linalg.norm(x)\n",
    "\n",
    "print(f\"L2 Norm:\")\n",
    "print(f\"  GPU: {gpu_norm:.6f}\")\n",
    "print(f\"  CPU: {cpu_norm:.6f}\")\n",
    "print(f\"  Match: {'‚úì' if np.isclose(gpu_norm, cpu_norm, rtol=1e-4) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec643cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Extended AXPY Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6974ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AXPBY: y = alpha*x + beta*y (more general)\n",
    "@cuda.jit\n",
    "def saxpby(alpha, x, beta, y, n):\n",
    "    \"\"\"y = alpha*x + beta*y\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = alpha * x[i] + beta * y[i]\n",
    "\n",
    "# WAXPBY: w = alpha*x + beta*y (output to separate array)\n",
    "@cuda.jit\n",
    "def swaxpby(alpha, x, beta, y, w, n):\n",
    "    \"\"\"w = alpha*x + beta*y\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        w[i] = alpha * x[i] + beta * y[i]\n",
    "\n",
    "# Triple AXPY: y = a1*x1 + a2*x2 + a3*x3\n",
    "@cuda.jit\n",
    "def saxpy3(a1, x1, a2, x2, a3, x3, y, n):\n",
    "    \"\"\"y = a1*x1 + a2*x2 + a3*x3\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = a1 * x1[i] + a2 * x2[i] + a3 * x3[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4123c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AXPBY\n",
    "n = 100_000\n",
    "alpha, beta = 2.0, 0.5\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "y_orig = y.copy()\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "saxpby[256, 256](alpha, d_x, beta, d_y, n)\n",
    "result = d_y.copy_to_host()\n",
    "expected = alpha * x + beta * y_orig\n",
    "\n",
    "print(f\"AXPBY (y = {alpha}*x + {beta}*y): {'‚úì' if np.allclose(result, expected) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd811e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Building a Vector Library Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a57a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUDAVector:\n",
    "    \"\"\"CUDA-accelerated vector operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, blocks=256, threads=256):\n",
    "        self.blocks = blocks\n",
    "        self.threads = threads\n",
    "    \n",
    "    def _get_device_arrays(self, *arrays):\n",
    "        \"\"\"Convert numpy arrays to device arrays if needed.\"\"\"\n",
    "        result = []\n",
    "        for arr in arrays:\n",
    "            if isinstance(arr, np.ndarray):\n",
    "                result.append(cuda.to_device(arr))\n",
    "            else:\n",
    "                result.append(arr)\n",
    "        return result\n",
    "    \n",
    "    def axpy(self, alpha, x, y):\n",
    "        \"\"\"y = alpha*x + y (in-place)\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        saxpy[self.blocks, self.threads](alpha, d_x, d_y, n)\n",
    "        return d_y\n",
    "    \n",
    "    def axpby(self, alpha, x, beta, y):\n",
    "        \"\"\"y = alpha*x + beta*y (in-place)\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        saxpby[self.blocks, self.threads](alpha, d_x, beta, d_y, n)\n",
    "        return d_y\n",
    "    \n",
    "    def scal(self, alpha, x):\n",
    "        \"\"\"x = alpha*x (in-place)\"\"\"\n",
    "        d_x, = self._get_device_arrays(x)\n",
    "        n = len(x)\n",
    "        sscal[self.blocks, self.threads](alpha, d_x, n)\n",
    "        return d_x\n",
    "    \n",
    "    def dot(self, x, y):\n",
    "        \"\"\"Return x¬∑y\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        total_threads = self.blocks * self.threads\n",
    "        d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "        \n",
    "        sdot_partial[self.blocks, self.threads](d_x, d_y, d_partial, n)\n",
    "        return d_partial.copy_to_host().sum()\n",
    "    \n",
    "    def nrm2(self, x):\n",
    "        \"\"\"Return ||x||_2\"\"\"\n",
    "        d_x, = self._get_device_arrays(x)\n",
    "        n = len(x)\n",
    "        total_threads = self.blocks * self.threads\n",
    "        d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "        \n",
    "        snrm2_partial[self.blocks, self.threads](d_x, d_partial, n)\n",
    "        return np.sqrt(d_partial.copy_to_host().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vector library\n",
    "vec = CUDAVector()\n",
    "\n",
    "n = 100_000\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "# Test all operations\n",
    "print(\"CUDAVector Library Tests:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# DOT\n",
    "gpu_dot = vec.dot(x, y)\n",
    "cpu_dot = np.dot(x, y)\n",
    "print(f\"dot(x, y): {'‚úì' if np.isclose(gpu_dot, cpu_dot, rtol=1e-4) else '‚úó'}\")\n",
    "\n",
    "# NRM2\n",
    "gpu_norm = vec.nrm2(x)\n",
    "cpu_norm = np.linalg.norm(x)\n",
    "print(f\"nrm2(x):   {'‚úì' if np.isclose(gpu_norm, cpu_norm, rtol=1e-4) else '‚úó'}\")\n",
    "\n",
    "# SCAL\n",
    "x_copy = x.copy()\n",
    "d_x = vec.scal(3.0, x_copy)\n",
    "result = d_x.copy_to_host()\n",
    "print(f\"scal(3,x): {'‚úì' if np.allclose(result, x * 3) else '‚úó'}\")\n",
    "\n",
    "# AXPY\n",
    "y_copy = y.copy()\n",
    "d_y = vec.axpy(2.0, x, y_copy)\n",
    "result = d_y.copy_to_host()\n",
    "print(f\"axpy:      {'‚úì' if np.allclose(result, 2*x + y) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5ac27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: ASUM - Sum of Absolute Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement SASUM: sum of |x_i|\n",
    "@cuda.jit\n",
    "def sasum_partial(x, partial_sums, n):\n",
    "    \"\"\"Compute sum of absolute values.\"\"\"\n",
    "    # Hint: Use math.fabs(x[i])\n",
    "    pass\n",
    "\n",
    "def sasum(x):\n",
    "    \"\"\"Return sum(|x_i|)\"\"\"\n",
    "    # Your implementation\n",
    "    pass\n",
    "\n",
    "# Test: x = [-1, 2, -3, 4, -5]\n",
    "# Expected: 1 + 2 + 3 + 4 + 5 = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345d153",
   "metadata": {},
   "source": [
    "### Exercise 2: IAMAX - Index of Maximum Absolute Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find index of max |x_i|\n",
    "# This is tricky with parallelism - think about how to do it!\n",
    "\n",
    "# Hint: Each thread finds max in its range, then combine\n",
    "@cuda.jit\n",
    "def isamax_partial(x, partial_max_vals, partial_max_idx, n):\n",
    "    \"\"\"Each thread finds local max abs value and its index.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: x = [1, -5, 3, -2]\n",
    "# Expected: index 1 (value -5, |‚àí5| = 5 is largest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196755e6",
   "metadata": {},
   "source": [
    "### Exercise 3: Batch AXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply AXPY to multiple vectors at once\n",
    "# Given: X (M x N matrix), Y (M x N matrix)\n",
    "# Compute: Y[i] = alpha * X[i] + Y[i] for each row i\n",
    "\n",
    "@cuda.jit\n",
    "def batch_saxpy(alpha, X, Y, M, N):\n",
    "    \"\"\"\n",
    "    Apply SAXPY to each row of X and Y.\n",
    "    X, Y are M√óN matrices.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with 10 vectors of length 1000 each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0848d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### BLAS Level-1 Operations\n",
    "\n",
    "| Operation | Formula | Memory (bytes/element) | FLOPs/element |\n",
    "|-----------|---------|------------------------|---------------|\n",
    "| SCAL | x = Œ±x | 8 (read+write) | 1 |\n",
    "| COPY | y = x | 8 (read+write) | 0 |\n",
    "| AXPY | y = Œ±x + y | 12 (2 read + 1 write) | 2 |\n",
    "| DOT | Œ± = x¬∑y | 8 (2 reads) | 2 |\n",
    "| NRM2 | Œ± = ‚Äñx‚Äñ‚ÇÇ | 4 (1 read) | 2 |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **BLAS Level-1 is memory-bound**: Low arithmetic intensity\n",
    "2. **SAXPY is the benchmark**: Measures memory bandwidth\n",
    "3. **Reduction operations need special handling**: (Week 4 topic)\n",
    "4. **Professional libraries use these patterns**: cuBLAS, etc.\n",
    "\n",
    "### Performance Formula\n",
    "\n",
    "```\n",
    "Effective Bandwidth = Bytes Transferred / Time\n",
    "\n",
    "For SAXPY:\n",
    "  Bandwidth = (3 √ó N √ó sizeof(float)) / Time\n",
    "            = (12 √ó N) / Time  [bytes/second]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7fe50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "üìã **Day 4:** Fused operations - combining multiple operations into single kernels for better performance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
