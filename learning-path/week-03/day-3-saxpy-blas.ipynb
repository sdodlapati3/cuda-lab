{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c235653a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"CUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3064af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Introduction to BLAS\n",
    "\n",
    "### What is BLAS?\n",
    "\n",
    "**BLAS** (Basic Linear Algebra Subprograms) is a specification for low-level linear algebra operations.\n",
    "\n",
    "```\n",
    "BLAS Levels:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Level 1: Vector-Vector operations  O(n)\n",
    "  â€¢ AXPY:  y = Î±x + y\n",
    "  â€¢ DOT:   Î± = xÂ·y\n",
    "  â€¢ SCAL:  x = Î±x\n",
    "  â€¢ NRM2:  Î± = ||x||â‚‚\n",
    "\n",
    "Level 2: Matrix-Vector operations  O(nÂ²)\n",
    "  â€¢ GEMV:  y = Î±Ax + Î²y\n",
    "  â€¢ SYMV:  y = Î±Ax + Î²y (A symmetric)\n",
    "\n",
    "Level 3: Matrix-Matrix operations  O(nÂ³)\n",
    "  â€¢ GEMM:  C = Î±AB + Î²C\n",
    "  â€¢ SYMM:  C = Î±AB + Î²C (A or B symmetric)\n",
    "```\n",
    "\n",
    "### Naming Convention\n",
    "\n",
    "```\n",
    "S = Single precision (float32)\n",
    "D = Double precision (float64)\n",
    "C = Complex single\n",
    "Z = Complex double\n",
    "\n",
    "Examples:\n",
    "  SAXPY = Single-precision A*X Plus Y\n",
    "  DGEMM = Double-precision GEneral Matrix Multiply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1468bb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: SAXPY - The Hello World of GPU Computing\n",
    "\n",
    "### The Operation\n",
    "\n",
    "$$y_i = \\alpha \\cdot x_i + y_i$$\n",
    "\n",
    "SAXPY is special because:\n",
    "1. **Simple**: One multiply, one add per element\n",
    "2. **Memory-bound**: More memory traffic than compute\n",
    "3. **Benchmark**: Used to measure memory bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da470e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def saxpy(alpha, x, y, n):\n",
    "    \"\"\"\n",
    "    SAXPY: y = alpha * x + y\n",
    "    \n",
    "    Modifies y in-place.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = alpha * x[i] + y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SAXPY\n",
    "n = 1_000_000\n",
    "alpha = 2.0\n",
    "\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "y_original = y.copy()\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "\n",
    "result = d_y.copy_to_host()\n",
    "expected = alpha * x + y_original\n",
    "\n",
    "print(f\"SAXPY: y = {alpha} * x + y\")\n",
    "print(f\"N = {n:,}\")\n",
    "print(f\"Correct: {np.allclose(result, expected)}\")\n",
    "print(f\"\\nSample results (first 5):\")\n",
    "print(f\"  x:        {x[:5]}\")\n",
    "print(f\"  y (orig): {y_original[:5]}\")\n",
    "print(f\"  y (new):  {result[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400494b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Bandwidth Analysis\n",
    "\n",
    "### SAXPY Memory Traffic\n",
    "\n",
    "```\n",
    "For each element:\n",
    "  Read:  x[i]   â†’ 4 bytes\n",
    "  Read:  y[i]   â†’ 4 bytes\n",
    "  Write: y[i]   â†’ 4 bytes\n",
    "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Total:          12 bytes\n",
    "\n",
    "For N elements: 12 * N bytes\n",
    "\n",
    "Arithmetic Intensity = FLOPs / Bytes\n",
    "                     = 2 / 12\n",
    "                     = 0.167 FLOPs/byte\n",
    "\n",
    "This is VERY low â†’ memory bandwidth bound!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_saxpy(n, iterations=100):\n",
    "    \"\"\"Benchmark SAXPY and calculate effective bandwidth.\"\"\"\n",
    "    alpha = 2.0\n",
    "    x = np.random.rand(n).astype(np.float32)\n",
    "    y = np.random.rand(n).astype(np.float32)\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # Warmup\n",
    "    saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "    cuda.synchronize()\n",
    "    elapsed = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    bytes_transferred = n * 12  # 3 arrays Ã— 4 bytes (read x, read y, write y)\n",
    "    bandwidth_gb_s = (bytes_transferred / elapsed) / 1e9\n",
    "    \n",
    "    return elapsed * 1000, bandwidth_gb_s  # ms, GB/s\n",
    "\n",
    "print(f\"{'N':>12} | {'Time (ms)':>10} | {'Bandwidth (GB/s)':>16}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for n in [100_000, 1_000_000, 10_000_000, 100_000_000]:\n",
    "    time_ms, bw = benchmark_saxpy(n)\n",
    "    print(f\"{n:>12,} | {time_ms:>10.3f} | {bw:>16.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ab367",
   "metadata": {},
   "source": [
    "### Interpreting Bandwidth Results\n",
    "\n",
    "```\n",
    "GPU Memory Bandwidth (theoretical peak):\n",
    "  T4:    320 GB/s\n",
    "  V100:  900 GB/s\n",
    "  A100: 1555 GB/s (HBM2e)\n",
    "  H100: 3350 GB/s (HBM3)\n",
    "\n",
    "Achievable: Usually 70-85% of peak\n",
    "\n",
    "If your measured bandwidth is:\n",
    "  > 70% of peak â†’ Well optimized\n",
    "  50-70% of peak â†’ Room for improvement\n",
    "  < 50% of peak â†’ Check for issues\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143e858",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Other BLAS Level-1 Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCAL: x = alpha * x\n",
    "@cuda.jit\n",
    "def sscal(alpha, x, n):\n",
    "    \"\"\"Scale vector: x = alpha * x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        x[i] = alpha * x[i]\n",
    "\n",
    "# COPY: y = x\n",
    "@cuda.jit\n",
    "def scopy(x, y, n):\n",
    "    \"\"\"Copy vector: y = x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = x[i]\n",
    "\n",
    "# SWAP: swap x and y\n",
    "@cuda.jit\n",
    "def sswap(x, y, n):\n",
    "    \"\"\"Swap vectors: x, y = y, x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        temp = x[i]\n",
    "        x[i] = y[i]\n",
    "        y[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOT product (partial - needs reduction for full result)\n",
    "@cuda.jit\n",
    "def sdot_partial(x, y, partial_sums, n):\n",
    "    \"\"\"\n",
    "    Partial dot product: each thread computes partial sum.\n",
    "    Full reduction needed to get final result.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(tid, n, stride):\n",
    "        local_sum += x[i] * y[i]\n",
    "    \n",
    "    partial_sums[tid] = local_sum\n",
    "\n",
    "def sdot(x, y):\n",
    "    \"\"\"Complete dot product implementation.\"\"\"\n",
    "    n = len(x)\n",
    "    blocks, threads = 256, 256\n",
    "    total_threads = blocks * threads\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "    \n",
    "    sdot_partial[blocks, threads](d_x, d_y, d_partial, n)\n",
    "    \n",
    "    # Final reduction on CPU (we'll learn GPU reduction in Week 4)\n",
    "    partial = d_partial.copy_to_host()\n",
    "    return partial.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BLAS operations\n",
    "n = 100_000\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Test SCAL\n",
    "d_x = cuda.to_device(x.copy())\n",
    "sscal[blocks, threads](2.0, d_x, n)\n",
    "result = d_x.copy_to_host()\n",
    "print(f\"SCAL (x *= 2): {'âœ“' if np.allclose(result, x * 2) else 'âœ—'}\")\n",
    "\n",
    "# Test COPY\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.device_array(n, dtype=np.float32)\n",
    "scopy[blocks, threads](d_x, d_y, n)\n",
    "result = d_y.copy_to_host()\n",
    "print(f\"COPY (y = x): {'âœ“' if np.allclose(result, x) else 'âœ—'}\")\n",
    "\n",
    "# Test SWAP\n",
    "d_x = cuda.to_device(x.copy())\n",
    "d_y = cuda.to_device(y.copy())\n",
    "sswap[blocks, threads](d_x, d_y, n)\n",
    "result_x = d_x.copy_to_host()\n",
    "result_y = d_y.copy_to_host()\n",
    "print(f\"SWAP: {'âœ“' if np.allclose(result_x, y) and np.allclose(result_y, x) else 'âœ—'}\")\n",
    "\n",
    "# Test DOT\n",
    "gpu_dot = sdot(x, y)\n",
    "cpu_dot = np.dot(x, y)\n",
    "print(f\"DOT: {'âœ“' if np.isclose(gpu_dot, cpu_dot, rtol=1e-4) else 'âœ—'} (GPU: {gpu_dot:.4f}, CPU: {cpu_dot:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7449df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: NRM2 - Vector Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def snrm2_partial(x, partial_sums, n):\n",
    "    \"\"\"\n",
    "    Partial L2 norm squared: each thread computes sum of squares.\n",
    "    Need to sqrt the final sum.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(tid, n, stride):\n",
    "        local_sum += x[i] * x[i]\n",
    "    \n",
    "    partial_sums[tid] = local_sum\n",
    "\n",
    "def snrm2(x):\n",
    "    \"\"\"Complete L2 norm: ||x||_2 = sqrt(sum(x_i^2))\"\"\"\n",
    "    n = len(x)\n",
    "    blocks, threads = 256, 256\n",
    "    total_threads = blocks * threads\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "    \n",
    "    snrm2_partial[blocks, threads](d_x, d_partial, n)\n",
    "    \n",
    "    partial = d_partial.copy_to_host()\n",
    "    return np.sqrt(partial.sum())\n",
    "\n",
    "# Test\n",
    "x = np.random.rand(100_000).astype(np.float32)\n",
    "gpu_norm = snrm2(x)\n",
    "cpu_norm = np.linalg.norm(x)\n",
    "\n",
    "print(f\"L2 Norm:\")\n",
    "print(f\"  GPU: {gpu_norm:.6f}\")\n",
    "print(f\"  CPU: {cpu_norm:.6f}\")\n",
    "print(f\"  Match: {'âœ“' if np.isclose(gpu_norm, cpu_norm, rtol=1e-4) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec643cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Extended AXPY Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6974ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AXPBY: y = alpha*x + beta*y (more general)\n",
    "@cuda.jit\n",
    "def saxpby(alpha, x, beta, y, n):\n",
    "    \"\"\"y = alpha*x + beta*y\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = alpha * x[i] + beta * y[i]\n",
    "\n",
    "# WAXPBY: w = alpha*x + beta*y (output to separate array)\n",
    "@cuda.jit\n",
    "def swaxpby(alpha, x, beta, y, w, n):\n",
    "    \"\"\"w = alpha*x + beta*y\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        w[i] = alpha * x[i] + beta * y[i]\n",
    "\n",
    "# Triple AXPY: y = a1*x1 + a2*x2 + a3*x3\n",
    "@cuda.jit\n",
    "def saxpy3(a1, x1, a2, x2, a3, x3, y, n):\n",
    "    \"\"\"y = a1*x1 + a2*x2 + a3*x3\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = a1 * x1[i] + a2 * x2[i] + a3 * x3[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4123c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AXPBY\n",
    "n = 100_000\n",
    "alpha, beta = 2.0, 0.5\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "y_orig = y.copy()\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "saxpby[256, 256](alpha, d_x, beta, d_y, n)\n",
    "result = d_y.copy_to_host()\n",
    "expected = alpha * x + beta * y_orig\n",
    "\n",
    "print(f\"AXPBY (y = {alpha}*x + {beta}*y): {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd811e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Building a Vector Library Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a57a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUDAVector:\n",
    "    \"\"\"CUDA-accelerated vector operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, blocks=256, threads=256):\n",
    "        self.blocks = blocks\n",
    "        self.threads = threads\n",
    "    \n",
    "    def _get_device_arrays(self, *arrays):\n",
    "        \"\"\"Convert numpy arrays to device arrays if needed.\"\"\"\n",
    "        result = []\n",
    "        for arr in arrays:\n",
    "            if isinstance(arr, np.ndarray):\n",
    "                result.append(cuda.to_device(arr))\n",
    "            else:\n",
    "                result.append(arr)\n",
    "        return result\n",
    "    \n",
    "    def axpy(self, alpha, x, y):\n",
    "        \"\"\"y = alpha*x + y (in-place)\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        saxpy[self.blocks, self.threads](alpha, d_x, d_y, n)\n",
    "        return d_y\n",
    "    \n",
    "    def axpby(self, alpha, x, beta, y):\n",
    "        \"\"\"y = alpha*x + beta*y (in-place)\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        saxpby[self.blocks, self.threads](alpha, d_x, beta, d_y, n)\n",
    "        return d_y\n",
    "    \n",
    "    def scal(self, alpha, x):\n",
    "        \"\"\"x = alpha*x (in-place)\"\"\"\n",
    "        d_x, = self._get_device_arrays(x)\n",
    "        n = len(x)\n",
    "        sscal[self.blocks, self.threads](alpha, d_x, n)\n",
    "        return d_x\n",
    "    \n",
    "    def dot(self, x, y):\n",
    "        \"\"\"Return xÂ·y\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        total_threads = self.blocks * self.threads\n",
    "        d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "        \n",
    "        sdot_partial[self.blocks, self.threads](d_x, d_y, d_partial, n)\n",
    "        return d_partial.copy_to_host().sum()\n",
    "    \n",
    "    def nrm2(self, x):\n",
    "        \"\"\"Return ||x||_2\"\"\"\n",
    "        d_x, = self._get_device_arrays(x)\n",
    "        n = len(x)\n",
    "        total_threads = self.blocks * self.threads\n",
    "        d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "        \n",
    "        snrm2_partial[self.blocks, self.threads](d_x, d_partial, n)\n",
    "        return np.sqrt(d_partial.copy_to_host().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vector library\n",
    "vec = CUDAVector()\n",
    "\n",
    "n = 100_000\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "# Test all operations\n",
    "print(\"CUDAVector Library Tests:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# DOT\n",
    "gpu_dot = vec.dot(x, y)\n",
    "cpu_dot = np.dot(x, y)\n",
    "print(f\"dot(x, y): {'âœ“' if np.isclose(gpu_dot, cpu_dot, rtol=1e-4) else 'âœ—'}\")\n",
    "\n",
    "# NRM2\n",
    "gpu_norm = vec.nrm2(x)\n",
    "cpu_norm = np.linalg.norm(x)\n",
    "print(f\"nrm2(x):   {'âœ“' if np.isclose(gpu_norm, cpu_norm, rtol=1e-4) else 'âœ—'}\")\n",
    "\n",
    "# SCAL\n",
    "x_copy = x.copy()\n",
    "d_x = vec.scal(3.0, x_copy)\n",
    "result = d_x.copy_to_host()\n",
    "print(f\"scal(3,x): {'âœ“' if np.allclose(result, x * 3) else 'âœ—'}\")\n",
    "\n",
    "# AXPY\n",
    "y_copy = y.copy()\n",
    "d_y = vec.axpy(2.0, x, y_copy)\n",
    "result = d_y.copy_to_host()\n",
    "print(f\"axpy:      {'âœ“' if np.allclose(result, 2*x + y) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5ac27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: ASUM - Sum of Absolute Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement SASUM: sum of |x_i|\n",
    "@cuda.jit\n",
    "def sasum_partial(x, partial_sums, n):\n",
    "    \"\"\"Compute sum of absolute values.\"\"\"\n",
    "    # Hint: Use math.fabs(x[i])\n",
    "    pass\n",
    "\n",
    "def sasum(x):\n",
    "    \"\"\"Return sum(|x_i|)\"\"\"\n",
    "    # Your implementation\n",
    "    pass\n",
    "\n",
    "# Test: x = [-1, 2, -3, 4, -5]\n",
    "# Expected: 1 + 2 + 3 + 4 + 5 = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345d153",
   "metadata": {},
   "source": [
    "### Exercise 2: IAMAX - Index of Maximum Absolute Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find index of max |x_i|\n",
    "# This is tricky with parallelism - think about how to do it!\n",
    "\n",
    "# Hint: Each thread finds max in its range, then combine\n",
    "@cuda.jit\n",
    "def isamax_partial(x, partial_max_vals, partial_max_idx, n):\n",
    "    \"\"\"Each thread finds local max abs value and its index.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: x = [1, -5, 3, -2]\n",
    "# Expected: index 1 (value -5, |âˆ’5| = 5 is largest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196755e6",
   "metadata": {},
   "source": [
    "### Exercise 3: Batch AXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply AXPY to multiple vectors at once\n",
    "# Given: X (M x N matrix), Y (M x N matrix)\n",
    "# Compute: Y[i] = alpha * X[i] + Y[i] for each row i\n",
    "\n",
    "@cuda.jit\n",
    "def batch_saxpy(alpha, X, Y, M, N):\n",
    "    \"\"\"\n",
    "    Apply SAXPY to each row of X and Y.\n",
    "    X, Y are MÃ—N matrices.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with 10 vectors of length 1000 each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0848d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### BLAS Level-1 Operations\n",
    "\n",
    "| Operation | Formula | Memory (bytes/element) | FLOPs/element |\n",
    "|-----------|---------|------------------------|---------------|\n",
    "| SCAL | x = Î±x | 8 (read+write) | 1 |\n",
    "| COPY | y = x | 8 (read+write) | 0 |\n",
    "| AXPY | y = Î±x + y | 12 (2 read + 1 write) | 2 |\n",
    "| DOT | Î± = xÂ·y | 8 (2 reads) | 2 |\n",
    "| NRM2 | Î± = â€–xâ€–â‚‚ | 4 (1 read) | 2 |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **BLAS Level-1 is memory-bound**: Low arithmetic intensity\n",
    "2. **SAXPY is the benchmark**: Measures memory bandwidth\n",
    "3. **Reduction operations need special handling**: (Week 4 topic)\n",
    "4. **Professional libraries use these patterns**: cuBLAS, etc.\n",
    "\n",
    "### Performance Formula\n",
    "\n",
    "```\n",
    "Effective Bandwidth = Bytes Transferred / Time\n",
    "\n",
    "For SAXPY:\n",
    "  Bandwidth = (3 Ã— N Ã— sizeof(float)) / Time\n",
    "            = (12 Ã— N) / Time  [bytes/second]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7fe50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "ðŸ“‹ **Day 4:** Fused operations - combining multiple operations into single kernels for better performance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
