{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c235653a",
   "metadata": {},
   "source": [
    "# ðŸš€ Day 3: SAXPY & BLAS Level-1 Operations\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-03/day-3-saxpy-blas.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ The Challenge\n",
    "\n",
    "*Why do NVIDIA, AMD, and Intel all spend millions optimizing a function that just computes `y = a*x + y`? What makes SAXPY the universal GPU benchmark?*\n",
    "\n",
    "The answer lies in understanding **memory bandwidth** â€” the true limiting factor for most GPU operations. Today we learn to think like library engineers and benchmark like hardware designers.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "| Objective | Skill Level |\n",
    "|-----------|-------------|\n",
    "| Understand BLAS levels and standard operations | Understand |\n",
    "| Implement SAXPY, DOT, SCAL, and AXPY operations | Apply |\n",
    "| Calculate theoretical memory bandwidth | Analyze |\n",
    "| Evaluate when to write custom kernels vs. use libraries | Evaluate |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ºï¸ Session Roadmap\n",
    "\n",
    "| Part | Topic | Duration |\n",
    "|------|-------|----------|\n",
    "| 1 | Introduction to BLAS | 10 min |\n",
    "| 2 | SAXPY - The Benchmark | 15 min |\n",
    "| 3 | Other BLAS Level-1 Operations | 15 min |\n",
    "| 4 | Bandwidth Analysis | 10 min |\n",
    "| 5 | Exercises | 10 min |\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ðŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3064af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Introduction to BLAS\n",
    "\n",
    "> ðŸ’¡ **Concept Card: BLAS as the Gold Standard**\n",
    "> \n",
    "> ```\n",
    "> ðŸ† BLAS - THE BENCHMARK FOR NUMERICAL COMPUTING\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   BLAS = Basic Linear Algebra Subprograms\n",
    ">   \n",
    ">   THE GOLD STANDARD because:\n",
    ">   âœ“ Defined in 1979, battle-tested for 45+ years\n",
    ">   âœ“ Every vendor optimizes their implementation\n",
    ">   âœ“ Portable interface, optimized internals\n",
    ">   âœ“ Basis for LAPACK, NumPy, PyTorch, TensorFlow\n",
    ">   \n",
    ">   WHEN TO USE LIBRARIES (cuBLAS):\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   âœ… Standard operations (GEMM, GEMV, etc.)\n",
    ">   âœ… Production code needing reliability\n",
    ">   âœ… When vendor-optimized is faster than DIY\n",
    ">   \n",
    ">   WHEN TO WRITE CUSTOM KERNELS:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   âœ… Fusion opportunities (combine ops)\n",
    ">   âœ… Non-standard operations\n",
    ">   âœ… Learning and understanding\n",
    ">   âœ… When library overhead matters\n",
    ">   \n",
    ">   TODAY: We learn BOTH to make informed decisions!\n",
    ">   \n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **The Key Insight:** Understanding BLAS teaches you how to think about performance â€” even if you end up using the library!\n",
    "\n",
    "### BLAS Levels\n",
    "\n",
    "```\n",
    "BLAS Levels:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Level 1: Vector-Vector operations  O(n)\n",
    "  â€¢ AXPY:  y = Î±x + y\n",
    "  â€¢ DOT:   Î± = xÂ·y\n",
    "  â€¢ SCAL:  x = Î±x\n",
    "  â€¢ NRM2:  Î± = ||x||â‚‚\n",
    "\n",
    "Level 2: Matrix-Vector operations  O(nÂ²)\n",
    "  â€¢ GEMV:  y = Î±Ax + Î²y\n",
    "  â€¢ SYMV:  y = Î±Ax + Î²y (A symmetric)\n",
    "\n",
    "Level 3: Matrix-Matrix operations  O(nÂ³)\n",
    "  â€¢ GEMM:  C = Î±AB + Î²C\n",
    "  â€¢ SYMM:  C = Î±AB + Î²C (A or B symmetric)\n",
    "```\n",
    "\n",
    "### Naming Convention\n",
    "\n",
    "```\n",
    "S = Single precision (float32)\n",
    "D = Double precision (float64)\n",
    "C = Complex single\n",
    "Z = Complex double\n",
    "\n",
    "Examples:\n",
    "  SAXPY = Single-precision A*X Plus Y\n",
    "  DGEMM = Double-precision GEneral Matrix Multiply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1468bb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: SAXPY - The Hello World of GPU Computing\n",
    "\n",
    "### The Operation\n",
    "\n",
    "$$y_i = \\alpha \\cdot x_i + y_i$$\n",
    "\n",
    "> ðŸ’¡ **Concept Card: Why SAXPY is THE Benchmark**\n",
    "> \n",
    "> ```\n",
    "> ðŸ“Š SAXPY: THE MEMORY BANDWIDTH BENCHMARK\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   y = Î± Ã— x + y  (for each element)\n",
    ">   \n",
    ">   MEMORY TRAFFIC ANALYSIS:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   â€¢ Read x[i]:   4 bytes (float32)\n",
    ">   â€¢ Read y[i]:   4 bytes (float32)\n",
    ">   â€¢ Write y[i]:  4 bytes (float32)\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   Total:         12 bytes per element\n",
    ">   \n",
    ">   COMPUTE:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   â€¢ 1 multiply (Î± Ã— x[i])\n",
    ">   â€¢ 1 add (result + y[i])\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   Total:         2 FLOPs per element\n",
    ">   \n",
    ">   ARITHMETIC INTENSITY:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   2 FLOPs Ã· 12 bytes = 0.17 FLOPs/byte\n",
    ">   \n",
    ">   âš ï¸ MEMORY-BOUND! (GPUs can do 100+ FLOPs/byte)\n",
    ">   \n",
    ">   WHY IT'S THE BENCHMARK:\n",
    ">   â†’ Measures raw memory bandwidth\n",
    ">   â†’ Cannot hide behind compute\n",
    ">   â†’ Honest performance indicator\n",
    ">   \n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **The Takeaway:** SAXPY performance = your GPU's memory system performance!\n",
    "\n",
    "### ðŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ðŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f023ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile saxpy.cu\n",
    "// saxpy.cu - The classic GPU benchmark\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// SAXPY: y = alpha * x + y\n",
    "__global__ void saxpy(float alpha, const float* x, float* y, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        y[i] = alpha * x[i] + y[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    float alpha = 2.0f;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_x = (float*)malloc(size);\n",
    "    float *h_y = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_x[i] = 1.0f;\n",
    "        h_y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_x, *d_y;\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMalloc(&d_y, size);\n",
    "    \n",
    "    // Copy to device\n",
    "    cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_y, h_y, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch SAXPY\n",
    "    int threads = 256;\n",
    "    int blocks = 256;\n",
    "    saxpy<<<blocks, threads>>>(alpha, d_x, d_y, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Copy back and verify\n",
    "    cudaMemcpy(h_y, d_y, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // y[0] should be: 2.0 * 1.0 + 2.0 = 4.0\n",
    "    printf(\"y[0] = %f (expected 4.0)\\n\", h_y[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_x); cudaFree(d_y);\n",
    "    free(h_x); free(h_y);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o saxpy saxpy.cu\n",
    "!./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile blas_level1.cu\n",
    "// blas_level1.cu - Other BLAS Level-1 Operations\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// SCAL: x = alpha * x\n",
    "__global__ void scal(float alpha, float* x, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        x[i] = alpha * x[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// AXPY: y = alpha * x + y (same as SAXPY)\n",
    "__global__ void axpy(float alpha, const float* x, float* y, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        y[i] = alpha * x[i] + y[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// DOT: result = sum(x[i] * y[i]) - requires reduction!\n",
    "// See Week 4 for proper implementation\n",
    "__global__ void dot_partial(const float* x, const float* y, float* partial, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        sum += x[i] * y[i];\n",
    "    }\n",
    "    partial[tid] = sum;  // Needs reduction to complete\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    float alpha = 2.0f;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_x = (float*)malloc(size);\n",
    "    for (int i = 0; i < n; i++) h_x[i] = 3.0f;\n",
    "    \n",
    "    float *d_x;\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threads = 256, blocks = 256;\n",
    "    \n",
    "    // Test SCAL\n",
    "    scal<<<blocks, threads>>>(alpha, d_x, n);\n",
    "    cudaMemcpy(h_x, d_x, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"SCAL: 3.0 * 2.0 = %f (expected 6.0)\\n\", h_x[0]);\n",
    "    \n",
    "    cudaFree(d_x);\n",
    "    free(h_x);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61776015",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o blas_level1 blas_level1.cu\n",
    "!./blas_level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da470e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for interactive testing\n",
    "@cuda.jit\n",
    "def saxpy(alpha, x, y, n):\n",
    "    \"\"\"\n",
    "    SAXPY: y = alpha * x + y\n",
    "    Modifies y in-place.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = alpha * x[i] + y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SAXPY\n",
    "n = 1_000_000\n",
    "alpha = 2.0\n",
    "\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "y_original = y.copy()\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "\n",
    "result = d_y.copy_to_host()\n",
    "expected = alpha * x + y_original\n",
    "\n",
    "print(f\"SAXPY: y = {alpha} * x + y\")\n",
    "print(f\"N = {n:,}\")\n",
    "print(f\"Correct: {np.allclose(result, expected)}\")\n",
    "print(f\"\\nSample results (first 5):\")\n",
    "print(f\"  x:        {x[:5]}\")\n",
    "print(f\"  y (orig): {y_original[:5]}\")\n",
    "print(f\"  y (new):  {result[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400494b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Other BLAS Level-1 Operations\n",
    "\n",
    "Now let's complete our BLAS Level-1 toolkit with SCAL, DOT, and AXPY operations.\n",
    "\n",
    "> ðŸ’¡ **Concept Card: BLAS Level-1 Memory Patterns**\n",
    "> \n",
    "> ```\n",
    "> ðŸ“¦ BLAS-1 OPERATIONS: Memory Traffic Analysis\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    ">   â”‚ Operationâ”‚ Formula     â”‚ Bytes/elem â”‚ FLOPs/elem â”‚\n",
    ">   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    ">   â”‚ SCAL     â”‚ x = Î±x      â”‚ 8 (R+W)    â”‚ 1          â”‚\n",
    ">   â”‚ COPY     â”‚ y = x       â”‚ 8 (R+W)    â”‚ 0          â”‚\n",
    ">   â”‚ AXPY     â”‚ y = Î±x + y  â”‚ 12 (2R+W)  â”‚ 2          â”‚\n",
    ">   â”‚ DOT      â”‚ Î± = xÂ·y     â”‚ 8 (2R)     â”‚ 2          â”‚\n",
    ">   â”‚ NRM2     â”‚ Î± = â€–xâ€–â‚‚    â”‚ 4 (1R)     â”‚ 2          â”‚\n",
    ">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    ">   \n",
    ">   ALL ARE MEMORY-BOUND! (Arithmetic intensity < 1)\n",
    ">   \n",
    ">   âš ï¸ DOT and NRM2 require REDUCTION (Week 4 topic)\n",
    ">   \n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **Notice:** All Level-1 operations are memory-bound. The GPU's compute units are mostly waiting for data!\n",
    "\n",
    "### ðŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ðŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_saxpy(n, iterations=100):\n",
    "    \"\"\"Benchmark SAXPY and calculate effective bandwidth.\"\"\"\n",
    "    alpha = 2.0\n",
    "    x = np.random.rand(n).astype(np.float32)\n",
    "    y = np.random.rand(n).astype(np.float32)\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # Warmup\n",
    "    saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        saxpy[blocks, threads](alpha, d_x, d_y, n)\n",
    "    cuda.synchronize()\n",
    "    elapsed = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    bytes_transferred = n * 12  # 3 arrays Ã— 4 bytes (read x, read y, write y)\n",
    "    bandwidth_gb_s = (bytes_transferred / elapsed) / 1e9\n",
    "    \n",
    "    return elapsed * 1000, bandwidth_gb_s  # ms, GB/s\n",
    "\n",
    "print(f\"{'N':>12} | {'Time (ms)':>10} | {'Bandwidth (GB/s)':>16}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for n in [100_000, 1_000_000, 10_000_000, 100_000_000]:\n",
    "    time_ms, bw = benchmark_saxpy(n)\n",
    "    print(f\"{n:>12,} | {time_ms:>10.3f} | {bw:>16.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ab367",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Bandwidth Analysis\n",
    "\n",
    "Let's measure and understand the actual memory bandwidth we're achieving.\n",
    "\n",
    "### The Bandwidth Formula\n",
    "\n",
    "```\n",
    "Effective Bandwidth = Bytes Transferred / Time\n",
    "\n",
    "For SAXPY:\n",
    "  Bandwidth = (3 Ã— N Ã— sizeof(float)) / Time\n",
    "            = (12 Ã— N) / Time  [bytes/second]\n",
    "```\n",
    "\n",
    "### Typical GPU Memory Bandwidths\n",
    "\n",
    "| GPU | Theoretical Bandwidth | Achievable (~80%) |\n",
    "|-----|----------------------|-------------------|\n",
    "| RTX 3090 | 936 GB/s | ~750 GB/s |\n",
    "| A100 | 2,039 GB/s | ~1,600 GB/s |\n",
    "| RTX 4090 | 1,008 GB/s | ~800 GB/s |\n",
    "| V100 | 900 GB/s | ~720 GB/s |\n",
    "\n",
    "> **Goal:** Get as close to theoretical bandwidth as possible!\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Exercises\n",
    "\n",
    "Now it's your turn to implement BLAS-like operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143e858",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Other BLAS Level-1 Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCAL: x = alpha * x\n",
    "@cuda.jit\n",
    "def sscal(alpha, x, n):\n",
    "    \"\"\"Scale vector: x = alpha * x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        x[i] = alpha * x[i]\n",
    "\n",
    "# COPY: y = x\n",
    "@cuda.jit\n",
    "def scopy(x, y, n):\n",
    "    \"\"\"Copy vector: y = x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = x[i]\n",
    "\n",
    "# SWAP: swap x and y\n",
    "@cuda.jit\n",
    "def sswap(x, y, n):\n",
    "    \"\"\"Swap vectors: x, y = y, x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        temp = x[i]\n",
    "        x[i] = y[i]\n",
    "        y[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOT product (partial - needs reduction for full result)\n",
    "@cuda.jit\n",
    "def sdot_partial(x, y, partial_sums, n):\n",
    "    \"\"\"\n",
    "    Partial dot product: each thread computes partial sum.\n",
    "    Full reduction needed to get final result.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(tid, n, stride):\n",
    "        local_sum += x[i] * y[i]\n",
    "    \n",
    "    partial_sums[tid] = local_sum\n",
    "\n",
    "def sdot(x, y):\n",
    "    \"\"\"Complete dot product implementation.\"\"\"\n",
    "    n = len(x)\n",
    "    blocks, threads = 256, 256\n",
    "    total_threads = blocks * threads\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "    \n",
    "    sdot_partial[blocks, threads](d_x, d_y, d_partial, n)\n",
    "    \n",
    "    # Final reduction on CPU (we'll learn GPU reduction in Week 4)\n",
    "    partial = d_partial.copy_to_host()\n",
    "    return partial.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BLAS operations\n",
    "n = 100_000\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Test SCAL\n",
    "d_x = cuda.to_device(x.copy())\n",
    "sscal[blocks, threads](2.0, d_x, n)\n",
    "result = d_x.copy_to_host()\n",
    "print(f\"SCAL (x *= 2): {'âœ“' if np.allclose(result, x * 2) else 'âœ—'}\")\n",
    "\n",
    "# Test COPY\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.device_array(n, dtype=np.float32)\n",
    "scopy[blocks, threads](d_x, d_y, n)\n",
    "result = d_y.copy_to_host()\n",
    "print(f\"COPY (y = x): {'âœ“' if np.allclose(result, x) else 'âœ—'}\")\n",
    "\n",
    "# Test SWAP\n",
    "d_x = cuda.to_device(x.copy())\n",
    "d_y = cuda.to_device(y.copy())\n",
    "sswap[blocks, threads](d_x, d_y, n)\n",
    "result_x = d_x.copy_to_host()\n",
    "result_y = d_y.copy_to_host()\n",
    "print(f\"SWAP: {'âœ“' if np.allclose(result_x, y) and np.allclose(result_y, x) else 'âœ—'}\")\n",
    "\n",
    "# Test DOT\n",
    "gpu_dot = sdot(x, y)\n",
    "cpu_dot = np.dot(x, y)\n",
    "print(f\"DOT: {'âœ“' if np.isclose(gpu_dot, cpu_dot, rtol=1e-4) else 'âœ—'} (GPU: {gpu_dot:.4f}, CPU: {cpu_dot:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7449df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: NRM2 - Vector Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def snrm2_partial(x, partial_sums, n):\n",
    "    \"\"\"\n",
    "    Partial L2 norm squared: each thread computes sum of squares.\n",
    "    Need to sqrt the final sum.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(tid, n, stride):\n",
    "        local_sum += x[i] * x[i]\n",
    "    \n",
    "    partial_sums[tid] = local_sum\n",
    "\n",
    "def snrm2(x):\n",
    "    \"\"\"Complete L2 norm: ||x||_2 = sqrt(sum(x_i^2))\"\"\"\n",
    "    n = len(x)\n",
    "    blocks, threads = 256, 256\n",
    "    total_threads = blocks * threads\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "    \n",
    "    snrm2_partial[blocks, threads](d_x, d_partial, n)\n",
    "    \n",
    "    partial = d_partial.copy_to_host()\n",
    "    return np.sqrt(partial.sum())\n",
    "\n",
    "# Test\n",
    "x = np.random.rand(100_000).astype(np.float32)\n",
    "gpu_norm = snrm2(x)\n",
    "cpu_norm = np.linalg.norm(x)\n",
    "\n",
    "print(f\"L2 Norm:\")\n",
    "print(f\"  GPU: {gpu_norm:.6f}\")\n",
    "print(f\"  CPU: {cpu_norm:.6f}\")\n",
    "print(f\"  Match: {'âœ“' if np.isclose(gpu_norm, cpu_norm, rtol=1e-4) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec643cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Extended AXPY Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6974ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AXPBY: y = alpha*x + beta*y (more general)\n",
    "@cuda.jit\n",
    "def saxpby(alpha, x, beta, y, n):\n",
    "    \"\"\"y = alpha*x + beta*y\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = alpha * x[i] + beta * y[i]\n",
    "\n",
    "# WAXPBY: w = alpha*x + beta*y (output to separate array)\n",
    "@cuda.jit\n",
    "def swaxpby(alpha, x, beta, y, w, n):\n",
    "    \"\"\"w = alpha*x + beta*y\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        w[i] = alpha * x[i] + beta * y[i]\n",
    "\n",
    "# Triple AXPY: y = a1*x1 + a2*x2 + a3*x3\n",
    "@cuda.jit\n",
    "def saxpy3(a1, x1, a2, x2, a3, x3, y, n):\n",
    "    \"\"\"y = a1*x1 + a2*x2 + a3*x3\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        y[i] = a1 * x1[i] + a2 * x2[i] + a3 * x3[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4123c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AXPBY\n",
    "n = 100_000\n",
    "alpha, beta = 2.0, 0.5\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "y_orig = y.copy()\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "saxpby[256, 256](alpha, d_x, beta, d_y, n)\n",
    "result = d_y.copy_to_host()\n",
    "expected = alpha * x + beta * y_orig\n",
    "\n",
    "print(f\"AXPBY (y = {alpha}*x + {beta}*y): {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd811e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Building a Vector Library Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a57a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUDAVector:\n",
    "    \"\"\"CUDA-accelerated vector operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, blocks=256, threads=256):\n",
    "        self.blocks = blocks\n",
    "        self.threads = threads\n",
    "    \n",
    "    def _get_device_arrays(self, *arrays):\n",
    "        \"\"\"Convert numpy arrays to device arrays if needed.\"\"\"\n",
    "        result = []\n",
    "        for arr in arrays:\n",
    "            if isinstance(arr, np.ndarray):\n",
    "                result.append(cuda.to_device(arr))\n",
    "            else:\n",
    "                result.append(arr)\n",
    "        return result\n",
    "    \n",
    "    def axpy(self, alpha, x, y):\n",
    "        \"\"\"y = alpha*x + y (in-place)\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        saxpy[self.blocks, self.threads](alpha, d_x, d_y, n)\n",
    "        return d_y\n",
    "    \n",
    "    def axpby(self, alpha, x, beta, y):\n",
    "        \"\"\"y = alpha*x + beta*y (in-place)\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        saxpby[self.blocks, self.threads](alpha, d_x, beta, d_y, n)\n",
    "        return d_y\n",
    "    \n",
    "    def scal(self, alpha, x):\n",
    "        \"\"\"x = alpha*x (in-place)\"\"\"\n",
    "        d_x, = self._get_device_arrays(x)\n",
    "        n = len(x)\n",
    "        sscal[self.blocks, self.threads](alpha, d_x, n)\n",
    "        return d_x\n",
    "    \n",
    "    def dot(self, x, y):\n",
    "        \"\"\"Return xÂ·y\"\"\"\n",
    "        d_x, d_y = self._get_device_arrays(x, y)\n",
    "        n = len(x)\n",
    "        total_threads = self.blocks * self.threads\n",
    "        d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "        \n",
    "        sdot_partial[self.blocks, self.threads](d_x, d_y, d_partial, n)\n",
    "        return d_partial.copy_to_host().sum()\n",
    "    \n",
    "    def nrm2(self, x):\n",
    "        \"\"\"Return ||x||_2\"\"\"\n",
    "        d_x, = self._get_device_arrays(x)\n",
    "        n = len(x)\n",
    "        total_threads = self.blocks * self.threads\n",
    "        d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "        \n",
    "        snrm2_partial[self.blocks, self.threads](d_x, d_partial, n)\n",
    "        return np.sqrt(d_partial.copy_to_host().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vector library\n",
    "vec = CUDAVector()\n",
    "\n",
    "n = 100_000\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "# Test all operations\n",
    "print(\"CUDAVector Library Tests:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# DOT\n",
    "gpu_dot = vec.dot(x, y)\n",
    "cpu_dot = np.dot(x, y)\n",
    "print(f\"dot(x, y): {'âœ“' if np.isclose(gpu_dot, cpu_dot, rtol=1e-4) else 'âœ—'}\")\n",
    "\n",
    "# NRM2\n",
    "gpu_norm = vec.nrm2(x)\n",
    "cpu_norm = np.linalg.norm(x)\n",
    "print(f\"nrm2(x):   {'âœ“' if np.isclose(gpu_norm, cpu_norm, rtol=1e-4) else 'âœ—'}\")\n",
    "\n",
    "# SCAL\n",
    "x_copy = x.copy()\n",
    "d_x = vec.scal(3.0, x_copy)\n",
    "result = d_x.copy_to_host()\n",
    "print(f\"scal(3,x): {'âœ“' if np.allclose(result, x * 3) else 'âœ—'}\")\n",
    "\n",
    "# AXPY\n",
    "y_copy = y.copy()\n",
    "d_y = vec.axpy(2.0, x, y_copy)\n",
    "result = d_y.copy_to_host()\n",
    "print(f\"axpy:      {'âœ“' if np.allclose(result, 2*x + y) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5ac27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercises\n",
    "\n",
    "### ðŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these BLAS-style exercises in CUDA C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b129e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile saxpy_exercises.cu\n",
    "// saxpy_exercises.cu - BLAS-style exercises\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: SASUM - Sum of Absolute Values\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void sasumPartial(const float* x, float* partialSums, int n) {\n",
    "    extern __shared__ float sdata[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load with absolute value\n",
    "    sdata[tid] = (idx < n) ? fabsf(x[idx]) : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Parallel reduction in shared memory\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] += sdata[tid + s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write result for this block\n",
    "    if (tid == 0) {\n",
    "        partialSums[blockIdx.x] = sdata[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "float sasum(const float* d_x, int n) {\n",
    "    int threads = 256;\n",
    "    int blocks = (n + threads - 1) / threads;\n",
    "    \n",
    "    float* d_partial;\n",
    "    CUDA_CHECK(cudaMalloc(&d_partial, blocks * sizeof(float)));\n",
    "    \n",
    "    sasumPartial<<<blocks, threads, threads * sizeof(float)>>>(d_x, d_partial, n);\n",
    "    \n",
    "    // Final reduction on CPU (or recursively on GPU for very large arrays)\n",
    "    float* h_partial = (float*)malloc(blocks * sizeof(float));\n",
    "    CUDA_CHECK(cudaMemcpy(h_partial, d_partial, blocks * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < blocks; i++) sum += h_partial[i];\n",
    "    \n",
    "    free(h_partial);\n",
    "    cudaFree(d_partial);\n",
    "    return sum;\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: ISAMAX - Index of Maximum Absolute Value\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void isamaxPartial(const float* x, float* maxVals, int* maxIdx, int n) {\n",
    "    extern __shared__ char sharedMem[];\n",
    "    float* svals = (float*)sharedMem;\n",
    "    int* sidx = (int*)(sharedMem + blockDim.x * sizeof(float));\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Initialize\n",
    "    svals[tid] = (idx < n) ? fabsf(x[idx]) : -1.0f;\n",
    "    sidx[tid] = idx;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Parallel reduction keeping track of max and index\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            if (svals[tid + s] > svals[tid]) {\n",
    "                svals[tid] = svals[tid + s];\n",
    "                sidx[tid] = sidx[tid + s];\n",
    "            }\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        maxVals[blockIdx.x] = svals[0];\n",
    "        maxIdx[blockIdx.x] = sidx[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "int isamax(const float* d_x, int n) {\n",
    "    int threads = 256;\n",
    "    int blocks = (n + threads - 1) / threads;\n",
    "    \n",
    "    float* d_maxVals;\n",
    "    int* d_maxIdx;\n",
    "    CUDA_CHECK(cudaMalloc(&d_maxVals, blocks * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc(&d_maxIdx, blocks * sizeof(int)));\n",
    "    \n",
    "    size_t sharedSize = threads * (sizeof(float) + sizeof(int));\n",
    "    isamaxPartial<<<blocks, threads, sharedSize>>>(d_x, d_maxVals, d_maxIdx, n);\n",
    "    \n",
    "    float* h_maxVals = (float*)malloc(blocks * sizeof(float));\n",
    "    int* h_maxIdx = (int*)malloc(blocks * sizeof(int));\n",
    "    CUDA_CHECK(cudaMemcpy(h_maxVals, d_maxVals, blocks * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    CUDA_CHECK(cudaMemcpy(h_maxIdx, d_maxIdx, blocks * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Final comparison on CPU\n",
    "    float maxVal = h_maxVals[0];\n",
    "    int maxIndex = h_maxIdx[0];\n",
    "    for (int i = 1; i < blocks; i++) {\n",
    "        if (h_maxVals[i] > maxVal) {\n",
    "            maxVal = h_maxVals[i];\n",
    "            maxIndex = h_maxIdx[i];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(h_maxVals); free(h_maxIdx);\n",
    "    cudaFree(d_maxVals); cudaFree(d_maxIdx);\n",
    "    return maxIndex;\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Batch SAXPY (apply to rows of matrices)\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void batchSaxpy(float alpha, const float* X, float* Y, int M, int N) {\n",
    "    // Each thread handles one element\n",
    "    int row = blockIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        int idx = row * N + col;\n",
    "        Y[idx] = alpha * X[idx] + Y[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Test harness\n",
    "// =============================================================================\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== SAXPY/BLAS Exercises ===\\n\\n\");\n",
    "    \n",
    "    // Exercise 1: SASUM\n",
    "    printf(\"Exercise 1: SASUM (Sum of Absolute Values)\\n\");\n",
    "    printf(\"-\" \"------------------------------------------\\n\");\n",
    "    {\n",
    "        float h_x[] = {-1.0f, 2.0f, -3.0f, 4.0f, -5.0f};\n",
    "        const int N = 5;\n",
    "        \n",
    "        float* d_x;\n",
    "        CUDA_CHECK(cudaMalloc(&d_x, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_x, h_x, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        float result = sasum(d_x, N);\n",
    "        printf(\"Input: [-1, 2, -3, 4, -5]\\n\");\n",
    "        printf(\"SASUM = %.0f (expected: 15)\\n\\n\", result);\n",
    "        \n",
    "        cudaFree(d_x);\n",
    "    }\n",
    "    \n",
    "    // Exercise 2: ISAMAX\n",
    "    printf(\"Exercise 2: ISAMAX (Index of Max Abs Value)\\n\");\n",
    "    printf(\"-\" \"-------------------------------------------\\n\");\n",
    "    {\n",
    "        float h_x[] = {1.0f, -5.0f, 3.0f, -2.0f};\n",
    "        const int N = 4;\n",
    "        \n",
    "        float* d_x;\n",
    "        CUDA_CHECK(cudaMalloc(&d_x, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_x, h_x, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        int result = isamax(d_x, N);\n",
    "        printf(\"Input: [1, -5, 3, -2]\\n\");\n",
    "        printf(\"ISAMAX = %d (expected: 1, value=-5, |âˆ’5|=5 is max)\\n\\n\", result);\n",
    "        \n",
    "        cudaFree(d_x);\n",
    "    }\n",
    "    \n",
    "    // Exercise 3: Batch SAXPY\n",
    "    printf(\"Exercise 3: Batch SAXPY\\n\");\n",
    "    printf(\"-\" \"-----------------------\\n\");\n",
    "    {\n",
    "        const int M = 10, N = 1000;\n",
    "        float* h_X = (float*)malloc(M * N * sizeof(float));\n",
    "        float* h_Y = (float*)malloc(M * N * sizeof(float));\n",
    "        \n",
    "        for (int i = 0; i < M * N; i++) {\n",
    "            h_X[i] = 1.0f;\n",
    "            h_Y[i] = 2.0f;\n",
    "        }\n",
    "        \n",
    "        float *d_X, *d_Y;\n",
    "        CUDA_CHECK(cudaMalloc(&d_X, M * N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMalloc(&d_Y, M * N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_X, h_X, M * N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        CUDA_CHECK(cudaMemcpy(d_Y, h_Y, M * N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        dim3 block(256);\n",
    "        dim3 grid((N + 255) / 256, M);\n",
    "        \n",
    "        batchSaxpy<<<grid, block>>>(3.0f, d_X, d_Y, M, N);\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "        \n",
    "        CUDA_CHECK(cudaMemcpy(h_Y, d_Y, M * N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Applied Y = 3*X + Y to %d vectors of length %d\\n\", M, N);\n",
    "        printf(\"Sample Y[0][0] = %.0f (expected: 3*1 + 2 = 5)\\n\", h_Y[0]);\n",
    "        \n",
    "        cudaFree(d_X); cudaFree(d_Y);\n",
    "        free(h_X); free(h_Y);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n=== All exercises complete! ===\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14442c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o saxpy_exercises saxpy_exercises.cu && ./saxpy_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd608a03",
   "metadata": {},
   "source": [
    "### ðŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: ASUM - Sum of Absolute Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement SASUM: sum of |x_i|\n",
    "@cuda.jit\n",
    "def sasum_partial(x, partial_sums, n):\n",
    "    \"\"\"Compute sum of absolute values.\"\"\"\n",
    "    # Hint: Use math.fabs(x[i])\n",
    "    pass\n",
    "\n",
    "def sasum(x):\n",
    "    \"\"\"Return sum(|x_i|)\"\"\"\n",
    "    # Your implementation\n",
    "    pass\n",
    "\n",
    "# Test: x = [-1, 2, -3, 4, -5]\n",
    "# Expected: 1 + 2 + 3 + 4 + 5 = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345d153",
   "metadata": {},
   "source": [
    "### Exercise 2: IAMAX - Index of Maximum Absolute Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find index of max |x_i|\n",
    "# This is tricky with parallelism - think about how to do it!\n",
    "\n",
    "# Hint: Each thread finds max in its range, then combine\n",
    "@cuda.jit\n",
    "def isamax_partial(x, partial_max_vals, partial_max_idx, n):\n",
    "    \"\"\"Each thread finds local max abs value and its index.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: x = [1, -5, 3, -2]\n",
    "# Expected: index 1 (value -5, |âˆ’5| = 5 is largest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196755e6",
   "metadata": {},
   "source": [
    "### Exercise 3: Batch AXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply AXPY to multiple vectors at once\n",
    "# Given: X (M x N matrix), Y (M x N matrix)\n",
    "# Compute: Y[i] = alpha * X[i] + Y[i] for each row i\n",
    "\n",
    "@cuda.jit\n",
    "def batch_saxpy(alpha, X, Y, M, N):\n",
    "    \"\"\"\n",
    "    Apply SAXPY to each row of X and Y.\n",
    "    X, Y are MÃ—N matrices.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with 10 vectors of length 1000 each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0848d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Key Takeaways\n",
    "\n",
    "### Quick Reference Card: BLAS Level-1\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  BLAS LEVEL-1 OPERATIONS                                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  OPERATION TABLE:                                               â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  â”‚ Op    â”‚ Formula     â”‚ Bytes/elem â”‚ FLOPs/elem â”‚ Notes    â”‚  â”‚\n",
    "â”‚  â”‚ SCAL  â”‚ x = Î±x      â”‚ 8 (R+W)    â”‚ 1          â”‚ In-place â”‚  â”‚\n",
    "â”‚  â”‚ COPY  â”‚ y = x       â”‚ 8 (R+W)    â”‚ 0          â”‚ Pure I/O â”‚  â”‚\n",
    "â”‚  â”‚ AXPY  â”‚ y = Î±x + y  â”‚ 12 (2R+W)  â”‚ 2          â”‚ SAXPY    â”‚  â”‚\n",
    "â”‚  â”‚ DOT   â”‚ Î± = xÂ·y     â”‚ 8 (2R)     â”‚ 2          â”‚ Reduce   â”‚  â”‚\n",
    "â”‚  â”‚ NRM2  â”‚ Î± = â€–xâ€–â‚‚    â”‚ 4 (1R)     â”‚ 2          â”‚ Reduce   â”‚  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  BANDWIDTH FORMULA:                                             â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  Effective BW = (Bytes Transferred) / Time                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  For SAXPY: BW = (3 Ã— N Ã— 4 bytes) / Time = 12N / Time          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  LIBRARY VS CUSTOM:                                             â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  cuBLAS: Use for standard ops, production code                  â”‚\n",
    "â”‚  Custom: Use for fusion, learning, non-standard ops             â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### âœ… What You Achieved Today\n",
    "\n",
    "| Skill | Status |\n",
    "|-------|--------|\n",
    "| Understand BLAS levels and standard operations | âœ… |\n",
    "| Implemented SAXPY, SCAL, AXPY | âœ… |\n",
    "| Calculated theoretical memory bandwidth | âœ… |\n",
    "| Understand when to use libraries vs custom | âœ… |\n",
    "\n",
    "### ðŸ§  Key Insights\n",
    "\n",
    "1. **BLAS Level-1 is memory-bound**: Low arithmetic intensity means we're limited by memory bandwidth\n",
    "2. **SAXPY is the benchmark**: It honestly measures memory system performance\n",
    "3. **Reduction operations need special handling**: DOT and NRM2 require techniques from Week 4\n",
    "4. **Professional libraries use these patterns**: cuBLAS, MKL, OpenBLAS all follow this standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7fe50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš€ What's Next?\n",
    "\n",
    "**Day 4: Kernel Fusion & Optimization** â€” Combining multiple operations into single kernels for massive performance gains!\n",
    "\n",
    "| Preview Topic | What You'll Learn |\n",
    "|---------------|-------------------|\n",
    "| The fusion problem | Why separate kernels waste bandwidth |\n",
    "| Fused operations | Combining ops in single kernel |\n",
    "| Memory traffic analysis | Measuring reduction in traffic |\n",
    "| Fused activations | Real-world neural network patterns |\n",
    "\n",
    "> ðŸ’¡ **Tomorrow's Hook:** What if you could do 4 operations for the memory cost of 1? That's the power of kernel fusion!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
