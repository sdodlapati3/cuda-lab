{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c838ed",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"CUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e34e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Kernel Fusion Problem\n",
    "\n",
    "### Why Fusion Matters\n",
    "\n",
    "Consider computing: `z = a*x + b*y + c`\n",
    "\n",
    "```\n",
    "Approach 1: Separate Kernels\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Kernel 1: temp1 = a * x        # Read x, Write temp1\n",
    "Kernel 2: temp2 = b * y        # Read y, Write temp2  \n",
    "Kernel 3: temp3 = temp1 + temp2 # Read temp1, temp2, Write temp3\n",
    "Kernel 4: z = temp3 + c        # Read temp3, Write z\n",
    "\n",
    "Memory traffic: MANY reads and writes!\n",
    "Kernel launches: 4 (overhead for each)\n",
    "\n",
    "Approach 2: Fused Kernel\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Kernel: z = a*x + b*y + c      # Read x, y, Write z\n",
    "\n",
    "Memory traffic: Minimal (2 reads, 1 write)\n",
    "Kernel launches: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate kernels (non-fused)\n",
    "@cuda.jit\n",
    "def scalar_multiply(a, x, out, n):\n",
    "    \"\"\"out = a * x\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a * x[i]\n",
    "\n",
    "@cuda.jit\n",
    "def vector_add(a, b, out, n):\n",
    "    \"\"\"out = a + b\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def scalar_add(x, c, out, n):\n",
    "    \"\"\"out = x + c\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = x[i] + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fused kernel\n",
    "@cuda.jit\n",
    "def fused_axpbyc(a, x, b, y, c, z, n):\n",
    "    \"\"\"z = a*x + b*y + c (all in one kernel)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        z[i] = a * x[i] + b * y[i] + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc279273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance\n",
    "n = 10_000_000\n",
    "a, b, c = 2.0, 3.0, 1.0\n",
    "\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "iterations = 100\n",
    "\n",
    "# Non-fused approach\n",
    "d_temp1 = cuda.device_array(n, dtype=np.float32)\n",
    "d_temp2 = cuda.device_array(n, dtype=np.float32)\n",
    "d_temp3 = cuda.device_array(n, dtype=np.float32)\n",
    "d_z_nonfused = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "# Warmup\n",
    "scalar_multiply[blocks, threads](a, d_x, d_temp1, n)\n",
    "scalar_multiply[blocks, threads](b, d_y, d_temp2, n)\n",
    "vector_add[blocks, threads](d_temp1, d_temp2, d_temp3, n)\n",
    "scalar_add[blocks, threads](d_temp3, c, d_z_nonfused, n)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    scalar_multiply[blocks, threads](a, d_x, d_temp1, n)\n",
    "    scalar_multiply[blocks, threads](b, d_y, d_temp2, n)\n",
    "    vector_add[blocks, threads](d_temp1, d_temp2, d_temp3, n)\n",
    "    scalar_add[blocks, threads](d_temp3, c, d_z_nonfused, n)\n",
    "cuda.synchronize()\n",
    "nonfused_time = (time.perf_counter() - start) / iterations * 1000\n",
    "\n",
    "# Fused approach\n",
    "d_z_fused = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "fused_axpbyc[blocks, threads](a, d_x, b, d_y, c, d_z_fused, n)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    fused_axpbyc[blocks, threads](a, d_x, b, d_y, c, d_z_fused, n)\n",
    "cuda.synchronize()\n",
    "fused_time = (time.perf_counter() - start) / iterations * 1000\n",
    "\n",
    "# Verify correctness\n",
    "expected = a * x + b * y + c\n",
    "result_nonfused = d_z_nonfused.copy_to_host()\n",
    "result_fused = d_z_fused.copy_to_host()\n",
    "\n",
    "print(f\"z = {a}*x + {b}*y + {c}\")\n",
    "print(f\"N = {n:,}\\n\")\n",
    "print(f\"Non-fused (4 kernels): {nonfused_time:.3f} ms\")\n",
    "print(f\"Fused (1 kernel):      {fused_time:.3f} ms\")\n",
    "print(f\"Speedup:               {nonfused_time/fused_time:.2f}x\")\n",
    "print(f\"\\nCorrectness: {'âœ“' if np.allclose(result_fused, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1402aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Traffic Analysis\n",
    "\n",
    "### Counting Memory Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_traffic(n, sizeof=4):\n",
    "    \"\"\"Analyze memory traffic for z = a*x + b*y + c\"\"\"\n",
    "    \n",
    "    # Non-fused approach\n",
    "    # Kernel 1: temp1 = a * x\n",
    "    k1_read = n * sizeof   # read x\n",
    "    k1_write = n * sizeof  # write temp1\n",
    "    \n",
    "    # Kernel 2: temp2 = b * y\n",
    "    k2_read = n * sizeof   # read y\n",
    "    k2_write = n * sizeof  # write temp2\n",
    "    \n",
    "    # Kernel 3: temp3 = temp1 + temp2\n",
    "    k3_read = 2 * n * sizeof   # read temp1, temp2\n",
    "    k3_write = n * sizeof      # write temp3\n",
    "    \n",
    "    # Kernel 4: z = temp3 + c\n",
    "    k4_read = n * sizeof   # read temp3\n",
    "    k4_write = n * sizeof  # write z\n",
    "    \n",
    "    nonfused_total = (k1_read + k1_write + k2_read + k2_write + \n",
    "                      k3_read + k3_write + k4_read + k4_write)\n",
    "    \n",
    "    # Fused approach\n",
    "    # Single kernel: z = a*x + b*y + c\n",
    "    fused_read = 2 * n * sizeof   # read x, y\n",
    "    fused_write = n * sizeof      # write z\n",
    "    fused_total = fused_read + fused_write\n",
    "    \n",
    "    return nonfused_total, fused_total\n",
    "\n",
    "n = 10_000_000\n",
    "nonfused_bytes, fused_bytes = analyze_memory_traffic(n)\n",
    "\n",
    "print(f\"Memory Traffic Analysis (N={n:,})\")\n",
    "print(f\"{'='*45}\")\n",
    "print(f\"Non-fused: {nonfused_bytes/1e9:.2f} GB\")\n",
    "print(f\"Fused:     {fused_bytes/1e9:.2f} GB\")\n",
    "print(f\"Reduction: {nonfused_bytes/fused_bytes:.1f}x less memory traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e613563",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Common Fusion Patterns\n",
    "\n",
    "### Pattern 1: Fused Multiply-Add (FMA)\n",
    "\n",
    "This is so common that GPUs have hardware support for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def fused_multiply_add(a, b, c, out, n):\n",
    "    \"\"\"out = a * b + c (element-wise FMA)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        # GPU actually has a single FMA instruction!\n",
    "        out[i] = a[i] * b[i] + c[i]\n",
    "\n",
    "# Test\n",
    "n = 1_000_000\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "b = np.random.rand(n).astype(np.float32)\n",
    "c = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "d_out = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "fused_multiply_add[256, 256](d_a, d_b, d_c, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = a * b + c\n",
    "\n",
    "print(f\"FMA (a*b + c): {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccd849",
   "metadata": {},
   "source": [
    "### Pattern 2: Activation Functions with Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a812f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def fused_scale_relu(x, scale, out, n):\n",
    "    \"\"\"out = ReLU(scale * x) = max(0, scale * x)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        val = scale * x[i]\n",
    "        out[i] = max(0.0, val)\n",
    "\n",
    "@cuda.jit\n",
    "def fused_linear_sigmoid(x, weight, bias, out, n):\n",
    "    \"\"\"out = sigmoid(weight * x + bias)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        linear = weight * x[i] + bias\n",
    "        out[i] = 1.0 / (1.0 + math.exp(-linear))\n",
    "\n",
    "@cuda.jit\n",
    "def fused_batchnorm_relu(x, mean, var, gamma, beta, epsilon, out, n):\n",
    "    \"\"\"BatchNorm + ReLU in one kernel.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        # Normalize\n",
    "        normalized = (x[i] - mean) / math.sqrt(var + epsilon)\n",
    "        # Scale and shift\n",
    "        bn_out = gamma * normalized + beta\n",
    "        # ReLU\n",
    "        out[i] = max(0.0, bn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: separate vs fused BatchNorm+ReLU\n",
    "\n",
    "@cuda.jit\n",
    "def batchnorm_only(x, mean, var, gamma, beta, epsilon, out, n):\n",
    "    \"\"\"BatchNorm without ReLU.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        normalized = (x[i] - mean) / math.sqrt(var + epsilon)\n",
    "        out[i] = gamma * normalized + beta\n",
    "\n",
    "@cuda.jit\n",
    "def relu_only(x, out, n):\n",
    "    \"\"\"ReLU only.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = max(0.0, x[i])\n",
    "\n",
    "n = 10_000_000\n",
    "x = np.random.randn(n).astype(np.float32)\n",
    "mean, var = 0.0, 1.0\n",
    "gamma, beta, epsilon = 1.0, 0.0, 1e-5\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_temp = cuda.device_array(n, dtype=np.float32)\n",
    "d_out_sep = cuda.device_array(n, dtype=np.float32)\n",
    "d_out_fused = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "iterations = 100\n",
    "\n",
    "# Separate kernels\n",
    "batchnorm_only[blocks, threads](d_x, mean, var, gamma, beta, epsilon, d_temp, n)\n",
    "relu_only[blocks, threads](d_temp, d_out_sep, n)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    batchnorm_only[blocks, threads](d_x, mean, var, gamma, beta, epsilon, d_temp, n)\n",
    "    relu_only[blocks, threads](d_temp, d_out_sep, n)\n",
    "cuda.synchronize()\n",
    "separate_time = (time.perf_counter() - start) / iterations * 1000\n",
    "\n",
    "# Fused kernel\n",
    "fused_batchnorm_relu[blocks, threads](d_x, mean, var, gamma, beta, epsilon, d_out_fused, n)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    fused_batchnorm_relu[blocks, threads](d_x, mean, var, gamma, beta, epsilon, d_out_fused, n)\n",
    "cuda.synchronize()\n",
    "fused_time = (time.perf_counter() - start) / iterations * 1000\n",
    "\n",
    "print(f\"BatchNorm + ReLU (N={n:,})\")\n",
    "print(f\"Separate (2 kernels): {separate_time:.3f} ms\")\n",
    "print(f\"Fused (1 kernel):     {fused_time:.3f} ms\")\n",
    "print(f\"Speedup:              {separate_time/fused_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdc27c",
   "metadata": {},
   "source": [
    "### Pattern 3: Polynomial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f78907",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def fused_polynomial(x, a, b, c, d, out, n):\n",
    "    \"\"\"Evaluate axÂ³ + bxÂ² + cx + d using Horner's method.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        xi = x[i]\n",
    "        # Horner's method: ((a*x + b)*x + c)*x + d\n",
    "        out[i] = ((a * xi + b) * xi + c) * xi + d\n",
    "\n",
    "# Compare with separate operations\n",
    "@cuda.jit\n",
    "def poly_step1(x, a, out, n):  # a*xÂ³\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a * x[i] * x[i] * x[i]\n",
    "\n",
    "@cuda.jit  \n",
    "def poly_step2(x, b, out, n):  # b*xÂ²\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = b * x[i] * x[i]\n",
    "\n",
    "# ... and so on for each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acac882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test polynomial evaluation\n",
    "n = 1_000_000\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "a, b, c, d = 2.0, -3.0, 1.0, 5.0\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_out = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "fused_polynomial[256, 256](d_x, a, b, c, d, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = a * x**3 + b * x**2 + c * x + d\n",
    "\n",
    "print(f\"Polynomial: {a}xÂ³ + {b}xÂ² + {c}x + {d}\")\n",
    "print(f\"Correct: {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828e1a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Advanced Fusion - Multi-Input Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5254d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def fused_weighted_average_3(x1, w1, x2, w2, x3, w3, out, n):\n",
    "    \"\"\"Weighted average of 3 vectors: (w1*x1 + w2*x2 + w3*x3) / (w1+w2+w3)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    weight_sum = w1 + w2 + w3\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = (w1 * x1[i] + w2 * x2[i] + w3 * x3[i]) / weight_sum\n",
    "\n",
    "@cuda.jit\n",
    "def fused_softmax_cross_entropy(logits, targets, loss, n):\n",
    "    \"\"\"\n",
    "    Simplified softmax cross-entropy for binary classification.\n",
    "    loss = -sum(target * log(sigmoid(logit)) + (1-target) * log(1-sigmoid(logit)))\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        logit = logits[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Numerically stable sigmoid\n",
    "        if logit >= 0:\n",
    "            prob = 1.0 / (1.0 + math.exp(-logit))\n",
    "        else:\n",
    "            exp_logit = math.exp(logit)\n",
    "            prob = exp_logit / (1.0 + exp_logit)\n",
    "        \n",
    "        # Cross-entropy (avoid log(0))\n",
    "        eps = 1e-7\n",
    "        prob = max(eps, min(1 - eps, prob))\n",
    "        loss[i] = -(target * math.log(prob) + (1 - target) * math.log(1 - prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce540536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weighted average\n",
    "n = 100_000\n",
    "x1 = np.random.rand(n).astype(np.float32)\n",
    "x2 = np.random.rand(n).astype(np.float32)\n",
    "x3 = np.random.rand(n).astype(np.float32)\n",
    "w1, w2, w3 = 0.5, 0.3, 0.2\n",
    "\n",
    "d_x1 = cuda.to_device(x1)\n",
    "d_x2 = cuda.to_device(x2)\n",
    "d_x3 = cuda.to_device(x3)\n",
    "d_out = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "fused_weighted_average_3[256, 256](d_x1, w1, d_x2, w2, d_x3, w3, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = (w1 * x1 + w2 * x2 + w3 * x3) / (w1 + w2 + w3)\n",
    "\n",
    "print(f\"Weighted Average: {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7736bac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: When NOT to Fuse\n",
    "\n",
    "### Fusion Trade-offs\n",
    "\n",
    "```\n",
    "âœ“ DO Fuse When:\n",
    "â€¢ Operations are element-wise (same index)\n",
    "â€¢ Intermediate results are only used once\n",
    "â€¢ Memory bandwidth is the bottleneck\n",
    "â€¢ Operations are simple (not register-heavy)\n",
    "\n",
    "âœ— DON'T Fuse When:\n",
    "â€¢ Intermediate results needed elsewhere\n",
    "â€¢ Fused kernel becomes too complex\n",
    "â€¢ Register pressure becomes too high\n",
    "â€¢ Operations have different access patterns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sometimes intermediate results are needed\n",
    "\n",
    "def compute_with_intermediate(x, y):\n",
    "    \"\"\"Case where intermediate result is needed.\"\"\"\n",
    "    # z1 = x + y (needed for both z2 AND for debugging/logging)\n",
    "    # z2 = z1 * 2\n",
    "    # z3 = z1 + 1\n",
    "    # In this case, z1 is used twice, so we might keep it separate\n",
    "    pass\n",
    "\n",
    "# If z1 is only used for z2, then fuse: z2 = (x + y) * 2\n",
    "@cuda.jit\n",
    "def can_fuse(x, y, z2, n):\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        z2[i] = (x[i] + y[i]) * 2  # No intermediate storage\n",
    "\n",
    "# If z1 is used multiple times, maybe keep separate\n",
    "@cuda.jit\n",
    "def keep_separate(x, y, z1, n):\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        z1[i] = x[i] + y[i]  # Store intermediate\n",
    "\n",
    "# Then z1 can be used by multiple downstream kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0704b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Comprehensive Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fusion_benchmark(n, iterations=50):\n",
    "    \"\"\"Compare fused vs non-fused for various operation chains.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    x = np.random.rand(n).astype(np.float32)\n",
    "    y = np.random.rand(n).astype(np.float32)\n",
    "    z = np.random.rand(n).astype(np.float32)\n",
    "    \n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    d_z = cuda.to_device(z)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # Test 1: a*x + b*y + c\n",
    "    d_temp1 = cuda.device_array(n, dtype=np.float32)\n",
    "    d_temp2 = cuda.device_array(n, dtype=np.float32)\n",
    "    d_out = cuda.device_array(n, dtype=np.float32)\n",
    "    \n",
    "    # Non-fused\n",
    "    cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        scalar_multiply[blocks, threads](2.0, d_x, d_temp1, n)\n",
    "        scalar_multiply[blocks, threads](3.0, d_y, d_temp2, n)\n",
    "        vector_add[blocks, threads](d_temp1, d_temp2, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    nonfused = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # Fused\n",
    "    @cuda.jit\n",
    "    def fused_2axpy(a, x, b, y, out, n):\n",
    "        tid = cuda.grid(1)\n",
    "        stride = cuda.gridsize(1)\n",
    "        for i in range(tid, n, stride):\n",
    "            out[i] = a * x[i] + b * y[i]\n",
    "    \n",
    "    cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        fused_2axpy[blocks, threads](2.0, d_x, 3.0, d_y, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    fused = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    results.append(('a*x + b*y', 3, nonfused, fused))\n",
    "    \n",
    "    # Test 2: sqrt(x*x + y*y)  (distance)\n",
    "    @cuda.jit\n",
    "    def nonfused_dist_step1(x, out, n):\n",
    "        tid = cuda.grid(1)\n",
    "        stride = cuda.gridsize(1)\n",
    "        for i in range(tid, n, stride):\n",
    "            out[i] = x[i] * x[i]\n",
    "    \n",
    "    @cuda.jit\n",
    "    def nonfused_dist_sqrt(x, out, n):\n",
    "        tid = cuda.grid(1)\n",
    "        stride = cuda.gridsize(1)\n",
    "        for i in range(tid, n, stride):\n",
    "            out[i] = math.sqrt(x[i])\n",
    "    \n",
    "    cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        nonfused_dist_step1[blocks, threads](d_x, d_temp1, n)\n",
    "        nonfused_dist_step1[blocks, threads](d_y, d_temp2, n)\n",
    "        vector_add[blocks, threads](d_temp1, d_temp2, d_out, n)\n",
    "        nonfused_dist_sqrt[blocks, threads](d_out, d_temp1, n)\n",
    "    cuda.synchronize()\n",
    "    nonfused = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    @cuda.jit\n",
    "    def fused_distance(x, y, out, n):\n",
    "        tid = cuda.grid(1)\n",
    "        stride = cuda.gridsize(1)\n",
    "        for i in range(tid, n, stride):\n",
    "            out[i] = math.sqrt(x[i]*x[i] + y[i]*y[i])\n",
    "    \n",
    "    cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        fused_distance[blocks, threads](d_x, d_y, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    fused = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    results.append(('sqrt(xÂ²+yÂ²)', 4, nonfused, fused))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "n = 10_000_000\n",
    "results = run_fusion_benchmark(n)\n",
    "\n",
    "print(f\"\\nFusion Benchmark (N={n:,})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Operation':<20} | {'Kernels':<8} | {'Non-fused':<10} | {'Fused':<10} | {'Speedup':<8}\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "for op, kernels, nf_time, f_time in results:\n",
    "    speedup = nf_time / f_time\n",
    "    print(f\"{op:<20} | {kernels:<8} | {nf_time:<10.3f} | {f_time:<10.3f} | {speedup:<8.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7edb1c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Fused Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9587f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement fused min-max normalization\n",
    "# normalized = (x - min) / (max - min)\n",
    "# Given pre-computed min and max values\n",
    "\n",
    "@cuda.jit\n",
    "def fused_minmax_normalize(x, min_val, max_val, out, n):\n",
    "    \"\"\"Normalize x to [0, 1] using pre-computed min and max.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: x = [0, 5, 10], min=0, max=10\n",
    "# Expected: [0, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ecddb",
   "metadata": {},
   "source": [
    "### Exercise 2: Fused Gradient Descent Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient descent update with momentum\n",
    "# velocity = momentum * velocity - lr * gradient\n",
    "# weights = weights + velocity\n",
    "\n",
    "@cuda.jit\n",
    "def fused_sgd_momentum(weights, gradients, velocity, lr, momentum, n):\n",
    "    \"\"\"SGD with momentum update (modifies weights and velocity in-place).\"\"\"\n",
    "    pass\n",
    "\n",
    "# This fuses 4 operations into 1 kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4610a08",
   "metadata": {},
   "source": [
    "### Exercise 3: Fused Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement fused brightness + contrast adjustment\n",
    "# output = clamp((input - 0.5) * contrast + 0.5 + brightness, 0, 1)\n",
    "\n",
    "@cuda.jit\n",
    "def fused_brightness_contrast(image, brightness, contrast, out, n):\n",
    "    \"\"\"Apply brightness and contrast adjustment in one kernel.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with various brightness/contrast values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459571b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Kernel Fusion Guidelines\n",
    "\n",
    "```\n",
    "1. IDENTIFY fusion opportunities:\n",
    "   â€¢ Chains of element-wise operations\n",
    "   â€¢ Same data accessed multiple times\n",
    "   â€¢ Temporary arrays only used once\n",
    "\n",
    "2. MEASURE before and after:\n",
    "   â€¢ Time both approaches\n",
    "   â€¢ Calculate memory traffic reduction\n",
    "   â€¢ Check correctness!\n",
    "\n",
    "3. BALANCE complexity:\n",
    "   â€¢ Readable code matters\n",
    "   â€¢ Too much fusion â†’ register pressure\n",
    "   â€¢ Document what's fused\n",
    "```\n",
    "\n",
    "### Memory Traffic Reduction\n",
    "\n",
    "| Chain Length | Non-fused Reads/Writes | Fused Reads/Writes | Reduction |\n",
    "|--------------|------------------------|--------------------|-----------|\n",
    "| 2 ops | 4N | 2N | 2x |\n",
    "| 3 ops | 6N | 2N | 3x |\n",
    "| 4 ops | 8N | 2N | 4x |\n",
    "| k ops | 2kN | 2N | kx |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Fusion dramatically reduces memory traffic**\n",
    "2. **Fewer kernel launches = less overhead**\n",
    "3. **GPUs are memory-bound for simple ops**\n",
    "4. **Fusion is free performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59b4a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Week 3 Complete! ðŸŽ‰\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "| Day | Topic | Key Skills |\n",
    "|-----|-------|------------|\n",
    "| 1 | Grid-Stride Loops | Handle any size, professional patterns |\n",
    "| 2 | Element-wise Ops | Math functions, benchmarking |\n",
    "| 3 | SAXPY & BLAS | Memory bandwidth, standard ops |\n",
    "| 4 | Fused Operations | Reduce traffic, combine kernels |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "ðŸ“‹ **Day 5:** Complete the checkpoint quiz\n",
    "\n",
    "ðŸ“‹ **Week 4 Preview:** Reduction & Atomics\n",
    "- Parallel reduction patterns\n",
    "- Warp-level primitives\n",
    "- Atomic operations\n",
    "- Histogram computation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
