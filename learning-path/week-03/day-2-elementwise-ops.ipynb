{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc4f2d2",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 2: Element-wise Vector Operations\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-03/day-2-elementwise-ops.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Challenge\n",
    "\n",
    "*You have 10 million data points and need to apply the same operation to each one. CPU takes seconds... but the GPU should finish in milliseconds. What makes this problem perfectly suited for GPUs?*\n",
    "\n",
    "Element-wise operations are the **embarrassingly parallel** workhorses of GPU computing. When every element can be processed independently, we unlock the GPU's full potential!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "| Objective | Skill Level |\n",
    "|-----------|-------------|\n",
    "| Implement basic arithmetic operations on vectors | Apply |\n",
    "| Apply transcendental math functions (sqrt, exp, log, trig) | Apply |\n",
    "| Build neural network activation functions (ReLU, sigmoid, tanh) | Create |\n",
    "| Combine operations efficiently with grid-stride loops | Apply |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ Session Roadmap\n",
    "\n",
    "| Part | Topic | Duration |\n",
    "|------|-------|----------|\n",
    "| 1 | Basic Arithmetic Operations | 10 min |\n",
    "| 2 | Math Functions | 15 min |\n",
    "| 3 | Activation Functions | 15 min |\n",
    "| 4 | Combined Operations | 10 min |\n",
    "| 5 | Exercises | 10 min |\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7bc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021e9ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Basic Arithmetic Operations\n",
    "\n",
    "> ğŸ’¡ **Concept Card: Embarrassingly Parallel Operations**\n",
    "> \n",
    "> ```\n",
    "> ğŸ¯ EMBARRASSINGLY PARALLEL = PERFECTLY GPU-FRIENDLY\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   What makes an operation \"embarrassingly parallel\"?\n",
    ">   \n",
    ">   âœ… NO dependencies between elements\n",
    ">   âœ… NO communication between threads needed\n",
    ">   âœ… NO shared data modified by multiple threads\n",
    ">   \n",
    ">   ELEMENT-WISE OPERATIONS:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   Input A:  [aâ‚€] [aâ‚] [aâ‚‚] [aâ‚ƒ] ... [aâ‚™]\n",
    ">   Input B:  [bâ‚€] [bâ‚] [bâ‚‚] [bâ‚ƒ] ... [bâ‚™]\n",
    ">              â†“    â†“    â†“    â†“   ...  â†“\n",
    ">   Output:  [câ‚€] [câ‚] [câ‚‚] [câ‚ƒ] ... [câ‚™]\n",
    ">   \n",
    ">   Thread 0 computes câ‚€ = f(aâ‚€, bâ‚€)  â† Independent!\n",
    ">   Thread 1 computes câ‚ = f(aâ‚, bâ‚)  â† Independent!\n",
    ">   Thread 2 computes câ‚‚ = f(aâ‚‚, bâ‚‚)  â† Independent!\n",
    ">   ...\n",
    ">   \n",
    ">   EXAMPLES:\n",
    ">   â€¢ Vector add:     c[i] = a[i] + b[i]\n",
    ">   â€¢ Scalar multiply: c[i] = Î± Ã— a[i]\n",
    ">   â€¢ Element-wise:   c[i] = a[i] Ã— b[i]  (Hadamard)\n",
    ">   â€¢ Math functions: c[i] = sin(a[i])\n",
    ">   \n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **Why GPUs Excel:** When threads don't need to talk to each other, we can unleash all of them simultaneously!\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile elementwise_ops.cu\n",
    "// elementwise_ops.cu - Basic vector operations\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Vector Addition\n",
    "__global__ void vectorAdd(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Vector Subtraction\n",
    "__global__ void vectorSub(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] - b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Element-wise Multiplication (Hadamard product)\n",
    "__global__ void vectorMul(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] * b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Element-wise Division\n",
    "__global__ void vectorDiv(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] / b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Scalar operations\n",
    "__global__ void scalarMul(const float* a, float scalar, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] * scalar;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate and initialize host arrays\n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_out = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device arrays\n",
    "    float *d_a, *d_b, *d_out;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_out, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch\n",
    "    int threads = 256;\n",
    "    int blocks = 256;\n",
    "    \n",
    "    vectorAdd<<<blocks, threads>>>(d_a, d_b, d_out, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"Add: %f + %f = %f\\n\", h_a[0], h_b[0], h_out[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_out);\n",
    "    free(h_a); free(h_b); free(h_out);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6edea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o elementwise_ops elementwise_ops.cu\n",
    "!./elementwise_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalents for interactive testing\n",
    "@cuda.jit\n",
    "def vector_add(a, b, out, n):\n",
    "    \"\"\"out[i] = a[i] + b[i]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def vector_sub(a, b, out, n):\n",
    "    \"\"\"out[i] = a[i] - b[i]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] - b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def vector_mul(a, b, out, n):\n",
    "    \"\"\"out[i] = a[i] * b[i]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] * b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def vector_div(a, b, out, n):\n",
    "    \"\"\"out[i] = a[i] / b[i]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] / b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic operations\n",
    "n = 1_000_000\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "b = np.random.rand(n).astype(np.float32) + 0.1  # Avoid div by zero\n",
    "out = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_out = cuda.to_device(out)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Test each operation\n",
    "ops = [\n",
    "    ('Add', vector_add, lambda a, b: a + b),\n",
    "    ('Sub', vector_sub, lambda a, b: a - b),\n",
    "    ('Mul', vector_mul, lambda a, b: a * b),\n",
    "    ('Div', vector_div, lambda a, b: a / b),\n",
    "]\n",
    "\n",
    "print(f\"Testing with {n:,} elements\\n\")\n",
    "for name, kernel, np_op in ops:\n",
    "    kernel[blocks, threads](d_a, d_b, d_out, n)\n",
    "    result = d_out.copy_to_host()\n",
    "    expected = np_op(a, b)\n",
    "    match = np.allclose(result, expected)\n",
    "    print(f\"{name}: {'âœ“' if match else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403f8c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Math Functions\n",
    "\n",
    "Excellent! You've mastered basic arithmetic. Now let's tap into the GPU's specialized math hardware.\n",
    "\n",
    "> ğŸ’¡ **Concept Card: GPU Math Unit (SFU)**\n",
    "> \n",
    "> ```\n",
    "> ğŸ§® SPECIAL FUNCTION UNIT - Dedicated Math Hardware\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   GPUs have dedicated hardware for fast approximations:\n",
    ">   \n",
    ">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    ">   â”‚ CUDA Math Functions (GPU Accelerated)          â”‚\n",
    ">   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    ">   â”‚ BASIC:    sqrtf, rsqrtf (1/âˆšx), powf           â”‚\n",
    ">   â”‚ EXP/LOG:  expf, exp2f, logf, log2f, log10f     â”‚\n",
    ">   â”‚ TRIG:     sinf, cosf, tanf                     â”‚\n",
    ">   â”‚ INVERSE:  asinf, acosf, atanf, atan2f          â”‚\n",
    ">   â”‚ HYPER:    sinhf, coshf, tanhf                  â”‚\n",
    ">   â”‚ UTILITY:  fabsf, fmodf, floorf, ceilf          â”‚\n",
    ">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    ">   \n",
    ">   PRECISION OPTIONS:\n",
    ">   â€¢ sinf()  - Full precision (slower)\n",
    ">   â€¢ __sinf() - Fast approximation (faster, less accurate)\n",
    ">   \n",
    ">   INTRINSICS (even faster):\n",
    ">   â€¢ __expf(), __logf(), __sinf(), __cosf()\n",
    ">   \n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **Pro Tip:** Use fast math for graphics/ML, full precision for scientific computing.\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e071876",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scalar_ops.cu\n",
    "// scalar_ops.cu - Scalar operations on vectors\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Scalar Add: out[i] = a[i] + scalar\n",
    "__global__ void scalarAdd(const float* a, float scalar, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] + scalar;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Scalar Multiply: out[i] = a[i] * scalar\n",
    "__global__ void scalarMul(const float* a, float scalar, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] * scalar;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Scalar Power: out[i] = a[i] ^ power\n",
    "__global__ void scalarPow(const float* a, float power, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = powf(a[i], power);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_out = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_out;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_out, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threads = 256, blocks = 256;\n",
    "    \n",
    "    // Test scalar add\n",
    "    scalarAdd<<<blocks, threads>>>(d_a, 3.0f, d_out, n);\n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"ScalarAdd: %f + 3.0 = %f\\n\", h_a[0], h_out[0]);\n",
    "    \n",
    "    // Test scalar multiply\n",
    "    scalarMul<<<blocks, threads>>>(d_a, 2.5f, d_out, n);\n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"ScalarMul: %f * 2.5 = %f\\n\", h_a[0], h_out[0]);\n",
    "    \n",
    "    // Test scalar power\n",
    "    scalarPow<<<blocks, threads>>>(d_a, 3.0f, d_out, n);\n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"ScalarPow: %f ^ 3.0 = %f\\n\", h_a[0], h_out[0]);\n",
    "    \n",
    "    cudaFree(d_a); cudaFree(d_out);\n",
    "    free(h_a); free(h_out);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o scalar_ops scalar_ops.cu\n",
    "!./scalar_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def scalar_add(a, scalar, out, n):\n",
    "    \"\"\"out[i] = a[i] + scalar\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] + scalar\n",
    "\n",
    "@cuda.jit\n",
    "def scalar_mul(a, scalar, out, n):\n",
    "    \"\"\"out[i] = a[i] * scalar\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] * scalar\n",
    "\n",
    "@cuda.jit\n",
    "def scalar_pow(a, power, out, n):\n",
    "    \"\"\"out[i] = a[i] ** power\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] ** power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scalar operations\n",
    "scalar_mul[blocks, threads](d_a, 2.5, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = a * 2.5\n",
    "print(f\"Scalar multiply by 2.5: {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")\n",
    "\n",
    "scalar_pow[blocks, threads](d_a, 2.0, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = a ** 2.0\n",
    "print(f\"Scalar power of 2: {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b05e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Activation Functions\n",
    "\n",
    "Now let's build something practical â€” the activation functions powering neural networks!\n",
    "\n",
    "> ğŸ’¡ **Concept Card: Neural Network Activations**\n",
    "> \n",
    "> ```\n",
    "> ğŸ§  ACTIVATION FUNCTIONS - The Heart of Deep Learning\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   Every neural network uses element-wise activations:\n",
    ">   \n",
    ">   ReLU (Rectified Linear Unit):\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   f(x) = max(0, x)\n",
    ">   \n",
    ">        â•±\n",
    ">       â•±\n",
    ">      â•±\n",
    ">   â”€â”€â€¢â”€â”€â”€â”€â”€â”€â”€â”€  â† \"Dead\" for x < 0\n",
    ">   \n",
    ">   Sigmoid:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   f(x) = 1 / (1 + exp(-x))\n",
    ">   \n",
    ">      â”Œâ”€â”€â”€â”€â”€ 1.0\n",
    ">     â•±\n",
    ">   â”€â€¢â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† Outputs (0, 1)\n",
    ">     \n",
    ">   Tanh:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    ">   \n",
    ">      â”Œâ”€â”€â”€â”€â”€ +1\n",
    ">     â•±\n",
    ">   â”€â€¢â”€\n",
    ">     â•²\n",
    ">      â””â”€â”€â”€â”€â”€ -1  â† Outputs (-1, +1)\n",
    ">   \n",
    ">   ALL ARE ELEMENT-WISE â†’ PERFECT FOR GPU!\n",
    ">   \n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **ML Context:** These run on every element of every layer for every sample in every batch!\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile math_functions.cu\n",
    "// math_functions.cu - Math operations on vectors\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void vectorSqrt(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = sqrtf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorExp(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = expf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorLog(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = logf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorSin(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = sinf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorCos(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = cosf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_out = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 4.0f;  // Use 4.0 for sqrt demo\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_out;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_out, size);\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threads = 256, blocks = 256;\n",
    "    \n",
    "    vectorSqrt<<<blocks, threads>>>(d_a, d_out, n);\n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"sqrt(%f) = %f\\n\", h_a[0], h_out[0]);\n",
    "    \n",
    "    // Reset input for exp\n",
    "    for (int i = 0; i < n; i++) h_a[i] = 1.0f;\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    vectorExp<<<blocks, threads>>>(d_a, d_out, n);\n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"exp(%f) = %f\\n\", h_a[0], h_out[0]);\n",
    "    \n",
    "    vectorSin<<<blocks, threads>>>(d_a, d_out, n);\n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"sin(%f) = %f\\n\", h_a[0], h_out[0]);\n",
    "    \n",
    "    cudaFree(d_a); cudaFree(d_out);\n",
    "    free(h_a); free(h_out);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95be3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o math_functions math_functions.cu\n",
    "!./math_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vector_sqrt(a, out, n):\n",
    "    \"\"\"out[i] = sqrt(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.sqrt(a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_exp(a, out, n):\n",
    "    \"\"\"out[i] = exp(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.exp(a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_log(a, out, n):\n",
    "    \"\"\"out[i] = log(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.log(a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_sin(a, out, n):\n",
    "    \"\"\"out[i] = sin(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.sin(a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_cos(a, out, n):\n",
    "    \"\"\"out[i] = cos(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.cos(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test math functions\n",
    "a_pos = np.abs(a) + 0.01  # Positive values for sqrt/log\n",
    "d_a_pos = cuda.to_device(a_pos)\n",
    "\n",
    "math_ops = [\n",
    "    ('sqrt', vector_sqrt, np.sqrt, d_a_pos, a_pos),\n",
    "    ('exp', vector_exp, np.exp, d_a, a * 0.1),  # Scale down to avoid overflow\n",
    "    ('log', vector_log, np.log, d_a_pos, a_pos),\n",
    "    ('sin', vector_sin, np.sin, d_a, a),\n",
    "    ('cos', vector_cos, np.cos, d_a, a),\n",
    "]\n",
    "\n",
    "print(\"Math function tests:\")\n",
    "for name, kernel, np_fn, d_input, h_input in math_ops:\n",
    "    kernel[blocks, threads](d_input, d_out, n)\n",
    "    result = d_out.copy_to_host()\n",
    "    expected = np_fn(h_input)\n",
    "    match = np.allclose(result, expected, rtol=1e-5)\n",
    "    print(f\"  {name}: {'âœ“' if match else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf92afe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Compound Operations\n",
    "\n",
    "### Combining Multiple Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff999805",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vector_normalize(a, out, n):\n",
    "    \"\"\"Normalize to [0, 1] assuming input in [0, max_val]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        # Sigmoid-like normalization\n",
    "        out[i] = 1.0 / (1.0 + math.exp(-a[i]))\n",
    "\n",
    "@cuda.jit\n",
    "def vector_relu(a, out, n):\n",
    "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = max(0.0, a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_leaky_relu(a, out, alpha, n):\n",
    "    \"\"\"Leaky ReLU: max(alpha*x, x)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        x = a[i]\n",
    "        out[i] = x if x > 0 else alpha * x\n",
    "\n",
    "@cuda.jit\n",
    "def vector_tanh(a, out, n):\n",
    "    \"\"\"Hyperbolic tangent activation\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.tanh(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test activation functions\n",
    "a_centered = (a - 0.5) * 4  # Values around 0\n",
    "d_a_centered = cuda.to_device(a_centered)\n",
    "\n",
    "# Sigmoid\n",
    "vector_normalize[blocks, threads](d_a_centered, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = 1 / (1 + np.exp(-a_centered))\n",
    "print(f\"Sigmoid: {'âœ“' if np.allclose(result, expected, rtol=1e-5) else 'âœ—'}\")\n",
    "\n",
    "# ReLU\n",
    "vector_relu[blocks, threads](d_a_centered, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = np.maximum(0, a_centered)\n",
    "print(f\"ReLU: {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")\n",
    "\n",
    "# Tanh\n",
    "vector_tanh[blocks, threads](d_a_centered, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = np.tanh(a_centered)\n",
    "print(f\"Tanh: {'âœ“' if np.allclose(result, expected, rtol=1e-5) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac25bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Combined Operations & Performance\n",
    "\n",
    "Now that you have a toolkit of element-wise operations, let's explore combining them efficiently.\n",
    "\n",
    "### Key Performance Insight\n",
    "\n",
    "| Operation Type | Compute Intensity | GPU Benefit |\n",
    "|----------------|-------------------|-------------|\n",
    "| Simple (add, mul) | Low | Memory-bound, ~100Ã— speedup |\n",
    "| Medium (sqrt, div) | Medium | Balanced, ~200Ã— speedup |\n",
    "| Complex (exp, sin) | High | Compute-bound, ~500Ã— speedup |\n",
    "\n",
    "> **Rule of Thumb:** More complex operations â†’ higher GPU speedups!\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Exercises\n",
    "\n",
    "Now it's your turn! Apply what you've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operation(kernel, np_op, a, b, name, iterations=100):\n",
    "    \"\"\"Benchmark GPU kernel vs NumPy.\"\"\"\n",
    "    n = len(a)\n",
    "    out = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b) if b is not None else None\n",
    "    d_out = cuda.to_device(out)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # Warmup\n",
    "    if d_b is not None:\n",
    "        kernel[blocks, threads](d_a, d_b, d_out, n)\n",
    "    else:\n",
    "        kernel[blocks, threads](d_a, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # GPU benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        if d_b is not None:\n",
    "            kernel[blocks, threads](d_a, d_b, d_out, n)\n",
    "        else:\n",
    "            kernel[blocks, threads](d_a, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    gpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # NumPy benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        if b is not None:\n",
    "            _ = np_op(a, b)\n",
    "        else:\n",
    "            _ = np_op(a)\n",
    "    numpy_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    speedup = numpy_time / gpu_time\n",
    "    return gpu_time, numpy_time, speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a400e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive benchmark\n",
    "n = 10_000_000\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "b = np.random.rand(n).astype(np.float32) + 0.1\n",
    "\n",
    "print(f\"Benchmarking with N = {n:,} elements\\n\")\n",
    "print(f\"{'Operation':<15} | {'GPU (ms)':<10} | {'NumPy (ms)':<10} | {'Speedup':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "benchmarks = [\n",
    "    ('Add', vector_add, lambda x, y: x + y, b),\n",
    "    ('Mul', vector_mul, lambda x, y: x * y, b),\n",
    "    ('Div', vector_div, lambda x, y: x / y, b),\n",
    "    ('Sqrt', vector_sqrt, np.sqrt, None),\n",
    "    ('Exp', vector_exp, np.exp, None),\n",
    "    ('Log', vector_log, np.log, None),\n",
    "    ('Sin', vector_sin, np.sin, None),\n",
    "    ('Cos', vector_cos, np.cos, None),\n",
    "]\n",
    "\n",
    "for name, kernel, np_op, b_arr in benchmarks:\n",
    "    a_input = np.abs(a) + 0.01 if name in ['Sqrt', 'Log'] else a\n",
    "    gpu_t, np_t, speedup = benchmark_operation(kernel, np_op, a_input, b_arr, name)\n",
    "    print(f\"{name:<15} | {gpu_t:<10.3f} | {np_t:<10.3f} | {speedup:<10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9928e",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "```\n",
    "Memory-bound ops (add, mul):\n",
    "â€¢ Moderate speedup (5-10x)\n",
    "â€¢ Limited by memory bandwidth\n",
    "â€¢ GPU has higher bandwidth than CPU\n",
    "\n",
    "Compute-bound ops (exp, sin, sqrt):\n",
    "â€¢ Higher speedup (10-50x)\n",
    "â€¢ GPU excels at parallel math\n",
    "â€¢ More compute per memory access\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde0dde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: In-Place Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a129c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def inplace_add(a, b, n):\n",
    "    \"\"\"a[i] += b[i] (modifies a in-place)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        a[i] += b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def inplace_scale(a, scalar, n):\n",
    "    \"\"\"a[i] *= scalar (modifies a in-place)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        a[i] *= scalar\n",
    "\n",
    "@cuda.jit\n",
    "def inplace_clamp(a, min_val, max_val, n):\n",
    "    \"\"\"Clamp values to [min_val, max_val] in-place\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        a[i] = max(min_val, min(max_val, a[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test in-place operations\n",
    "test_a = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float32)\n",
    "test_b = np.array([0.5, 0.5, 0.5, 0.5, 0.5], dtype=np.float32)\n",
    "\n",
    "d_test_a = cuda.to_device(test_a.copy())\n",
    "d_test_b = cuda.to_device(test_b)\n",
    "\n",
    "print(f\"Original a: {test_a}\")\n",
    "\n",
    "inplace_add[1, 32](d_test_a, d_test_b, len(test_a))\n",
    "print(f\"After a += b: {d_test_a.copy_to_host()}\")\n",
    "\n",
    "inplace_scale[1, 32](d_test_a, 2.0, len(test_a))\n",
    "print(f\"After a *= 2: {d_test_a.copy_to_host()}\")\n",
    "\n",
    "inplace_clamp[1, 32](d_test_a, 2.0, 8.0, len(test_a))\n",
    "print(f\"After clamp[2,8]: {d_test_a.copy_to_host()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6e572",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these elementwise operation exercises in CUDA C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65644664",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile elementwise_exercises.cu\n",
    "// elementwise_exercises.cu - Elementwise operation exercises\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Vector Absolute Value\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void vectorAbs(const float* a, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        out[idx] = fabsf(a[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Softplus Activation: log(1 + exp(x))\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void vectorSoftplus(const float* a, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float x = a[idx];\n",
    "        // Numerical stability: for large x, softplus(x) â‰ˆ x\n",
    "        if (x > 20.0f) {\n",
    "            out[idx] = x;\n",
    "        } else {\n",
    "            out[idx] = logf(1.0f + expf(x));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Polynomial Evaluation: ax^2 + bx + c\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void polynomialEval(const float* x, float a, float b, float c, \n",
    "                                float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float xi = x[idx];\n",
    "        out[idx] = a * xi * xi + b * xi + c;\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 4: Distance from Origin (2D vectors)\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void vectorDistance2D(const float* x, const float* y, float* dist, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        dist[idx] = sqrtf(x[idx] * x[idx] + y[idx] * y[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Test harness\n",
    "// =============================================================================\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Elementwise Operation Exercises ===\\n\\n\");\n",
    "    \n",
    "    // Exercise 1: Vector Absolute Value\n",
    "    printf(\"Exercise 1: Vector Absolute Value\\n\");\n",
    "    printf(\"-\" \"---------------------------------\\n\");\n",
    "    {\n",
    "        float h_a[] = {-3.0f, -1.0f, 0.0f, 1.0f, 3.0f};\n",
    "        float h_out[5];\n",
    "        const int N = 5;\n",
    "        \n",
    "        float *d_a, *d_out;\n",
    "        CUDA_CHECK(cudaMalloc(&d_a, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMalloc(&d_out, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        vectorAbs<<<1, 256>>>(d_a, d_out, N);\n",
    "        CUDA_CHECK(cudaMemcpy(h_out, d_out, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Input:  [%.0f, %.0f, %.0f, %.0f, %.0f]\\n\", \n",
    "               h_a[0], h_a[1], h_a[2], h_a[3], h_a[4]);\n",
    "        printf(\"Output: [%.0f, %.0f, %.0f, %.0f, %.0f]\\n\", \n",
    "               h_out[0], h_out[1], h_out[2], h_out[3], h_out[4]);\n",
    "        printf(\"Expected: [3, 1, 0, 1, 3]\\n\\n\");\n",
    "        \n",
    "        cudaFree(d_a); cudaFree(d_out);\n",
    "    }\n",
    "    \n",
    "    // Exercise 2: Softplus\n",
    "    printf(\"Exercise 2: Softplus Activation\\n\");\n",
    "    printf(\"-\" \"-------------------------------\\n\");\n",
    "    {\n",
    "        float h_a[] = {-5.0f, 0.0f, 1.0f, 5.0f, 25.0f};\n",
    "        float h_out[5];\n",
    "        const int N = 5;\n",
    "        \n",
    "        float *d_a, *d_out;\n",
    "        CUDA_CHECK(cudaMalloc(&d_a, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMalloc(&d_out, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        vectorSoftplus<<<1, 256>>>(d_a, d_out, N);\n",
    "        CUDA_CHECK(cudaMemcpy(h_out, d_out, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Input:  [%.1f, %.1f, %.1f, %.1f, %.1f]\\n\", \n",
    "               h_a[0], h_a[1], h_a[2], h_a[3], h_a[4]);\n",
    "        printf(\"Output: [%.3f, %.3f, %.3f, %.3f, %.3f]\\n\", \n",
    "               h_out[0], h_out[1], h_out[2], h_out[3], h_out[4]);\n",
    "        printf(\"Note: softplus(0)=ln(2)â‰ˆ0.693, large xâ†’x\\n\\n\");\n",
    "        \n",
    "        cudaFree(d_a); cudaFree(d_out);\n",
    "    }\n",
    "    \n",
    "    // Exercise 3: Polynomial\n",
    "    printf(\"Exercise 3: Polynomial (x^2 + 2x + 1)\\n\");\n",
    "    printf(\"-\" \"-------------------------------------\\n\");\n",
    "    {\n",
    "        float h_x[] = {0.0f, 1.0f, 2.0f, 3.0f};\n",
    "        float h_out[4];\n",
    "        const int N = 4;\n",
    "        \n",
    "        float *d_x, *d_out;\n",
    "        CUDA_CHECK(cudaMalloc(&d_x, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMalloc(&d_out, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_x, h_x, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        polynomialEval<<<1, 256>>>(d_x, 1.0f, 2.0f, 1.0f, d_out, N);\n",
    "        CUDA_CHECK(cudaMemcpy(h_out, d_out, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Input x: [%.0f, %.0f, %.0f, %.0f]\\n\", h_x[0], h_x[1], h_x[2], h_x[3]);\n",
    "        printf(\"Output:  [%.0f, %.0f, %.0f, %.0f]\\n\", h_out[0], h_out[1], h_out[2], h_out[3]);\n",
    "        printf(\"Expected: [1, 4, 9, 16] = (x+1)^2\\n\\n\");\n",
    "        \n",
    "        cudaFree(d_x); cudaFree(d_out);\n",
    "    }\n",
    "    \n",
    "    // Exercise 4: Distance 2D\n",
    "    printf(\"Exercise 4: Distance from Origin\\n\");\n",
    "    printf(\"-\" \"--------------------------------\\n\");\n",
    "    {\n",
    "        float h_x[] = {3.0f, 0.0f, 4.0f};\n",
    "        float h_y[] = {4.0f, 5.0f, 3.0f};\n",
    "        float h_dist[3];\n",
    "        const int N = 3;\n",
    "        \n",
    "        float *d_x, *d_y, *d_dist;\n",
    "        CUDA_CHECK(cudaMalloc(&d_x, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMalloc(&d_y, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMalloc(&d_dist, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_x, h_x, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        CUDA_CHECK(cudaMemcpy(d_y, h_y, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        vectorDistance2D<<<1, 256>>>(d_x, d_y, d_dist, N);\n",
    "        CUDA_CHECK(cudaMemcpy(h_dist, d_dist, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"x: [%.0f, %.0f, %.0f], y: [%.0f, %.0f, %.0f]\\n\", \n",
    "               h_x[0], h_x[1], h_x[2], h_y[0], h_y[1], h_y[2]);\n",
    "        printf(\"Distance: [%.0f, %.0f, %.0f]\\n\", h_dist[0], h_dist[1], h_dist[2]);\n",
    "        printf(\"Expected: [5, 5, 5] (3-4-5 triangles!)\\n\");\n",
    "        \n",
    "        cudaFree(d_x); cudaFree(d_y); cudaFree(d_dist);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n=== All exercises complete! ===\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a612a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o elementwise_exercises elementwise_exercises.cu && ./elementwise_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763349e",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Vector Absolute Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement vector absolute value\n",
    "@cuda.jit\n",
    "def vector_abs(a, out, n):\n",
    "    \"\"\"out[i] = |a[i]|\"\"\"\n",
    "    # Hint: Use math.fabs(x)\n",
    "    pass\n",
    "\n",
    "# Test with [-3, -1, 0, 1, 3]\n",
    "# Expected: [3, 1, 0, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a326b3",
   "metadata": {},
   "source": [
    "### Exercise 2: Softplus Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d198ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement softplus: log(1 + exp(x))\n",
    "@cuda.jit\n",
    "def vector_softplus(a, out, n):\n",
    "    \"\"\"Softplus activation: out[i] = log(1 + exp(a[i]))\"\"\"\n",
    "    # Hint: For numerical stability, use:\n",
    "    # if x > 20: return x (avoid exp overflow)\n",
    "    # else: return log(1 + exp(x))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022f0ac",
   "metadata": {},
   "source": [
    "### Exercise 3: Polynomial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate polynomial a*x^2 + b*x + c\n",
    "@cuda.jit\n",
    "def polynomial_eval(x, a_coef, b_coef, c_coef, out, n):\n",
    "    \"\"\"Evaluate ax^2 + bx + c for each element.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: x = [0, 1, 2, 3], a=1, b=2, c=1\n",
    "# Expected (x^2 + 2x + 1): [1, 4, 9, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b40bb0",
   "metadata": {},
   "source": [
    "### Exercise 4: Distance from Origin (2D vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f974fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute distance from origin for 2D points\n",
    "@cuda.jit\n",
    "def vector_distance_2d(x, y, dist, n):\n",
    "    \"\"\"dist[i] = sqrt(x[i]^2 + y[i]^2)\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: x = [3, 0, 4], y = [4, 5, 3]\n",
    "# Expected: [5, 5, 5] (3-4-5 triangles!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f51e1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### Quick Reference Card: Element-wise Operations\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ELEMENT-WISE OPERATION TEMPLATE                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  BINARY OPERATION (2 inputs):                                   â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  @cuda.jit                                                      â”‚\n",
    "â”‚  def binary_op(a, b, out, n):                                   â”‚\n",
    "â”‚      tid = cuda.grid(1)                                         â”‚\n",
    "â”‚      stride = cuda.gridsize(1)                                  â”‚\n",
    "â”‚      for i in range(tid, n, stride):                            â”‚\n",
    "â”‚          out[i] = a[i] â—‹ b[i]   # â—‹ = +, -, *, /               â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  UNARY OPERATION (1 input):                                     â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  @cuda.jit                                                      â”‚\n",
    "â”‚  def unary_op(a, out, n):                                       â”‚\n",
    "â”‚      tid = cuda.grid(1)                                         â”‚\n",
    "â”‚      stride = cuda.gridsize(1)                                  â”‚\n",
    "â”‚      for i in range(tid, n, stride):                            â”‚\n",
    "â”‚          out[i] = func(a[i])    # sqrt, exp, sin, etc.         â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  AVAILABLE MATH FUNCTIONS:                                      â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  â€¢ Basic:  +, -, *, /, **, %                                    â”‚\n",
    "â”‚  â€¢ Math:   math.sqrt, math.exp, math.log, math.log10            â”‚\n",
    "â”‚  â€¢ Trig:   math.sin, math.cos, math.tan                         â”‚\n",
    "â”‚  â€¢ Inv:    math.asin, math.acos, math.atan                      â”‚\n",
    "â”‚  â€¢ Other:  math.fabs, math.floor, math.ceil, math.tanh          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### âœ… What You Achieved Today\n",
    "\n",
    "| Skill | Status |\n",
    "|-------|--------|\n",
    "| Implemented arithmetic operations on vectors | âœ… |\n",
    "| Applied transcendental math functions | âœ… |\n",
    "| Built neural network activation functions | âœ… |\n",
    "| Combined operations with grid-stride loops | âœ… |\n",
    "\n",
    "### ğŸ§  Performance Notes\n",
    "\n",
    "| Operation Type | Characteristic | GPU Advantage |\n",
    "|----------------|----------------|---------------|\n",
    "| Simple ops | Memory bandwidth bound | Moderate speedup |\n",
    "| Complex ops | Compute bound | Massive speedup |\n",
    "| Combined ops | Best of both | Use grid-stride! |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746a42d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ What's Next?\n",
    "\n",
    "**Day 3: SAXPY & BLAS-like Operations** â€” We'll combine multiple operations and learn about the industry-standard BLAS specification!\n",
    "\n",
    "| Preview Topic | What You'll Learn |\n",
    "|---------------|-------------------|\n",
    "| BLAS Introduction | The standard for linear algebra |\n",
    "| SAXPY | The \"Hello World\" of GPU benchmarks |\n",
    "| Memory bandwidth | Understanding performance limits |\n",
    "| DOT, SCAL, AXPY | Building blocks of linear algebra |\n",
    "\n",
    "> ğŸ’¡ **Tomorrow's Hook:** SAXPY is just `y = a*x + y`â€”so why is it the most famous GPU benchmark in the world?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
