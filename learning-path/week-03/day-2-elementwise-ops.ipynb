{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc4f2d2",
   "metadata": {},
   "source": [
    "# üöÄ Day 2: Element-wise Vector Operations\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-03/day-2-elementwise-ops.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement arithmetic operations on vectors (add, sub, mul, div)\n",
    "- Apply math functions (sqrt, exp, log, sin, cos)\n",
    "- Build neural network activation functions\n",
    "- Combine operations efficiently\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7bc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021e9ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Basic Arithmetic Operations\n",
    "\n",
    "### CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// elementwise_ops.cu - Basic vector operations\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Vector Addition\n",
    "__global__ void vectorAdd(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Vector Subtraction\n",
    "__global__ void vectorSub(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] - b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Element-wise Multiplication (Hadamard product)\n",
    "__global__ void vectorMul(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] * b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Element-wise Division\n",
    "__global__ void vectorDiv(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] / b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Scalar operations\n",
    "__global__ void scalarMul(const float* a, float scalar, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] * scalar;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate and initialize host arrays\n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_out = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device arrays\n",
    "    float *d_a, *d_b, *d_out;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_out, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch\n",
    "    int threads = 256;\n",
    "    int blocks = 256;\n",
    "    \n",
    "    vectorAdd<<<blocks, threads>>>(d_a, d_b, d_out, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"Add: %f + %f = %f\\n\", h_a[0], h_b[0], h_out[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_out);\n",
    "    free(h_a); free(h_b); free(h_out);\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Python/Numba (Optional - Interactive Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalents for interactive testing\n",
    "@cuda.jit\n",
    "def vector_add(a, b, out, n):\n",
    "    \"\"\"out[i] = a[i] + b[i]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def vector_sub(a, b, out, n):\n",
    "    \"\"\"out[i] = a[i] - b[i]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] - b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def vector_mul(a, b, out, n):\n",
    "    \"\"\"out[i] = a[i] * b[i]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] * b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def vector_div(a, b, out, n):\n",
    "    \"\"\"out[i] = a[i] / b[i]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] / b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic operations\n",
    "n = 1_000_000\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "b = np.random.rand(n).astype(np.float32) + 0.1  # Avoid div by zero\n",
    "out = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_out = cuda.to_device(out)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Test each operation\n",
    "ops = [\n",
    "    ('Add', vector_add, lambda a, b: a + b),\n",
    "    ('Sub', vector_sub, lambda a, b: a - b),\n",
    "    ('Mul', vector_mul, lambda a, b: a * b),\n",
    "    ('Div', vector_div, lambda a, b: a / b),\n",
    "]\n",
    "\n",
    "print(f\"Testing with {n:,} elements\\n\")\n",
    "for name, kernel, np_op in ops:\n",
    "    kernel[blocks, threads](d_a, d_b, d_out, n)\n",
    "    result = d_out.copy_to_host()\n",
    "    expected = np_op(a, b)\n",
    "    match = np.allclose(result, expected)\n",
    "    print(f\"{name}: {'‚úì' if match else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403f8c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Scalar Operations\n",
    "\n",
    "### CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// Scalar Add: out[i] = a[i] + scalar\n",
    "__global__ void scalarAdd(const float* a, float scalar, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] + scalar;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Scalar Multiply: out[i] = a[i] * scalar\n",
    "__global__ void scalarMul(const float* a, float scalar, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] * scalar;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Scalar Power: out[i] = a[i] ^ power\n",
    "__global__ void scalarPow(const float* a, float power, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = powf(a[i], power);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Python/Numba (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def scalar_add(a, scalar, out, n):\n",
    "    \"\"\"out[i] = a[i] + scalar\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] + scalar\n",
    "\n",
    "@cuda.jit\n",
    "def scalar_mul(a, scalar, out, n):\n",
    "    \"\"\"out[i] = a[i] * scalar\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] * scalar\n",
    "\n",
    "@cuda.jit\n",
    "def scalar_pow(a, power, out, n):\n",
    "    \"\"\"out[i] = a[i] ** power\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] ** power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scalar operations\n",
    "scalar_mul[blocks, threads](d_a, 2.5, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = a * 2.5\n",
    "print(f\"Scalar multiply by 2.5: {'‚úì' if np.allclose(result, expected) else '‚úó'}\")\n",
    "\n",
    "scalar_pow[blocks, threads](d_a, 2.0, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = a ** 2.0\n",
    "print(f\"Scalar power of 2: {'‚úì' if np.allclose(result, expected) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b05e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Math Functions\n",
    "\n",
    "### CUDA C++ Math Functions\n",
    "\n",
    "| CUDA C++ | Python/Numba | Description |\n",
    "|----------|--------------|-------------|\n",
    "| `sqrtf(x)` | `math.sqrt(x)` | Square root |\n",
    "| `sinf(x)` | `math.sin(x)` | Sine |\n",
    "| `cosf(x)` | `math.cos(x)` | Cosine |\n",
    "| `tanf(x)` | `math.tan(x)` | Tangent |\n",
    "| `expf(x)` | `math.exp(x)` | Exponential e^x |\n",
    "| `logf(x)` | `math.log(x)` | Natural log |\n",
    "| `log10f(x)` | `math.log10(x)` | Log base 10 |\n",
    "| `fabsf(x)` | `math.fabs(x)` | Absolute value |\n",
    "| `floorf(x)` | `math.floor(x)` | Floor |\n",
    "| `ceilf(x)` | `math.ceil(x)` | Ceiling |\n",
    "| `powf(x,y)` | `x ** y` | Power |\n",
    "\n",
    "### CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// math_functions.cu\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void vectorSqrt(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = sqrtf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorExp(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = expf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorLog(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = logf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorSin(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = sinf(a[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorCos(const float* a, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = cosf(a[i]);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Python/Numba (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vector_sqrt(a, out, n):\n",
    "    \"\"\"out[i] = sqrt(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.sqrt(a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_exp(a, out, n):\n",
    "    \"\"\"out[i] = exp(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.exp(a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_log(a, out, n):\n",
    "    \"\"\"out[i] = log(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.log(a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_sin(a, out, n):\n",
    "    \"\"\"out[i] = sin(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.sin(a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_cos(a, out, n):\n",
    "    \"\"\"out[i] = cos(a[i])\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.cos(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test math functions\n",
    "a_pos = np.abs(a) + 0.01  # Positive values for sqrt/log\n",
    "d_a_pos = cuda.to_device(a_pos)\n",
    "\n",
    "math_ops = [\n",
    "    ('sqrt', vector_sqrt, np.sqrt, d_a_pos, a_pos),\n",
    "    ('exp', vector_exp, np.exp, d_a, a * 0.1),  # Scale down to avoid overflow\n",
    "    ('log', vector_log, np.log, d_a_pos, a_pos),\n",
    "    ('sin', vector_sin, np.sin, d_a, a),\n",
    "    ('cos', vector_cos, np.cos, d_a, a),\n",
    "]\n",
    "\n",
    "print(\"Math function tests:\")\n",
    "for name, kernel, np_fn, d_input, h_input in math_ops:\n",
    "    kernel[blocks, threads](d_input, d_out, n)\n",
    "    result = d_out.copy_to_host()\n",
    "    expected = np_fn(h_input)\n",
    "    match = np.allclose(result, expected, rtol=1e-5)\n",
    "    print(f\"  {name}: {'‚úì' if match else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf92afe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Compound Operations\n",
    "\n",
    "### Combining Multiple Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff999805",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vector_normalize(a, out, n):\n",
    "    \"\"\"Normalize to [0, 1] assuming input in [0, max_val]\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        # Sigmoid-like normalization\n",
    "        out[i] = 1.0 / (1.0 + math.exp(-a[i]))\n",
    "\n",
    "@cuda.jit\n",
    "def vector_relu(a, out, n):\n",
    "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = max(0.0, a[i])\n",
    "\n",
    "@cuda.jit\n",
    "def vector_leaky_relu(a, out, alpha, n):\n",
    "    \"\"\"Leaky ReLU: max(alpha*x, x)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        x = a[i]\n",
    "        out[i] = x if x > 0 else alpha * x\n",
    "\n",
    "@cuda.jit\n",
    "def vector_tanh(a, out, n):\n",
    "    \"\"\"Hyperbolic tangent activation\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = math.tanh(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test activation functions\n",
    "a_centered = (a - 0.5) * 4  # Values around 0\n",
    "d_a_centered = cuda.to_device(a_centered)\n",
    "\n",
    "# Sigmoid\n",
    "vector_normalize[blocks, threads](d_a_centered, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = 1 / (1 + np.exp(-a_centered))\n",
    "print(f\"Sigmoid: {'‚úì' if np.allclose(result, expected, rtol=1e-5) else '‚úó'}\")\n",
    "\n",
    "# ReLU\n",
    "vector_relu[blocks, threads](d_a_centered, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = np.maximum(0, a_centered)\n",
    "print(f\"ReLU: {'‚úì' if np.allclose(result, expected) else '‚úó'}\")\n",
    "\n",
    "# Tanh\n",
    "vector_tanh[blocks, threads](d_a_centered, d_out, n)\n",
    "result = d_out.copy_to_host()\n",
    "expected = np.tanh(a_centered)\n",
    "print(f\"Tanh: {'‚úì' if np.allclose(result, expected, rtol=1e-5) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac25bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Performance Analysis\n",
    "\n",
    "### Memory Bandwidth vs Compute Bound\n",
    "\n",
    "```\n",
    "Memory Bandwidth Bound:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚Ä¢ Simple ops like add, mul\n",
    "‚Ä¢ Read 2 values, write 1 ‚Üí mostly waiting on memory\n",
    "‚Ä¢ GPU advantage: Higher memory bandwidth\n",
    "\n",
    "Compute Bound:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚Ä¢ Complex ops like exp, sin, sqrt\n",
    "‚Ä¢ Many cycles per element\n",
    "‚Ä¢ GPU advantage: Massive parallelism\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operation(kernel, np_op, a, b, name, iterations=100):\n",
    "    \"\"\"Benchmark GPU kernel vs NumPy.\"\"\"\n",
    "    n = len(a)\n",
    "    out = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b) if b is not None else None\n",
    "    d_out = cuda.to_device(out)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # Warmup\n",
    "    if d_b is not None:\n",
    "        kernel[blocks, threads](d_a, d_b, d_out, n)\n",
    "    else:\n",
    "        kernel[blocks, threads](d_a, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # GPU benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        if d_b is not None:\n",
    "            kernel[blocks, threads](d_a, d_b, d_out, n)\n",
    "        else:\n",
    "            kernel[blocks, threads](d_a, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    gpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # NumPy benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        if b is not None:\n",
    "            _ = np_op(a, b)\n",
    "        else:\n",
    "            _ = np_op(a)\n",
    "    numpy_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    speedup = numpy_time / gpu_time\n",
    "    return gpu_time, numpy_time, speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a400e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive benchmark\n",
    "n = 10_000_000\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "b = np.random.rand(n).astype(np.float32) + 0.1\n",
    "\n",
    "print(f\"Benchmarking with N = {n:,} elements\\n\")\n",
    "print(f\"{'Operation':<15} | {'GPU (ms)':<10} | {'NumPy (ms)':<10} | {'Speedup':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "benchmarks = [\n",
    "    ('Add', vector_add, lambda x, y: x + y, b),\n",
    "    ('Mul', vector_mul, lambda x, y: x * y, b),\n",
    "    ('Div', vector_div, lambda x, y: x / y, b),\n",
    "    ('Sqrt', vector_sqrt, np.sqrt, None),\n",
    "    ('Exp', vector_exp, np.exp, None),\n",
    "    ('Log', vector_log, np.log, None),\n",
    "    ('Sin', vector_sin, np.sin, None),\n",
    "    ('Cos', vector_cos, np.cos, None),\n",
    "]\n",
    "\n",
    "for name, kernel, np_op, b_arr in benchmarks:\n",
    "    a_input = np.abs(a) + 0.01 if name in ['Sqrt', 'Log'] else a\n",
    "    gpu_t, np_t, speedup = benchmark_operation(kernel, np_op, a_input, b_arr, name)\n",
    "    print(f\"{name:<15} | {gpu_t:<10.3f} | {np_t:<10.3f} | {speedup:<10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9928e",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "```\n",
    "Memory-bound ops (add, mul):\n",
    "‚Ä¢ Moderate speedup (5-10x)\n",
    "‚Ä¢ Limited by memory bandwidth\n",
    "‚Ä¢ GPU has higher bandwidth than CPU\n",
    "\n",
    "Compute-bound ops (exp, sin, sqrt):\n",
    "‚Ä¢ Higher speedup (10-50x)\n",
    "‚Ä¢ GPU excels at parallel math\n",
    "‚Ä¢ More compute per memory access\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde0dde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: In-Place Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a129c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def inplace_add(a, b, n):\n",
    "    \"\"\"a[i] += b[i] (modifies a in-place)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        a[i] += b[i]\n",
    "\n",
    "@cuda.jit\n",
    "def inplace_scale(a, scalar, n):\n",
    "    \"\"\"a[i] *= scalar (modifies a in-place)\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        a[i] *= scalar\n",
    "\n",
    "@cuda.jit\n",
    "def inplace_clamp(a, min_val, max_val, n):\n",
    "    \"\"\"Clamp values to [min_val, max_val] in-place\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        a[i] = max(min_val, min(max_val, a[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test in-place operations\n",
    "test_a = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float32)\n",
    "test_b = np.array([0.5, 0.5, 0.5, 0.5, 0.5], dtype=np.float32)\n",
    "\n",
    "d_test_a = cuda.to_device(test_a.copy())\n",
    "d_test_b = cuda.to_device(test_b)\n",
    "\n",
    "print(f\"Original a: {test_a}\")\n",
    "\n",
    "inplace_add[1, 32](d_test_a, d_test_b, len(test_a))\n",
    "print(f\"After a += b: {d_test_a.copy_to_host()}\")\n",
    "\n",
    "inplace_scale[1, 32](d_test_a, 2.0, len(test_a))\n",
    "print(f\"After a *= 2: {d_test_a.copy_to_host()}\")\n",
    "\n",
    "inplace_clamp[1, 32](d_test_a, 2.0, 8.0, len(test_a))\n",
    "print(f\"After clamp[2,8]: {d_test_a.copy_to_host()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6e572",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Vector Absolute Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement vector absolute value\n",
    "@cuda.jit\n",
    "def vector_abs(a, out, n):\n",
    "    \"\"\"out[i] = |a[i]|\"\"\"\n",
    "    # Hint: Use math.fabs(x)\n",
    "    pass\n",
    "\n",
    "# Test with [-3, -1, 0, 1, 3]\n",
    "# Expected: [3, 1, 0, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a326b3",
   "metadata": {},
   "source": [
    "### Exercise 2: Softplus Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d198ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement softplus: log(1 + exp(x))\n",
    "@cuda.jit\n",
    "def vector_softplus(a, out, n):\n",
    "    \"\"\"Softplus activation: out[i] = log(1 + exp(a[i]))\"\"\"\n",
    "    # Hint: For numerical stability, use:\n",
    "    # if x > 20: return x (avoid exp overflow)\n",
    "    # else: return log(1 + exp(x))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022f0ac",
   "metadata": {},
   "source": [
    "### Exercise 3: Polynomial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate polynomial a*x^2 + b*x + c\n",
    "@cuda.jit\n",
    "def polynomial_eval(x, a_coef, b_coef, c_coef, out, n):\n",
    "    \"\"\"Evaluate ax^2 + bx + c for each element.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: x = [0, 1, 2, 3], a=1, b=2, c=1\n",
    "# Expected (x^2 + 2x + 1): [1, 4, 9, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b40bb0",
   "metadata": {},
   "source": [
    "### Exercise 4: Distance from Origin (2D vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f974fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute distance from origin for 2D points\n",
    "@cuda.jit\n",
    "def vector_distance_2d(x, y, dist, n):\n",
    "    \"\"\"dist[i] = sqrt(x[i]^2 + y[i]^2)\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: x = [3, 0, 4], y = [4, 5, 3]\n",
    "# Expected: [5, 5, 5] (3-4-5 triangles!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f51e1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Element-wise Operation Template\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def vector_operation(a, b, out, n):  # or (a, out, n) for unary\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = operation(a[i], b[i])  # or operation(a[i])\n",
    "```\n",
    "\n",
    "### Math Functions Available\n",
    "- Basic: `+`, `-`, `*`, `/`, `**`, `%`\n",
    "- Math: `sqrt`, `exp`, `log`, `log10`\n",
    "- Trig: `sin`, `cos`, `tan`, `asin`, `acos`, `atan`\n",
    "- Other: `fabs`, `floor`, `ceil`, `fmod`, `tanh`\n",
    "\n",
    "### Performance Notes\n",
    "- Simple ops: Memory bandwidth bound\n",
    "- Complex ops: Compute bound, higher speedups\n",
    "- Always use grid-stride for flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746a42d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "üìã **Day 3:** SAXPY and BLAS-like operations\n",
    "\n",
    "We'll combine multiple operations and learn about the BLAS standard!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
