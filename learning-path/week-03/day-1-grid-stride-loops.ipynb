{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b718310",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 1: Grid-Stride Loops - The Professional Pattern\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-03/day-1-grid-stride-loops.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Challenge\n",
    "\n",
    "*You've launched 1 million threads for 1 million elements. What happens when you have 10 million elements tomorrow?*\n",
    "\n",
    "The naive \"one thread per element\" approach breaks down with real-world data sizes. Professional CUDA developers use a single elegant pattern that handles **any data size** with **fixed launch configurations**: the **grid-stride loop**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "| Objective | Skill Level |\n",
    "|-----------|-------------|\n",
    "| Identify limitations of naive \"one thread per element\" approach | Understand |\n",
    "| Implement grid-stride loops for arbitrary data sizes | Apply |\n",
    "| Configure optimal thread blocks independent of problem size | Apply |\n",
    "| Extend patterns to 1D and 2D grid-stride loops | Create |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ Session Roadmap\n",
    "\n",
    "| Part | Topic | Duration |\n",
    "|------|-------|----------|\n",
    "| 1 | The Problem with Naive Kernels | 10 min |\n",
    "| 2 | Grid-Stride Loop Pattern | 15 min |\n",
    "| 3 | 2D Grid-Stride Loops | 15 min |\n",
    "| 4 | Performance Analysis | 10 min |\n",
    "| 5 | Exercises | 10 min |\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02124ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Verify GPU\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")\n",
    "    print(f\"Compute capability: {device.compute_capability}\")\n",
    "    print(f\"Max threads per block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "    print(f\"Max blocks per grid: {device.MAX_GRID_DIM_X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da705d12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Problem with Naive Kernels\n",
    "\n",
    "> ğŸ’¡ **Concept Card: The Scalability Problem**\n",
    "> \n",
    "> ```\n",
    "> ğŸ¯ THE NAIVE APPROACH LIMITATION\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   NAIVE: One Thread = One Element\n",
    ">   \n",
    ">   Thread 0 â†’ Element 0    âœ… Works for small arrays\n",
    ">   Thread 1 â†’ Element 1    \n",
    ">   Thread 2 â†’ Element 2    âŒ But what if N > max_threads?\n",
    ">   ...                     âŒ What if N changes at runtime?\n",
    ">   \n",
    ">   PROBLEM: Must recalculate grid dimensions for every N!\n",
    ">   \n",
    ">   blocks = (N + 255) / 256   â† Changes with data size\n",
    ">   kernel<<<blocks, 256>>>()  â† Must relaunch differently\n",
    ">   \n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **Why It Matters:** Production code processes varying data sizes. Reconfiguring launches is error-prone and inflexible.\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "Compile and run:\n",
    "```bash\n",
    "nvcc -arch=sm_75 -o naive_add naive_add.cu\n",
    "./naive_add\n",
    "```\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda34343",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naive_add.cu\n",
    "// naive_add.cu - Naive vector addition\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n",
    "                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Naive kernel: one thread handles exactly one element\n",
    "__global__ void naiveAdd(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (tid < n) {  // Bounds check\n",
    "        out[tid] = a[tid] + b[tid];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_a, *h_b, *h_out;\n",
    "    h_a = (float*)malloc(size);\n",
    "    h_b = (float*)malloc(size);\n",
    "    h_out = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize data\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_a, *d_b, *d_out;\n",
    "    CUDA_CHECK(cudaMalloc(&d_a, size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_b, size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_out, size));\n",
    "    \n",
    "    // Copy to device\n",
    "    CUDA_CHECK(cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice));\n",
    "    CUDA_CHECK(cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Launch configuration\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    printf(\"Launching with %d blocks, %d threads/block\\n\", blocksPerGrid, threadsPerBlock);\n",
    "    \n",
    "    naiveAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_out, n);\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Copy back and verify\n",
    "    CUDA_CHECK(cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost));\n",
    "    printf(\"Result[0] = %f (expected 3.0)\\n\", h_out[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_out);\n",
    "    free(h_a); free(h_b); free(h_out);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a306a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o naive_add naive_add.cu\n",
    "!./naive_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for interactive testing\n",
    "@cuda.jit\n",
    "def naive_add(a, b, out, n):\n",
    "    \"\"\"Naive: One thread handles exactly one element.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    \n",
    "    if tid < n:  # Bounds check\n",
    "        out[tid] = a[tid] + b[tid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef1641",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Grid-Stride Loop Pattern\n",
    "\n",
    "> ğŸ’¡ **Concept Card: The Grid-Stride Loop Pattern**\n",
    "> \n",
    "> ```\n",
    "> ğŸ”„ GRID-STRIDE: Handle ANY Size with FIXED Config\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   Array: [0][1][2][3][4][5][6][7][8][9][10][11][12][13]\n",
    ">                                                    \n",
    ">   Grid has 4 threads total (e.g., 2 blocks Ã— 2 threads)\n",
    ">   \n",
    ">   ITERATION 1 (initial positions):\n",
    ">   T0 â†’ [0]     T1 â†’ [1]     T2 â†’ [2]     T3 â†’ [3]\n",
    ">   \n",
    ">   ITERATION 2 (stride by gridSize = 4):\n",
    ">   T0 â†’ [4]     T1 â†’ [5]     T2 â†’ [6]     T3 â†’ [7]\n",
    ">   \n",
    ">   ITERATION 3:\n",
    ">   T0 â†’ [8]     T1 â†’ [9]     T2 â†’ [10]    T3 â†’ [11]\n",
    ">   \n",
    ">   ITERATION 4:\n",
    ">   T0 â†’ [12]    T1 â†’ [13]    T2 â†’ done    T3 â†’ done\n",
    ">   \n",
    ">   PATTERN:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   tid = blockIdx.x * blockDim.x + threadIdx.x\n",
    ">   stride = blockDim.x * gridDim.x  // Total threads\n",
    ">   \n",
    ">   for (i = tid; i < N; i += stride) {\n",
    ">       process(data[i]);\n",
    ">   }\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **Key Insight:** Each thread handles elements `tid`, `tid+stride`, `tid+2*stride`, ... until done.\n",
    "\n",
    "Now let's see the grid-stride loop in action. Notice how we configure the grid **once** and let it handle any size:\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b34284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the limitation\n",
    "device = cuda.get_current_device()\n",
    "max_blocks = device.MAX_GRID_DIM_X\n",
    "threads_per_block = 256\n",
    "max_elements_naive = max_blocks * threads_per_block\n",
    "\n",
    "print(f\"Max blocks in X dimension: {max_blocks:,}\")\n",
    "print(f\"Threads per block: {threads_per_block}\")\n",
    "print(f\"Max elements with naive approach: {max_elements_naive:,}\")\n",
    "print(f\"That's only {max_elements_naive / 1e9:.2f} billion elements\")\n",
    "print(f\"\\nModern datasets can have billions of elements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ff382",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: 2D Grid-Stride Loops\n",
    "\n",
    "Excellent! You've mastered the 1D pattern. Now let's extend it to 2D - essential for images, matrices, and spatial data.\n",
    "\n",
    "> ğŸ’¡ **Concept Card: 2D Grid-Stride Pattern**\n",
    "> \n",
    "> ```\n",
    "> ğŸ® 2D GRID-STRIDE: For Images & Matrices\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> \n",
    ">   Image/Matrix (8Ã—6):              Thread Block (2Ã—2):\n",
    ">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”¬â”€â”€â”€â”\n",
    ">   â”‚ 0,0  0,1  0,2 ... 0,7   â”‚      â”‚T00â”‚T01â”‚\n",
    ">   â”‚ 1,0  1,1  1,2 ... 1,7   â”‚      â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    ">   â”‚ ...                     â”‚      â”‚T10â”‚T11â”‚\n",
    ">   â”‚ 5,0  5,1  5,2 ... 5,7   â”‚      â””â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    ">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    ">   \n",
    ">   Each thread strides in BOTH X and Y:\n",
    ">   \n",
    ">   T00: (0,0), (0,2), (0,4), (0,6),  â† stride X\n",
    ">        (2,0), (2,2), (2,4), (2,6),  â† stride Y\n",
    ">        (4,0), (4,2), (4,4), (4,6)\n",
    ">        \n",
    ">   PATTERN:\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    ">   x = blockIdx.x * blockDim.x + threadIdx.x\n",
    ">   y = blockIdx.y * blockDim.y + threadIdx.y\n",
    ">   strideX = blockDim.x * gridDim.x\n",
    ">   strideY = blockDim.y * gridDim.y\n",
    ">   \n",
    ">   for (row = y; row < height; row += strideY)\n",
    ">       for (col = x; col < width; col += strideX)\n",
    ">           idx = row * width + col\n",
    ">           process(data[idx])\n",
    ">   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "> â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "> ```\n",
    "> \n",
    "> **Remember:** Outer loop strides rows (Y), inner loop strides columns (X) for coalesced access.\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35060fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile grid_stride_add.cu\n",
    "// grid_stride_add.cu - Professional grid-stride pattern\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Grid-stride kernel: each thread processes multiple elements\n",
    "__global__ void gridStrideAdd(const float* a, const float* b, float* out, int n) {\n",
    "    // Global thread ID\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // Total number of threads in the grid\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Each thread processes elements at tid, tid+stride, tid+2*stride, ...\n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 100000000;  // 100M elements - works with any size!\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate\n",
    "    float *d_a, *d_b, *d_out;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_out, size);\n",
    "    \n",
    "    // Fixed launch config - works for ANY data size\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = 256;  // Fixed, not dependent on n!\n",
    "    \n",
    "    printf(\"Processing %d elements with %d total threads\\n\", \n",
    "           n, threadsPerBlock * blocksPerGrid);\n",
    "    printf(\"Each thread handles ~%d elements\\n\", \n",
    "           n / (threadsPerBlock * blocksPerGrid));\n",
    "    \n",
    "    gridStrideAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_out, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_out);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f98ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o grid_stride_add grid_stride_add.cu\n",
    "!./grid_stride_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45079b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for interactive testing\n",
    "@cuda.jit\n",
    "def grid_stride_add(a, b, out, n):\n",
    "    \"\"\"Grid-stride loop: Each thread processes multiple elements.\"\"\"\n",
    "    tid = cuda.grid(1)           # Global thread ID (= blockIdx.x * blockDim.x + threadIdx.x)\n",
    "    stride = cuda.gridsize(1)    # Total threads (= blockDim.x * gridDim.x)\n",
    "    \n",
    "    # Each thread processes elements at tid, tid+stride, tid+2*stride, ...\n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02754b1c",
   "metadata": {},
   "source": [
    "### Key Functions\n",
    "\n",
    "| Function | Returns | Description |\n",
    "|----------|---------|-------------|\n",
    "| `cuda.grid(1)` | int | Global thread ID (1D) |\n",
    "| `cuda.gridsize(1)` | int | Total threads in grid = blocks Ã— threads_per_block |\n",
    "| `cuda.grid(2)` | (x, y) | Global thread ID (2D) |\n",
    "| `cuda.gridsize(2)` | (x, y) | Total threads in each dimension |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid-stride pattern\n",
    "@cuda.jit\n",
    "def show_grid_stride(output, n):\n",
    "    \"\"\"Store which thread processed each element.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        output[i] = tid  # Store thread ID\n",
    "\n",
    "# Small example\n",
    "n = 20\n",
    "blocks = 2\n",
    "threads = 4\n",
    "total_threads = blocks * threads\n",
    "\n",
    "output = np.zeros(n, dtype=np.int32)\n",
    "d_output = cuda.to_device(output)\n",
    "\n",
    "show_grid_stride[blocks, threads](d_output, n)\n",
    "result = d_output.copy_to_host()\n",
    "\n",
    "print(f\"Configuration: {blocks} blocks Ã— {threads} threads = {total_threads} total threads\")\n",
    "print(f\"Processing {n} elements\")\n",
    "print(f\"\\nElement index:  {list(range(n))}\")\n",
    "print(f\"Processed by:   {list(result)}\")\n",
    "print(f\"\\nEach thread processes {n // total_threads} elements (plus remainder)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e853ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Benefits of Grid-Stride Loops\n",
    "\n",
    "### Benefit 1: Handle ANY Data Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same kernel config works for different sizes\n",
    "def test_sizes(kernel, sizes, blocks=256, threads=256):\n",
    "    \"\"\"Test kernel with various data sizes.\"\"\"\n",
    "    for n in sizes:\n",
    "        a = np.random.rand(n).astype(np.float32)\n",
    "        b = np.random.rand(n).astype(np.float32)\n",
    "        out = np.zeros(n, dtype=np.float32)\n",
    "        \n",
    "        d_a = cuda.to_device(a)\n",
    "        d_b = cuda.to_device(b)\n",
    "        d_out = cuda.to_device(out)\n",
    "        \n",
    "        kernel[blocks, threads](d_a, d_b, d_out, n)\n",
    "        result = d_out.copy_to_host()\n",
    "        \n",
    "        # Verify\n",
    "        expected = a + b\n",
    "        is_correct = np.allclose(result, expected)\n",
    "        status = \"âœ“\" if is_correct else \"âœ—\"\n",
    "        print(f\"{status} N = {n:>12,} | Blocks = {blocks}, Threads = {threads}\")\n",
    "\n",
    "# Test grid-stride with various sizes\n",
    "print(\"Grid-Stride Loop (same config for all sizes):\")\n",
    "test_sizes(grid_stride_add, [100, 1000, 10000, 100000, 1000000, 10000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfd5ba",
   "metadata": {},
   "source": [
    "### Benefit 2: Tune for Occupancy, Not Data Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different launch configs, same result\n",
    "n = 1_000_000\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "b = np.random.rand(n).astype(np.float32)\n",
    "out = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_out = cuda.to_device(out)\n",
    "\n",
    "configs = [\n",
    "    (32, 64),    # Few threads\n",
    "    (128, 128),  # Moderate\n",
    "    (256, 256),  # Typical\n",
    "    (512, 512),  # High occupancy\n",
    "]\n",
    "\n",
    "print(f\"Testing different configs with N = {n:,}\\n\")\n",
    "for blocks, threads in configs:\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        grid_stride_add[blocks, threads](d_a, d_b, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    elapsed = (time.perf_counter() - start) / 100 * 1000\n",
    "    \n",
    "    total_threads = blocks * threads\n",
    "    elements_per_thread = n / total_threads\n",
    "    print(f\"Blocks={blocks:3}, Threads={threads:3} | \"\n",
    "          f\"Total={total_threads:>7,} | \"\n",
    "          f\"Elem/Thread={elements_per_thread:>6.1f} | \"\n",
    "          f\"Time={elapsed:.3f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751a109",
   "metadata": {},
   "source": [
    "### Benefit 3: Better Memory Access Patterns\n",
    "\n",
    "Grid-stride loops naturally provide **coalesced memory access**:\n",
    "\n",
    "```\n",
    "Pass 1: Threads 0,1,2,...,31 access elements 0,1,2,...,31  â† Coalesced!\n",
    "Pass 2: Threads 0,1,2,...,31 access elements 256,257,...,287  â† Coalesced!\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604caaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Common Patterns\n",
    "\n",
    "### Pattern 1: Basic 1D Grid-Stride (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// 1D Grid-Stride Template\n",
    "__global__ void gridStride1D(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        data[i] = data[i] * 2.0f;  // Some operation\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Pattern 2: 2D Grid-Stride (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// 2D Grid-Stride for images/matrices\n",
    "__global__ void gridStride2D(float* data, int height, int width) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int strideX = blockDim.x * gridDim.x;\n",
    "    int strideY = blockDim.y * gridDim.y;\n",
    "    \n",
    "    for (int row = y; row < height; row += strideY) {\n",
    "        for (int col = x; col < width; col += strideX) {\n",
    "            int idx = row * width + col;\n",
    "            data[idx] = data[idx] * 2.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Launch with 2D config:\n",
    "// dim3 threads(16, 16);\n",
    "// dim3 blocks(32, 32);\n",
    "// gridStride2D<<<blocks, threads>>>(d_data, height, width);\n",
    "```\n",
    "\n",
    "### Pattern 3: Grid-Stride with Local Accumulation (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// Each thread computes partial sum of its elements\n",
    "__global__ void gridStrideSum(const float* data, float* partialSums, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    float localSum = 0.0f;\n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        localSum += data[i];\n",
    "    }\n",
    "    \n",
    "    // Store partial sum (will need reduction to complete)\n",
    "    partialSums[tid] = localSum;\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc459085",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ 2D Grid-Stride and Accumulation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c56a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile grid_stride_2d.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// 2D Grid-Stride for images/matrices\n",
    "__global__ void gridStride2D(float* data, int height, int width) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int strideX = blockDim.x * gridDim.x;\n",
    "    int strideY = blockDim.y * gridDim.y;\n",
    "    \n",
    "    for (int row = y; row < height; row += strideY) {\n",
    "        for (int col = x; col < width; col += strideX) {\n",
    "            int idx = row * width + col;\n",
    "            data[idx] = data[idx] * 2.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Grid-Stride with Local Accumulation (partial sums)\n",
    "__global__ void gridStrideSum(const float* data, float* partialSums, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    float localSum = 0.0f;\n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        localSum += data[i];\n",
    "    }\n",
    "    \n",
    "    partialSums[tid] = localSum;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Test 2D Grid-Stride\n",
    "    const int HEIGHT = 1000;\n",
    "    const int WIDTH = 1000;\n",
    "    const int SIZE = HEIGHT * WIDTH;\n",
    "    const size_t bytes = SIZE * sizeof(float);\n",
    "    \n",
    "    float *h_data = (float*)malloc(bytes);\n",
    "    for (int i = 0; i < SIZE; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, bytes);\n",
    "    cudaMemcpy(d_data, h_data, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 threads(16, 16);\n",
    "    dim3 blocks(32, 32);  // Fixed config works for any image size!\n",
    "    \n",
    "    gridStride2D<<<blocks, threads>>>(d_data, HEIGHT, WIDTH);\n",
    "    \n",
    "    cudaMemcpy(h_data, d_data, bytes, cudaMemcpyDeviceToHost);\n",
    "    printf(\"2D Grid-Stride: data[0] = %.1f (expected 2.0)\\n\", h_data[0]);\n",
    "    \n",
    "    // Test Grid-Stride Sum\n",
    "    const int N = 1000000;\n",
    "    float *h_input = (float*)malloc(N * sizeof(float));\n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1.0f;\n",
    "    \n",
    "    float *d_input;\n",
    "    cudaMalloc(&d_input, N * sizeof(float));\n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int totalThreads = 256 * 256;\n",
    "    float *d_partial;\n",
    "    cudaMalloc(&d_partial, totalThreads * sizeof(float));\n",
    "    \n",
    "    gridStrideSum<<<256, 256>>>(d_input, d_partial, N);\n",
    "    \n",
    "    float *h_partial = (float*)malloc(totalThreads * sizeof(float));\n",
    "    cudaMemcpy(h_partial, d_partial, totalThreads * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < totalThreads; i++) sum += h_partial[i];\n",
    "    \n",
    "    printf(\"Grid-Stride Sum: %.0f (expected %.0f)\\n\", sum, (float)N);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_partial);\n",
    "    free(h_data);\n",
    "    free(h_input);\n",
    "    free(h_partial);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c165d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o grid_stride_2d grid_stride_2d.cu && ./grid_stride_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Basic 1D Grid-Stride (Python)\n",
    "@cuda.jit\n",
    "def grid_stride_1d(data, n):\n",
    "    \"\"\"Basic 1D grid-stride pattern.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        data[i] = data[i] * 2  # Some operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e689f",
   "metadata": {},
   "source": [
    "### Pattern 2: 2D Grid-Stride (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a4ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def grid_stride_2d(data, height, width):\n",
    "    \"\"\"2D grid-stride for images/matrices.\"\"\"\n",
    "    start_x, start_y = cuda.grid(2)\n",
    "    stride_x, stride_y = cuda.gridsize(2)\n",
    "    \n",
    "    for y in range(start_y, height, stride_y):\n",
    "        for x in range(start_x, width, stride_x):\n",
    "            data[y, x] = data[y, x] * 2\n",
    "\n",
    "# Test 2D grid-stride\n",
    "height, width = 1000, 1000\n",
    "data = np.random.rand(height, width).astype(np.float32)\n",
    "d_data = cuda.to_device(data)\n",
    "\n",
    "threads = (16, 16)\n",
    "blocks = (32, 32)  # Can be any size, not dependent on image size\n",
    "\n",
    "grid_stride_2d[blocks, threads](d_data, height, width)\n",
    "result = d_data.copy_to_host()\n",
    "\n",
    "print(f\"Image: {height}x{width} = {height*width:,} pixels\")\n",
    "print(f\"Grid: {blocks[0]*blocks[1]*threads[0]*threads[1]:,} threads\")\n",
    "print(f\"Correct: {np.allclose(result, data * 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f834c7c",
   "metadata": {},
   "source": [
    "### Pattern 3: Grid-Stride with Local Accumulation (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def grid_stride_sum(data, partial_sums, n):\n",
    "    \"\"\"Each thread computes partial sum of its elements.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(tid, n, stride):\n",
    "        local_sum += data[i]\n",
    "    \n",
    "    # Store partial sum (will need reduction to complete)\n",
    "    partial_sums[tid] = local_sum\n",
    "\n",
    "# Test\n",
    "n = 1_000_000\n",
    "data = np.random.rand(n).astype(np.float32)\n",
    "blocks, threads = 256, 256\n",
    "total_threads = blocks * threads\n",
    "\n",
    "d_data = cuda.to_device(data)\n",
    "d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "\n",
    "grid_stride_sum[blocks, threads](d_data, d_partial, n)\n",
    "partial = d_partial.copy_to_host()\n",
    "\n",
    "gpu_sum = partial.sum()  # Final reduction on CPU\n",
    "cpu_sum = data.sum()\n",
    "\n",
    "print(f\"CPU sum: {cpu_sum:.6f}\")\n",
    "print(f\"GPU sum: {gpu_sum:.6f}\")\n",
    "print(f\"Match: {np.isclose(cpu_sum, gpu_sum, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459d3fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Optimal Launch Configuration\n",
    "\n",
    "### Guidelines for Choosing Blocks and Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb48d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda.get_current_device()\n",
    "\n",
    "print(\"Device Properties:\")\n",
    "print(f\"  Max threads per block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "print(f\"  Warp size: {device.WARP_SIZE}\")\n",
    "print(f\"  Max blocks per SM: {device.MAX_BLOCK_DIM_X}\")\n",
    "print(f\"  Multiprocessors: {device.MULTIPROCESSOR_COUNT}\")\n",
    "\n",
    "# Recommended config\n",
    "threads = 256  # Multiple of warp size (32)\n",
    "blocks = device.MULTIPROCESSOR_COUNT * 4  # Enough for good occupancy\n",
    "\n",
    "print(f\"\\nRecommended starting config:\")\n",
    "print(f\"  Threads per block: {threads}\")\n",
    "print(f\"  Blocks: {blocks}\")\n",
    "print(f\"  Total threads: {blocks * threads:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9c095",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Grid-Stride is Superior\n",
    "\n",
    "Now let's compare the approaches side-by-side to understand why grid-stride is the professional choice:\n",
    "\n",
    "| Aspect | Naive (1 thread = 1 element) | Grid-Stride Loop |\n",
    "|--------|------------------------------|------------------|\n",
    "| **Grid sizing** | Must match data size | Fixed, tuned once |\n",
    "| **Large data** | Huge grids, overhead | Same small grid |\n",
    "| **Small data** | Underutilized threads | Same config works |\n",
    "| **Occupancy tuning** | Tied to data size | Independent tuning |\n",
    "| **Code reuse** | Different configs | One kernel fits all |\n",
    "| **Memory coalescing** | Natural | Natural |\n",
    "\n",
    "> ğŸ”‘ **Professional Insight:** Grid-stride loops let you tune your kernel launch configuration for **maximum occupancy** on your specific GPU, regardless of problem size.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Performance Analysis\n",
    "\n",
    "Let's measure the real-world impact of grid-stride loops:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cb8e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851de17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_add(n, iterations=100):\n",
    "    \"\"\"Compare naive vs grid-stride for vector addition.\"\"\"\n",
    "    a = np.random.rand(n).astype(np.float32)\n",
    "    b = np.random.rand(n).astype(np.float32)\n",
    "    out = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    d_out = cuda.to_device(out)\n",
    "    \n",
    "    # Naive config: one thread per element\n",
    "    threads_naive = 256\n",
    "    blocks_naive = (n + threads_naive - 1) // threads_naive\n",
    "    \n",
    "    # Grid-stride config: fixed size\n",
    "    threads_gs = 256\n",
    "    blocks_gs = 256\n",
    "    \n",
    "    # Warmup\n",
    "    naive_add[blocks_naive, threads_naive](d_a, d_b, d_out, n)\n",
    "    grid_stride_add[blocks_gs, threads_gs](d_a, d_b, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark naive\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        naive_add[blocks_naive, threads_naive](d_a, d_b, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    naive_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # Benchmark grid-stride\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        grid_stride_add[blocks_gs, threads_gs](d_a, d_b, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    gs_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # NumPy baseline\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        _ = a + b\n",
    "    numpy_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    return naive_time, gs_time, numpy_time, blocks_naive\n",
    "\n",
    "print(f\"{'N':>12} | {'Naive (ms)':>10} | {'Grid-Stride':>11} | {'NumPy':>10} | {'GPU Speedup':>11}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for n in [10_000, 100_000, 1_000_000, 10_000_000]:\n",
    "    naive_t, gs_t, np_t, blocks = benchmark_add(n)\n",
    "    speedup = np_t / gs_t\n",
    "    print(f\"{n:>12,} | {naive_t:>10.3f} | {gs_t:>11.3f} | {np_t:>10.3f} | {speedup:>10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f89ae3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these grid-stride loop exercises in CUDA C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed835f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile grid_stride_exercises.cu\n",
    "// grid_stride_exercises.cu - Grid-stride loop exercises\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Vector Scaling with Grid-Stride Loop\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void vectorScale(float* data, float scalar, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Grid-stride loop\n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        data[i] *= scalar;\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Vector Square Root with Grid-Stride Loop\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void vectorSqrt(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        data[i] = sqrtf(data[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Conditional Processing (threshold filter)\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void thresholdFilter(const float* input, float* output, \n",
    "                                 float threshold, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        output[i] = (input[i] > threshold) ? input[i] : 0.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 4: 2D Brightness Adjustment with Grid-Stride\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void adjustBrightness2D(float* image, float delta, \n",
    "                                    int width, int height) {\n",
    "    // 2D grid-stride loop\n",
    "    int startX = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int startY = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int strideX = blockDim.x * gridDim.x;\n",
    "    int strideY = blockDim.y * gridDim.y;\n",
    "    \n",
    "    for (int y = startY; y < height; y += strideY) {\n",
    "        for (int x = startX; x < width; x += strideX) {\n",
    "            int idx = y * width + x;\n",
    "            float val = image[idx] + delta;\n",
    "            image[idx] = fminf(fmaxf(val, 0.0f), 1.0f);  // Clamp to [0,1]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Test harness\n",
    "// =============================================================================\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Grid-Stride Loop Exercises ===\\n\\n\");\n",
    "    \n",
    "    // Exercise 1: Vector Scaling\n",
    "    printf(\"Exercise 1: Vector Scaling\\n\");\n",
    "    printf(\"-\" \"-------------------------\\n\");\n",
    "    {\n",
    "        const int N = 100000000;  // 100M elements - more than any grid can handle\n",
    "        float* h_data = (float*)malloc(N * sizeof(float));\n",
    "        for (int i = 0; i < N; i++) h_data[i] = (float)(i + 1);\n",
    "        \n",
    "        float* d_data;\n",
    "        CUDA_CHECK(cudaMalloc(&d_data, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        // Use limited grid - grid-stride handles the rest\n",
    "        int threads = 256;\n",
    "        int blocks = 256;  // Only 65K threads for 100M elements!\n",
    "        \n",
    "        vectorScale<<<blocks, threads>>>(d_data, 3.0f, N);\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "        \n",
    "        CUDA_CHECK(cudaMemcpy(h_data, d_data, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Scaled 100M elements with only %d threads\\n\", blocks * threads);\n",
    "        printf(\"Sample: data[0]=%.0f, data[4]=%.0f (expected: 3, 15)\\n\\n\", \n",
    "               h_data[0], h_data[4]);\n",
    "        \n",
    "        cudaFree(d_data);\n",
    "        free(h_data);\n",
    "    }\n",
    "    \n",
    "    // Exercise 2: Vector Sqrt\n",
    "    printf(\"Exercise 2: Vector Square Root\\n\");\n",
    "    printf(\"-\" \"------------------------------\\n\");\n",
    "    {\n",
    "        const int N = 10000;\n",
    "        float* h_data = (float*)malloc(N * sizeof(float));\n",
    "        for (int i = 0; i < N; i++) h_data[i] = (float)((i + 1) * (i + 1));  // Perfect squares\n",
    "        \n",
    "        float* d_data;\n",
    "        CUDA_CHECK(cudaMalloc(&d_data, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        vectorSqrt<<<32, 256>>>(d_data, N);\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "        \n",
    "        CUDA_CHECK(cudaMemcpy(h_data, d_data, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Sample: sqrt(1)=%.0f, sqrt(4)=%.0f, sqrt(9)=%.0f\\n\\n\", \n",
    "               h_data[0], h_data[1], h_data[2]);\n",
    "        \n",
    "        cudaFree(d_data);\n",
    "        free(h_data);\n",
    "    }\n",
    "    \n",
    "    // Exercise 3: Threshold Filter\n",
    "    printf(\"Exercise 3: Threshold Filter\\n\");\n",
    "    printf(\"-\" \"----------------------------\\n\");\n",
    "    {\n",
    "        const int N = 1000;\n",
    "        float* h_input = (float*)malloc(N * sizeof(float));\n",
    "        float* h_output = (float*)malloc(N * sizeof(float));\n",
    "        for (int i = 0; i < N; i++) h_input[i] = (float)(i % 100) / 100.0f;\n",
    "        \n",
    "        float *d_input, *d_output;\n",
    "        CUDA_CHECK(cudaMalloc(&d_input, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMalloc(&d_output, N * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        thresholdFilter<<<4, 256>>>(d_input, d_output, 0.5f, N);\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "        \n",
    "        CUDA_CHECK(cudaMemcpy(h_output, d_output, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        int above = 0, zeroed = 0;\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            if (h_output[i] > 0) above++; else zeroed++;\n",
    "        }\n",
    "        printf(\"Threshold=0.5: %d above, %d zeroed\\n\\n\", above, zeroed);\n",
    "        \n",
    "        cudaFree(d_input); cudaFree(d_output);\n",
    "        free(h_input); free(h_output);\n",
    "    }\n",
    "    \n",
    "    // Exercise 4: 2D Brightness\n",
    "    printf(\"Exercise 4: 2D Brightness Adjustment\\n\");\n",
    "    printf(\"-\" \"------------------------------------\\n\");\n",
    "    {\n",
    "        const int W = 4096, H = 4096;\n",
    "        float* h_image = (float*)malloc(W * H * sizeof(float));\n",
    "        for (int i = 0; i < W * H; i++) h_image[i] = 0.3f;  // Dark image\n",
    "        \n",
    "        float* d_image;\n",
    "        CUDA_CHECK(cudaMalloc(&d_image, W * H * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_image, h_image, W * H * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        // Small grid for large image\n",
    "        dim3 block(16, 16);\n",
    "        dim3 grid(16, 16);  // Only 256 blocks for 16M pixels\n",
    "        \n",
    "        adjustBrightness2D<<<grid, block>>>(d_image, 0.5f, W, H);\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "        \n",
    "        CUDA_CHECK(cudaMemcpy(h_image, d_image, W * H * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Adjusted 4096x4096 image with %d blocks\\n\", 16*16);\n",
    "        printf(\"Sample: pixel=%.1f (was 0.3, +0.5 = 0.8)\\n\", h_image[0]);\n",
    "        \n",
    "        cudaFree(d_image);\n",
    "        free(h_image);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n=== All exercises complete! ===\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eab896",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o grid_stride_exercises grid_stride_exercises.cu && ./grid_stride_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3724a23",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Vector Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3455355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Vector Scaling (Python)\n",
    "@cuda.jit\n",
    "def vector_scale(data, scalar, n):\n",
    "    \"\"\"Multiply every element by scalar: data[i] *= scalar\"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
    "# Expected after scale by 3: [3, 6, 9, 12, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade333c",
   "metadata": {},
   "source": [
    "### Exercise 2: Vector Square Root (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41611f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement grid-stride sqrt\n",
    "@cuda.jit\n",
    "def vector_sqrt(input_data, output_data, n):\n",
    "    \"\"\"Compute sqrt of every element.\"\"\"\n",
    "    # Hint: Use math.sqrt(x) inside the kernel\n",
    "    pass\n",
    "\n",
    "# Test with input = [1, 4, 9, 16, 25]\n",
    "# Expected output = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44974cf5",
   "metadata": {},
   "source": [
    "### Exercise 3: Conditional Processing (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3996b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Double only positive values\n",
    "@cuda.jit\n",
    "def double_positives(data, n):\n",
    "    \"\"\"Double the value of positive elements, leave others unchanged.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test with input = [-2, -1, 0, 1, 2]\n",
    "# Expected output = [-2, -1, 0, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98916fb",
   "metadata": {},
   "source": [
    "### Exercise 4: 2D Brightness Adjustment (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adjust image brightness\n",
    "@cuda.jit\n",
    "def adjust_brightness(image, factor, height, width):\n",
    "    \"\"\"Multiply all pixel values by factor, clamping to [0, 1].\"\"\"\n",
    "    # Use 2D grid-stride pattern\n",
    "    # Clamp: max(0, min(1, value))\n",
    "    pass\n",
    "\n",
    "# Test with a 100x100 image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb426a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### Quick Reference Card: Grid-Stride Loops\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  GRID-STRIDE LOOP PATTERN                                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1D PATTERN:                                                    â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  int tid = blockIdx.x * blockDim.x + threadIdx.x;               â”‚\n",
    "â”‚  int stride = blockDim.x * gridDim.x;                           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  for (int i = tid; i < n; i += stride) {                        â”‚\n",
    "â”‚      data[i] = process(data[i]);                                â”‚\n",
    "â”‚  }                                                              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  2D PATTERN:                                                    â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  int x = blockIdx.x * blockDim.x + threadIdx.x;                 â”‚\n",
    "â”‚  int y = blockIdx.y * blockDim.y + threadIdx.y;                 â”‚\n",
    "â”‚  int strideX = blockDim.x * gridDim.x;                          â”‚\n",
    "â”‚  int strideY = blockDim.y * gridDim.y;                          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  for (int row = y; row < height; row += strideY)                â”‚\n",
    "â”‚      for (int col = x; col < width; col += strideX) {           â”‚\n",
    "â”‚          int idx = row * width + col;                           â”‚\n",
    "â”‚          data[idx] = process(data[idx]);                        â”‚\n",
    "â”‚      }                                                          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  RECOMMENDED LAUNCH CONFIG:                                     â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  int threadsPerBlock = 256;        // Multiple of 32 (warp)     â”‚\n",
    "â”‚  int blocksPerGrid = numSMs * 4;   // Good occupancy            â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### âœ… What You Achieved Today\n",
    "\n",
    "| Skill | Status |\n",
    "|-------|--------|\n",
    "| Identified naive kernel limitations | âœ… |\n",
    "| Implemented 1D grid-stride loops | âœ… |\n",
    "| Extended to 2D grid-stride loops | âœ… |\n",
    "| Configured optimal launch parameters | âœ… |\n",
    "\n",
    "### ğŸ§  Key Benefits to Remember\n",
    "\n",
    "1. âœ“ **Handle any data size** with fixed launch config\n",
    "2. âœ“ **Tune occupancy independently** of data\n",
    "3. âœ“ **Natural coalesced access** pattern\n",
    "4. âœ“ **Professional, reusable** code template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91190d1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ What's Next?\n",
    "\n",
    "**Day 2: Element-wise Vector Operations** â€” We'll use grid-stride loops as the foundation for all our vector operations!\n",
    "\n",
    "| Preview Topic | What You'll Learn |\n",
    "|---------------|-------------------|\n",
    "| Basic arithmetic | add, sub, mul, div on vectors |\n",
    "| Math functions | sqrt, exp, log, sin, cos |\n",
    "| Activation functions | ReLU, sigmoid, tanh |\n",
    "| Compound operations | Combining multiple ops |\n",
    "\n",
    "> ğŸ’¡ **Tomorrow's Hook:** What's the fastest way to apply the same operation to millions of independent elements? (Hint: you already know the pattern!)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
