{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b718310",
   "metadata": {},
   "source": [
    "# üöÄ Day 1: Grid-Stride Loops - The Professional Pattern\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-03/day-1-grid-stride-loops.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand limitations of naive \"one thread per element\" approach\n",
    "- Master the grid-stride loop pattern for production code\n",
    "- Handle arbitrary data sizes with fixed launch configurations\n",
    "- Apply 1D and 2D grid-stride patterns\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02124ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Verify GPU\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")\n",
    "    print(f\"Compute capability: {device.compute_capability}\")\n",
    "    print(f\"Max threads per block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "    print(f\"Max blocks per grid: {device.MAX_GRID_DIM_X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da705d12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Problem with Naive Kernels\n",
    "\n",
    "### Naive Approach: One Thread Per Element\n",
    "\n",
    "The simplest approach assigns exactly one thread to each element.\n",
    "\n",
    "### CUDA C++ Implementation (Primary)\n",
    "\n",
    "Compile and run:\n",
    "```bash\n",
    "nvcc -arch=sm_75 -o naive_add naive_add.cu\n",
    "./naive_add\n",
    "```\n",
    "\n",
    "### Python/Numba (Optional - Interactive Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda34343",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naive_add.cu\n",
    "// naive_add.cu - Naive vector addition\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n",
    "                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Naive kernel: one thread handles exactly one element\n",
    "__global__ void naiveAdd(const float* a, const float* b, float* out, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (tid < n) {  // Bounds check\n",
    "        out[tid] = a[tid] + b[tid];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_a, *h_b, *h_out;\n",
    "    h_a = (float*)malloc(size);\n",
    "    h_b = (float*)malloc(size);\n",
    "    h_out = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize data\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_a, *d_b, *d_out;\n",
    "    CUDA_CHECK(cudaMalloc(&d_a, size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_b, size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_out, size));\n",
    "    \n",
    "    // Copy to device\n",
    "    CUDA_CHECK(cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice));\n",
    "    CUDA_CHECK(cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Launch configuration\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    printf(\"Launching with %d blocks, %d threads/block\\n\", blocksPerGrid, threadsPerBlock);\n",
    "    \n",
    "    naiveAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_out, n);\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Copy back and verify\n",
    "    CUDA_CHECK(cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost));\n",
    "    printf(\"Result[0] = %f (expected 3.0)\\n\", h_out[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_out);\n",
    "    free(h_a); free(h_b); free(h_out);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a306a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o naive_add naive_add.cu\n",
    "!./naive_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for interactive testing\n",
    "@cuda.jit\n",
    "def naive_add(a, b, out, n):\n",
    "    \"\"\"Naive: One thread handles exactly one element.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    \n",
    "    if tid < n:  # Bounds check\n",
    "        out[tid] = a[tid] + b[tid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef1641",
   "metadata": {},
   "source": [
    "### Problems with Naive Approach\n",
    "\n",
    "```\n",
    "Problem 1: Block Size Dependency\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "N = 1000 elements\n",
    "Block size = 256\n",
    "Blocks needed = ceil(1000/256) = 4\n",
    "Total threads = 4 √ó 256 = 1024\n",
    "Wasted threads = 1024 - 1000 = 24 (2.4%)\n",
    "\n",
    "Problem 2: Large Data\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "N = 1 billion elements\n",
    "Block size = 256\n",
    "Blocks needed = ceil(1B/256) = 3,906,250\n",
    "Max blocks (on some GPUs) = 65,535\n",
    "‚ùå FAILS for very large data!\n",
    "\n",
    "Problem 3: Fixed Launch Config\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Every different N requires different grid size.\n",
    "Can't tune for occupancy independently of data size.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b34284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the limitation\n",
    "device = cuda.get_current_device()\n",
    "max_blocks = device.MAX_GRID_DIM_X\n",
    "threads_per_block = 256\n",
    "max_elements_naive = max_blocks * threads_per_block\n",
    "\n",
    "print(f\"Max blocks in X dimension: {max_blocks:,}\")\n",
    "print(f\"Threads per block: {threads_per_block}\")\n",
    "print(f\"Max elements with naive approach: {max_elements_naive:,}\")\n",
    "print(f\"That's only {max_elements_naive / 1e9:.2f} billion elements\")\n",
    "print(f\"\\nModern datasets can have billions of elements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ff382",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Grid-Stride Loop Pattern\n",
    "\n",
    "### The Solution: Each Thread Processes Multiple Elements\n",
    "\n",
    "```\n",
    "Grid-Stride Loop Concept:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Data: [0][1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]...\n",
    "       ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ\n",
    "Grid:  T0 T1 T2 T3 T0 T1 T2 T3 T0  T1  T2  T3  T0  T1  T2  T3\n",
    "       \\________/  \\________/  \\_________/   \\__________/\n",
    "        Pass 1      Pass 2       Pass 3         Pass 4\n",
    "\n",
    "Each thread processes elements at: tid, tid+gridsize, tid+2*gridsize, ...\n",
    "```\n",
    "\n",
    "### CUDA C++ Implementation (Primary)\n",
    "\n",
    "**Key Difference:** \n",
    "- `gridDim.x * blockDim.x` = total threads in grid\n",
    "- Loop `for (i = tid; i < n; i += stride)` processes multiple elements per thread\n",
    "\n",
    "### CUDA C++ vs Python Comparison\n",
    "\n",
    "| CUDA C++ | Python/Numba |\n",
    "|----------|--------------|\n",
    "| `blockIdx.x * blockDim.x + threadIdx.x` | `cuda.grid(1)` |\n",
    "| `blockDim.x * gridDim.x` | `cuda.gridsize(1)` |\n",
    "\n",
    "### Python/Numba (Optional - Interactive Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35060fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile grid_stride_add.cu\n",
    "// grid_stride_add.cu - Professional grid-stride pattern\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Grid-stride kernel: each thread processes multiple elements\n",
    "__global__ void gridStrideAdd(const float* a, const float* b, float* out, int n) {\n",
    "    // Global thread ID\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // Total number of threads in the grid\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Each thread processes elements at tid, tid+stride, tid+2*stride, ...\n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 100000000;  // 100M elements - works with any size!\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate\n",
    "    float *d_a, *d_b, *d_out;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_out, size);\n",
    "    \n",
    "    // Fixed launch config - works for ANY data size\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = 256;  // Fixed, not dependent on n!\n",
    "    \n",
    "    printf(\"Processing %d elements with %d total threads\\n\", \n",
    "           n, threadsPerBlock * blocksPerGrid);\n",
    "    printf(\"Each thread handles ~%d elements\\n\", \n",
    "           n / (threadsPerBlock * blocksPerGrid));\n",
    "    \n",
    "    gridStrideAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_out, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_out);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f98ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o grid_stride_add grid_stride_add.cu\n",
    "!./grid_stride_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45079b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for interactive testing\n",
    "@cuda.jit\n",
    "def grid_stride_add(a, b, out, n):\n",
    "    \"\"\"Grid-stride loop: Each thread processes multiple elements.\"\"\"\n",
    "    tid = cuda.grid(1)           # Global thread ID (= blockIdx.x * blockDim.x + threadIdx.x)\n",
    "    stride = cuda.gridsize(1)    # Total threads (= blockDim.x * gridDim.x)\n",
    "    \n",
    "    # Each thread processes elements at tid, tid+stride, tid+2*stride, ...\n",
    "    for i in range(tid, n, stride):\n",
    "        out[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02754b1c",
   "metadata": {},
   "source": [
    "### Key Functions\n",
    "\n",
    "| Function | Returns | Description |\n",
    "|----------|---------|-------------|\n",
    "| `cuda.grid(1)` | int | Global thread ID (1D) |\n",
    "| `cuda.gridsize(1)` | int | Total threads in grid = blocks √ó threads_per_block |\n",
    "| `cuda.grid(2)` | (x, y) | Global thread ID (2D) |\n",
    "| `cuda.gridsize(2)` | (x, y) | Total threads in each dimension |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid-stride pattern\n",
    "@cuda.jit\n",
    "def show_grid_stride(output, n):\n",
    "    \"\"\"Store which thread processed each element.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        output[i] = tid  # Store thread ID\n",
    "\n",
    "# Small example\n",
    "n = 20\n",
    "blocks = 2\n",
    "threads = 4\n",
    "total_threads = blocks * threads\n",
    "\n",
    "output = np.zeros(n, dtype=np.int32)\n",
    "d_output = cuda.to_device(output)\n",
    "\n",
    "show_grid_stride[blocks, threads](d_output, n)\n",
    "result = d_output.copy_to_host()\n",
    "\n",
    "print(f\"Configuration: {blocks} blocks √ó {threads} threads = {total_threads} total threads\")\n",
    "print(f\"Processing {n} elements\")\n",
    "print(f\"\\nElement index:  {list(range(n))}\")\n",
    "print(f\"Processed by:   {list(result)}\")\n",
    "print(f\"\\nEach thread processes {n // total_threads} elements (plus remainder)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e853ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Benefits of Grid-Stride Loops\n",
    "\n",
    "### Benefit 1: Handle ANY Data Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same kernel config works for different sizes\n",
    "def test_sizes(kernel, sizes, blocks=256, threads=256):\n",
    "    \"\"\"Test kernel with various data sizes.\"\"\"\n",
    "    for n in sizes:\n",
    "        a = np.random.rand(n).astype(np.float32)\n",
    "        b = np.random.rand(n).astype(np.float32)\n",
    "        out = np.zeros(n, dtype=np.float32)\n",
    "        \n",
    "        d_a = cuda.to_device(a)\n",
    "        d_b = cuda.to_device(b)\n",
    "        d_out = cuda.to_device(out)\n",
    "        \n",
    "        kernel[blocks, threads](d_a, d_b, d_out, n)\n",
    "        result = d_out.copy_to_host()\n",
    "        \n",
    "        # Verify\n",
    "        expected = a + b\n",
    "        is_correct = np.allclose(result, expected)\n",
    "        status = \"‚úì\" if is_correct else \"‚úó\"\n",
    "        print(f\"{status} N = {n:>12,} | Blocks = {blocks}, Threads = {threads}\")\n",
    "\n",
    "# Test grid-stride with various sizes\n",
    "print(\"Grid-Stride Loop (same config for all sizes):\")\n",
    "test_sizes(grid_stride_add, [100, 1000, 10000, 100000, 1000000, 10000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfd5ba",
   "metadata": {},
   "source": [
    "### Benefit 2: Tune for Occupancy, Not Data Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different launch configs, same result\n",
    "n = 1_000_000\n",
    "a = np.random.rand(n).astype(np.float32)\n",
    "b = np.random.rand(n).astype(np.float32)\n",
    "out = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_out = cuda.to_device(out)\n",
    "\n",
    "configs = [\n",
    "    (32, 64),    # Few threads\n",
    "    (128, 128),  # Moderate\n",
    "    (256, 256),  # Typical\n",
    "    (512, 512),  # High occupancy\n",
    "]\n",
    "\n",
    "print(f\"Testing different configs with N = {n:,}\\n\")\n",
    "for blocks, threads in configs:\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        grid_stride_add[blocks, threads](d_a, d_b, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    elapsed = (time.perf_counter() - start) / 100 * 1000\n",
    "    \n",
    "    total_threads = blocks * threads\n",
    "    elements_per_thread = n / total_threads\n",
    "    print(f\"Blocks={blocks:3}, Threads={threads:3} | \"\n",
    "          f\"Total={total_threads:>7,} | \"\n",
    "          f\"Elem/Thread={elements_per_thread:>6.1f} | \"\n",
    "          f\"Time={elapsed:.3f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751a109",
   "metadata": {},
   "source": [
    "### Benefit 3: Better Memory Access Patterns\n",
    "\n",
    "Grid-stride loops naturally provide **coalesced memory access**:\n",
    "\n",
    "```\n",
    "Pass 1: Threads 0,1,2,...,31 access elements 0,1,2,...,31  ‚Üê Coalesced!\n",
    "Pass 2: Threads 0,1,2,...,31 access elements 256,257,...,287  ‚Üê Coalesced!\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604caaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Common Patterns\n",
    "\n",
    "### Pattern 1: Basic 1D Grid-Stride (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// 1D Grid-Stride Template\n",
    "__global__ void gridStride1D(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        data[i] = data[i] * 2.0f;  // Some operation\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Pattern 2: 2D Grid-Stride (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// 2D Grid-Stride for images/matrices\n",
    "__global__ void gridStride2D(float* data, int height, int width) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int strideX = blockDim.x * gridDim.x;\n",
    "    int strideY = blockDim.y * gridDim.y;\n",
    "    \n",
    "    for (int row = y; row < height; row += strideY) {\n",
    "        for (int col = x; col < width; col += strideX) {\n",
    "            int idx = row * width + col;\n",
    "            data[idx] = data[idx] * 2.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Launch with 2D config:\n",
    "// dim3 threads(16, 16);\n",
    "// dim3 blocks(32, 32);\n",
    "// gridStride2D<<<blocks, threads>>>(d_data, height, width);\n",
    "```\n",
    "\n",
    "### Pattern 3: Grid-Stride with Local Accumulation (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// Each thread computes partial sum of its elements\n",
    "__global__ void gridStrideSum(const float* data, float* partialSums, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    float localSum = 0.0f;\n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        localSum += data[i];\n",
    "    }\n",
    "    \n",
    "    // Store partial sum (will need reduction to complete)\n",
    "    partialSums[tid] = localSum;\n",
    "}\n",
    "```\n",
    "\n",
    "### Python/Numba Equivalents (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Basic 1D Grid-Stride (Python)\n",
    "@cuda.jit\n",
    "def grid_stride_1d(data, n):\n",
    "    \"\"\"Basic 1D grid-stride pattern.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        data[i] = data[i] * 2  # Some operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e689f",
   "metadata": {},
   "source": [
    "### Pattern 2: 2D Grid-Stride (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a4ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def grid_stride_2d(data, height, width):\n",
    "    \"\"\"2D grid-stride for images/matrices.\"\"\"\n",
    "    start_x, start_y = cuda.grid(2)\n",
    "    stride_x, stride_y = cuda.gridsize(2)\n",
    "    \n",
    "    for y in range(start_y, height, stride_y):\n",
    "        for x in range(start_x, width, stride_x):\n",
    "            data[y, x] = data[y, x] * 2\n",
    "\n",
    "# Test 2D grid-stride\n",
    "height, width = 1000, 1000\n",
    "data = np.random.rand(height, width).astype(np.float32)\n",
    "d_data = cuda.to_device(data)\n",
    "\n",
    "threads = (16, 16)\n",
    "blocks = (32, 32)  # Can be any size, not dependent on image size\n",
    "\n",
    "grid_stride_2d[blocks, threads](d_data, height, width)\n",
    "result = d_data.copy_to_host()\n",
    "\n",
    "print(f\"Image: {height}x{width} = {height*width:,} pixels\")\n",
    "print(f\"Grid: {blocks[0]*blocks[1]*threads[0]*threads[1]:,} threads\")\n",
    "print(f\"Correct: {np.allclose(result, data * 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f834c7c",
   "metadata": {},
   "source": [
    "### Pattern 3: Grid-Stride with Local Accumulation (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def grid_stride_sum(data, partial_sums, n):\n",
    "    \"\"\"Each thread computes partial sum of its elements.\"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(tid, n, stride):\n",
    "        local_sum += data[i]\n",
    "    \n",
    "    # Store partial sum (will need reduction to complete)\n",
    "    partial_sums[tid] = local_sum\n",
    "\n",
    "# Test\n",
    "n = 1_000_000\n",
    "data = np.random.rand(n).astype(np.float32)\n",
    "blocks, threads = 256, 256\n",
    "total_threads = blocks * threads\n",
    "\n",
    "d_data = cuda.to_device(data)\n",
    "d_partial = cuda.device_array(total_threads, dtype=np.float32)\n",
    "\n",
    "grid_stride_sum[blocks, threads](d_data, d_partial, n)\n",
    "partial = d_partial.copy_to_host()\n",
    "\n",
    "gpu_sum = partial.sum()  # Final reduction on CPU\n",
    "cpu_sum = data.sum()\n",
    "\n",
    "print(f\"CPU sum: {cpu_sum:.6f}\")\n",
    "print(f\"GPU sum: {gpu_sum:.6f}\")\n",
    "print(f\"Match: {np.isclose(cpu_sum, gpu_sum, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459d3fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Optimal Launch Configuration\n",
    "\n",
    "### Guidelines for Choosing Blocks and Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb48d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda.get_current_device()\n",
    "\n",
    "print(\"Device Properties:\")\n",
    "print(f\"  Max threads per block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "print(f\"  Warp size: {device.WARP_SIZE}\")\n",
    "print(f\"  Max blocks per SM: {device.MAX_BLOCK_DIM_X}\")\n",
    "print(f\"  Multiprocessors: {device.MULTIPROCESSOR_COUNT}\")\n",
    "\n",
    "# Recommended config\n",
    "threads = 256  # Multiple of warp size (32)\n",
    "blocks = device.MULTIPROCESSOR_COUNT * 4  # Enough for good occupancy\n",
    "\n",
    "print(f\"\\nRecommended starting config:\")\n",
    "print(f\"  Threads per block: {threads}\")\n",
    "print(f\"  Blocks: {blocks}\")\n",
    "print(f\"  Total threads: {blocks * threads:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9c095",
   "metadata": {},
   "source": [
    "### Rules of Thumb\n",
    "\n",
    "```\n",
    "Thread count per block:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚Ä¢ Always multiple of 32 (warp size)\n",
    "‚Ä¢ 128-256 is usually good\n",
    "‚Ä¢ 512 for compute-heavy kernels\n",
    "‚Ä¢ Max 1024 on most GPUs\n",
    "\n",
    "Block count:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚Ä¢ At least SMs √ó 2 (hide latency)\n",
    "‚Ä¢ SMs √ó 4 to SMs √ó 8 is often optimal\n",
    "‚Ä¢ More blocks = more flexibility for scheduler\n",
    "\n",
    "With grid-stride, you can always use:\n",
    "  blocks = SM_count * 4\n",
    "  threads = 256\n",
    "And it will work for any data size!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cb8e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851de17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_add(n, iterations=100):\n",
    "    \"\"\"Compare naive vs grid-stride for vector addition.\"\"\"\n",
    "    a = np.random.rand(n).astype(np.float32)\n",
    "    b = np.random.rand(n).astype(np.float32)\n",
    "    out = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    d_out = cuda.to_device(out)\n",
    "    \n",
    "    # Naive config: one thread per element\n",
    "    threads_naive = 256\n",
    "    blocks_naive = (n + threads_naive - 1) // threads_naive\n",
    "    \n",
    "    # Grid-stride config: fixed size\n",
    "    threads_gs = 256\n",
    "    blocks_gs = 256\n",
    "    \n",
    "    # Warmup\n",
    "    naive_add[blocks_naive, threads_naive](d_a, d_b, d_out, n)\n",
    "    grid_stride_add[blocks_gs, threads_gs](d_a, d_b, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark naive\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        naive_add[blocks_naive, threads_naive](d_a, d_b, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    naive_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # Benchmark grid-stride\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        grid_stride_add[blocks_gs, threads_gs](d_a, d_b, d_out, n)\n",
    "    cuda.synchronize()\n",
    "    gs_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # NumPy baseline\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        _ = a + b\n",
    "    numpy_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    return naive_time, gs_time, numpy_time, blocks_naive\n",
    "\n",
    "print(f\"{'N':>12} | {'Naive (ms)':>10} | {'Grid-Stride':>11} | {'NumPy':>10} | {'GPU Speedup':>11}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for n in [10_000, 100_000, 1_000_000, 10_000_000]:\n",
    "    naive_t, gs_t, np_t, blocks = benchmark_add(n)\n",
    "    speedup = np_t / gs_t\n",
    "    print(f\"{n:>12,} | {naive_t:>10.3f} | {gs_t:>11.3f} | {np_t:>10.3f} | {speedup:>10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f89ae3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Complete these exercises in CUDA C++ first, then optionally test with Python.\n",
    "\n",
    "### Exercise 1: Vector Scaling (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// TODO: Implement grid-stride vector scaling\n",
    "// File: vector_scale.cu\n",
    "\n",
    "__global__ void vectorScale(float* data, float scalar, int n) {\n",
    "    // TODO: Implement grid-stride loop\n",
    "    // Multiply every element by scalar: data[i] *= scalar\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Test with data = [1, 2, 3, 4, 5], scalar = 3\n",
    "    // Expected: [3, 6, 9, 12, 15]\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Exercise 2: Vector Square Root (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// TODO: Apply sqrt to every element\n",
    "// File: vector_sqrt.cu\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void vectorSqrt(const float* input, float* output, int n) {\n",
    "    // TODO: Implement grid-stride loop with sqrtf()\n",
    "}\n",
    "```\n",
    "\n",
    "### Exercise 3: Conditional Processing (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// TODO: Double only positive values\n",
    "// File: double_positives.cu\n",
    "\n",
    "__global__ void doublePositives(float* data, int n) {\n",
    "    // TODO: Double positive elements, leave others unchanged\n",
    "    // Input:  [-2, -1, 0, 1, 2]\n",
    "    // Output: [-2, -1, 0, 2, 4]\n",
    "}\n",
    "```\n",
    "\n",
    "### Exercise 4: 2D Brightness Adjustment (CUDA C++)\n",
    "\n",
    "```cpp\n",
    "// TODO: Adjust image brightness\n",
    "// File: brightness.cu\n",
    "\n",
    "__global__ void adjustBrightness(float* image, float factor, int height, int width) {\n",
    "    // TODO: Use 2D grid-stride pattern\n",
    "    // Multiply all pixels by factor, clamp to [0, 1]\n",
    "    // Clamp: fminf(1.0f, fmaxf(0.0f, value))\n",
    "}\n",
    "```\n",
    "\n",
    "### Python/Numba Practice (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3455355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Vector Scaling (Python)\n",
    "@cuda.jit\n",
    "def vector_scale(data, scalar, n):\n",
    "    \"\"\"Multiply every element by scalar: data[i] *= scalar\"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
    "# Expected after scale by 3: [3, 6, 9, 12, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade333c",
   "metadata": {},
   "source": [
    "### Exercise 2: Vector Square Root (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41611f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement grid-stride sqrt\n",
    "@cuda.jit\n",
    "def vector_sqrt(input_data, output_data, n):\n",
    "    \"\"\"Compute sqrt of every element.\"\"\"\n",
    "    # Hint: Use math.sqrt(x) inside the kernel\n",
    "    pass\n",
    "\n",
    "# Test with input = [1, 4, 9, 16, 25]\n",
    "# Expected output = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44974cf5",
   "metadata": {},
   "source": [
    "### Exercise 3: Conditional Processing (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3996b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Double only positive values\n",
    "@cuda.jit\n",
    "def double_positives(data, n):\n",
    "    \"\"\"Double the value of positive elements, leave others unchanged.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test with input = [-2, -1, 0, 1, 2]\n",
    "# Expected output = [-2, -1, 0, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98916fb",
   "metadata": {},
   "source": [
    "### Exercise 4: 2D Brightness Adjustment (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adjust image brightness\n",
    "@cuda.jit\n",
    "def adjust_brightness(image, factor, height, width):\n",
    "    \"\"\"Multiply all pixel values by factor, clamping to [0, 1].\"\"\"\n",
    "    # Use 2D grid-stride pattern\n",
    "    # Clamp: max(0, min(1, value))\n",
    "    pass\n",
    "\n",
    "# Test with a 100x100 image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb426a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### CUDA C++ Grid-Stride Template\n",
    "\n",
    "```cpp\n",
    "// 1D Grid-Stride\n",
    "__global__ void kernel1D(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        data[i] = process(data[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// 2D Grid-Stride\n",
    "__global__ void kernel2D(float* data, int height, int width) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int strideX = blockDim.x * gridDim.x;\n",
    "    int strideY = blockDim.y * gridDim.y;\n",
    "    \n",
    "    for (int row = y; row < height; row += strideY) {\n",
    "        for (int col = x; col < width; col += strideX) {\n",
    "            int idx = row * width + col;\n",
    "            data[idx] = process(data[idx]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Python/Numba Equivalent (Optional Reference)\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def kernel_1d(data, n):\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(tid, n, stride):\n",
    "        data[i] = process(data[i])\n",
    "\n",
    "@cuda.jit\n",
    "def kernel_2d(data, height, width):\n",
    "    x, y = cuda.grid(2)\n",
    "    stride_x, stride_y = cuda.gridsize(2)\n",
    "    for row in range(y, height, stride_y):\n",
    "        for col in range(x, width, stride_x):\n",
    "            data[row, col] = process(data[row, col])\n",
    "```\n",
    "\n",
    "### Key Benefits\n",
    "1. ‚úì Handle any data size with fixed launch config\n",
    "2. ‚úì Tune occupancy independently of data\n",
    "3. ‚úì Natural coalesced access\n",
    "4. ‚úì Professional, reusable pattern\n",
    "\n",
    "### Recommended Config\n",
    "```cpp\n",
    "int threadsPerBlock = 256;  // Multiple of 32\n",
    "int blocksPerGrid = numSMs * 4;  // Good occupancy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91190d1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "üìã **Day 2:** Element-wise vector operations (add, sub, mul, div, math functions)\n",
    "\n",
    "We'll use grid-stride loops as the foundation for all our vector operations!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
