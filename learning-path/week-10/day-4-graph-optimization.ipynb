{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca98408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e522ef3",
   "metadata": {},
   "source": [
    "# Day 4: Graph Optimization\n",
    "\n",
    "## ğŸ¯ The Hook: Finding the Fastest Recipe\n",
    "\n",
    "**A Michelin chef doesn't just follow recipesâ€”they optimize them.** Should you prep all vegetables first, or interleave with cooking? Can two cooks work in parallel? Is the overhead of coordinating them worth it for a simple dish?\n",
    "\n",
    "```\n",
    "The Optimization Questions:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "1. WHEN to use graphs?\n",
    "   â€¢ 10 small kernels: YES! (50% overhead â†’ 5%)\n",
    "   â€¢ 1 large kernel:   NO!  (0.1% overhead â†’ still 0.1%)\n",
    "\n",
    "2. HOW to structure graphs?\n",
    "   â€¢ Deep chain: K1â†’K2â†’K3â†’K4â†’K5 (serial, simple)\n",
    "   â€¢ Wide fork:  K1â†’[K2,K3,K4]â†’K5 (parallel, faster!)\n",
    "\n",
    "3. WHAT'S the breakeven point?\n",
    "   â€¢ Setup cost: ~100Î¼s (capture + instantiate)\n",
    "   â€¢ Savings per launch: ~50Î¼s (10 kernels Ã— 5Î¼s)\n",
    "   â€¢ Breakeven: 100Ã·50 = 2 iterations!\n",
    "   â€¢ After 100 iterations: 50x faster total!\n",
    "```\n",
    "\n",
    "**Today you'll learn:** How to analyze, optimize, and know when graphs helpâ€”turning good code into great code!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Analyze** when CUDA graphs provide benefit vs. add overhead\n",
    "2. **Calculate breakeven points** for graph usage decisions\n",
    "3. **Optimize graph structure** for maximum parallelism\n",
    "4. **Profile** graph execution to identify bottlenecks\n",
    "5. **Apply best practices** for production graph usage\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸƒ Concept Card: Finding the Fastest Recipe\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  GRAPH OPTIMIZATION = CHEF'S EFFICIENCY ANALYSIS               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  DECISION MATRIX - When to Use Graphs:                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â”‚  âœ… USE GRAPHS when:                                    â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Many small kernels (10+ at <0.1ms each)              â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Repeated execution (100+ times)                      â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Fixed topology with variable data                    â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Training loops, inference pipelines                  â”‚   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â”‚  âŒ AVOID GRAPHS when:                                  â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Dynamic control flow (if/else per iteration)        â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ One-shot execution (setup > savings)                 â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Large kernels (launch overhead negligible)           â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Grid/block dimensions change per iteration           â”‚   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  BREAKEVEN ANALYSIS:                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â”‚  Setup Cost:    Capture (~100Î¼s) + Instantiate (~50Î¼s)  â”‚   â”‚\n",
    "â”‚  â”‚  Per-Launch:    Regular: ~10Î¼s Ã— N kernels              â”‚   â”‚\n",
    "â”‚  â”‚                 Graph:   ~5Î¼s total                     â”‚   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â”‚  Example (10 kernels):                                  â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Regular: 10 Ã— 10Î¼s = 100Î¼s per launch                â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Graph: 150Î¼s setup + 5Î¼s per launch                  â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Breakeven: 150 Ã· (100-5) â‰ˆ 2 iterations!            â”‚   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â”‚  After 1000 iterations:                                 â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Regular: 1000 Ã— 100Î¼s = 100,000Î¼s                    â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Graph:   150Î¼s + 1000 Ã— 5Î¼s = 5,150Î¼s                â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Speedup: ~20x!                                       â”‚   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  STRUCTURE OPTIMIZATION:                                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚  Before (Deep):        After (Wide):                    â”‚   â”‚\n",
    "â”‚  â”‚  K1â†’K2â†’K3â†’K4           K1â†’[K2,K3]â†’K4                    â”‚   â”‚\n",
    "â”‚  â”‚  Time: 4 steps         Time: 3 steps (parallel!)        â”‚   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â”‚  Optimization Tips:                                     â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Minimize graph depth (critical path)                 â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Maximize width (parallel kernels)                    â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Fuse tiny kernels into larger ones                   â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Use updates, never rebuild unnecessarily             â”‚   â”‚\n",
    "â”‚  â”‚                                                         â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ’¡ Key Insight: Graphs are an optimization toolâ€”measure first,â”‚\n",
    "â”‚     then apply. The best optimization is knowing when NOT to!  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: When to Use Graphs\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘               WHEN TO USE CUDA GRAPHS                    â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                          â•‘\n",
    "â•‘  âœ… USE GRAPHS when:                                     â•‘\n",
    "â•‘  â€¢ Same sequence repeated many times (1000+)             â•‘\n",
    "â•‘  â€¢ Many small kernels (<0.1ms each)                      â•‘\n",
    "â•‘  â€¢ Fixed workflow with variable data                     â•‘\n",
    "â•‘  â€¢ Launch overhead is significant portion of time        â•‘\n",
    "â•‘  â€¢ Inference in production (fixed model)                 â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•‘  âŒ AVOID GRAPHS when:                                   â•‘\n",
    "â•‘  â€¢ Dynamic control flow (if/else, variable loops)        â•‘\n",
    "â•‘  â€¢ One-shot execution                                    â•‘\n",
    "â•‘  â€¢ Large kernels where launch overhead negligible        â•‘\n",
    "â•‘  â€¢ Frequently changing graph topology                    â•‘\n",
    "â•‘  â€¢ Grid/block dimensions change per iteration            â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "### Breakeven Analysis\n",
    "\n",
    "```cpp\n",
    "// When does a graph pay off?\n",
    "\n",
    "// Costs:\n",
    "// - Capture:     ~100-500Î¼s\n",
    "// - Instantiate: ~10-50Î¼s\n",
    "// - Launch:      ~1-5Î¼s (vs ~5-15Î¼s regular)\n",
    "\n",
    "// Breakeven example:\n",
    "// Regular: 10 kernels Ã— 10Î¼s launch = 100Î¼s per iteration\n",
    "// Graph:   Setup 150Î¼s + 5Î¼s per iteration\n",
    "// Breakeven at 150Î¼s / (100 - 5)Î¼s â‰ˆ 2 iterations\n",
    "\n",
    "// After 100 iterations:\n",
    "// Regular: 100 Ã— 100Î¼s = 10,000Î¼s\n",
    "// Graph:   150Î¼s + 100 Ã— 5Î¼s = 650Î¼s\n",
    "// â†’ 15x improvement!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db66a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Graph Structure Optimization\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates techniques for optimizing graph structure including minimizing depth and fusing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph_optimization.cu\n",
    "// graph_optimization.cu - Graph structure optimization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Technique 1: Minimize graph depth\n",
    "// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "// BAD: Sequential chain\n",
    "//   A â†’ B â†’ C â†’ D â†’ E â†’ F\n",
    "//   Depth = 6, minimal parallelism\n",
    "\n",
    "// GOOD: Parallel branches\n",
    "//   A â†’ B â†’ C\n",
    "//             \\â†’ F\n",
    "//   D â†’ E â†’  /\n",
    "//   Depth = 4, more parallelism\n",
    "\n",
    "__global__ void kernel(float* d, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) d[tid] *= 2.0f;\n",
    "}\n",
    "\n",
    "// Technique 2: Batch similar operations\n",
    "// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "__global__ void fusedKernel(float* a, float* b, float* c, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        // Multiple operations in one kernel\n",
    "        float x = a[tid];\n",
    "        x = x * 2.0f;  // Was kernel 1\n",
    "        x = x + 1.0f;  // Was kernel 2\n",
    "        x = sqrtf(x);  // Was kernel 3\n",
    "        c[tid] = x;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_a, *d_b;\n",
    "    cudaMalloc(&d_a, N * sizeof(float));\n",
    "    cudaMalloc(&d_b, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // Capture Optimized Graph\n",
    "    // ============================================\n",
    "    cudaGraph_t graph;\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Parallel memcpys (device-to-device)\n",
    "    cudaMemcpyAsync(d_b, d_a, N/2 * sizeof(float), \n",
    "                    cudaMemcpyDeviceToDevice, stream);\n",
    "    \n",
    "    // Fused kernel instead of multiple small ones\n",
    "    fusedKernel<<<256, 256, 0, stream>>>(d_a, d_a, d_b, N);\n",
    "    \n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    \n",
    "    // ============================================\n",
    "    // Verify Graph Structure\n",
    "    // ============================================\n",
    "    size_t numNodes;\n",
    "    cudaGraphGetNodes(graph, NULL, &numNodes);\n",
    "    printf(\"Optimized graph has %zu nodes\\n\", numNodes);\n",
    "    \n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Warmup\n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 1000;\n",
    "    cudaEventRecord(start, stream);\n",
    "    \n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Average launch time: %.2f Î¼s\\n\", (ms * 1000) / ITERATIONS);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26affe48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Graphs with Streams\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates running multiple graphs concurrently on different streams for maximum GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a289dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile concurrent_graphs.cu\n",
    "// concurrent_graphs.cu - Running multiple graphs concurrently\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] = sqrtf(data[tid] + 1.0f);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 18;\n",
    "    const int NUM_GRAPHS = 4;\n",
    "    \n",
    "    float* d_data[NUM_GRAPHS];\n",
    "    cudaStream_t streams[NUM_GRAPHS];\n",
    "    cudaGraphExec_t graphExecs[NUM_GRAPHS];\n",
    "    \n",
    "    // ============================================\n",
    "    // Create Multiple Graphs\n",
    "    // ============================================\n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaMalloc(&d_data[i], N * sizeof(float));\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "        \n",
    "        // Capture graph for this stream\n",
    "        cudaGraph_t graph;\n",
    "        cudaStreamBeginCapture(streams[i], cudaStreamCaptureModeGlobal);\n",
    "        \n",
    "        process<<<256, 256, 0, streams[i]>>>(d_data[i], N);\n",
    "        \n",
    "        cudaStreamEndCapture(streams[i], &graph);\n",
    "        cudaGraphInstantiate(&graphExecs[i], graph, NULL, NULL, 0);\n",
    "        cudaGraphDestroy(graph);  // Can destroy after instantiate\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch All Graphs Concurrently\n",
    "    // ============================================\n",
    "    // Different graphs on different streams run in parallel!\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start, 0);\n",
    "    \n",
    "    for (int iter = 0; iter < 100; iter++) {\n",
    "        for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "            // Each graph on its own stream\n",
    "            cudaGraphLaunch(graphExecs[i], streams[i]);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop, 0);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"%d concurrent graphs, 100 iterations: %.2f ms\\n\", NUM_GRAPHS, ms);\n",
    "    printf(\"Average per iteration: %.2f Î¼s\\n\", (ms * 1000) / 100);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaGraphExecDestroy(graphExecs[i]);\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o concurrent_graphs concurrent_graphs.cu && ./concurrent_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7427b2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Real-World Pattern - Inference Pipeline\n",
    "\n",
    "### ğŸ”· CUDA C++ Inference Graph (Primary)\n",
    "\n",
    "This example demonstrates a practical neural network inference pipeline using CUDA Graphs. The graph captures multiple layer operations (Linear + ReLU activations + Softmax) and benchmarks the throughput achievable with graph-based execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15173f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_graph.cu\n",
    "// inference_graph.cu - Neural network inference pattern\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "// Simplified layer kernels\n",
    "__global__ void linearLayer(float* out, const float* in, const float* W, \n",
    "                            const float* b, int out_dim, int in_dim) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < out_dim) {\n",
    "        float sum = b[tid];\n",
    "        for (int i = 0; i < in_dim; i++) {\n",
    "            sum += W[tid * in_dim + i] * in[i];\n",
    "        }\n",
    "        out[tid] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void relu(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] = fmaxf(0.0f, data[tid]);\n",
    "}\n",
    "\n",
    "__global__ void softmaxKernel(float* out, const float* in, int n) {\n",
    "    __shared__ float maxVal;\n",
    "    __shared__ float sum;\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Find max (simplified for small n)\n",
    "    if (tid == 0) {\n",
    "        maxVal = in[0];\n",
    "        for (int i = 1; i < n; i++) {\n",
    "            if (in[i] > maxVal) maxVal = in[i];\n",
    "        }\n",
    "        sum = 0.0f;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute exp sum\n",
    "    if (tid < n) {\n",
    "        float val = expf(in[tid] - maxVal);\n",
    "        atomicAdd(&sum, val);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Normalize\n",
    "    if (tid < n) out[tid] = expf(in[tid] - maxVal) / sum;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int INPUT_DIM = 784;\n",
    "    const int HIDDEN_DIM = 256;\n",
    "    const int OUTPUT_DIM = 10;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_hidden1, *d_hidden2, *d_output;\n",
    "    float *d_W1, *d_b1, *d_W2, *d_b2, *d_W3, *d_b3;\n",
    "    \n",
    "    cudaMalloc(&d_input, INPUT_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_hidden1, HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_hidden2, HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_output, OUTPUT_DIM * sizeof(float));\n",
    "    \n",
    "    // Weights (simplified - just allocate, don't init)\n",
    "    cudaMalloc(&d_W1, HIDDEN_DIM * INPUT_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_b1, HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_W2, HIDDEN_DIM * HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_b2, HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_W3, OUTPUT_DIM * HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_b3, OUTPUT_DIM * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // Build inference graph\n",
    "    cudaGraph_t graph;\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Layer 1: Linear + ReLU\n",
    "    linearLayer<<<1, 256, 0, stream>>>(d_hidden1, d_input, d_W1, d_b1,\n",
    "                                        HIDDEN_DIM, INPUT_DIM);\n",
    "    relu<<<1, 256, 0, stream>>>(d_hidden1, HIDDEN_DIM);\n",
    "    \n",
    "    // Layer 2: Linear + ReLU\n",
    "    linearLayer<<<1, 256, 0, stream>>>(d_hidden2, d_hidden1, d_W2, d_b2,\n",
    "                                        HIDDEN_DIM, HIDDEN_DIM);\n",
    "    relu<<<1, 256, 0, stream>>>(d_hidden2, HIDDEN_DIM);\n",
    "    \n",
    "    // Layer 3: Linear + Softmax\n",
    "    linearLayer<<<1, 256, 0, stream>>>(d_output, d_hidden2, d_W3, d_b3,\n",
    "                                        OUTPUT_DIM, HIDDEN_DIM);\n",
    "    softmaxKernel<<<1, 32, 0, stream>>>(d_output, d_output, OUTPUT_DIM);\n",
    "    \n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    \n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    size_t numNodes;\n",
    "    cudaGraphGetNodes(graph, NULL, &numNodes);\n",
    "    printf(\"Inference graph has %zu nodes (6 kernels expected)\\n\", numNodes);\n",
    "    \n",
    "    cudaGraphDestroy(graph);  // Template no longer needed\n",
    "    \n",
    "    // Benchmark inference\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 10000;\n",
    "    \n",
    "    // Warmup\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEventRecord(start, stream);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    cudaEventRecord(stop, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Inference: %d iterations in %.2f ms\\n\", ITERATIONS, ms);\n",
    "    printf(\"Per inference: %.2f Î¼s\\n\", (ms * 1000) / ITERATIONS);\n",
    "    printf(\"Throughput: %.0f inferences/sec\\n\", ITERATIONS / (ms / 1000));\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_hidden1);\n",
    "    cudaFree(d_hidden2);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_W1); cudaFree(d_b1);\n",
    "    cudaFree(d_W2); cudaFree(d_b2);\n",
    "    cudaFree(d_W3); cudaFree(d_b3);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o inference_graph inference_graph.cu && ./inference_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf2749",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Best Practices Summary\n",
    "\n",
    "### Graph Lifecycle\n",
    "\n",
    "```cpp\n",
    "// BEST PRACTICE: Proper lifecycle management\n",
    "\n",
    "// 1. Create/capture graph\n",
    "cudaGraph_t graph;\n",
    "// ... capture or build ...\n",
    "\n",
    "// 2. Instantiate once\n",
    "cudaGraphExec_t graphExec;\n",
    "cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "\n",
    "// 3. Destroy template (optional - saves memory)\n",
    "cudaGraphDestroy(graph);\n",
    "\n",
    "// 4. Launch many times\n",
    "for (int i = 0; i < 1000000; i++) {\n",
    "    // Update if needed\n",
    "    if (parametersChanged) {\n",
    "        cudaGraphExecKernelNodeSetParams(...);\n",
    "    }\n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "}\n",
    "\n",
    "// 5. Cleanup at end\n",
    "cudaGraphExecDestroy(graphExec);\n",
    "```\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "```\n",
    "âŒ MISTAKE 1: Rebuilding graphs every iteration\n",
    "   â†’ Build once, update parameters\n",
    "\n",
    "âŒ MISTAKE 2: Using graphs for single-shot work\n",
    "   â†’ Use regular launches for one-off tasks\n",
    "\n",
    "âŒ MISTAKE 3: Capturing allocations in graph\n",
    "   â†’ Allocate outside, only use inside\n",
    "\n",
    "âŒ MISTAKE 4: Forgetting to sync before reading results\n",
    "   â†’ Always cudaStreamSynchronize before host access\n",
    "\n",
    "âŒ MISTAKE 5: Using host-side conditionals in capture\n",
    "   â†’ All conditionals evaluated at capture time!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba361b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph_optimization_exercises.cu\n",
    "/*\n",
    " * CUDA Graph Optimization Exercises\n",
    " * Exercise 1: Breakeven Analysis - When graphs beat regular launches\n",
    " * Exercise 2: Multi-Stream Graphs - Concurrent execution within graphs\n",
    " * Exercise 3: Complete Inference Pipeline - Full pipeline with memcpy nodes\n",
    " */\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <chrono>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Simple kernel for benchmarking\n",
    "__global__ void simpleKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = data[idx] * 2.0f + 1.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Simulated layer kernels for inference pipeline\n",
    "__global__ void layer1Kernel(float* input, float* output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        output[idx] = tanhf(input[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void layer2Kernel(float* input, float* output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        output[idx] = input[idx] * 0.5f + 0.5f;  // Sigmoid approx\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void layer3Kernel(float* input, float* output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        output[idx] = fmaxf(0.0f, input[idx]);  // ReLU\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================\n",
    "// Exercise 1: Breakeven Analysis\n",
    "// Measure at what iteration count graphs become faster\n",
    "// =============================================================\n",
    "void exercise1_breakeven_analysis() {\n",
    "    printf(\"\\n=== Exercise 1: Breakeven Analysis ===\\n\");\n",
    "    \n",
    "    const int N = 1024;  // Small N to emphasize launch overhead\n",
    "    const int BLOCK_SIZE = 256;\n",
    "    const int GRID_SIZE = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMemset(d_data, 0, N * sizeof(float)));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    CHECK_CUDA(cudaStreamCreate(&stream));\n",
    "    \n",
    "    // Create graph\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t graphExec;\n",
    "    \n",
    "    CHECK_CUDA(cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal));\n",
    "    simpleKernel<<<GRID_SIZE, BLOCK_SIZE, 0, stream>>>(d_data, N);\n",
    "    CHECK_CUDA(cudaStreamEndCapture(stream, &graph));\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0));\n",
    "    \n",
    "    // Test different iteration counts\n",
    "    int testCounts[] = {1, 5, 10, 25, 50, 100, 200, 500, 1000};\n",
    "    int numTests = sizeof(testCounts) / sizeof(testCounts[0]);\n",
    "    \n",
    "    printf(\"\\n%-12s %-15s %-15s %-10s\\n\", \"Iterations\", \"Regular (Âµs)\", \"Graph (Âµs)\", \"Winner\");\n",
    "    printf(\"%-12s %-15s %-15s %-10s\\n\", \"----------\", \"-----------\", \"---------\", \"------\");\n",
    "    \n",
    "    int breakeven = -1;\n",
    "    \n",
    "    for (int t = 0; t < numTests; t++) {\n",
    "        int iters = testCounts[t];\n",
    "        \n",
    "        // Warm up\n",
    "        for (int i = 0; i < 10; i++) {\n",
    "            simpleKernel<<<GRID_SIZE, BLOCK_SIZE, 0, stream>>>(d_data, N);\n",
    "            CHECK_CUDA(cudaGraphLaunch(graphExec, stream));\n",
    "        }\n",
    "        CHECK_CUDA(cudaStreamSynchronize(stream));\n",
    "        \n",
    "        // Time regular launches\n",
    "        auto start = std::chrono::high_resolution_clock::now();\n",
    "        for (int i = 0; i < iters; i++) {\n",
    "            simpleKernel<<<GRID_SIZE, BLOCK_SIZE, 0, stream>>>(d_data, N);\n",
    "        }\n",
    "        CHECK_CUDA(cudaStreamSynchronize(stream));\n",
    "        auto end = std::chrono::high_resolution_clock::now();\n",
    "        double regularTime = std::chrono::duration<double, std::micro>(end - start).count();\n",
    "        \n",
    "        // Time graph launches\n",
    "        start = std::chrono::high_resolution_clock::now();\n",
    "        for (int i = 0; i < iters; i++) {\n",
    "            CHECK_CUDA(cudaGraphLaunch(graphExec, stream));\n",
    "        }\n",
    "        CHECK_CUDA(cudaStreamSynchronize(stream));\n",
    "        end = std::chrono::high_resolution_clock::now();\n",
    "        double graphTime = std::chrono::duration<double, std::micro>(end - start).count();\n",
    "        \n",
    "        const char* winner = graphTime < regularTime ? \"Graph\" : \"Regular\";\n",
    "        printf(\"%-12d %-15.2f %-15.2f %-10s\\n\", iters, regularTime, graphTime, winner);\n",
    "        \n",
    "        if (breakeven == -1 && graphTime < regularTime) {\n",
    "            breakeven = iters;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (breakeven != -1) {\n",
    "        printf(\"\\nBreakeven point: Around %d iterations\\n\", breakeven);\n",
    "    } else {\n",
    "        printf(\"\\nGraph not faster in tested range\\n\");\n",
    "    }\n",
    "    \n",
    "    CHECK_CUDA(cudaGraphExecDestroy(graphExec));\n",
    "    CHECK_CUDA(cudaGraphDestroy(graph));\n",
    "    CHECK_CUDA(cudaStreamDestroy(stream));\n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "}\n",
    "\n",
    "// =============================================================\n",
    "// Exercise 2: Multi-Stream Graphs\n",
    "// Create a graph with parallel branches using stream forking\n",
    "// =============================================================\n",
    "void exercise2_multi_stream_graphs() {\n",
    "    printf(\"\\n=== Exercise 2: Multi-Stream Graphs ===\\n\");\n",
    "    \n",
    "    const int N = 1024 * 256;\n",
    "    const int BLOCK_SIZE = 256;\n",
    "    const int GRID_SIZE = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    float *d_A, *d_B, *d_C;\n",
    "    CHECK_CUDA(cudaMalloc(&d_A, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_B, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_C, N * sizeof(float)));\n",
    "    \n",
    "    cudaStream_t mainStream, forkStreamA, forkStreamB;\n",
    "    CHECK_CUDA(cudaStreamCreate(&mainStream));\n",
    "    CHECK_CUDA(cudaStreamCreate(&forkStreamA));\n",
    "    CHECK_CUDA(cudaStreamCreate(&forkStreamB));\n",
    "    \n",
    "    // Create graph with parallel branches (fork-join pattern)\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t graphExec;\n",
    "    \n",
    "    CHECK_CUDA(cudaStreamBeginCapture(mainStream, cudaStreamCaptureModeGlobal));\n",
    "    \n",
    "    // Fork into parallel streams\n",
    "    cudaEvent_t forkEvent, joinEventA, joinEventB;\n",
    "    CHECK_CUDA(cudaEventCreate(&forkEvent));\n",
    "    CHECK_CUDA(cudaEventCreate(&joinEventA));\n",
    "    CHECK_CUDA(cudaEventCreate(&joinEventB));\n",
    "    \n",
    "    // Record fork point\n",
    "    CHECK_CUDA(cudaEventRecord(forkEvent, mainStream));\n",
    "    CHECK_CUDA(cudaStreamWaitEvent(forkStreamA, forkEvent, 0));\n",
    "    CHECK_CUDA(cudaStreamWaitEvent(forkStreamB, forkEvent, 0));\n",
    "    \n",
    "    // Parallel work on branches\n",
    "    simpleKernel<<<GRID_SIZE, BLOCK_SIZE, 0, forkStreamA>>>(d_A, N);\n",
    "    simpleKernel<<<GRID_SIZE, BLOCK_SIZE, 0, forkStreamA>>>(d_A, N);\n",
    "    \n",
    "    simpleKernel<<<GRID_SIZE, BLOCK_SIZE, 0, forkStreamB>>>(d_B, N);\n",
    "    simpleKernel<<<GRID_SIZE, BLOCK_SIZE, 0, forkStreamB>>>(d_B, N);\n",
    "    \n",
    "    // Join back\n",
    "    CHECK_CUDA(cudaEventRecord(joinEventA, forkStreamA));\n",
    "    CHECK_CUDA(cudaEventRecord(joinEventB, forkStreamB));\n",
    "    CHECK_CUDA(cudaStreamWaitEvent(mainStream, joinEventA, 0));\n",
    "    CHECK_CUDA(cudaStreamWaitEvent(mainStream, joinEventB, 0));\n",
    "    \n",
    "    // Final kernel after join\n",
    "    simpleKernel<<<GRID_SIZE, BLOCK_SIZE, 0, mainStream>>>(d_C, N);\n",
    "    \n",
    "    CHECK_CUDA(cudaStreamEndCapture(mainStream, &graph));\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0));\n",
    "    \n",
    "    // Analyze graph structure\n",
    "    size_t numNodes, numEdges;\n",
    "    CHECK_CUDA(cudaGraphGetNodes(graph, nullptr, &numNodes));\n",
    "    CHECK_CUDA(cudaGraphGetEdges(graph, nullptr, nullptr, &numEdges));\n",
    "    \n",
    "    printf(\"Graph structure:\\n\");\n",
    "    printf(\"  Nodes: %zu\\n\", numNodes);\n",
    "    printf(\"  Edges: %zu\\n\", numEdges);\n",
    "    printf(\"  Pattern: Fork-Join (2 parallel branches)\\n\");\n",
    "    \n",
    "    // Execute and time\n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        CHECK_CUDA(cudaGraphLaunch(graphExec, mainStream));\n",
    "    }\n",
    "    CHECK_CUDA(cudaStreamSynchronize(mainStream));\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    double graphTime = std::chrono::duration<double, std::micro>(end - start).count();\n",
    "    \n",
    "    printf(\"  100 launches: %.2f Âµs (%.2f Âµs/launch)\\n\", graphTime, graphTime / 100);\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaEventDestroy(forkEvent));\n",
    "    CHECK_CUDA(cudaEventDestroy(joinEventA));\n",
    "    CHECK_CUDA(cudaEventDestroy(joinEventB));\n",
    "    CHECK_CUDA(cudaGraphExecDestroy(graphExec));\n",
    "    CHECK_CUDA(cudaGraphDestroy(graph));\n",
    "    CHECK_CUDA(cudaStreamDestroy(mainStream));\n",
    "    CHECK_CUDA(cudaStreamDestroy(forkStreamA));\n",
    "    CHECK_CUDA(cudaStreamDestroy(forkStreamB));\n",
    "    CHECK_CUDA(cudaFree(d_A));\n",
    "    CHECK_CUDA(cudaFree(d_B));\n",
    "    CHECK_CUDA(cudaFree(d_C));\n",
    "}\n",
    "\n",
    "// =============================================================\n",
    "// Exercise 3: Complete Inference Pipeline\n",
    "// Full pipeline with memcpy nodes for input/output\n",
    "// =============================================================\n",
    "void exercise3_complete_inference_pipeline() {\n",
    "    printf(\"\\n=== Exercise 3: Complete Inference Pipeline ===\\n\");\n",
    "    \n",
    "    const int N = 1024 * 32;  // Batch size * features\n",
    "    const int BLOCK_SIZE = 256;\n",
    "    const int GRID_SIZE = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    // Host memory (pinned for async transfers)\n",
    "    float *h_input, *h_output;\n",
    "    CHECK_CUDA(cudaMallocHost(&h_input, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMallocHost(&h_output, N * sizeof(float)));\n",
    "    \n",
    "    // Initialize input\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_input[i] = (float)(i % 100) / 100.0f - 0.5f;\n",
    "    }\n",
    "    \n",
    "    // Device memory\n",
    "    float *d_input, *d_layer1, *d_layer2, *d_output;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_layer1, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_layer2, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, N * sizeof(float)));\n",
    "    \n",
    "    // Build explicit graph with memcpy + compute nodes\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t graphExec;\n",
    "    CHECK_CUDA(cudaGraphCreate(&graph, 0));\n",
    "    \n",
    "    // Node 1: Copy input H2D\n",
    "    cudaGraphNode_t memcpyH2DNode;\n",
    "    cudaMemcpy3DParms h2dParams = {0};\n",
    "    h2dParams.srcPtr = make_cudaPitchedPtr(h_input, N * sizeof(float), N, 1);\n",
    "    h2dParams.dstPtr = make_cudaPitchedPtr(d_input, N * sizeof(float), N, 1);\n",
    "    h2dParams.extent = make_cudaExtent(N * sizeof(float), 1, 1);\n",
    "    h2dParams.kind = cudaMemcpyHostToDevice;\n",
    "    CHECK_CUDA(cudaGraphAddMemcpyNode(&memcpyH2DNode, graph, nullptr, 0, &h2dParams));\n",
    "    \n",
    "    // Node 2: Layer 1 kernel\n",
    "    cudaGraphNode_t layer1Node;\n",
    "    void* layer1Args[] = { &d_input, &d_layer1, (void*)&N };\n",
    "    cudaKernelNodeParams layer1Params = {0};\n",
    "    layer1Params.func = (void*)layer1Kernel;\n",
    "    layer1Params.gridDim = dim3(GRID_SIZE);\n",
    "    layer1Params.blockDim = dim3(BLOCK_SIZE);\n",
    "    layer1Params.kernelParams = layer1Args;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&layer1Node, graph, &memcpyH2DNode, 1, &layer1Params));\n",
    "    \n",
    "    // Node 3: Layer 2 kernel\n",
    "    cudaGraphNode_t layer2Node;\n",
    "    void* layer2Args[] = { &d_layer1, &d_layer2, (void*)&N };\n",
    "    cudaKernelNodeParams layer2Params = {0};\n",
    "    layer2Params.func = (void*)layer2Kernel;\n",
    "    layer2Params.gridDim = dim3(GRID_SIZE);\n",
    "    layer2Params.blockDim = dim3(BLOCK_SIZE);\n",
    "    layer2Params.kernelParams = layer2Args;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&layer2Node, graph, &layer1Node, 1, &layer2Params));\n",
    "    \n",
    "    // Node 4: Layer 3 kernel\n",
    "    cudaGraphNode_t layer3Node;\n",
    "    void* layer3Args[] = { &d_layer2, &d_output, (void*)&N };\n",
    "    cudaKernelNodeParams layer3Params = {0};\n",
    "    layer3Params.func = (void*)layer3Kernel;\n",
    "    layer3Params.gridDim = dim3(GRID_SIZE);\n",
    "    layer3Params.blockDim = dim3(BLOCK_SIZE);\n",
    "    layer3Params.kernelParams = layer3Args;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&layer3Node, graph, &layer2Node, 1, &layer3Params));\n",
    "    \n",
    "    // Node 5: Copy output D2H\n",
    "    cudaGraphNode_t memcpyD2HNode;\n",
    "    cudaMemcpy3DParms d2hParams = {0};\n",
    "    d2hParams.srcPtr = make_cudaPitchedPtr(d_output, N * sizeof(float), N, 1);\n",
    "    d2hParams.dstPtr = make_cudaPitchedPtr(h_output, N * sizeof(float), N, 1);\n",
    "    d2hParams.extent = make_cudaExtent(N * sizeof(float), 1, 1);\n",
    "    d2hParams.kind = cudaMemcpyDeviceToHost;\n",
    "    CHECK_CUDA(cudaGraphAddMemcpyNode(&memcpyD2HNode, graph, &layer3Node, 1, &d2hParams));\n",
    "    \n",
    "    // Instantiate\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0));\n",
    "    \n",
    "    // Print graph info\n",
    "    size_t numNodes;\n",
    "    CHECK_CUDA(cudaGraphGetNodes(graph, nullptr, &numNodes));\n",
    "    printf(\"Inference pipeline graph:\\n\");\n",
    "    printf(\"  Total nodes: %zu\\n\", numNodes);\n",
    "    printf(\"  Structure: H2D -> Layer1 -> Layer2 -> Layer3 -> D2H\\n\");\n",
    "    \n",
    "    // Execute pipeline\n",
    "    cudaStream_t stream;\n",
    "    CHECK_CUDA(cudaStreamCreate(&stream));\n",
    "    \n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    const int NUM_INFERENCES = 100;\n",
    "    for (int i = 0; i < NUM_INFERENCES; i++) {\n",
    "        CHECK_CUDA(cudaGraphLaunch(graphExec, stream));\n",
    "    }\n",
    "    CHECK_CUDA(cudaStreamSynchronize(stream));\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    double totalTime = std::chrono::duration<double, std::milli>(end - start).count();\n",
    "    printf(\"\\nPerformance:\\n\");\n",
    "    printf(\"  %d inferences: %.2f ms\\n\", NUM_INFERENCES, totalTime);\n",
    "    printf(\"  Per inference: %.3f ms\\n\", totalTime / NUM_INFERENCES);\n",
    "    printf(\"  Throughput: %.1f inferences/sec\\n\", NUM_INFERENCES / (totalTime / 1000.0));\n",
    "    \n",
    "    // Verify output (sample)\n",
    "    printf(\"\\nSample output (first 5 elements):\\n\");\n",
    "    for (int i = 0; i < 5; i++) {\n",
    "        printf(\"  output[%d] = %.4f\\n\", i, h_output[i]);\n",
    "    }\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaGraphExecDestroy(graphExec));\n",
    "    CHECK_CUDA(cudaGraphDestroy(graph));\n",
    "    CHECK_CUDA(cudaStreamDestroy(stream));\n",
    "    CHECK_CUDA(cudaFreeHost(h_input));\n",
    "    CHECK_CUDA(cudaFreeHost(h_output));\n",
    "    CHECK_CUDA(cudaFree(d_input));\n",
    "    CHECK_CUDA(cudaFree(d_layer1));\n",
    "    CHECK_CUDA(cudaFree(d_layer2));\n",
    "    CHECK_CUDA(cudaFree(d_output));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"CUDA Graph Optimization Exercises\\n\");\n",
    "    printf(\"==================================\\n\");\n",
    "    \n",
    "    int device;\n",
    "    cudaDeviceProp prop;\n",
    "    CHECK_CUDA(cudaGetDevice(&device));\n",
    "    CHECK_CUDA(cudaGetDeviceProperties(&prop, device));\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    \n",
    "    exercise1_breakeven_analysis();\n",
    "    exercise2_multi_stream_graphs();\n",
    "    exercise3_complete_inference_pipeline();\n",
    "    \n",
    "    printf(\"\\nâœ“ All exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87953402",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o graph_optimization_exercises graph_optimization_exercises.cu && ./graph_optimization_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef6cb1",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Breakeven Analysis\n",
    "Measure at what iteration count graphs become faster than regular launches.\n",
    "\n",
    "### Exercise 2: Multi-Stream Graphs\n",
    "Create a graph that uses multiple internal streams.\n",
    "\n",
    "### Exercise 3: Complete Inference Pipeline\n",
    "Extend the inference example with memcpy nodes in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bd87d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       GRAPH OPTIMIZATION: FINDING THE FASTEST RECIPE    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  ğŸ¯ Pattern: Measure, Then Optimize                     â”‚\n",
    "â”‚  â€¢ Not all code benefits from graphs                    â”‚\n",
    "â”‚  â€¢ The best optimization is knowing when NOT to use it! â”‚\n",
    "â”‚  â€¢ Always calculate breakeven before adding complexity  â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  When to Use:                                           â”‚\n",
    "â”‚  â€¢ Many small kernels (launch-bound, not compute-bound) â”‚\n",
    "â”‚  â€¢ Repeated execution (100+ times)                      â”‚\n",
    "â”‚  â€¢ Fixed topology with variable data                    â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Optimization Tips:                                     â”‚\n",
    "â”‚  â€¢ Minimize graph depth (shorter critical path)         â”‚\n",
    "â”‚  â€¢ Maximize parallelism (fork-join patterns)            â”‚\n",
    "â”‚  â€¢ Fuse small kernels when possible                     â”‚\n",
    "â”‚  â€¢ Use updates, never rebuild unnecessarily             â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Lifecycle Best Practices:                              â”‚\n",
    "â”‚  â€¢ Capture/build â†’ Instantiate â†’ Launch(N) â†’ Destroy    â”‚\n",
    "â”‚  â€¢ Can destroy template graph after instantiate         â”‚\n",
    "â”‚  â€¢ Keep exec alive for entire repetition sequence       â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Concurrency:                                           â”‚\n",
    "â”‚  â€¢ Different graphs on different streams = parallel     â”‚\n",
    "â”‚  â€¢ Same graphExec = serialized                          â”‚\n",
    "â”‚  â€¢ Multiple graphExec from same template = parallel OK  â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  âš ï¸ Week 10 Summary:                                    â”‚\n",
    "â”‚  â€¢ Day 1: Capture (record cooking â†’ recipe)             â”‚\n",
    "â”‚  â€¢ Day 2: Explicit (write recipes from scratch)         â”‚\n",
    "â”‚  â€¢ Day 3: Updates (change ingredients, keep recipe)     â”‚\n",
    "â”‚  â€¢ Day 4: Optimize (chef's efficiency analysis)         â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ğŸ‰ Week 10 Complete: CUDA Graphs Mastery!\n",
    "\n",
    "You've learned to think like a **head chef** managing a GPU kitchen:\n",
    "- **Recipe Cards** (graphs) eliminate per-instruction overhead\n",
    "- **Capture** records existing cooking into recipes\n",
    "- **Explicit Build** designs custom parallel workflows\n",
    "- **Updates** change ingredients without rewriting recipes\n",
    "- **Optimization** knows when recipes help vs. hurt\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "With CUDA Graphs, you can now **eliminate CPU-GPU coordination overhead**. But what about coordination **within the GPU itself**? Next week explores **Cooperative Groups & Dynamic Parallelism**â€”letting GPU threads coordinate and launch their own work!\n",
    "\n",
    "**â†’ Week 11: Cooperative Groups & Dynamic Parallelism** (Threads that think for themselves)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
