{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca98408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e522ef3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: When to Use Graphs\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "```\n",
    "╔══════════════════════════════════════════════════════════╗\n",
    "║               WHEN TO USE CUDA GRAPHS                    ║\n",
    "╠══════════════════════════════════════════════════════════╣\n",
    "║                                                          ║\n",
    "║  ✅ USE GRAPHS when:                                     ║\n",
    "║  • Same sequence repeated many times (1000+)             ║\n",
    "║  • Many small kernels (<0.1ms each)                      ║\n",
    "║  • Fixed workflow with variable data                     ║\n",
    "║  • Launch overhead is significant portion of time        ║\n",
    "║  • Inference in production (fixed model)                 ║\n",
    "║                                                          ║\n",
    "║  ❌ AVOID GRAPHS when:                                   ║\n",
    "║  • Dynamic control flow (if/else, variable loops)        ║\n",
    "║  • One-shot execution                                    ║\n",
    "║  • Large kernels where launch overhead negligible        ║\n",
    "║  • Frequently changing graph topology                    ║\n",
    "║  • Grid/block dimensions change per iteration            ║\n",
    "║                                                          ║\n",
    "╚══════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "### Breakeven Analysis\n",
    "\n",
    "```cpp\n",
    "// When does a graph pay off?\n",
    "\n",
    "// Costs:\n",
    "// - Capture:     ~100-500μs\n",
    "// - Instantiate: ~10-50μs\n",
    "// - Launch:      ~1-5μs (vs ~5-15μs regular)\n",
    "\n",
    "// Breakeven example:\n",
    "// Regular: 10 kernels × 10μs launch = 100μs per iteration\n",
    "// Graph:   Setup 150μs + 5μs per iteration\n",
    "// Breakeven at 150μs / (100 - 5)μs ≈ 2 iterations\n",
    "\n",
    "// After 100 iterations:\n",
    "// Regular: 100 × 100μs = 10,000μs\n",
    "// Graph:   150μs + 100 × 5μs = 650μs\n",
    "// → 15x improvement!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db66a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Graph Structure Optimization\n",
    "\n",
    "### CUDA C++ Optimized Graph (Primary)\n",
    "\n",
    "This example demonstrates techniques for optimizing graph structure including minimizing depth and fusing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph_optimization.cu\n",
    "// graph_optimization.cu - Graph structure optimization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Technique 1: Minimize graph depth\n",
    "// ─────────────────────────────────\n",
    "// BAD: Sequential chain\n",
    "//   A → B → C → D → E → F\n",
    "//   Depth = 6, minimal parallelism\n",
    "\n",
    "// GOOD: Parallel branches\n",
    "//   A → B → C\n",
    "//             \\→ F\n",
    "//   D → E →  /\n",
    "//   Depth = 4, more parallelism\n",
    "\n",
    "__global__ void kernel(float* d, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) d[tid] *= 2.0f;\n",
    "}\n",
    "\n",
    "// Technique 2: Batch similar operations\n",
    "// ─────────────────────────────────────\n",
    "__global__ void fusedKernel(float* a, float* b, float* c, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        // Multiple operations in one kernel\n",
    "        float x = a[tid];\n",
    "        x = x * 2.0f;  // Was kernel 1\n",
    "        x = x + 1.0f;  // Was kernel 2\n",
    "        x = sqrtf(x);  // Was kernel 3\n",
    "        c[tid] = x;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_a, *d_b;\n",
    "    cudaMalloc(&d_a, N * sizeof(float));\n",
    "    cudaMalloc(&d_b, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // Capture Optimized Graph\n",
    "    // ============================================\n",
    "    cudaGraph_t graph;\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Parallel memcpys (device-to-device)\n",
    "    cudaMemcpyAsync(d_b, d_a, N/2 * sizeof(float), \n",
    "                    cudaMemcpyDeviceToDevice, stream);\n",
    "    \n",
    "    // Fused kernel instead of multiple small ones\n",
    "    fusedKernel<<<256, 256, 0, stream>>>(d_a, d_a, d_b, N);\n",
    "    \n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    \n",
    "    // ============================================\n",
    "    // Verify Graph Structure\n",
    "    // ============================================\n",
    "    size_t numNodes;\n",
    "    cudaGraphGetNodes(graph, NULL, &numNodes);\n",
    "    printf(\"Optimized graph has %zu nodes\\n\", numNodes);\n",
    "    \n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Warmup\n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 1000;\n",
    "    cudaEventRecord(start, stream);\n",
    "    \n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Average launch time: %.2f μs\\n\", (ms * 1000) / ITERATIONS);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26affe48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Graphs with Streams\n",
    "\n",
    "### Concurrent Graph Execution\n",
    "\n",
    "```cpp\n",
    "// concurrent_graphs.cu - Running multiple graphs concurrently\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] = sqrtf(data[tid]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 18;\n",
    "    const int NUM_GRAPHS = 4;\n",
    "    \n",
    "    float* d_data[NUM_GRAPHS];\n",
    "    cudaStream_t streams[NUM_GRAPHS];\n",
    "    cudaGraphExec_t graphExecs[NUM_GRAPHS];\n",
    "    \n",
    "    // ============================================\n",
    "    // Create Multiple Graphs\n",
    "    // ============================================\n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaMalloc(&d_data[i], N * sizeof(float));\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "        \n",
    "        // Capture graph for this stream\n",
    "        cudaGraph_t graph;\n",
    "        cudaStreamBeginCapture(streams[i], cudaStreamCaptureModeGlobal);\n",
    "        \n",
    "        process<<<256, 256, 0, streams[i]>>>(d_data[i], N);\n",
    "        \n",
    "        cudaStreamEndCapture(streams[i], &graph);\n",
    "        cudaGraphInstantiate(&graphExecs[i], graph, NULL, NULL, 0);\n",
    "        cudaGraphDestroy(graph);  // Can destroy after instantiate\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch All Graphs Concurrently\n",
    "    // ============================================\n",
    "    // Different graphs on different streams run in parallel!\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start, 0);\n",
    "    \n",
    "    for (int iter = 0; iter < 100; iter++) {\n",
    "        for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "            // Each graph on its own stream\n",
    "            cudaGraphLaunch(graphExecs[i], streams[i]);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop, 0);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"%d concurrent graphs, 100 iterations: %.2f ms\\n\", NUM_GRAPHS, ms);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaGraphExecDestroy(graphExecs[i]);\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7427b2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Real-World Pattern - Inference Pipeline\n",
    "\n",
    "### CUDA C++ Inference Graph (Primary)\n",
    "\n",
    "```cpp\n",
    "// inference_graph.cu - Neural network inference pattern\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Simplified layer kernels\n",
    "__global__ void linearLayer(float* out, float* in, float* W, float* b,\n",
    "                            int out_dim, int in_dim) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < out_dim) {\n",
    "        float sum = b[tid];\n",
    "        for (int i = 0; i < in_dim; i++) {\n",
    "            sum += W[tid * in_dim + i] * in[i];\n",
    "        }\n",
    "        out[tid] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void relu(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] = fmaxf(0.0f, data[tid]);\n",
    "}\n",
    "\n",
    "__global__ void softmax(float* out, float* in, int n) {\n",
    "    __shared__ float sum;\n",
    "    if (threadIdx.x == 0) sum = 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    float val = (tid < n) ? expf(in[tid]) : 0.0f;\n",
    "    atomicAdd(&sum, val);\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (tid < n) out[tid] = val / sum;\n",
    "}\n",
    "\n",
    "struct InferenceContext {\n",
    "    float* d_input;\n",
    "    float* d_hidden1;\n",
    "    float* d_hidden2;\n",
    "    float* d_output;\n",
    "    float* d_W1, *d_b1;\n",
    "    float* d_W2, *d_b2;\n",
    "    float* d_W3, *d_b3;\n",
    "    cudaStream_t stream;\n",
    "    cudaGraphExec_t graphExec;\n",
    "    \n",
    "    int input_dim;\n",
    "    int hidden_dim;\n",
    "    int output_dim;\n",
    "};\n",
    "\n",
    "void buildInferenceGraph(InferenceContext* ctx) {\n",
    "    cudaGraph_t graph;\n",
    "    cudaStreamBeginCapture(ctx->stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Layer 1: Linear + ReLU\n",
    "    linearLayer<<<1, 256, 0, ctx->stream>>>(\n",
    "        ctx->d_hidden1, ctx->d_input, \n",
    "        ctx->d_W1, ctx->d_b1,\n",
    "        ctx->hidden_dim, ctx->input_dim);\n",
    "    relu<<<1, 256, 0, ctx->stream>>>(ctx->d_hidden1, ctx->hidden_dim);\n",
    "    \n",
    "    // Layer 2: Linear + ReLU\n",
    "    linearLayer<<<1, 256, 0, ctx->stream>>>(\n",
    "        ctx->d_hidden2, ctx->d_hidden1,\n",
    "        ctx->d_W2, ctx->d_b2,\n",
    "        ctx->hidden_dim, ctx->hidden_dim);\n",
    "    relu<<<1, 256, 0, ctx->stream>>>(ctx->d_hidden2, ctx->hidden_dim);\n",
    "    \n",
    "    // Layer 3: Linear + Softmax\n",
    "    linearLayer<<<1, 256, 0, ctx->stream>>>(\n",
    "        ctx->d_output, ctx->d_hidden2,\n",
    "        ctx->d_W3, ctx->d_b3,\n",
    "        ctx->output_dim, ctx->hidden_dim);\n",
    "    softmax<<<1, 32, 0, ctx->stream>>>(\n",
    "        ctx->d_output, ctx->d_output, ctx->output_dim);\n",
    "    \n",
    "    cudaStreamEndCapture(ctx->stream, &graph);\n",
    "    cudaGraphInstantiate(&ctx->graphExec, graph, NULL, NULL, 0);\n",
    "    cudaGraphDestroy(graph);\n",
    "}\n",
    "\n",
    "void runInference(InferenceContext* ctx, float* h_input, float* h_output) {\n",
    "    // Copy input (H2D)\n",
    "    cudaMemcpyAsync(ctx->d_input, h_input,\n",
    "                    ctx->input_dim * sizeof(float),\n",
    "                    cudaMemcpyHostToDevice, ctx->stream);\n",
    "    \n",
    "    // Launch inference graph\n",
    "    cudaGraphLaunch(ctx->graphExec, ctx->stream);\n",
    "    \n",
    "    // Copy output (D2H)\n",
    "    cudaMemcpyAsync(h_output, ctx->d_output,\n",
    "                    ctx->output_dim * sizeof(float),\n",
    "                    cudaMemcpyDeviceToHost, ctx->stream);\n",
    "    \n",
    "    cudaStreamSynchronize(ctx->stream);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    InferenceContext ctx;\n",
    "    ctx.input_dim = 784;    // e.g., MNIST\n",
    "    ctx.hidden_dim = 256;\n",
    "    ctx.output_dim = 10;\n",
    "    \n",
    "    // Allocate buffers...\n",
    "    cudaStreamCreate(&ctx.stream);\n",
    "    cudaMalloc(&ctx.d_input, ctx.input_dim * sizeof(float));\n",
    "    cudaMalloc(&ctx.d_hidden1, ctx.hidden_dim * sizeof(float));\n",
    "    cudaMalloc(&ctx.d_hidden2, ctx.hidden_dim * sizeof(float));\n",
    "    cudaMalloc(&ctx.d_output, ctx.output_dim * sizeof(float));\n",
    "    // ... allocate weights ...\n",
    "    \n",
    "    // Build graph ONCE\n",
    "    buildInferenceGraph(&ctx);\n",
    "    \n",
    "    // Run inference 10000 times\n",
    "    float h_input[784], h_output[10];\n",
    "    for (int i = 0; i < 10000; i++) {\n",
    "        // Only graph launch, no CPU overhead!\n",
    "        runInference(&ctx, h_input, h_output);\n",
    "    }\n",
    "    \n",
    "    printf(\"Completed 10000 inference runs\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf2749",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Best Practices Summary\n",
    "\n",
    "### Graph Lifecycle\n",
    "\n",
    "```cpp\n",
    "// BEST PRACTICE: Proper lifecycle management\n",
    "\n",
    "// 1. Create/capture graph\n",
    "cudaGraph_t graph;\n",
    "// ... capture or build ...\n",
    "\n",
    "// 2. Instantiate once\n",
    "cudaGraphExec_t graphExec;\n",
    "cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "\n",
    "// 3. Destroy template (optional - saves memory)\n",
    "cudaGraphDestroy(graph);\n",
    "\n",
    "// 4. Launch many times\n",
    "for (int i = 0; i < 1000000; i++) {\n",
    "    // Update if needed\n",
    "    if (parametersChanged) {\n",
    "        cudaGraphExecKernelNodeSetParams(...);\n",
    "    }\n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "}\n",
    "\n",
    "// 5. Cleanup at end\n",
    "cudaGraphExecDestroy(graphExec);\n",
    "```\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "```\n",
    "❌ MISTAKE 1: Rebuilding graphs every iteration\n",
    "   → Build once, update parameters\n",
    "\n",
    "❌ MISTAKE 2: Using graphs for single-shot work\n",
    "   → Use regular launches for one-off tasks\n",
    "\n",
    "❌ MISTAKE 3: Capturing allocations in graph\n",
    "   → Allocate outside, only use inside\n",
    "\n",
    "❌ MISTAKE 4: Forgetting to sync before reading results\n",
    "   → Always cudaStreamSynchronize before host access\n",
    "\n",
    "❌ MISTAKE 5: Using host-side conditionals in capture\n",
    "   → All conditionals evaluated at capture time!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba361b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Breakeven Analysis\n",
    "Measure at what iteration count graphs become faster than regular launches.\n",
    "\n",
    "### Exercise 2: Multi-Stream Graphs\n",
    "Create a graph that uses multiple internal streams.\n",
    "\n",
    "### Exercise 3: Complete Inference Pipeline\n",
    "Extend the inference example with memcpy nodes in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bd87d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│           GRAPH OPTIMIZATION BEST PRACTICES             │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  When to Use:                                           │\n",
    "│  • Many small kernels (launch-bound)                    │\n",
    "│  • Repeated execution (100+ times)                      │\n",
    "│  • Fixed topology with variable data                    │\n",
    "│                                                         │\n",
    "│  Optimization Tips:                                     │\n",
    "│  • Minimize graph depth                                 │\n",
    "│  • Fuse small kernels                                   │\n",
    "│  • Maximize parallelism (fork-join)                     │\n",
    "│  • Use updates, not rebuilds                            │\n",
    "│                                                         │\n",
    "│  Lifecycle:                                             │\n",
    "│  • Capture/build → Instantiate → Launch(N) → Destroy    │\n",
    "│  • Can destroy template after instantiate               │\n",
    "│                                                         │\n",
    "│  Concurrency:                                           │\n",
    "│  • Different graphs on different streams = parallel     │\n",
    "│  • Same graphExec = serialized                          │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Week 10 Complete! Next: Week 11 - Cooperative Groups & Dynamic Parallelism"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
