{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca98408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e522ef3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: When to Use Graphs\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘               WHEN TO USE CUDA GRAPHS                    â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                          â•‘\n",
    "â•‘  âœ… USE GRAPHS when:                                     â•‘\n",
    "â•‘  â€¢ Same sequence repeated many times (1000+)             â•‘\n",
    "â•‘  â€¢ Many small kernels (<0.1ms each)                      â•‘\n",
    "â•‘  â€¢ Fixed workflow with variable data                     â•‘\n",
    "â•‘  â€¢ Launch overhead is significant portion of time        â•‘\n",
    "â•‘  â€¢ Inference in production (fixed model)                 â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•‘  âŒ AVOID GRAPHS when:                                   â•‘\n",
    "â•‘  â€¢ Dynamic control flow (if/else, variable loops)        â•‘\n",
    "â•‘  â€¢ One-shot execution                                    â•‘\n",
    "â•‘  â€¢ Large kernels where launch overhead negligible        â•‘\n",
    "â•‘  â€¢ Frequently changing graph topology                    â•‘\n",
    "â•‘  â€¢ Grid/block dimensions change per iteration            â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "### Breakeven Analysis\n",
    "\n",
    "```cpp\n",
    "// When does a graph pay off?\n",
    "\n",
    "// Costs:\n",
    "// - Capture:     ~100-500Î¼s\n",
    "// - Instantiate: ~10-50Î¼s\n",
    "// - Launch:      ~1-5Î¼s (vs ~5-15Î¼s regular)\n",
    "\n",
    "// Breakeven example:\n",
    "// Regular: 10 kernels Ã— 10Î¼s launch = 100Î¼s per iteration\n",
    "// Graph:   Setup 150Î¼s + 5Î¼s per iteration\n",
    "// Breakeven at 150Î¼s / (100 - 5)Î¼s â‰ˆ 2 iterations\n",
    "\n",
    "// After 100 iterations:\n",
    "// Regular: 100 Ã— 100Î¼s = 10,000Î¼s\n",
    "// Graph:   150Î¼s + 100 Ã— 5Î¼s = 650Î¼s\n",
    "// â†’ 15x improvement!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db66a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Graph Structure Optimization\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates techniques for optimizing graph structure including minimizing depth and fusing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph_optimization.cu\n",
    "// graph_optimization.cu - Graph structure optimization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Technique 1: Minimize graph depth\n",
    "// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "// BAD: Sequential chain\n",
    "//   A â†’ B â†’ C â†’ D â†’ E â†’ F\n",
    "//   Depth = 6, minimal parallelism\n",
    "\n",
    "// GOOD: Parallel branches\n",
    "//   A â†’ B â†’ C\n",
    "//             \\â†’ F\n",
    "//   D â†’ E â†’  /\n",
    "//   Depth = 4, more parallelism\n",
    "\n",
    "__global__ void kernel(float* d, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) d[tid] *= 2.0f;\n",
    "}\n",
    "\n",
    "// Technique 2: Batch similar operations\n",
    "// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "__global__ void fusedKernel(float* a, float* b, float* c, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        // Multiple operations in one kernel\n",
    "        float x = a[tid];\n",
    "        x = x * 2.0f;  // Was kernel 1\n",
    "        x = x + 1.0f;  // Was kernel 2\n",
    "        x = sqrtf(x);  // Was kernel 3\n",
    "        c[tid] = x;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_a, *d_b;\n",
    "    cudaMalloc(&d_a, N * sizeof(float));\n",
    "    cudaMalloc(&d_b, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // Capture Optimized Graph\n",
    "    // ============================================\n",
    "    cudaGraph_t graph;\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Parallel memcpys (device-to-device)\n",
    "    cudaMemcpyAsync(d_b, d_a, N/2 * sizeof(float), \n",
    "                    cudaMemcpyDeviceToDevice, stream);\n",
    "    \n",
    "    // Fused kernel instead of multiple small ones\n",
    "    fusedKernel<<<256, 256, 0, stream>>>(d_a, d_a, d_b, N);\n",
    "    \n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    \n",
    "    // ============================================\n",
    "    // Verify Graph Structure\n",
    "    // ============================================\n",
    "    size_t numNodes;\n",
    "    cudaGraphGetNodes(graph, NULL, &numNodes);\n",
    "    printf(\"Optimized graph has %zu nodes\\n\", numNodes);\n",
    "    \n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Warmup\n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 1000;\n",
    "    cudaEventRecord(start, stream);\n",
    "    \n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Average launch time: %.2f Î¼s\\n\", (ms * 1000) / ITERATIONS);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26affe48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Graphs with Streams\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates running multiple graphs concurrently on different streams for maximum GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a289dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile concurrent_graphs.cu\n",
    "// concurrent_graphs.cu - Running multiple graphs concurrently\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] = sqrtf(data[tid] + 1.0f);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 18;\n",
    "    const int NUM_GRAPHS = 4;\n",
    "    \n",
    "    float* d_data[NUM_GRAPHS];\n",
    "    cudaStream_t streams[NUM_GRAPHS];\n",
    "    cudaGraphExec_t graphExecs[NUM_GRAPHS];\n",
    "    \n",
    "    // ============================================\n",
    "    // Create Multiple Graphs\n",
    "    // ============================================\n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaMalloc(&d_data[i], N * sizeof(float));\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "        \n",
    "        // Capture graph for this stream\n",
    "        cudaGraph_t graph;\n",
    "        cudaStreamBeginCapture(streams[i], cudaStreamCaptureModeGlobal);\n",
    "        \n",
    "        process<<<256, 256, 0, streams[i]>>>(d_data[i], N);\n",
    "        \n",
    "        cudaStreamEndCapture(streams[i], &graph);\n",
    "        cudaGraphInstantiate(&graphExecs[i], graph, NULL, NULL, 0);\n",
    "        cudaGraphDestroy(graph);  // Can destroy after instantiate\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch All Graphs Concurrently\n",
    "    // ============================================\n",
    "    // Different graphs on different streams run in parallel!\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start, 0);\n",
    "    \n",
    "    for (int iter = 0; iter < 100; iter++) {\n",
    "        for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "            // Each graph on its own stream\n",
    "            cudaGraphLaunch(graphExecs[i], streams[i]);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop, 0);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"%d concurrent graphs, 100 iterations: %.2f ms\\n\", NUM_GRAPHS, ms);\n",
    "    printf(\"Average per iteration: %.2f Î¼s\\n\", (ms * 1000) / 100);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_GRAPHS; i++) {\n",
    "        cudaGraphExecDestroy(graphExecs[i]);\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o concurrent_graphs concurrent_graphs.cu && ./concurrent_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7427b2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Real-World Pattern - Inference Pipeline\n",
    "\n",
    "### ğŸ”· CUDA C++ Inference Graph (Primary)\n",
    "\n",
    "This example demonstrates a practical neural network inference pipeline using CUDA Graphs. The graph captures multiple layer operations (Linear + ReLU activations + Softmax) and benchmarks the throughput achievable with graph-based execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15173f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_graph.cu\n",
    "// inference_graph.cu - Neural network inference pattern\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "// Simplified layer kernels\n",
    "__global__ void linearLayer(float* out, const float* in, const float* W, \n",
    "                            const float* b, int out_dim, int in_dim) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < out_dim) {\n",
    "        float sum = b[tid];\n",
    "        for (int i = 0; i < in_dim; i++) {\n",
    "            sum += W[tid * in_dim + i] * in[i];\n",
    "        }\n",
    "        out[tid] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void relu(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] = fmaxf(0.0f, data[tid]);\n",
    "}\n",
    "\n",
    "__global__ void softmaxKernel(float* out, const float* in, int n) {\n",
    "    __shared__ float maxVal;\n",
    "    __shared__ float sum;\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Find max (simplified for small n)\n",
    "    if (tid == 0) {\n",
    "        maxVal = in[0];\n",
    "        for (int i = 1; i < n; i++) {\n",
    "            if (in[i] > maxVal) maxVal = in[i];\n",
    "        }\n",
    "        sum = 0.0f;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute exp sum\n",
    "    if (tid < n) {\n",
    "        float val = expf(in[tid] - maxVal);\n",
    "        atomicAdd(&sum, val);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Normalize\n",
    "    if (tid < n) out[tid] = expf(in[tid] - maxVal) / sum;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int INPUT_DIM = 784;\n",
    "    const int HIDDEN_DIM = 256;\n",
    "    const int OUTPUT_DIM = 10;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_hidden1, *d_hidden2, *d_output;\n",
    "    float *d_W1, *d_b1, *d_W2, *d_b2, *d_W3, *d_b3;\n",
    "    \n",
    "    cudaMalloc(&d_input, INPUT_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_hidden1, HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_hidden2, HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_output, OUTPUT_DIM * sizeof(float));\n",
    "    \n",
    "    // Weights (simplified - just allocate, don't init)\n",
    "    cudaMalloc(&d_W1, HIDDEN_DIM * INPUT_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_b1, HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_W2, HIDDEN_DIM * HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_b2, HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_W3, OUTPUT_DIM * HIDDEN_DIM * sizeof(float));\n",
    "    cudaMalloc(&d_b3, OUTPUT_DIM * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // Build inference graph\n",
    "    cudaGraph_t graph;\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Layer 1: Linear + ReLU\n",
    "    linearLayer<<<1, 256, 0, stream>>>(d_hidden1, d_input, d_W1, d_b1,\n",
    "                                        HIDDEN_DIM, INPUT_DIM);\n",
    "    relu<<<1, 256, 0, stream>>>(d_hidden1, HIDDEN_DIM);\n",
    "    \n",
    "    // Layer 2: Linear + ReLU\n",
    "    linearLayer<<<1, 256, 0, stream>>>(d_hidden2, d_hidden1, d_W2, d_b2,\n",
    "                                        HIDDEN_DIM, HIDDEN_DIM);\n",
    "    relu<<<1, 256, 0, stream>>>(d_hidden2, HIDDEN_DIM);\n",
    "    \n",
    "    // Layer 3: Linear + Softmax\n",
    "    linearLayer<<<1, 256, 0, stream>>>(d_output, d_hidden2, d_W3, d_b3,\n",
    "                                        OUTPUT_DIM, HIDDEN_DIM);\n",
    "    softmaxKernel<<<1, 32, 0, stream>>>(d_output, d_output, OUTPUT_DIM);\n",
    "    \n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    \n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    size_t numNodes;\n",
    "    cudaGraphGetNodes(graph, NULL, &numNodes);\n",
    "    printf(\"Inference graph has %zu nodes (6 kernels expected)\\n\", numNodes);\n",
    "    \n",
    "    cudaGraphDestroy(graph);  // Template no longer needed\n",
    "    \n",
    "    // Benchmark inference\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 10000;\n",
    "    \n",
    "    // Warmup\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEventRecord(start, stream);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    cudaEventRecord(stop, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Inference: %d iterations in %.2f ms\\n\", ITERATIONS, ms);\n",
    "    printf(\"Per inference: %.2f Î¼s\\n\", (ms * 1000) / ITERATIONS);\n",
    "    printf(\"Throughput: %.0f inferences/sec\\n\", ITERATIONS / (ms / 1000));\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_hidden1);\n",
    "    cudaFree(d_hidden2);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_W1); cudaFree(d_b1);\n",
    "    cudaFree(d_W2); cudaFree(d_b2);\n",
    "    cudaFree(d_W3); cudaFree(d_b3);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o inference_graph inference_graph.cu && ./inference_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf2749",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Best Practices Summary\n",
    "\n",
    "### Graph Lifecycle\n",
    "\n",
    "```cpp\n",
    "// BEST PRACTICE: Proper lifecycle management\n",
    "\n",
    "// 1. Create/capture graph\n",
    "cudaGraph_t graph;\n",
    "// ... capture or build ...\n",
    "\n",
    "// 2. Instantiate once\n",
    "cudaGraphExec_t graphExec;\n",
    "cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "\n",
    "// 3. Destroy template (optional - saves memory)\n",
    "cudaGraphDestroy(graph);\n",
    "\n",
    "// 4. Launch many times\n",
    "for (int i = 0; i < 1000000; i++) {\n",
    "    // Update if needed\n",
    "    if (parametersChanged) {\n",
    "        cudaGraphExecKernelNodeSetParams(...);\n",
    "    }\n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "}\n",
    "\n",
    "// 5. Cleanup at end\n",
    "cudaGraphExecDestroy(graphExec);\n",
    "```\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "```\n",
    "âŒ MISTAKE 1: Rebuilding graphs every iteration\n",
    "   â†’ Build once, update parameters\n",
    "\n",
    "âŒ MISTAKE 2: Using graphs for single-shot work\n",
    "   â†’ Use regular launches for one-off tasks\n",
    "\n",
    "âŒ MISTAKE 3: Capturing allocations in graph\n",
    "   â†’ Allocate outside, only use inside\n",
    "\n",
    "âŒ MISTAKE 4: Forgetting to sync before reading results\n",
    "   â†’ Always cudaStreamSynchronize before host access\n",
    "\n",
    "âŒ MISTAKE 5: Using host-side conditionals in capture\n",
    "   â†’ All conditionals evaluated at capture time!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba361b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Breakeven Analysis\n",
    "Measure at what iteration count graphs become faster than regular launches.\n",
    "\n",
    "### Exercise 2: Multi-Stream Graphs\n",
    "Create a graph that uses multiple internal streams.\n",
    "\n",
    "### Exercise 3: Complete Inference Pipeline\n",
    "Extend the inference example with memcpy nodes in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bd87d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           GRAPH OPTIMIZATION BEST PRACTICES             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  When to Use:                                           â”‚\n",
    "â”‚  â€¢ Many small kernels (launch-bound)                    â”‚\n",
    "â”‚  â€¢ Repeated execution (100+ times)                      â”‚\n",
    "â”‚  â€¢ Fixed topology with variable data                    â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Optimization Tips:                                     â”‚\n",
    "â”‚  â€¢ Minimize graph depth                                 â”‚\n",
    "â”‚  â€¢ Fuse small kernels                                   â”‚\n",
    "â”‚  â€¢ Maximize parallelism (fork-join)                     â”‚\n",
    "â”‚  â€¢ Use updates, not rebuilds                            â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Lifecycle:                                             â”‚\n",
    "â”‚  â€¢ Capture/build â†’ Instantiate â†’ Launch(N) â†’ Destroy    â”‚\n",
    "â”‚  â€¢ Can destroy template after instantiate               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Concurrency:                                           â”‚\n",
    "â”‚  â€¢ Different graphs on different streams = parallel     â”‚\n",
    "â”‚  â€¢ Same graphExec = serialized                          â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Week 10 Complete! Next: Week 11 - Cooperative Groups & Dynamic Parallelism"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
