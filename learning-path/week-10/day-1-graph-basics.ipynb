{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6fa35d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Problem Graphs Solve\n",
    "\n",
    "### Kernel Launch Overhead\n",
    "\n",
    "```\n",
    "Traditional Stream Execution:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "CPU: [Launch K1][Launch K2][Launch K3][Launch K4]\n",
    "          ↓          ↓          ↓          ↓\n",
    "GPU:   [  K1  ]  [  K2  ]  [  K3  ]  [  K4  ]\n",
    "\n",
    "Each launch: ~5-10 μs overhead\n",
    "If kernels are fast (10 μs), overhead = 50% of time!\n",
    "\n",
    "With CUDA Graph:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "CPU: [Launch Graph]  (single call!)\n",
    "          ↓\n",
    "GPU: [K1][K2][K3][K4]  (all pre-planned)\n",
    "\n",
    "Launch overhead: ~5-10 μs TOTAL\n",
    "```\n",
    "\n",
    "### When Graphs Help\n",
    "\n",
    "```\n",
    "✅ Good for Graphs:\n",
    "• Repetitive workflows (training loops)\n",
    "• Many small kernels\n",
    "• Fixed computation pattern\n",
    "• Inference pipelines\n",
    "\n",
    "❌ Not Ideal:\n",
    "• Dynamic control flow\n",
    "• Frequently changing shapes\n",
    "• Single large kernel\n",
    "• One-time computations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a252e4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Stream Capture\n",
    "\n",
    "### CUDA C++ Stream Capture (Primary)\n",
    "\n",
    "The following example demonstrates creating CUDA graphs via stream capture - the easiest way to create graphs from existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph_capture.cu\n",
    "// graph_capture.cu - Creating graphs via stream capture\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void scaleKernel(float* data, float scale, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] *= scale;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void addKernel(float* data, float value, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] += value;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const size_t BYTES = N * sizeof(float);\n",
    "    \n",
    "    // Allocate pinned host and device memory\n",
    "    float *h_data, *d_data;\n",
    "    cudaMallocHost(&h_data, BYTES);\n",
    "    cudaMalloc(&d_data, BYTES);\n",
    "    \n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // STEP 1: Begin Stream Capture\n",
    "    // ============================================\n",
    "    cudaGraph_t graph;\n",
    "    \n",
    "    // Start capturing operations\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // All operations on this stream are now recorded (not executed!)\n",
    "    cudaMemcpyAsync(d_data, h_data, BYTES, cudaMemcpyHostToDevice, stream);\n",
    "    scaleKernel<<<256, 256, 0, stream>>>(d_data, 2.0f, N);\n",
    "    addKernel<<<256, 256, 0, stream>>>(d_data, 1.0f, N);\n",
    "    scaleKernel<<<256, 256, 0, stream>>>(d_data, 0.5f, N);\n",
    "    cudaMemcpyAsync(h_data, d_data, BYTES, cudaMemcpyDeviceToHost, stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // STEP 2: End Capture and Get Graph\n",
    "    // ============================================\n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    \n",
    "    // ============================================\n",
    "    // STEP 3: Instantiate Graph (compile it)\n",
    "    // ============================================\n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // ============================================\n",
    "    // STEP 4: Launch Graph (can do many times!)\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 100;\n",
    "    \n",
    "    // Time graph launches\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float graphTime;\n",
    "    cudaEventElapsedTime(&graphTime, start, stop);\n",
    "    \n",
    "    printf(\"Graph: %d iterations in %.2f ms (%.2f us/iter)\\n\",\n",
    "           ITERATIONS, graphTime, graphTime * 1000 / ITERATIONS);\n",
    "    \n",
    "    // ============================================\n",
    "    // Compare with Stream (no graph)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        cudaMemcpyAsync(d_data, h_data, BYTES, cudaMemcpyHostToDevice, stream);\n",
    "        scaleKernel<<<256, 256, 0, stream>>>(d_data, 2.0f, N);\n",
    "        addKernel<<<256, 256, 0, stream>>>(d_data, 1.0f, N);\n",
    "        scaleKernel<<<256, 256, 0, stream>>>(d_data, 0.5f, N);\n",
    "        cudaMemcpyAsync(h_data, d_data, BYTES, cudaMemcpyDeviceToHost, stream);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float streamTime;\n",
    "    cudaEventElapsedTime(&streamTime, start, stop);\n",
    "    \n",
    "    printf(\"Stream: %d iterations in %.2f ms (%.2f us/iter)\\n\",\n",
    "           ITERATIONS, streamTime, streamTime * 1000 / ITERATIONS);\n",
    "    printf(\"Speedup: %.2fx\\n\", streamTime / graphTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad919d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o graph_capture graph_capture.cu\n",
    "!./graph_capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c271e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Graph Lifecycle\n",
    "\n",
    "### The Three Objects\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                  CUDA GRAPH LIFECYCLE                   │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  1. cudaGraph_t (Template)                              │\n",
    "│     └─ Definition of operations and dependencies       │\n",
    "│     └─ Created by capture or explicit construction     │\n",
    "│     └─ Can be inspected, modified, cloned              │\n",
    "│                                                         │\n",
    "│  2. cudaGraphExec_t (Executable)                        │\n",
    "│     └─ Compiled/instantiated version of graph          │\n",
    "│     └─ Ready for launch                                 │\n",
    "│     └─ Some parameters can be updated                   │\n",
    "│                                                         │\n",
    "│  3. cudaStream_t (Where it runs)                        │\n",
    "│     └─ Graph launches into a stream                     │\n",
    "│     └─ Follows stream ordering rules                    │\n",
    "│                                                         │\n",
    "│  Workflow:                                              │\n",
    "│  Capture/Build → Graph → Instantiate → GraphExec       │\n",
    "│                                            ↓            │\n",
    "│                              Launch (many times!)       │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Capture Modes\n",
    "\n",
    "```cpp\n",
    "// Capture mode options:\n",
    "\n",
    "// cudaStreamCaptureModeGlobal\n",
    "// - Any operation in any thread on capturing stream is captured\n",
    "// - Most common for single-threaded code\n",
    "cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "\n",
    "// cudaStreamCaptureModeThreadLocal  \n",
    "// - Only operations from this thread are captured\n",
    "cudaStreamBeginCapture(stream, cudaStreamCaptureModeThreadLocal);\n",
    "\n",
    "// cudaStreamCaptureModeRelaxed\n",
    "// - Doesn't insert sync barriers, slightly faster capture\n",
    "cudaStreamBeginCapture(stream, cudaStreamCaptureModeRelaxed);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715bdf8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Capture Rules and Restrictions\n",
    "\n",
    "### What Can Be Captured?\n",
    "\n",
    "```\n",
    "✅ CAN be captured:\n",
    "• Kernel launches\n",
    "• cudaMemcpyAsync\n",
    "• cudaMemsetAsync\n",
    "• Events (record/wait)\n",
    "• Child graph launches\n",
    "\n",
    "❌ CANNOT be captured:\n",
    "• cudaMemcpy (synchronous!)\n",
    "• cudaMalloc/cudaFree\n",
    "• cudaDeviceSynchronize\n",
    "• CPU operations\n",
    "• Cross-stream sync (without events)\n",
    "```\n",
    "\n",
    "### Multi-Stream Capture\n",
    "\n",
    "The following example shows how to capture operations across multiple streams into a single graph, creating parallel execution branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multi_stream_capture.cu\n",
    "// multi_stream_capture.cu - Capturing multiple streams\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void workA(float* a, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) a[tid] *= 2.0f;\n",
    "}\n",
    "\n",
    "__global__ void workB(float* b, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) b[tid] += 1.0f;\n",
    "}\n",
    "\n",
    "__global__ void combine(float* a, float* b, float* c, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) c[tid] = a[tid] + b[tid];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, N * sizeof(float));\n",
    "    cudaMalloc(&d_b, N * sizeof(float));\n",
    "    cudaMalloc(&d_c, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t streamMain, streamA, streamB;\n",
    "    cudaStreamCreate(&streamMain);\n",
    "    cudaStreamCreate(&streamA);\n",
    "    cudaStreamCreate(&streamB);\n",
    "    \n",
    "    cudaEvent_t forkEvent, joinA, joinB;\n",
    "    cudaEventCreate(&forkEvent);\n",
    "    cudaEventCreate(&joinA);\n",
    "    cudaEventCreate(&joinB);\n",
    "    \n",
    "    cudaGraph_t graph;\n",
    "    \n",
    "    // Begin capture on main stream\n",
    "    cudaStreamBeginCapture(streamMain, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Record fork point\n",
    "    cudaEventRecord(forkEvent, streamMain);\n",
    "    \n",
    "    // Stream A waits for fork, does work\n",
    "    cudaStreamWaitEvent(streamA, forkEvent);\n",
    "    workA<<<256, 256, 0, streamA>>>(d_a, N);\n",
    "    cudaEventRecord(joinA, streamA);\n",
    "    \n",
    "    // Stream B waits for fork, does work\n",
    "    cudaStreamWaitEvent(streamB, forkEvent);\n",
    "    workB<<<256, 256, 0, streamB>>>(d_b, N);\n",
    "    cudaEventRecord(joinB, streamB);\n",
    "    \n",
    "    // Main stream waits for both, combines\n",
    "    cudaStreamWaitEvent(streamMain, joinA);\n",
    "    cudaStreamWaitEvent(streamMain, joinB);\n",
    "    combine<<<256, 256, 0, streamMain>>>(d_a, d_b, d_c, N);\n",
    "    \n",
    "    // End capture\n",
    "    cudaStreamEndCapture(streamMain, &graph);\n",
    "    \n",
    "    // Graph now contains:\n",
    "    //     fork\n",
    "    //    /    \\\n",
    "    // workA  workB\n",
    "    //    \\    /\n",
    "    //   combine\n",
    "    \n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Launch!\n",
    "    cudaGraphLaunch(graphExec, streamMain);\n",
    "    cudaStreamSynchronize(streamMain);\n",
    "    \n",
    "    printf(\"Multi-stream graph executed!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(streamMain);\n",
    "    cudaStreamDestroy(streamA);\n",
    "    cudaStreamDestroy(streamB);\n",
    "    cudaEventDestroy(forkEvent);\n",
    "    cudaEventDestroy(joinA);\n",
    "    cudaEventDestroy(joinB);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o multi_stream_capture multi_stream_capture.cu\n",
    "!./multi_stream_capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Note (OPTIONAL)\n",
    "# CUDA Graphs are not directly supported in Numba\n",
    "# For graph functionality, use CUDA C++ or libraries like CuPy\n",
    "\n",
    "# Here's a simulation of the concept:\n",
    "@cuda.jit\n",
    "def kernel1(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        data[tid] *= 2.0\n",
    "\n",
    "@cuda.jit\n",
    "def kernel2(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        data[tid] += 1.0\n",
    "\n",
    "# Without graphs: each call has overhead\n",
    "n = 1 << 18\n",
    "d_data = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "# Warmup\n",
    "kernel1[(n+255)//256, 256](d_data)\n",
    "kernel2[(n+255)//256, 256](d_data)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Time\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    kernel1[(n+255)//256, 256](d_data)\n",
    "    kernel2[(n+255)//256, 256](d_data)\n",
    "cuda.synchronize()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"100 iterations (no graphs): {elapsed*1000:.2f} ms\")\n",
    "print(\"Note: CUDA Graphs require CUDA C++ for direct usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3cd2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Basic Capture\n",
    "Capture a pipeline with 5 kernels and measure speedup.\n",
    "\n",
    "### Exercise 2: Find Break-Even\n",
    "At what number of kernels does graph overhead pay off?\n",
    "\n",
    "### Exercise 3: Multi-Stream Graph\n",
    "Create a graph with fork-join pattern (parallel branches)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0811e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                 GRAPH BASICS                            │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Stream Capture:                                        │\n",
    "│  1. cudaStreamBeginCapture(stream, mode)                │\n",
    "│  2. ... operations ...                                  │\n",
    "│  3. cudaStreamEndCapture(stream, &graph)                │\n",
    "│                                                         │\n",
    "│  Execution:                                             │\n",
    "│  4. cudaGraphInstantiate(&exec, graph, ...)             │\n",
    "│  5. cudaGraphLaunch(exec, stream)                       │\n",
    "│                                                         │\n",
    "│  Benefits:                                              │\n",
    "│  • Reduced launch overhead                              │\n",
    "│  • Pre-planned dependencies                             │\n",
    "│  • Good for repetitive patterns                         │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 2 - Explicit Graph Construction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
