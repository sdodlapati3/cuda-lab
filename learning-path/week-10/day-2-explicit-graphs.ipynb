{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c32303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e472d5",
   "metadata": {},
   "source": [
    "# Day 2: Explicit Graph Construction\n",
    "\n",
    "## ğŸ¯ The Hook: Writing Your Own Recipes\n",
    "\n",
    "**Yesterday we recorded cooking to create recipes. But what if you're inventing a new dish?** You can't record what doesn't exist yet. You need to write the recipe from scratchâ€”deciding which steps can happen in parallel, which must wait for others, and how everything connects.\n",
    "\n",
    "```\n",
    "Stream Capture = Recording a cooking session:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"I'll just cook and the recipe writes itself!\"\n",
    "âœ… Easy    âŒ Limited to what you can physically do\n",
    "\n",
    "Explicit Construction = Writing a recipe book:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"While the pasta boils (20min), ALSO prep sauce (15min)\"\n",
    "                  â†“                    â†“\n",
    "              [Boil Pasta]        [Prep Sauce]\n",
    "                     \\              /\n",
    "                      [Combine & Serve]\n",
    "\n",
    "You design the workflowâ€”including parallelism you couldn't\n",
    "physically do alone but a team of chefs could!\n",
    "```\n",
    "\n",
    "**Today you'll learn:** How to build CUDA graphs from scratch, creating custom workflows with parallel branches and precise dependency control!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Create** CUDA graphs programmatically using the explicit API\n",
    "2. **Add kernel nodes** with custom parameters and dependencies\n",
    "3. **Build fork-join patterns** for parallel execution paths\n",
    "4. **Add memory operation nodes** (memcpy, memset) to graphs\n",
    "5. **Choose** between stream capture and explicit construction\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸƒ Concept Card: Building Your Own Recipes\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  EXPLICIT GRAPHS = CUSTOM RECIPE DESIGN                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  CAPTURE VS EXPLICIT:                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
    "â”‚  â”‚    Stream Capture      â”‚   Explicit Build       â”‚           â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚\n",
    "â”‚  â”‚ Record existing code   â”‚ Design from scratch    â”‚           â”‚\n",
    "â”‚  â”‚ Quick & easy           â”‚ Full control           â”‚           â”‚\n",
    "â”‚  â”‚ Serial structure       â”‚ Custom parallelism     â”‚           â”‚\n",
    "â”‚  â”‚ Converting legacy      â”‚ Building new systems   â”‚           â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  EXPLICIT CONSTRUCTION STEPS:                                   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ 1. Create empty graph:  cudaGraphCreate(&graph, 0)       â”‚  â”‚\n",
    "â”‚  â”‚                                                          â”‚  â”‚\n",
    "â”‚  â”‚ 2. Add nodes:           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚  â”‚\n",
    "â”‚  â”‚    cudaGraphAddKernelNode(&nodeA, graph, NULL, 0, &p)   â”‚  â”‚\n",
    "â”‚  â”‚                              â”‚                           â”‚  â”‚\n",
    "â”‚  â”‚ 3. Specify dependencies: â”Œâ”€â”€â”´â”€â”€â”                        â”‚  â”‚\n",
    "â”‚  â”‚    deps[] = {nodeA}      â”‚     â”‚                        â”‚  â”‚\n",
    "â”‚  â”‚    cudaGraphAddKernelNode(&B, graph, deps, 1, &p2)      â”‚  â”‚\n",
    "â”‚  â”‚    cudaGraphAddKernelNode(&C, graph, deps, 1, &p3)      â”‚  â”‚\n",
    "â”‚  â”‚                           B     C   (parallel!)         â”‚  â”‚\n",
    "â”‚  â”‚ 4. Join parallel branches:â””â”€â”€â”¬â”€â”€â”˜                       â”‚  â”‚\n",
    "â”‚  â”‚    deps[] = {B, C}           â”‚                          â”‚  â”‚\n",
    "â”‚  â”‚    cudaGraphAddKernelNode(&D, graph, deps, 2, &p4)      â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  FORK-JOIN PATTERN (Team Cooking):                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚         [Prep]                              â”‚               â”‚\n",
    "â”‚  â”‚           â”‚                                 â”‚               â”‚\n",
    "â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”     FORK: No deps = run   â”‚               â”‚\n",
    "â”‚  â”‚     â–¼     â–¼     â–¼          in parallel!    â”‚               â”‚\n",
    "â”‚  â”‚  [Chop] [Boil] [Fry]                        â”‚               â”‚\n",
    "â”‚  â”‚     â”‚     â”‚     â”‚                           â”‚               â”‚\n",
    "â”‚  â”‚     â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜     JOIN: Multiple deps   â”‚               â”‚\n",
    "â”‚  â”‚           â–¼                = wait for all   â”‚               â”‚\n",
    "â”‚  â”‚       [Combine]                             â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ’¡ Key Insight: You're the head chef designing the workflowâ€” â”‚\n",
    "â”‚     explicit graphs let you parallelize like a team of chefs!  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Why Explicit Construction?\n",
    "\n",
    "### Capture vs Explicit\n",
    "\n",
    "```\n",
    "Stream Capture:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ… Easy - just run code in capture mode\n",
    "âœ… Natural for converting existing code\n",
    "âŒ Limited control over structure\n",
    "âŒ Can't build graphs dynamically\n",
    "\n",
    "Explicit Construction:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ… Full control over graph structure\n",
    "âœ… Can build programmatically\n",
    "âœ… More flexible dependencies\n",
    "âŒ More verbose code\n",
    "âŒ Need to manage node handles\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c31796",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building a Simple Graph\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates building a graph node by node, giving you full control over the graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1da4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile explicit_graph.cu\n",
    "// explicit_graph.cu - Building graphs node by node\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernelA(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] *= 2.0f;\n",
    "}\n",
    "\n",
    "__global__ void kernelB(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] += 1.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const size_t BYTES = N * sizeof(float);\n",
    "    \n",
    "    float *h_data, *d_data;\n",
    "    cudaMallocHost(&h_data, BYTES);\n",
    "    cudaMalloc(&d_data, BYTES);\n",
    "    \n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // Create Empty Graph\n",
    "    // ============================================\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphCreate(&graph, 0);  // 0 = flags (none)\n",
    "    \n",
    "    // ============================================\n",
    "    // Add Memcpy Node (H2D)\n",
    "    // ============================================\n",
    "    cudaGraphNode_t h2dNode;\n",
    "    cudaMemcpy3DParms h2dParams = {0};\n",
    "    h2dParams.srcPtr = make_cudaPitchedPtr((void*)h_data, BYTES, N, 1);\n",
    "    h2dParams.dstPtr = make_cudaPitchedPtr((void*)d_data, BYTES, N, 1);\n",
    "    h2dParams.extent = make_cudaExtent(BYTES, 1, 1);\n",
    "    h2dParams.kind = cudaMemcpyHostToDevice;\n",
    "    \n",
    "    cudaGraphAddMemcpyNode(&h2dNode, graph, \n",
    "                           NULL, 0,  // No dependencies\n",
    "                           &h2dParams);\n",
    "    \n",
    "    // ============================================\n",
    "    // Add Kernel Node A (depends on H2D)\n",
    "    // ============================================\n",
    "    cudaGraphNode_t kernelANode;\n",
    "    \n",
    "    cudaKernelNodeParams kernelAParams = {0};\n",
    "    void* argsA[] = { &d_data, (void*)&N };\n",
    "    \n",
    "    kernelAParams.func = (void*)kernelA;\n",
    "    kernelAParams.gridDim = dim3(256);\n",
    "    kernelAParams.blockDim = dim3(256);\n",
    "    kernelAParams.sharedMemBytes = 0;\n",
    "    kernelAParams.kernelParams = argsA;\n",
    "    kernelAParams.extra = NULL;\n",
    "    \n",
    "    cudaGraphNode_t depA[] = { h2dNode };  // Depends on H2D\n",
    "    cudaGraphAddKernelNode(&kernelANode, graph, \n",
    "                           depA, 1,  // 1 dependency\n",
    "                           &kernelAParams);\n",
    "    \n",
    "    // ============================================\n",
    "    // Add Kernel Node B (depends on Kernel A)\n",
    "    // ============================================\n",
    "    cudaGraphNode_t kernelBNode;\n",
    "    \n",
    "    cudaKernelNodeParams kernelBParams = {0};\n",
    "    void* argsB[] = { &d_data, (void*)&N };\n",
    "    \n",
    "    kernelBParams.func = (void*)kernelB;\n",
    "    kernelBParams.gridDim = dim3(256);\n",
    "    kernelBParams.blockDim = dim3(256);\n",
    "    kernelBParams.sharedMemBytes = 0;\n",
    "    kernelBParams.kernelParams = argsB;\n",
    "    kernelBParams.extra = NULL;\n",
    "    \n",
    "    cudaGraphNode_t depB[] = { kernelANode };  // Depends on A\n",
    "    cudaGraphAddKernelNode(&kernelBNode, graph, \n",
    "                           depB, 1,\n",
    "                           &kernelBParams);\n",
    "    \n",
    "    // ============================================\n",
    "    // Add Memcpy Node (D2H, depends on Kernel B)\n",
    "    // ============================================\n",
    "    cudaGraphNode_t d2hNode;\n",
    "    cudaMemcpy3DParms d2hParams = {0};\n",
    "    d2hParams.srcPtr = make_cudaPitchedPtr((void*)d_data, BYTES, N, 1);\n",
    "    d2hParams.dstPtr = make_cudaPitchedPtr((void*)h_data, BYTES, N, 1);\n",
    "    d2hParams.extent = make_cudaExtent(BYTES, 1, 1);\n",
    "    d2hParams.kind = cudaMemcpyDeviceToHost;\n",
    "    \n",
    "    cudaGraphNode_t depD2H[] = { kernelBNode };\n",
    "    cudaGraphAddMemcpyNode(&d2hNode, graph, \n",
    "                           depD2H, 1,\n",
    "                           &d2hParams);\n",
    "    \n",
    "    // ============================================\n",
    "    // Instantiate and Execute\n",
    "    // ============================================\n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    // Verify\n",
    "    printf(\"Result[0] = %.1f (expected 3.0)\\n\", h_data[0]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e480be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o explicit_graph explicit_graph.cu\n",
    "!./explicit_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e1bc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Node Types\n",
    "\n",
    "### Available Node Types\n",
    "\n",
    "```cpp\n",
    "// All available graph node types:\n",
    "\n",
    "// 1. Kernel Node\n",
    "cudaGraphAddKernelNode(&node, graph, deps, numDeps, &kernelParams);\n",
    "\n",
    "// 2. Memcpy Node\n",
    "cudaGraphAddMemcpyNode(&node, graph, deps, numDeps, &memcpyParams);\n",
    "\n",
    "// 3. Memset Node\n",
    "cudaGraphAddMemsetNode(&node, graph, deps, numDeps, &memsetParams);\n",
    "\n",
    "// 4. Host Node (CPU callback)\n",
    "cudaGraphAddHostNode(&node, graph, deps, numDeps, &hostParams);\n",
    "\n",
    "// 5. Child Graph Node (nested graph)\n",
    "cudaGraphAddChildGraphNode(&node, graph, deps, numDeps, childGraph);\n",
    "\n",
    "// 6. Empty Node (synchronization point)\n",
    "cudaGraphAddEmptyNode(&node, graph, deps, numDeps);\n",
    "\n",
    "// 7. Event Record Node\n",
    "cudaGraphAddEventRecordNode(&node, graph, deps, numDeps, event);\n",
    "\n",
    "// 8. Event Wait Node\n",
    "cudaGraphAddEventWaitNode(&node, graph, deps, numDeps, event);\n",
    "```\n",
    "\n",
    "### Empty Nodes for Synchronization\n",
    "\n",
    "```cpp\n",
    "// Use empty nodes as synchronization barriers\n",
    "//\n",
    "//    A1    A2    A3\n",
    "//     \\    |    /\n",
    "//      [Empty]     <- Sync point\n",
    "//         |\n",
    "//         B\n",
    "\n",
    "cudaGraphNode_t syncNode;\n",
    "cudaGraphNode_t deps[] = { nodeA1, nodeA2, nodeA3 };\n",
    "cudaGraphAddEmptyNode(&syncNode, graph, deps, 3);\n",
    "\n",
    "// B depends on sync point\n",
    "cudaGraphNode_t depB[] = { syncNode };\n",
    "cudaGraphAddKernelNode(&nodeB, graph, depB, 1, &paramsB);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba693f5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Complex DAG Patterns\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates creating a graph with parallel branches that merge - a common pattern for concurrent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fork_join_graph.cu\n",
    "// fork_join_graph.cu - Parallel branches that merge\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processA(float* a, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) a[tid] = a[tid] * 2.0f;\n",
    "}\n",
    "\n",
    "__global__ void processB(float* b, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) b[tid] = b[tid] + 1.0f;\n",
    "}\n",
    "\n",
    "__global__ void combine(float* a, float* b, float* c, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) c[tid] = a[tid] + b[tid];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, N * sizeof(float));\n",
    "    cudaMalloc(&d_b, N * sizeof(float));\n",
    "    cudaMalloc(&d_c, N * sizeof(float));\n",
    "    \n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphCreate(&graph, 0);\n",
    "    \n",
    "    // ============================================\n",
    "    // Fork: Two independent parallel kernels\n",
    "    // ============================================\n",
    "    cudaGraphNode_t nodeA, nodeB;\n",
    "    \n",
    "    cudaKernelNodeParams paramsA = {0};\n",
    "    void* argsA[] = { &d_a, (void*)&N };\n",
    "    paramsA.func = (void*)processA;\n",
    "    paramsA.gridDim = dim3(256);\n",
    "    paramsA.blockDim = dim3(256);\n",
    "    paramsA.kernelParams = argsA;\n",
    "    \n",
    "    cudaKernelNodeParams paramsB = {0};\n",
    "    void* argsB[] = { &d_b, (void*)&N };\n",
    "    paramsB.func = (void*)processB;\n",
    "    paramsB.gridDim = dim3(256);\n",
    "    paramsB.blockDim = dim3(256);\n",
    "    paramsB.kernelParams = argsB;\n",
    "    \n",
    "    // No dependencies - they can run in parallel!\n",
    "    cudaGraphAddKernelNode(&nodeA, graph, NULL, 0, &paramsA);\n",
    "    cudaGraphAddKernelNode(&nodeB, graph, NULL, 0, &paramsB);\n",
    "    \n",
    "    // ============================================\n",
    "    // Join: Combine depends on both A and B\n",
    "    // ============================================\n",
    "    cudaGraphNode_t nodeC;\n",
    "    \n",
    "    cudaKernelNodeParams paramsC = {0};\n",
    "    void* argsC[] = { &d_a, &d_b, &d_c, (void*)&N };\n",
    "    paramsC.func = (void*)combine;\n",
    "    paramsC.gridDim = dim3(256);\n",
    "    paramsC.blockDim = dim3(256);\n",
    "    paramsC.kernelParams = argsC;\n",
    "    \n",
    "    cudaGraphNode_t depsC[] = { nodeA, nodeB };  // Depends on BOTH\n",
    "    cudaGraphAddKernelNode(&nodeC, graph, depsC, 2, &paramsC);\n",
    "    \n",
    "    /*\n",
    "    Graph structure:\n",
    "    \n",
    "    [processA]    [processB]\n",
    "           \\      /\n",
    "          [combine]\n",
    "    */\n",
    "    \n",
    "    // Instantiate and run\n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    printf(\"Fork-join graph executed!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67284c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o fork_join_graph fork_join_graph.cu\n",
    "!./fork_join_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161c6a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Graph Inspection\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates how to examine a graph's properties, including the number of nodes, node types, and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph_inspection.cu\n",
    "// graph_inspection.cu - Examining graph properties\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void dummyKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] *= 2.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // Capture a sample graph\n",
    "    cudaGraph_t graph;\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    dummyKernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    dummyKernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    dummyKernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    \n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    \n",
    "    // Get number of nodes\n",
    "    size_t numNodes;\n",
    "    cudaGraphGetNodes(graph, NULL, &numNodes);\n",
    "    printf(\"Graph has %zu nodes\\n\", numNodes);\n",
    "    \n",
    "    // Get all nodes\n",
    "    cudaGraphNode_t* nodes = new cudaGraphNode_t[numNodes];\n",
    "    cudaGraphGetNodes(graph, nodes, &numNodes);\n",
    "    \n",
    "    // For each node, get type\n",
    "    for (size_t i = 0; i < numNodes; i++) {\n",
    "        cudaGraphNodeType type;\n",
    "        cudaGraphNodeGetType(nodes[i], &type);\n",
    "        \n",
    "        switch (type) {\n",
    "            case cudaGraphNodeTypeKernel:\n",
    "                printf(\"Node %zu: Kernel\\n\", i);\n",
    "                break;\n",
    "            case cudaGraphNodeTypeMemcpy:\n",
    "                printf(\"Node %zu: Memcpy\\n\", i);\n",
    "                break;\n",
    "            case cudaGraphNodeTypeMemset:\n",
    "                printf(\"Node %zu: Memset\\n\", i);\n",
    "                break;\n",
    "            case cudaGraphNodeTypeHost:\n",
    "                printf(\"Node %zu: Host callback\\n\", i);\n",
    "                break;\n",
    "            case cudaGraphNodeTypeGraph:\n",
    "                printf(\"Node %zu: Child graph\\n\", i);\n",
    "                break;\n",
    "            case cudaGraphNodeTypeEmpty:\n",
    "                printf(\"Node %zu: Empty (sync)\\n\", i);\n",
    "                break;\n",
    "            default:\n",
    "                printf(\"Node %zu: Other\\n\", i);\n",
    "        }\n",
    "        \n",
    "        // Get dependencies\n",
    "        size_t numDeps;\n",
    "        cudaGraphNodeGetDependencies(nodes[i], NULL, &numDeps);\n",
    "        printf(\"  Has %zu dependencies\\n\", numDeps);\n",
    "    }\n",
    "    \n",
    "    delete[] nodes;\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o graph_inspection graph_inspection.cu\n",
    "!./graph_inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199a1e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a33ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile explicit_graph_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void kernel1(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] + 1.0f;\n",
    "}\n",
    "\n",
    "__global__ void kernel2(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] * 2.0f;\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Pipeline Graph (H2D -> K1 -> K2 -> D2H)\n",
    "// ============================================================\n",
    "\n",
    "void exercise1_pipelineGraph() {\n",
    "    printf(\"=== Exercise 1: Pipeline Graph ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 18;\n",
    "    size_t bytes = n * sizeof(float);\n",
    "    \n",
    "    float *h_data, *d_data;\n",
    "    CHECK_CUDA(cudaMallocHost(&h_data, bytes));\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, bytes));\n",
    "    \n",
    "    for (int i = 0; i < n; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    int grid = (n + 255) / 256;\n",
    "    \n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphNode_t h2dNode, k1Node, k2Node, d2hNode;\n",
    "    \n",
    "    CHECK_CUDA(cudaGraphCreate(&graph, 0));\n",
    "    \n",
    "    // H2D memcpy node\n",
    "    cudaMemcpy3DParms h2dParams = {0};\n",
    "    h2dParams.srcPtr = make_cudaPitchedPtr(h_data, bytes, n, 1);\n",
    "    h2dParams.dstPtr = make_cudaPitchedPtr(d_data, bytes, n, 1);\n",
    "    h2dParams.extent = make_cudaExtent(bytes, 1, 1);\n",
    "    h2dParams.kind = cudaMemcpyHostToDevice;\n",
    "    CHECK_CUDA(cudaGraphAddMemcpyNode(&h2dNode, graph, NULL, 0, &h2dParams));\n",
    "    \n",
    "    // Kernel 1 node\n",
    "    cudaKernelNodeParams k1Params = {0};\n",
    "    void* k1Args[] = {&d_data, (void*)&n};\n",
    "    k1Params.func = (void*)kernel1;\n",
    "    k1Params.gridDim = dim3(grid);\n",
    "    k1Params.blockDim = dim3(256);\n",
    "    k1Params.kernelParams = k1Args;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&k1Node, graph, &h2dNode, 1, &k1Params));\n",
    "    \n",
    "    // Kernel 2 node\n",
    "    cudaKernelNodeParams k2Params = {0};\n",
    "    void* k2Args[] = {&d_data, (void*)&n};\n",
    "    k2Params.func = (void*)kernel2;\n",
    "    k2Params.gridDim = dim3(grid);\n",
    "    k2Params.blockDim = dim3(256);\n",
    "    k2Params.kernelParams = k2Args;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&k2Node, graph, &k1Node, 1, &k2Params));\n",
    "    \n",
    "    // D2H memcpy node\n",
    "    cudaMemcpy3DParms d2hParams = {0};\n",
    "    d2hParams.srcPtr = make_cudaPitchedPtr(d_data, bytes, n, 1);\n",
    "    d2hParams.dstPtr = make_cudaPitchedPtr(h_data, bytes, n, 1);\n",
    "    d2hParams.extent = make_cudaExtent(bytes, 1, 1);\n",
    "    d2hParams.kind = cudaMemcpyDeviceToHost;\n",
    "    CHECK_CUDA(cudaGraphAddMemcpyNode(&d2hNode, graph, &k2Node, 1, &d2hParams));\n",
    "    \n",
    "    // Instantiate and execute\n",
    "    cudaGraphExec_t graphExec;\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        cudaGraphLaunch(graphExec, 0);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    printf(\"Pipeline: H2D -> K1 -> K2 -> D2H\\n\");\n",
    "    printf(\"100 iterations: %.2f ms (%.3f ms/iter)\\n\", ms, ms / 100);\n",
    "    printf(\"Result check: h_data[0] = %.1f (expected: 4.0)\\n\\n\", h_data[0]);\n",
    "    \n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Diamond Pattern\n",
    "// ============================================================\n",
    "\n",
    "__global__ void kernelB(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = sinf(data[idx]);\n",
    "}\n",
    "\n",
    "__global__ void kernelC(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = cosf(data[idx]);\n",
    "}\n",
    "\n",
    "__global__ void kernelD(const float* b, const float* c, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = b[idx] + c[idx];\n",
    "}\n",
    "\n",
    "void exercise2_diamondPattern() {\n",
    "    printf(\"=== Exercise 2: Diamond Pattern ===\\n\");\n",
    "    printf(\"    A\\n   / \\\\\\n  B   C\\n   \\\\ /\\n    D\\n\\n\");\n",
    "    \n",
    "    const int n = 1 << 18;\n",
    "    \n",
    "    float *d_input, *d_b, *d_c, *d_output;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_b, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_c, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, n * sizeof(float)));\n",
    "    \n",
    "    int grid = (n + 255) / 256;\n",
    "    \n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphNode_t nodeA, nodeB, nodeC, nodeD;\n",
    "    \n",
    "    CHECK_CUDA(cudaGraphCreate(&graph, 0));\n",
    "    \n",
    "    // Node A\n",
    "    cudaKernelNodeParams aParams = {0};\n",
    "    void* aArgs[] = {&d_input, (void*)&n};\n",
    "    aParams.func = (void*)kernel1;\n",
    "    aParams.gridDim = dim3(grid);\n",
    "    aParams.blockDim = dim3(256);\n",
    "    aParams.kernelParams = aArgs;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&nodeA, graph, NULL, 0, &aParams));\n",
    "    \n",
    "    // Node B (depends on A)\n",
    "    cudaKernelNodeParams bParams = {0};\n",
    "    void* bArgs[] = {&d_b, (void*)&n};\n",
    "    bParams.func = (void*)kernelB;\n",
    "    bParams.gridDim = dim3(grid);\n",
    "    bParams.blockDim = dim3(256);\n",
    "    bParams.kernelParams = bArgs;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&nodeB, graph, &nodeA, 1, &bParams));\n",
    "    \n",
    "    // Node C (depends on A)\n",
    "    cudaKernelNodeParams cParams = {0};\n",
    "    void* cArgs[] = {&d_c, (void*)&n};\n",
    "    cParams.func = (void*)kernelC;\n",
    "    cParams.gridDim = dim3(grid);\n",
    "    cParams.blockDim = dim3(256);\n",
    "    cParams.kernelParams = cArgs;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&nodeC, graph, &nodeA, 1, &cParams));\n",
    "    \n",
    "    // Node D (depends on B and C)\n",
    "    cudaGraphNode_t bcDeps[] = {nodeB, nodeC};\n",
    "    cudaKernelNodeParams dParams = {0};\n",
    "    void* dArgs[] = {&d_b, &d_c, &d_output, (void*)&n};\n",
    "    dParams.func = (void*)kernelD;\n",
    "    dParams.gridDim = dim3(grid);\n",
    "    dParams.blockDim = dim3(256);\n",
    "    dParams.kernelParams = dArgs;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&nodeD, graph, bcDeps, 2, &dParams));\n",
    "    \n",
    "    // Instantiate and execute\n",
    "    cudaGraphExec_t graphExec;\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        cudaGraphLaunch(graphExec, 0);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    printf(\"Diamond graph 100 iterations: %.2f ms\\n\\n\", ms);\n",
    "    \n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    cudaFree(d_output);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Graph Cloning\n",
    "// ============================================================\n",
    "\n",
    "void exercise3_graphCloning() {\n",
    "    printf(\"=== Exercise 3: Graph Cloning ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 18;\n",
    "    \n",
    "    float *d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    int grid = (n + 255) / 256;\n",
    "    \n",
    "    // Create original graph\n",
    "    cudaGraph_t originalGraph;\n",
    "    cudaGraphNode_t k1Node;\n",
    "    \n",
    "    CHECK_CUDA(cudaGraphCreate(&originalGraph, 0));\n",
    "    \n",
    "    cudaKernelNodeParams k1Params = {0};\n",
    "    void* k1Args[] = {&d_data, (void*)&n};\n",
    "    k1Params.func = (void*)kernel1;\n",
    "    k1Params.gridDim = dim3(grid);\n",
    "    k1Params.blockDim = dim3(256);\n",
    "    k1Params.kernelParams = k1Args;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&k1Node, originalGraph, NULL, 0, &k1Params));\n",
    "    \n",
    "    // Clone the graph\n",
    "    cudaGraph_t clonedGraph;\n",
    "    CHECK_CUDA(cudaGraphClone(&clonedGraph, originalGraph));\n",
    "    \n",
    "    // Get nodes from cloned graph and modify\n",
    "    size_t numNodes;\n",
    "    cudaGraphGetNodes(clonedGraph, NULL, &numNodes);\n",
    "    printf(\"Original graph has %zu nodes\\n\", numNodes);\n",
    "    \n",
    "    cudaGraphNode_t* nodes = (cudaGraphNode_t*)malloc(numNodes * sizeof(cudaGraphNode_t));\n",
    "    cudaGraphGetNodes(clonedGraph, nodes, &numNodes);\n",
    "    \n",
    "    // Modify the cloned node (change grid size)\n",
    "    cudaKernelNodeParams modifiedParams;\n",
    "    cudaGraphKernelNodeGetParams(nodes[0], &modifiedParams);\n",
    "    modifiedParams.gridDim = dim3(grid * 2);  // Double the grid\n",
    "    cudaGraphKernelNodeSetParams(nodes[0], &modifiedParams);\n",
    "    \n",
    "    printf(\"Cloned and modified graph (doubled grid size)\\n\");\n",
    "    \n",
    "    // Instantiate both\n",
    "    cudaGraphExec_t origExec, cloneExec;\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&origExec, originalGraph, NULL, NULL, 0));\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&cloneExec, clonedGraph, NULL, NULL, 0));\n",
    "    \n",
    "    printf(\"Both graphs instantiated successfully\\n\\n\");\n",
    "    \n",
    "    free(nodes);\n",
    "    cudaGraphExecDestroy(origExec);\n",
    "    cudaGraphExecDestroy(cloneExec);\n",
    "    cudaGraphDestroy(originalGraph);\n",
    "    cudaGraphDestroy(clonedGraph);\n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘           Explicit Graph Construction Exercises              â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\\n\", prop.name);\n",
    "    \n",
    "    exercise1_pipelineGraph();\n",
    "    exercise2_diamondPattern();\n",
    "    exercise3_graphCloning();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f38551",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o explicit_graph_exercises explicit_graph_exercises.cu && ./explicit_graph_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a61d8c",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Pipeline Graph\n",
    "Build a 4-stage pipeline graph explicitly (H2D â†’ K1 â†’ K2 â†’ D2H).\n",
    "\n",
    "### Exercise 2: Diamond Pattern\n",
    "```\n",
    "    A\n",
    "   / \\\n",
    "  B   C\n",
    "   \\ /\n",
    "    D\n",
    "```\n",
    "\n",
    "### Exercise 3: Graph Cloning\n",
    "Use `cudaGraphClone` to create a modified copy of a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d49eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       EXPLICIT GRAPHS: CUSTOM RECIPE DESIGN             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  ğŸ¯ Pattern: Writing Recipes vs Recording Cooking       â”‚\n",
    "â”‚  â€¢ Capture: Record what you already do                  â”‚\n",
    "â”‚  â€¢ Explicit: Design workflows that weren't possible     â”‚\n",
    "â”‚  â€¢ Explicit enables parallelism beyond single-threaded! â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Create Graph:                                          â”‚\n",
    "â”‚  â€¢ cudaGraphCreate(&graph, 0)                           â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Add Nodes (Ingredients/Steps):                         â”‚\n",
    "â”‚  â€¢ cudaGraphAddKernelNode(&node, graph, deps, n, &p)    â”‚\n",
    "â”‚  â€¢ cudaGraphAddMemcpyNode(...)                          â”‚\n",
    "â”‚  â€¢ cudaGraphAddMemsetNode(...)                          â”‚\n",
    "â”‚  â€¢ cudaGraphAddEmptyNode(...)  // sync barrier          â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Dependencies (Recipe Order):                           â”‚\n",
    "â”‚  â€¢ Pass array of dependency nodes                       â”‚\n",
    "â”‚  â€¢ NULL, 0 = no dependencies (can run immediately)      â”‚\n",
    "â”‚  â€¢ Multiple deps = waits for ALL (join point)           â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Fork-Join (Team Cooking):                              â”‚\n",
    "â”‚  â€¢ Fork: Multiple nodes with same deps â†’ parallel       â”‚\n",
    "â”‚  â€¢ Join: One node depending on multiple â†’ sync point    â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  âš ï¸ Connection to Day 1:                                â”‚\n",
    "â”‚  Capture is easier, explicit gives control.             â”‚\n",
    "â”‚  Use capture for existing code, explicit for new!       â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "You can now **build any graph structure**. But what about those training loops where data changes every iteration? Do you rebuild the graph each time? **No!** Tomorrow we'll learn **graph updates**â€”modifying recipe ingredients without rewriting the whole recipe!\n",
    "\n",
    "**â†’ Day 3: Graph Updates** (Modifying existing recipes efficiently)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
