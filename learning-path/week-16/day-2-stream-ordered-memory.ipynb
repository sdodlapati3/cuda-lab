{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35de823",
   "metadata": {},
   "source": [
    "# Day 2: Stream-Ordered Memory Allocation\n",
    "\n",
    "> ğŸ“¦ **\"What if GPU memory allocation worked like just-in-time inventoryâ€”items arriving exactly when the assembly line needs them, never blocking production?\"**\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "1. Use `cudaMallocAsync` and `cudaFreeAsync` for non-blocking allocation\n",
    "2. Configure memory pools for efficient memory reuse\n",
    "3. Tune release thresholds to balance memory usage and performance\n",
    "4. Coordinate cross-stream memory dependencies using events\n",
    "\n",
    "## The Problem with Synchronous Allocation\n",
    "\n",
    "Traditional `cudaMalloc`:\n",
    "- Implicitly synchronizes the device\n",
    "- Breaks async execution pipelines\n",
    "- Overhead on every allocation\n",
    "\n",
    "Stream-ordered allocation:\n",
    "- Allocation tied to stream ordering\n",
    "- No implicit synchronization\n",
    "- Memory pools for fast reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d375bcf",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Concept Card: Stream-Ordered Allocation as Just-in-Time Inventory\n",
    "\n",
    "**The Analogy:** Traditional memory allocation is like a warehouse that **stops the entire factory** every time you need new parts. Stream-ordered allocation is like **just-in-time (JIT) inventory**:\n",
    "\n",
    "```\n",
    "ğŸ“¦ JUST-IN-TIME INVENTORY SYSTEM\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Traditional cudaMalloc:          Stream-Ordered:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ­ FACTORY HALTS!  â”‚          â”‚  ğŸ­ FACTORY CONTINUES   â”‚\n",
    "â”‚                     â”‚          â”‚                         â”‚\n",
    "â”‚  \"Need 10 parts\"    â”‚          â”‚  ğŸ“¦ Order placed        â”‚\n",
    "â”‚      â†“              â”‚          â”‚      â†“                  â”‚\n",
    "â”‚  â¸ï¸ All lines stop  â”‚          â”‚  Assembly Line A: ğŸ”§... â”‚\n",
    "â”‚      â†“              â”‚          â”‚  Assembly Line B: ğŸ”§... â”‚\n",
    "â”‚  ğŸ“¦ Parts arrive    â”‚          â”‚      â†“                  â”‚\n",
    "â”‚      â†“              â”‚          â”‚  ğŸ“¦ Parts arrive JUST   â”‚\n",
    "â”‚  â–¶ï¸ Resume work     â”‚          â”‚     when Line A needs   â”‚\n",
    "â”‚                     â”‚          â”‚     them!               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“                                  â†“\n",
    "   Implicit sync                   Async-friendly\n",
    "   breaks pipeline!                zero blocking!\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "**The Inventory System:**\n",
    "| Component | Warehouse Analogy | CUDA Concept |\n",
    "|-----------|------------------|--------------|\n",
    "| **Memory Pool** | Warehouse storage area | Pre-allocated pool for fast reuse |\n",
    "| **MallocAsync** | Order on assembly line | Allocation tied to stream order |\n",
    "| **FreeAsync** | Return to warehouse | Memory goes back to pool, not OS |\n",
    "| **Release Threshold** | Warehouse capacity limit | When pool releases excess to OS |\n",
    "| **Cross-Stream Events** | Inter-line coordination | Sync memory availability across streams |\n",
    "\n",
    "**Why This Matters:** Like JIT manufacturing that keeps the factory running smoothly, stream-ordered allocation keeps your GPU kernels executing without allocation-induced stalls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6be95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_ordered_alloc.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = sqrtf((float)idx);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // Traditional approach - synchronous allocation\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 100;\n",
    "    \n",
    "    // Benchmark synchronous\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        float* d_data;\n",
    "        cudaMalloc(&d_data, size);  // Synchronizes!\n",
    "        processKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N);\n",
    "        cudaFree(d_data);  // Synchronizes!\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float syncMs;\n",
    "    cudaEventElapsedTime(&syncMs, start, stop);\n",
    "    \n",
    "    // Benchmark stream-ordered (async)\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        float* d_data;\n",
    "        cudaMallocAsync(&d_data, size, stream);  // Non-blocking!\n",
    "        processKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N);\n",
    "        cudaFreeAsync(d_data, stream);  // Non-blocking!\n",
    "    }\n",
    "    cudaStreamSynchronize(stream);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float asyncMs;\n",
    "    cudaEventElapsedTime(&asyncMs, start, stop);\n",
    "    \n",
    "    printf(\"Synchronous (cudaMalloc/cudaFree): %.2f ms\\n\", syncMs);\n",
    "    printf(\"Stream-ordered (Async):            %.2f ms\\n\", asyncMs);\n",
    "    printf(\"Speedup: %.1fx\\n\", syncMs / asyncMs);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c75e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc stream_ordered_alloc.cu -o stream_ordered_alloc && ./stream_ordered_alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51543523",
   "metadata": {},
   "source": [
    "## Memory Pool Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f40e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mempool_config.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    int device = 0;\n",
    "    cudaSetDevice(device);\n",
    "    \n",
    "    // Get default memory pool\n",
    "    cudaMemPool_t pool;\n",
    "    cudaDeviceGetDefaultMemPool(&pool, device);\n",
    "    \n",
    "    // Configure pool: set release threshold\n",
    "    // Memory below this threshold is kept for reuse\n",
    "    uint64_t threshold = 256 * 1024 * 1024;  // 256 MB\n",
    "    cudaMemPoolSetAttribute(pool, cudaMemPoolAttrReleaseThreshold, &threshold);\n",
    "    printf(\"Set release threshold: %llu MB\\n\", threshold / (1024*1024));\n",
    "    \n",
    "    // Query pool attributes\n",
    "    uint64_t usedBytes, reservedBytes;\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"Used: %llu bytes, Reserved: %llu bytes\\n\", usedBytes, reservedBytes);\n",
    "    \n",
    "    // Allocate some memory\n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    float* d_data;\n",
    "    size_t size = 100 * 1024 * 1024;  // 100 MB\n",
    "    cudaMallocAsync(&d_data, size, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After 100MB alloc - Used: %llu MB, Reserved: %llu MB\\n\", \n",
    "           usedBytes/(1024*1024), reservedBytes/(1024*1024));\n",
    "    \n",
    "    // Free it\n",
    "    cudaFreeAsync(d_data, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After free - Used: %llu MB, Reserved: %llu MB\\n\", \n",
    "           usedBytes/(1024*1024), reservedBytes/(1024*1024));\n",
    "    printf(\"(Memory kept for reuse up to threshold)\\n\");\n",
    "    \n",
    "    // Trim pool to release memory\n",
    "    cudaMemPoolTrimTo(pool, 0);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After trim - Reserved: %llu MB\\n\", reservedBytes/(1024*1024));\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158714ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc mempool_config.cu -o mempool_config && ./mempool_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b85fc",
   "metadata": {},
   "source": [
    "## Multi-Stream Pool Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multistream_pool.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel(float* data, int n, float val) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = val;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    cudaStream_t stream1, stream2;\n",
    "    cudaStreamCreate(&stream1);\n",
    "    cudaStreamCreate(&stream2);\n",
    "    \n",
    "    // Allocate in stream1\n",
    "    float* d_data;\n",
    "    cudaMallocAsync(&d_data, size, stream1);\n",
    "    kernel<<<(N+255)/256, 256, 0, stream1>>>(d_data, N, 1.0f);\n",
    "    \n",
    "    // Create dependency: stream2 waits for stream1\n",
    "    cudaEvent_t event;\n",
    "    cudaEventCreate(&event);\n",
    "    cudaEventRecord(event, stream1);\n",
    "    cudaStreamWaitEvent(stream2, event);\n",
    "    \n",
    "    // Now stream2 can use the data\n",
    "    kernel<<<(N+255)/256, 256, 0, stream2>>>(d_data, N, 2.0f);\n",
    "    \n",
    "    // Free in stream2\n",
    "    cudaFreeAsync(d_data, stream2);\n",
    "    \n",
    "    cudaStreamSynchronize(stream2);\n",
    "    \n",
    "    printf(\"Multi-stream pool sharing successful!\\n\");\n",
    "    \n",
    "    cudaEventDestroy(event);\n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(stream2);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc multistream_pool.cu -o multistream_pool && ./multistream_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bde407",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to practice stream-ordered memory allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_ordered_exercises.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <vector>\n",
    "\n",
    "/*\n",
    " * Stream-Ordered Memory Allocation Exercises\n",
    " * \n",
    " * Exercise 1: Implement a pipeline with async allocation\n",
    " * - Create multiple streams\n",
    " * - Use cudaMallocAsync in each stream\n",
    " * - Overlap allocation, compute, and deallocation\n",
    " * \n",
    " * Exercise 2: Memory pool configuration\n",
    " * - Create a custom memory pool\n",
    " * - Set release threshold\n",
    " * - Measure memory reuse efficiency\n",
    " * \n",
    " * Exercise 3: Cross-stream memory sharing\n",
    " * - Allocate in stream A\n",
    " * - Use cudaEventRecord to synchronize\n",
    " * - Access memory in stream B\n",
    " */\n",
    "\n",
    "#define CUDA_CHECK(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "__global__ void processKernel(float* data, int n, float multiplier) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = sqrtf((float)idx) * multiplier;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void reduceKernel(float* data, int n, float* result) {\n",
    "    __shared__ float sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? data[idx] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = 128; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(result, sdata[0]);\n",
    "}\n",
    "\n",
    "// Exercise 1: Async Allocation Pipeline\n",
    "void exercise1_asyncPipeline() {\n",
    "    printf(\"=== Exercise 1: Async Allocation Pipeline ===\\n\");\n",
    "    \n",
    "    const int NUM_STREAMS = 4;\n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    const int ITERATIONS = 10;\n",
    "    \n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        CUDA_CHECK(cudaStreamCreate(&streams[i]));\n",
    "    }\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CUDA_CHECK(cudaEventCreate(&start));\n",
    "    CUDA_CHECK(cudaEventCreate(&stop));\n",
    "    \n",
    "    // TODO Exercise 1a: Implement pipelined async allocation\n",
    "    // For each iteration:\n",
    "    //   1. cudaMallocAsync a buffer in stream[i % NUM_STREAMS]\n",
    "    //   2. Launch processKernel in that stream\n",
    "    //   3. cudaFreeAsync in that stream\n",
    "    \n",
    "    printf(\"Implement async allocation pipeline with %d streams\\n\", NUM_STREAMS);\n",
    "    printf(\"Each iteration should:\\n\");\n",
    "    printf(\"  - cudaMallocAsync %zu bytes\\n\", size);\n",
    "    printf(\"  - Launch processKernel\\n\");\n",
    "    printf(\"  - cudaFreeAsync\\n\\n\");\n",
    "    \n",
    "    // YOUR CODE HERE:\n",
    "    CUDA_CHECK(cudaEventRecord(start));\n",
    "    for (int iter = 0; iter < ITERATIONS; iter++) {\n",
    "        cudaStream_t stream = streams[iter % NUM_STREAMS];\n",
    "        float* d_data;\n",
    "        \n",
    "        // TODO: Uncomment and complete:\n",
    "        // CUDA_CHECK(cudaMallocAsync(&d_data, size, stream));\n",
    "        // processKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N, 1.0f);\n",
    "        // CUDA_CHECK(cudaFreeAsync(d_data, stream));\n",
    "    }\n",
    "    CUDA_CHECK(cudaEventRecord(stop));\n",
    "    \n",
    "    // Sync all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        CUDA_CHECK(cudaStreamSynchronize(streams[i]));\n",
    "    }\n",
    "    \n",
    "    float ms;\n",
    "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "    printf(\"Pipeline time: %.3f ms for %d iterations\\n\\n\", ms, ITERATIONS);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        CUDA_CHECK(cudaStreamDestroy(streams[i]));\n",
    "    }\n",
    "    CUDA_CHECK(cudaEventDestroy(start));\n",
    "    CUDA_CHECK(cudaEventDestroy(stop));\n",
    "}\n",
    "\n",
    "// Exercise 2: Memory Pool Configuration\n",
    "void exercise2_poolConfiguration() {\n",
    "    printf(\"=== Exercise 2: Memory Pool Configuration ===\\n\");\n",
    "    \n",
    "    const size_t SIZE = 1 << 24;  // 16 MB\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    CUDA_CHECK(cudaStreamCreate(&stream));\n",
    "    \n",
    "    // TODO Exercise 2a: Get the current device's memory pool\n",
    "    cudaMemPool_t memPool;\n",
    "    int device;\n",
    "    CUDA_CHECK(cudaGetDevice(&device));\n",
    "    \n",
    "    printf(\"Get current device memory pool and configure it:\\n\");\n",
    "    printf(\"  - cudaDeviceGetMemPool\\n\");\n",
    "    printf(\"  - cudaMemPoolSetAttribute for release threshold\\n\\n\");\n",
    "    \n",
    "    // YOUR CODE HERE:\n",
    "    // CUDA_CHECK(cudaDeviceGetMemPool(&memPool, device));\n",
    "    \n",
    "    // TODO Exercise 2b: Set release threshold\n",
    "    // uint64_t threshold = UINT64_MAX;  // Never release back to OS\n",
    "    // CUDA_CHECK(cudaMemPoolSetAttribute(memPool, \n",
    "    //            cudaMemPoolAttrReleaseThreshold, &threshold));\n",
    "    \n",
    "    // TODO Exercise 2c: Allocate and free multiple times, measure reuse\n",
    "    printf(\"Allocate/free %zu bytes multiple times and observe memory reuse\\n\", SIZE);\n",
    "    \n",
    "    for (int i = 0; i < 5; i++) {\n",
    "        float* d_data;\n",
    "        CUDA_CHECK(cudaMallocAsync(&d_data, SIZE, stream));\n",
    "        \n",
    "        // Get pool statistics\n",
    "        size_t currentSize = 0, highWatermark = 0;\n",
    "        // CUDA_CHECK(cudaMemPoolGetAttribute(memPool, \n",
    "        //            cudaMemPoolAttrUsedMemCurrent, &currentSize));\n",
    "        // CUDA_CHECK(cudaMemPoolGetAttribute(memPool,\n",
    "        //            cudaMemPoolAttrUsedMemHigh, &highWatermark));\n",
    "        \n",
    "        printf(\"  Iteration %d: allocated, current=%zu, high=%zu\\n\", \n",
    "               i, currentSize, highWatermark);\n",
    "        \n",
    "        CUDA_CHECK(cudaFreeAsync(d_data, stream));\n",
    "    }\n",
    "    CUDA_CHECK(cudaStreamSynchronize(stream));\n",
    "    \n",
    "    printf(\"\\n\");\n",
    "    CUDA_CHECK(cudaStreamDestroy(stream));\n",
    "}\n",
    "\n",
    "// Exercise 3: Cross-Stream Memory Sharing\n",
    "void exercise3_crossStreamSharing() {\n",
    "    printf(\"=== Exercise 3: Cross-Stream Memory Sharing ===\\n\");\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    cudaStream_t streamA, streamB;\n",
    "    CUDA_CHECK(cudaStreamCreate(&streamA));\n",
    "    CUDA_CHECK(cudaStreamCreate(&streamB));\n",
    "    \n",
    "    cudaEvent_t dataReady, computeDone;\n",
    "    CUDA_CHECK(cudaEventCreate(&dataReady));\n",
    "    CUDA_CHECK(cudaEventCreate(&computeDone));\n",
    "    \n",
    "    printf(\"Pattern: Allocate in streamA, use in streamB\\n\");\n",
    "    printf(\"  1. cudaMallocAsync in streamA\\n\");\n",
    "    printf(\"  2. cudaEventRecord(dataReady, streamA)\\n\");\n",
    "    printf(\"  3. cudaStreamWaitEvent(streamB, dataReady)\\n\");\n",
    "    printf(\"  4. Use memory in streamB\\n\");\n",
    "    printf(\"  5. cudaEventRecord(computeDone, streamB)\\n\");\n",
    "    printf(\"  6. cudaStreamWaitEvent(streamA, computeDone)\\n\");\n",
    "    printf(\"  7. cudaFreeAsync in streamA\\n\\n\");\n",
    "    \n",
    "    // TODO Exercise 3: Implement the cross-stream pattern\n",
    "    float* d_data;\n",
    "    float* d_result;\n",
    "    \n",
    "    // YOUR CODE HERE:\n",
    "    // Step 1: Allocate in streamA\n",
    "    // CUDA_CHECK(cudaMallocAsync(&d_data, size, streamA));\n",
    "    // CUDA_CHECK(cudaMallocAsync(&d_result, sizeof(float), streamA));\n",
    "    // CUDA_CHECK(cudaMemsetAsync(d_result, 0, sizeof(float), streamA));\n",
    "    \n",
    "    // Step 2: Signal data is ready\n",
    "    // CUDA_CHECK(cudaEventRecord(dataReady, streamA));\n",
    "    \n",
    "    // Step 3: streamB waits for data\n",
    "    // CUDA_CHECK(cudaStreamWaitEvent(streamB, dataReady));\n",
    "    \n",
    "    // Step 4: Use in streamB\n",
    "    // processKernel<<<(N+255)/256, 256, 0, streamB>>>(d_data, N, 2.0f);\n",
    "    // reduceKernel<<<(N+255)/256, 256, 0, streamB>>>(d_data, N, d_result);\n",
    "    \n",
    "    // Step 5-6: Signal compute done, streamA waits\n",
    "    // CUDA_CHECK(cudaEventRecord(computeDone, streamB));\n",
    "    // CUDA_CHECK(cudaStreamWaitEvent(streamA, computeDone));\n",
    "    \n",
    "    // Step 7: Free in streamA\n",
    "    // CUDA_CHECK(cudaFreeAsync(d_data, streamA));\n",
    "    // CUDA_CHECK(cudaFreeAsync(d_result, streamA));\n",
    "    \n",
    "    CUDA_CHECK(cudaStreamSynchronize(streamA));\n",
    "    CUDA_CHECK(cudaStreamSynchronize(streamB));\n",
    "    \n",
    "    printf(\"Cross-stream sharing pattern complete!\\n\\n\");\n",
    "    \n",
    "    CUDA_CHECK(cudaEventDestroy(dataReady));\n",
    "    CUDA_CHECK(cudaEventDestroy(computeDone));\n",
    "    CUDA_CHECK(cudaStreamDestroy(streamA));\n",
    "    CUDA_CHECK(cudaStreamDestroy(streamB));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Stream-Ordered Memory Allocation Exercises ===\\n\\n\");\n",
    "    \n",
    "    exercise1_asyncPipeline();\n",
    "    exercise2_poolConfiguration();\n",
    "    exercise3_crossStreamSharing();\n",
    "    \n",
    "    printf(\"=== Exercises Complete ===\\n\");\n",
    "    printf(\"Uncomment the TODO sections to complete each exercise!\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o stream_ordered_exercises stream_ordered_exercises.cu && ./stream_ordered_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b50f82",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises explore stream-ordered concepts accessible in Python:\n",
    "\n",
    "1. **Stream management**: Create multiple CUDA streams with `cuda.stream()` and measure overlap efficiency\n",
    "2. **Async transfers**: Use `cuda.to_device(data, stream=s)` for async memory transfers between streams\n",
    "3. **Event synchronization**: Implement producer-consumer pattern using `cuda.event()` for cross-stream sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86d90a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### ğŸ“¦ Memory Management Patterns Learned\n",
    "\n",
    "| Pattern | Traditional | Stream-Ordered | Benefit |\n",
    "|---------|-------------|----------------|---------|\n",
    "| **Iterative Processing** | Alloc/free each iteration | Pool reuses memory | No allocation overhead |\n",
    "| **Multi-Stream Pipelines** | Sync breaks overlap | Async alloc maintains overlap | Better GPU utilization |\n",
    "| **Temporary Buffers** | OS round-trip each time | Pool returns instantly | Lower latency |\n",
    "\n",
    "### Core Stream-Ordered Concepts\n",
    "\n",
    "1. **`cudaMallocAsync`** - Non-blocking allocation tied to stream ordering\n",
    "2. **`cudaFreeAsync`** - Non-blocking deallocation returning to pool\n",
    "3. **Memory Pools** - Reuse memory without expensive OS round-trips\n",
    "4. **Release Threshold** - Control when pool releases memory back to system\n",
    "5. **Cross-Stream Events** - Use `cudaEventRecord`/`cudaStreamWaitEvent` for dependencies\n",
    "\n",
    "### ğŸ“¦ The JIT Inventory Model\n",
    "```\n",
    "Pool (Warehouse) â†’ MallocAsync (Order) â†’ Kernel Uses â†’ FreeAsync (Return)\n",
    "      ğŸ“¦               ğŸ“‹ Queue            ğŸ”§ Work         â™»ï¸ Recycle\n",
    "```\n",
    "\n",
    "**Next:** Multi-GPU VMM for sharing memory across devices!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
