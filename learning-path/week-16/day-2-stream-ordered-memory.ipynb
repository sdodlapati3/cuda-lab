{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35de823",
   "metadata": {},
   "source": [
    "## The Problem with Synchronous Allocation\n",
    "\n",
    "Traditional `cudaMalloc`:\n",
    "- Implicitly synchronizes the device\n",
    "- Breaks async execution pipelines\n",
    "- Overhead on every allocation\n",
    "\n",
    "Stream-ordered allocation:\n",
    "- Allocation tied to stream ordering\n",
    "- No implicit synchronization\n",
    "- Memory pools for fast reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6be95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_ordered_alloc.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = sqrtf((float)idx);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // Traditional approach - synchronous allocation\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 100;\n",
    "    \n",
    "    // Benchmark synchronous\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        float* d_data;\n",
    "        cudaMalloc(&d_data, size);  // Synchronizes!\n",
    "        processKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N);\n",
    "        cudaFree(d_data);  // Synchronizes!\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float syncMs;\n",
    "    cudaEventElapsedTime(&syncMs, start, stop);\n",
    "    \n",
    "    // Benchmark stream-ordered (async)\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        float* d_data;\n",
    "        cudaMallocAsync(&d_data, size, stream);  // Non-blocking!\n",
    "        processKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N);\n",
    "        cudaFreeAsync(d_data, stream);  // Non-blocking!\n",
    "    }\n",
    "    cudaStreamSynchronize(stream);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float asyncMs;\n",
    "    cudaEventElapsedTime(&asyncMs, start, stop);\n",
    "    \n",
    "    printf(\"Synchronous (cudaMalloc/cudaFree): %.2f ms\\n\", syncMs);\n",
    "    printf(\"Stream-ordered (Async):            %.2f ms\\n\", asyncMs);\n",
    "    printf(\"Speedup: %.1fx\\n\", syncMs / asyncMs);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c75e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc stream_ordered_alloc.cu -o stream_ordered_alloc && ./stream_ordered_alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51543523",
   "metadata": {},
   "source": [
    "## Memory Pool Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f40e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mempool_config.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    int device = 0;\n",
    "    cudaSetDevice(device);\n",
    "    \n",
    "    // Get default memory pool\n",
    "    cudaMemPool_t pool;\n",
    "    cudaDeviceGetDefaultMemPool(&pool, device);\n",
    "    \n",
    "    // Configure pool: set release threshold\n",
    "    // Memory below this threshold is kept for reuse\n",
    "    uint64_t threshold = 256 * 1024 * 1024;  // 256 MB\n",
    "    cudaMemPoolSetAttribute(pool, cudaMemPoolAttrReleaseThreshold, &threshold);\n",
    "    printf(\"Set release threshold: %llu MB\\n\", threshold / (1024*1024));\n",
    "    \n",
    "    // Query pool attributes\n",
    "    uint64_t usedBytes, reservedBytes;\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"Used: %llu bytes, Reserved: %llu bytes\\n\", usedBytes, reservedBytes);\n",
    "    \n",
    "    // Allocate some memory\n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    float* d_data;\n",
    "    size_t size = 100 * 1024 * 1024;  // 100 MB\n",
    "    cudaMallocAsync(&d_data, size, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After 100MB alloc - Used: %llu MB, Reserved: %llu MB\\n\", \n",
    "           usedBytes/(1024*1024), reservedBytes/(1024*1024));\n",
    "    \n",
    "    // Free it\n",
    "    cudaFreeAsync(d_data, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After free - Used: %llu MB, Reserved: %llu MB\\n\", \n",
    "           usedBytes/(1024*1024), reservedBytes/(1024*1024));\n",
    "    printf(\"(Memory kept for reuse up to threshold)\\n\");\n",
    "    \n",
    "    // Trim pool to release memory\n",
    "    cudaMemPoolTrimTo(pool, 0);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After trim - Reserved: %llu MB\\n\", reservedBytes/(1024*1024));\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158714ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc mempool_config.cu -o mempool_config && ./mempool_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b85fc",
   "metadata": {},
   "source": [
    "## Multi-Stream Pool Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multistream_pool.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel(float* data, int n, float val) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = val;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    cudaStream_t stream1, stream2;\n",
    "    cudaStreamCreate(&stream1);\n",
    "    cudaStreamCreate(&stream2);\n",
    "    \n",
    "    // Allocate in stream1\n",
    "    float* d_data;\n",
    "    cudaMallocAsync(&d_data, size, stream1);\n",
    "    kernel<<<(N+255)/256, 256, 0, stream1>>>(d_data, N, 1.0f);\n",
    "    \n",
    "    // Create dependency: stream2 waits for stream1\n",
    "    cudaEvent_t event;\n",
    "    cudaEventCreate(&event);\n",
    "    cudaEventRecord(event, stream1);\n",
    "    cudaStreamWaitEvent(stream2, event);\n",
    "    \n",
    "    // Now stream2 can use the data\n",
    "    kernel<<<(N+255)/256, 256, 0, stream2>>>(d_data, N, 2.0f);\n",
    "    \n",
    "    // Free in stream2\n",
    "    cudaFreeAsync(d_data, stream2);\n",
    "    \n",
    "    cudaStreamSynchronize(stream2);\n",
    "    \n",
    "    printf(\"Multi-stream pool sharing successful!\\n\");\n",
    "    \n",
    "    cudaEventDestroy(event);\n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(stream2);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc multistream_pool.cu -o multistream_pool && ./multistream_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86d90a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **`cudaMallocAsync`** - Non-blocking allocation tied to stream\n",
    "2. **`cudaFreeAsync`** - Non-blocking deallocation\n",
    "3. **Memory pools** - Reuse memory without returning to OS\n",
    "4. **Release threshold** - Control when pool releases memory\n",
    "5. **Multi-stream** - Use events for cross-stream dependencies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
