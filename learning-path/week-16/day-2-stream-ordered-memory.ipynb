{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35de823",
   "metadata": {},
   "source": [
    "## The Problem with Synchronous Allocation\n",
    "\n",
    "Traditional `cudaMalloc`:\n",
    "- Implicitly synchronizes the device\n",
    "- Breaks async execution pipelines\n",
    "- Overhead on every allocation\n",
    "\n",
    "Stream-ordered allocation:\n",
    "- Allocation tied to stream ordering\n",
    "- No implicit synchronization\n",
    "- Memory pools for fast reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6be95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_ordered_alloc.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = sqrtf((float)idx);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // Traditional approach - synchronous allocation\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int ITERATIONS = 100;\n",
    "    \n",
    "    // Benchmark synchronous\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        float* d_data;\n",
    "        cudaMalloc(&d_data, size);  // Synchronizes!\n",
    "        processKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N);\n",
    "        cudaFree(d_data);  // Synchronizes!\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float syncMs;\n",
    "    cudaEventElapsedTime(&syncMs, start, stop);\n",
    "    \n",
    "    // Benchmark stream-ordered (async)\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        float* d_data;\n",
    "        cudaMallocAsync(&d_data, size, stream);  // Non-blocking!\n",
    "        processKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N);\n",
    "        cudaFreeAsync(d_data, stream);  // Non-blocking!\n",
    "    }\n",
    "    cudaStreamSynchronize(stream);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float asyncMs;\n",
    "    cudaEventElapsedTime(&asyncMs, start, stop);\n",
    "    \n",
    "    printf(\"Synchronous (cudaMalloc/cudaFree): %.2f ms\\n\", syncMs);\n",
    "    printf(\"Stream-ordered (Async):            %.2f ms\\n\", asyncMs);\n",
    "    printf(\"Speedup: %.1fx\\n\", syncMs / asyncMs);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c75e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc stream_ordered_alloc.cu -o stream_ordered_alloc && ./stream_ordered_alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51543523",
   "metadata": {},
   "source": [
    "## Memory Pool Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f40e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mempool_config.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    int device = 0;\n",
    "    cudaSetDevice(device);\n",
    "    \n",
    "    // Get default memory pool\n",
    "    cudaMemPool_t pool;\n",
    "    cudaDeviceGetDefaultMemPool(&pool, device);\n",
    "    \n",
    "    // Configure pool: set release threshold\n",
    "    // Memory below this threshold is kept for reuse\n",
    "    uint64_t threshold = 256 * 1024 * 1024;  // 256 MB\n",
    "    cudaMemPoolSetAttribute(pool, cudaMemPoolAttrReleaseThreshold, &threshold);\n",
    "    printf(\"Set release threshold: %llu MB\\n\", threshold / (1024*1024));\n",
    "    \n",
    "    // Query pool attributes\n",
    "    uint64_t usedBytes, reservedBytes;\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"Used: %llu bytes, Reserved: %llu bytes\\n\", usedBytes, reservedBytes);\n",
    "    \n",
    "    // Allocate some memory\n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    float* d_data;\n",
    "    size_t size = 100 * 1024 * 1024;  // 100 MB\n",
    "    cudaMallocAsync(&d_data, size, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After 100MB alloc - Used: %llu MB, Reserved: %llu MB\\n\", \n",
    "           usedBytes/(1024*1024), reservedBytes/(1024*1024));\n",
    "    \n",
    "    // Free it\n",
    "    cudaFreeAsync(d_data, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrUsedMemCurrent, &usedBytes);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After free - Used: %llu MB, Reserved: %llu MB\\n\", \n",
    "           usedBytes/(1024*1024), reservedBytes/(1024*1024));\n",
    "    printf(\"(Memory kept for reuse up to threshold)\\n\");\n",
    "    \n",
    "    // Trim pool to release memory\n",
    "    cudaMemPoolTrimTo(pool, 0);\n",
    "    cudaMemPoolGetAttribute(pool, cudaMemPoolAttrReservedMemCurrent, &reservedBytes);\n",
    "    printf(\"After trim - Reserved: %llu MB\\n\", reservedBytes/(1024*1024));\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158714ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc mempool_config.cu -o mempool_config && ./mempool_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b85fc",
   "metadata": {},
   "source": [
    "## Multi-Stream Pool Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multistream_pool.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel(float* data, int n, float val) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = val;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    cudaStream_t stream1, stream2;\n",
    "    cudaStreamCreate(&stream1);\n",
    "    cudaStreamCreate(&stream2);\n",
    "    \n",
    "    // Allocate in stream1\n",
    "    float* d_data;\n",
    "    cudaMallocAsync(&d_data, size, stream1);\n",
    "    kernel<<<(N+255)/256, 256, 0, stream1>>>(d_data, N, 1.0f);\n",
    "    \n",
    "    // Create dependency: stream2 waits for stream1\n",
    "    cudaEvent_t event;\n",
    "    cudaEventCreate(&event);\n",
    "    cudaEventRecord(event, stream1);\n",
    "    cudaStreamWaitEvent(stream2, event);\n",
    "    \n",
    "    // Now stream2 can use the data\n",
    "    kernel<<<(N+255)/256, 256, 0, stream2>>>(d_data, N, 2.0f);\n",
    "    \n",
    "    // Free in stream2\n",
    "    cudaFreeAsync(d_data, stream2);\n",
    "    \n",
    "    cudaStreamSynchronize(stream2);\n",
    "    \n",
    "    printf(\"Multi-stream pool sharing successful!\\n\");\n",
    "    \n",
    "    cudaEventDestroy(event);\n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(stream2);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc multistream_pool.cu -o multistream_pool && ./multistream_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bde407",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercises\n",
    "\n",
    "### ðŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to practice stream-ordered memory allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_ordered_exercises.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <vector>\n",
    "\n",
    "/*\n",
    " * Stream-Ordered Memory Allocation Exercises\n",
    " * \n",
    " * Exercise 1: Implement a pipeline with async allocation\n",
    " * - Create multiple streams\n",
    " * - Use cudaMallocAsync in each stream\n",
    " * - Overlap allocation, compute, and deallocation\n",
    " * \n",
    " * Exercise 2: Memory pool configuration\n",
    " * - Create a custom memory pool\n",
    " * - Set release threshold\n",
    " * - Measure memory reuse efficiency\n",
    " * \n",
    " * Exercise 3: Cross-stream memory sharing\n",
    " * - Allocate in stream A\n",
    " * - Use cudaEventRecord to synchronize\n",
    " * - Access memory in stream B\n",
    " */\n",
    "\n",
    "#define CUDA_CHECK(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "__global__ void processKernel(float* data, int n, float multiplier) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = sqrtf((float)idx) * multiplier;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void reduceKernel(float* data, int n, float* result) {\n",
    "    __shared__ float sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? data[idx] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = 128; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(result, sdata[0]);\n",
    "}\n",
    "\n",
    "// Exercise 1: Async Allocation Pipeline\n",
    "void exercise1_asyncPipeline() {\n",
    "    printf(\"=== Exercise 1: Async Allocation Pipeline ===\\n\");\n",
    "    \n",
    "    const int NUM_STREAMS = 4;\n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    const int ITERATIONS = 10;\n",
    "    \n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        CUDA_CHECK(cudaStreamCreate(&streams[i]));\n",
    "    }\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CUDA_CHECK(cudaEventCreate(&start));\n",
    "    CUDA_CHECK(cudaEventCreate(&stop));\n",
    "    \n",
    "    // TODO Exercise 1a: Implement pipelined async allocation\n",
    "    // For each iteration:\n",
    "    //   1. cudaMallocAsync a buffer in stream[i % NUM_STREAMS]\n",
    "    //   2. Launch processKernel in that stream\n",
    "    //   3. cudaFreeAsync in that stream\n",
    "    \n",
    "    printf(\"Implement async allocation pipeline with %d streams\\n\", NUM_STREAMS);\n",
    "    printf(\"Each iteration should:\\n\");\n",
    "    printf(\"  - cudaMallocAsync %zu bytes\\n\", size);\n",
    "    printf(\"  - Launch processKernel\\n\");\n",
    "    printf(\"  - cudaFreeAsync\\n\\n\");\n",
    "    \n",
    "    // YOUR CODE HERE:\n",
    "    CUDA_CHECK(cudaEventRecord(start));\n",
    "    for (int iter = 0; iter < ITERATIONS; iter++) {\n",
    "        cudaStream_t stream = streams[iter % NUM_STREAMS];\n",
    "        float* d_data;\n",
    "        \n",
    "        // TODO: Uncomment and complete:\n",
    "        // CUDA_CHECK(cudaMallocAsync(&d_data, size, stream));\n",
    "        // processKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N, 1.0f);\n",
    "        // CUDA_CHECK(cudaFreeAsync(d_data, stream));\n",
    "    }\n",
    "    CUDA_CHECK(cudaEventRecord(stop));\n",
    "    \n",
    "    // Sync all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        CUDA_CHECK(cudaStreamSynchronize(streams[i]));\n",
    "    }\n",
    "    \n",
    "    float ms;\n",
    "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "    printf(\"Pipeline time: %.3f ms for %d iterations\\n\\n\", ms, ITERATIONS);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        CUDA_CHECK(cudaStreamDestroy(streams[i]));\n",
    "    }\n",
    "    CUDA_CHECK(cudaEventDestroy(start));\n",
    "    CUDA_CHECK(cudaEventDestroy(stop));\n",
    "}\n",
    "\n",
    "// Exercise 2: Memory Pool Configuration\n",
    "void exercise2_poolConfiguration() {\n",
    "    printf(\"=== Exercise 2: Memory Pool Configuration ===\\n\");\n",
    "    \n",
    "    const size_t SIZE = 1 << 24;  // 16 MB\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    CUDA_CHECK(cudaStreamCreate(&stream));\n",
    "    \n",
    "    // TODO Exercise 2a: Get the current device's memory pool\n",
    "    cudaMemPool_t memPool;\n",
    "    int device;\n",
    "    CUDA_CHECK(cudaGetDevice(&device));\n",
    "    \n",
    "    printf(\"Get current device memory pool and configure it:\\n\");\n",
    "    printf(\"  - cudaDeviceGetMemPool\\n\");\n",
    "    printf(\"  - cudaMemPoolSetAttribute for release threshold\\n\\n\");\n",
    "    \n",
    "    // YOUR CODE HERE:\n",
    "    // CUDA_CHECK(cudaDeviceGetMemPool(&memPool, device));\n",
    "    \n",
    "    // TODO Exercise 2b: Set release threshold\n",
    "    // uint64_t threshold = UINT64_MAX;  // Never release back to OS\n",
    "    // CUDA_CHECK(cudaMemPoolSetAttribute(memPool, \n",
    "    //            cudaMemPoolAttrReleaseThreshold, &threshold));\n",
    "    \n",
    "    // TODO Exercise 2c: Allocate and free multiple times, measure reuse\n",
    "    printf(\"Allocate/free %zu bytes multiple times and observe memory reuse\\n\", SIZE);\n",
    "    \n",
    "    for (int i = 0; i < 5; i++) {\n",
    "        float* d_data;\n",
    "        CUDA_CHECK(cudaMallocAsync(&d_data, SIZE, stream));\n",
    "        \n",
    "        // Get pool statistics\n",
    "        size_t currentSize = 0, highWatermark = 0;\n",
    "        // CUDA_CHECK(cudaMemPoolGetAttribute(memPool, \n",
    "        //            cudaMemPoolAttrUsedMemCurrent, &currentSize));\n",
    "        // CUDA_CHECK(cudaMemPoolGetAttribute(memPool,\n",
    "        //            cudaMemPoolAttrUsedMemHigh, &highWatermark));\n",
    "        \n",
    "        printf(\"  Iteration %d: allocated, current=%zu, high=%zu\\n\", \n",
    "               i, currentSize, highWatermark);\n",
    "        \n",
    "        CUDA_CHECK(cudaFreeAsync(d_data, stream));\n",
    "    }\n",
    "    CUDA_CHECK(cudaStreamSynchronize(stream));\n",
    "    \n",
    "    printf(\"\\n\");\n",
    "    CUDA_CHECK(cudaStreamDestroy(stream));\n",
    "}\n",
    "\n",
    "// Exercise 3: Cross-Stream Memory Sharing\n",
    "void exercise3_crossStreamSharing() {\n",
    "    printf(\"=== Exercise 3: Cross-Stream Memory Sharing ===\\n\");\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    cudaStream_t streamA, streamB;\n",
    "    CUDA_CHECK(cudaStreamCreate(&streamA));\n",
    "    CUDA_CHECK(cudaStreamCreate(&streamB));\n",
    "    \n",
    "    cudaEvent_t dataReady, computeDone;\n",
    "    CUDA_CHECK(cudaEventCreate(&dataReady));\n",
    "    CUDA_CHECK(cudaEventCreate(&computeDone));\n",
    "    \n",
    "    printf(\"Pattern: Allocate in streamA, use in streamB\\n\");\n",
    "    printf(\"  1. cudaMallocAsync in streamA\\n\");\n",
    "    printf(\"  2. cudaEventRecord(dataReady, streamA)\\n\");\n",
    "    printf(\"  3. cudaStreamWaitEvent(streamB, dataReady)\\n\");\n",
    "    printf(\"  4. Use memory in streamB\\n\");\n",
    "    printf(\"  5. cudaEventRecord(computeDone, streamB)\\n\");\n",
    "    printf(\"  6. cudaStreamWaitEvent(streamA, computeDone)\\n\");\n",
    "    printf(\"  7. cudaFreeAsync in streamA\\n\\n\");\n",
    "    \n",
    "    // TODO Exercise 3: Implement the cross-stream pattern\n",
    "    float* d_data;\n",
    "    float* d_result;\n",
    "    \n",
    "    // YOUR CODE HERE:\n",
    "    // Step 1: Allocate in streamA\n",
    "    // CUDA_CHECK(cudaMallocAsync(&d_data, size, streamA));\n",
    "    // CUDA_CHECK(cudaMallocAsync(&d_result, sizeof(float), streamA));\n",
    "    // CUDA_CHECK(cudaMemsetAsync(d_result, 0, sizeof(float), streamA));\n",
    "    \n",
    "    // Step 2: Signal data is ready\n",
    "    // CUDA_CHECK(cudaEventRecord(dataReady, streamA));\n",
    "    \n",
    "    // Step 3: streamB waits for data\n",
    "    // CUDA_CHECK(cudaStreamWaitEvent(streamB, dataReady));\n",
    "    \n",
    "    // Step 4: Use in streamB\n",
    "    // processKernel<<<(N+255)/256, 256, 0, streamB>>>(d_data, N, 2.0f);\n",
    "    // reduceKernel<<<(N+255)/256, 256, 0, streamB>>>(d_data, N, d_result);\n",
    "    \n",
    "    // Step 5-6: Signal compute done, streamA waits\n",
    "    // CUDA_CHECK(cudaEventRecord(computeDone, streamB));\n",
    "    // CUDA_CHECK(cudaStreamWaitEvent(streamA, computeDone));\n",
    "    \n",
    "    // Step 7: Free in streamA\n",
    "    // CUDA_CHECK(cudaFreeAsync(d_data, streamA));\n",
    "    // CUDA_CHECK(cudaFreeAsync(d_result, streamA));\n",
    "    \n",
    "    CUDA_CHECK(cudaStreamSynchronize(streamA));\n",
    "    CUDA_CHECK(cudaStreamSynchronize(streamB));\n",
    "    \n",
    "    printf(\"Cross-stream sharing pattern complete!\\n\\n\");\n",
    "    \n",
    "    CUDA_CHECK(cudaEventDestroy(dataReady));\n",
    "    CUDA_CHECK(cudaEventDestroy(computeDone));\n",
    "    CUDA_CHECK(cudaStreamDestroy(streamA));\n",
    "    CUDA_CHECK(cudaStreamDestroy(streamB));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Stream-Ordered Memory Allocation Exercises ===\\n\\n\");\n",
    "    \n",
    "    exercise1_asyncPipeline();\n",
    "    exercise2_poolConfiguration();\n",
    "    exercise3_crossStreamSharing();\n",
    "    \n",
    "    printf(\"=== Exercises Complete ===\\n\");\n",
    "    printf(\"Uncomment the TODO sections to complete each exercise!\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o stream_ordered_exercises stream_ordered_exercises.cu && ./stream_ordered_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b50f82",
   "metadata": {},
   "source": [
    "### ðŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises explore stream-ordered concepts accessible in Python:\n",
    "\n",
    "1. **Stream management**: Create multiple CUDA streams with `cuda.stream()` and measure overlap efficiency\n",
    "2. **Async transfers**: Use `cuda.to_device(data, stream=s)` for async memory transfers between streams\n",
    "3. **Event synchronization**: Implement producer-consumer pattern using `cuda.event()` for cross-stream sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86d90a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **`cudaMallocAsync`** - Non-blocking allocation tied to stream\n",
    "2. **`cudaFreeAsync`** - Non-blocking deallocation\n",
    "3. **Memory pools** - Reuse memory without returning to OS\n",
    "4. **Release threshold** - Control when pool releases memory\n",
    "5. **Multi-stream** - Use events for cross-stream dependencies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
