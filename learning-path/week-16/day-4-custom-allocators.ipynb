{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c10def",
   "metadata": {},
   "source": [
    "## Why Custom Allocators?\n",
    "\n",
    "- **Reduce fragmentation** - Pool similar-sized allocations\n",
    "- **Avoid synchronization** - Pre-allocate memory\n",
    "- **Enable growth** - Expand without copy using VMM\n",
    "- **Application-specific** - Optimize for your workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd23296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_pool_allocator.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <vector>\n",
    "#include <stack>\n",
    "\n",
    "// Simple fixed-size block pool allocator\n",
    "class BlockPoolAllocator {\n",
    "    char* pool;\n",
    "    size_t blockSize;\n",
    "    size_t numBlocks;\n",
    "    std::stack<void*> freeBlocks;\n",
    "    \n",
    "public:\n",
    "    BlockPoolAllocator(size_t blockSz, size_t nBlocks) \n",
    "        : blockSize(blockSz), numBlocks(nBlocks) {\n",
    "        // Allocate entire pool\n",
    "        cudaMalloc(&pool, blockSize * numBlocks);\n",
    "        \n",
    "        // Initialize free list\n",
    "        for (size_t i = 0; i < numBlocks; i++) {\n",
    "            freeBlocks.push(pool + i * blockSize);\n",
    "        }\n",
    "        \n",
    "        printf(\"Created pool: %zu blocks of %zu bytes\\n\", numBlocks, blockSize);\n",
    "    }\n",
    "    \n",
    "    void* allocate() {\n",
    "        if (freeBlocks.empty()) return nullptr;\n",
    "        \n",
    "        void* block = freeBlocks.top();\n",
    "        freeBlocks.pop();\n",
    "        return block;\n",
    "    }\n",
    "    \n",
    "    void deallocate(void* ptr) {\n",
    "        freeBlocks.push(ptr);\n",
    "    }\n",
    "    \n",
    "    size_t available() const { return freeBlocks.size(); }\n",
    "    \n",
    "    ~BlockPoolAllocator() {\n",
    "        cudaFree(pool);\n",
    "    }\n",
    "};\n",
    "\n",
    "__global__ void useBlock(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = (float)idx;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Create pool of 1KB blocks\n",
    "    size_t blockSize = 1024 * sizeof(float);  // 4KB\n",
    "    size_t numBlocks = 100;\n",
    "    \n",
    "    BlockPoolAllocator pool(blockSize, numBlocks);\n",
    "    \n",
    "    printf(\"Available blocks: %zu\\n\", pool.available());\n",
    "    \n",
    "    // Allocate several blocks\n",
    "    std::vector<void*> allocated;\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        void* ptr = pool.allocate();\n",
    "        if (ptr) {\n",
    "            allocated.push_back(ptr);\n",
    "            useBlock<<<1, 256>>>((float*)ptr, 1024);\n",
    "        }\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"After 10 allocations: %zu available\\n\", pool.available());\n",
    "    \n",
    "    // Return blocks to pool\n",
    "    for (void* ptr : allocated) {\n",
    "        pool.deallocate(ptr);\n",
    "    }\n",
    "    \n",
    "    printf(\"After deallocation: %zu available\\n\", pool.available());\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ebd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc simple_pool_allocator.cu -o simple_pool_allocator && ./simple_pool_allocator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110a638",
   "metadata": {},
   "source": [
    "## VMM-Based Growable Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vmm_growable_buffer.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <vector>\n",
    "\n",
    "class GrowableGPUBuffer {\n",
    "    CUdeviceptr ptr;\n",
    "    size_t reservedSize;\n",
    "    size_t mappedSize;\n",
    "    size_t granularity;\n",
    "    int device;\n",
    "    CUmemAllocationProp prop;\n",
    "    CUmemAccessDesc accessDesc;\n",
    "    std::vector<CUmemGenericAllocationHandle> handles;\n",
    "    \n",
    "public:\n",
    "    GrowableGPUBuffer(size_t maxSize) : mappedSize(0) {\n",
    "        cuInit(0);\n",
    "        cudaGetDevice(&device);\n",
    "        \n",
    "        // Setup allocation properties\n",
    "        memset(&prop, 0, sizeof(prop));\n",
    "        prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n",
    "        prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        prop.location.id = device;\n",
    "        \n",
    "        cuMemGetAllocationGranularity(&granularity, &prop,\n",
    "                                      CU_MEM_ALLOC_GRANULARITY_MINIMUM);\n",
    "        \n",
    "        // Reserve virtual address space\n",
    "        reservedSize = align(maxSize);\n",
    "        cuMemAddressReserve(&ptr, reservedSize, granularity, 0, 0);\n",
    "        \n",
    "        // Setup access descriptor\n",
    "        memset(&accessDesc, 0, sizeof(accessDesc));\n",
    "        accessDesc.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        accessDesc.location.id = device;\n",
    "        accessDesc.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "        \n",
    "        printf(\"GrowableBuffer: Reserved %zu MB\\n\", reservedSize / (1024*1024));\n",
    "    }\n",
    "    \n",
    "    size_t align(size_t size) {\n",
    "        return ((size + granularity - 1) / granularity) * granularity;\n",
    "    }\n",
    "    \n",
    "    bool grow(size_t newSize) {\n",
    "        newSize = align(newSize);\n",
    "        if (newSize <= mappedSize) return true;\n",
    "        if (newSize > reservedSize) return false;\n",
    "        \n",
    "        size_t toMap = newSize - mappedSize;\n",
    "        \n",
    "        // Create new physical allocation\n",
    "        CUmemGenericAllocationHandle handle;\n",
    "        CUresult res = cuMemCreate(&handle, toMap, &prop, 0);\n",
    "        if (res != CUDA_SUCCESS) return false;\n",
    "        \n",
    "        // Map to virtual address\n",
    "        res = cuMemMap(ptr + mappedSize, toMap, 0, handle, 0);\n",
    "        if (res != CUDA_SUCCESS) {\n",
    "            cuMemRelease(handle);\n",
    "            return false;\n",
    "        }\n",
    "        \n",
    "        // Set access\n",
    "        res = cuMemSetAccess(ptr + mappedSize, toMap, &accessDesc, 1);\n",
    "        if (res != CUDA_SUCCESS) return false;\n",
    "        \n",
    "        handles.push_back(handle);\n",
    "        mappedSize = newSize;\n",
    "        \n",
    "        return true;\n",
    "    }\n",
    "    \n",
    "    void* data() { return (void*)ptr; }\n",
    "    size_t size() { return mappedSize; }\n",
    "    size_t capacity() { return reservedSize; }\n",
    "    \n",
    "    ~GrowableGPUBuffer() {\n",
    "        cuMemUnmap(ptr, mappedSize);\n",
    "        for (auto& h : handles) cuMemRelease(h);\n",
    "        cuMemAddressFree(ptr, reservedSize);\n",
    "    }\n",
    "};\n",
    "\n",
    "__global__ void fillKernel(int* data, int n, int value) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = value;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Create buffer with 1GB max capacity\n",
    "    GrowableGPUBuffer buffer(1ULL << 30);\n",
    "    \n",
    "    // Start with 1MB\n",
    "    size_t size1 = 1 << 20;\n",
    "    buffer.grow(size1);\n",
    "    printf(\"After grow(1MB): %zu bytes mapped\\n\", buffer.size());\n",
    "    \n",
    "    int n1 = size1 / sizeof(int);\n",
    "    fillKernel<<<(n1+255)/256, 256>>>((int*)buffer.data(), n1, 1);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Grow to 100MB - NO COPY NEEDED!\n",
    "    size_t size2 = 100 << 20;\n",
    "    buffer.grow(size2);\n",
    "    printf(\"After grow(100MB): %zu bytes mapped\\n\", buffer.size());\n",
    "    \n",
    "    // Original data still at same address, new space available\n",
    "    int n2 = size2 / sizeof(int);\n",
    "    fillKernel<<<(n2+255)/256, 256>>>((int*)buffer.data() + n1, n2 - n1, 2);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Verify\n",
    "    int h_val1, h_val2;\n",
    "    cudaMemcpy(&h_val1, buffer.data(), sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(&h_val2, (int*)buffer.data() + n1, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Original region value: %d (expected 1)\\n\", h_val1);\n",
    "    printf(\"New region value: %d (expected 2)\\n\", h_val2);\n",
    "    \n",
    "    printf(\"\\nGrowable buffer SUCCESS - no copying during growth!\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc vmm_growable_buffer.cu -o vmm_growable_buffer -lcuda && ./vmm_growable_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b78f0a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Block pools** - Fast allocation for fixed-size objects\n",
    "2. **VMM growable** - Expand without copying\n",
    "3. **Pre-allocation** - Avoid runtime allocation overhead\n",
    "4. **Application-specific** - Design for your workload patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
