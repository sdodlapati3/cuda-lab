{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4499d586",
   "metadata": {},
   "source": [
    "## Multi-GPU Memory Sharing\n",
    "\n",
    "VMM enables fine-grained control over which GPUs can access which memory:\n",
    "\n",
    "```cpp\n",
    "// Grant GPU 1 access to memory on GPU 0\n",
    "CUmemAccessDesc accessDesc;\n",
    "accessDesc.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "accessDesc.location.id = 1;  // GPU 1 gets access\n",
    "accessDesc.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "cuMemSetAccess(ptr_on_gpu0, size, &accessDesc, 1);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multi_gpu_vmm.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void writeKernel(int* data, int n, int value) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = value;\n",
    "}\n",
    "\n",
    "__global__ void readKernel(int* data, int n, long long* sum) {\n",
    "    __shared__ long long sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? data[idx] : 0;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = 128; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(sum, sdata[0]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    printf(\"Found %d GPU(s)\\n\", deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Multi-GPU VMM requires 2+ GPUs. Demonstrating single-GPU case.\\n\");\n",
    "        \n",
    "        // Single GPU demonstration\n",
    "        cuInit(0);\n",
    "        \n",
    "        CUmemAllocationProp prop = {};\n",
    "        prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n",
    "        prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        prop.location.id = 0;\n",
    "        \n",
    "        size_t granularity;\n",
    "        cuMemGetAllocationGranularity(&granularity, &prop,\n",
    "                                      CU_MEM_ALLOC_GRANULARITY_MINIMUM);\n",
    "        \n",
    "        size_t size = 1 << 20;\n",
    "        size = ((size + granularity - 1) / granularity) * granularity;\n",
    "        \n",
    "        CUdeviceptr ptr;\n",
    "        cuMemAddressReserve(&ptr, size, granularity, 0, 0);\n",
    "        \n",
    "        CUmemGenericAllocationHandle handle;\n",
    "        cuMemCreate(&handle, size, &prop, 0);\n",
    "        cuMemMap(ptr, size, 0, handle, 0);\n",
    "        \n",
    "        CUmemAccessDesc accessDesc = {};\n",
    "        accessDesc.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        accessDesc.location.id = 0;\n",
    "        accessDesc.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "        cuMemSetAccess(ptr, size, &accessDesc, 1);\n",
    "        \n",
    "        int n = size / sizeof(int);\n",
    "        writeKernel<<<(n+255)/256, 256>>>((int*)ptr, n, 1);\n",
    "        \n",
    "        long long* d_sum;\n",
    "        cudaMalloc(&d_sum, sizeof(long long));\n",
    "        cudaMemset(d_sum, 0, sizeof(long long));\n",
    "        readKernel<<<(n+255)/256, 256>>>((int*)ptr, n, d_sum);\n",
    "        \n",
    "        long long h_sum;\n",
    "        cudaMemcpy(&h_sum, d_sum, sizeof(long long), cudaMemcpyDeviceToHost);\n",
    "        printf(\"Sum: %lld (expected %d)\\n\", h_sum, n);\n",
    "        \n",
    "        cudaFree(d_sum);\n",
    "        cuMemUnmap(ptr, size);\n",
    "        cuMemRelease(handle);\n",
    "        cuMemAddressFree(ptr, size);\n",
    "        \n",
    "        return 0;\n",
    "    }\n",
    "    \n",
    "    // Multi-GPU case\n",
    "    cuInit(0);\n",
    "    \n",
    "    // Check P2P capability\n",
    "    int canAccess;\n",
    "    cudaDeviceCanAccessPeer(&canAccess, 1, 0);\n",
    "    if (!canAccess) {\n",
    "        printf(\"GPU 1 cannot access GPU 0 memory\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    printf(\"P2P access supported between GPU 0 and GPU 1\\n\");\n",
    "    \n",
    "    // Allocate on GPU 0 with VMM\n",
    "    cudaSetDevice(0);\n",
    "    \n",
    "    CUmemAllocationProp prop = {};\n",
    "    prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n",
    "    prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "    prop.location.id = 0;\n",
    "    \n",
    "    size_t granularity;\n",
    "    cuMemGetAllocationGranularity(&granularity, &prop,\n",
    "                                  CU_MEM_ALLOC_GRANULARITY_MINIMUM);\n",
    "    \n",
    "    size_t size = 1 << 20;\n",
    "    size = ((size + granularity - 1) / granularity) * granularity;\n",
    "    \n",
    "    CUdeviceptr ptr;\n",
    "    cuMemAddressReserve(&ptr, size, granularity, 0, 0);\n",
    "    \n",
    "    CUmemGenericAllocationHandle handle;\n",
    "    cuMemCreate(&handle, size, &prop, 0);\n",
    "    cuMemMap(ptr, size, 0, handle, 0);\n",
    "    \n",
    "    // Grant access to BOTH GPUs\n",
    "    CUmemAccessDesc accessDescs[2];\n",
    "    for (int i = 0; i < 2; i++) {\n",
    "        accessDescs[i].location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        accessDescs[i].location.id = i;\n",
    "        accessDescs[i].flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "    }\n",
    "    cuMemSetAccess(ptr, size, accessDescs, 2);\n",
    "    printf(\"Granted access to GPU 0 and GPU 1\\n\");\n",
    "    \n",
    "    // GPU 0 writes\n",
    "    cudaSetDevice(0);\n",
    "    int n = size / sizeof(int);\n",
    "    writeKernel<<<(n+255)/256, 256>>>((int*)ptr, n, 42);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"GPU 0 wrote data\\n\");\n",
    "    \n",
    "    // GPU 1 reads (P2P access!)\n",
    "    cudaSetDevice(1);\n",
    "    long long* d_sum;\n",
    "    cudaMalloc(&d_sum, sizeof(long long));\n",
    "    cudaMemset(d_sum, 0, sizeof(long long));\n",
    "    readKernel<<<(n+255)/256, 256>>>((int*)ptr, n, d_sum);\n",
    "    \n",
    "    long long h_sum;\n",
    "    cudaMemcpy(&h_sum, d_sum, sizeof(long long), cudaMemcpyDeviceToHost);\n",
    "    printf(\"GPU 1 read sum: %lld (expected %lld)\\n\", h_sum, (long long)n * 42);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_sum);\n",
    "    cuMemUnmap(ptr, size);\n",
    "    cuMemRelease(handle);\n",
    "    cuMemAddressFree(ptr, size);\n",
    "    \n",
    "    printf(\"Multi-GPU VMM complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b818cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc multi_gpu_vmm.cu -o multi_gpu_vmm -lcuda && ./multi_gpu_vmm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d7319",
   "metadata": {},
   "source": [
    "## Fabric Handles for NVLink Clusters\n",
    "\n",
    "For larger clusters with NVLink/NVSwitch, fabric handles enable efficient memory sharing:\n",
    "\n",
    "```cpp\n",
    "// Export allocation as fabric handle\n",
    "CUmemFabricHandle fabricHandle;\n",
    "cuMemExportToShareableHandle(&fabricHandle, handle, \n",
    "                              CU_MEM_HANDLE_TYPE_FABRIC, 0);\n",
    "\n",
    "// Import on another GPU\n",
    "CUmemGenericAllocationHandle importedHandle;\n",
    "cuMemImportFromShareableHandle(&importedHandle, &fabricHandle,\n",
    "                                CU_MEM_HANDLE_TYPE_FABRIC);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe33f3a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Fine-grained access** - Control exactly which GPUs access which memory\n",
    "2. **`cuMemSetAccess`** - Grant R/W permissions per device\n",
    "3. **P2P over VMM** - More control than `cudaEnablePeerAccess`\n",
    "4. **Fabric handles** - Scale to large NVLink clusters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
