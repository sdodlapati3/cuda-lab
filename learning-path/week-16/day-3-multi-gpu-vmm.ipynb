{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4499d586",
   "metadata": {},
   "source": [
    "# Day 3: Multi-GPU Virtual Memory Management\n",
    "\n",
    "> ğŸ›£ï¸ **\"What if different GPUs could access the same memory like reserved highway lanesâ€”you control exactly who can use which lanes and when?\"**\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "1. Configure cross-GPU memory access permissions using VMM\n",
    "2. Compare VMM-based sharing vs traditional peer access\n",
    "3. Map virtual memory across multiple devices\n",
    "4. Use fabric handles for large NVLink cluster deployments\n",
    "\n",
    "## Multi-GPU Memory Sharing\n",
    "\n",
    "VMM enables fine-grained control over which GPUs can access which memory:\n",
    "\n",
    "```cpp\n",
    "// Grant GPU 1 access to memory on GPU 0\n",
    "CUmemAccessDesc accessDesc;\n",
    "accessDesc.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "accessDesc.location.id = 1;  // GPU 1 gets access\n",
    "accessDesc.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "cuMemSetAccess(ptr_on_gpu0, size, &accessDesc, 1);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994df408",
   "metadata": {},
   "source": [
    "## ğŸ›£ï¸ Concept Card: Multi-GPU VMM as Reserved Highway Lanes\n",
    "\n",
    "**The Analogy:** Traditional peer access is like opening all highway lanes to everyoneâ€”simple but no control. VMM is like having **reserved lanes** where you control exactly which vehicles (GPUs) can use which sections:\n",
    "\n",
    "```\n",
    "ğŸ›£ï¸ RESERVED HIGHWAY LANE SYSTEM\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Traditional P2P Access:          VMM Access Control:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸš— ALL OR NOTHING  â”‚          â”‚  ğŸ›£ï¸ RESERVED LANES      â”‚\n",
    "â”‚                     â”‚          â”‚                         â”‚\n",
    "â”‚  GPU0 â†â†’ GPU1       â”‚          â”‚  Lane 1: GPU0 only â•â•â•  â”‚\n",
    "â”‚  Full access or     â”‚          â”‚  Lane 2: GPU0+GPU1 â•â•â•  â”‚\n",
    "â”‚  no access          â”‚          â”‚  Lane 3: GPU1 only â•â•â•  â”‚\n",
    "â”‚                     â”‚          â”‚  Lane 4: All GPUs â•â•â•â•â• â”‚\n",
    "â”‚                     â”‚          â”‚                         â”‚\n",
    "â”‚  No per-buffer      â”‚          â”‚  Per-buffer control!    â”‚\n",
    "â”‚  control            â”‚          â”‚                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“                                  â†“\n",
    "   Binary on/off                   Fine-grained permissions\n",
    "   for entire GPU                  per memory region!\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "**The Lane Reservation System:**\n",
    "| Highway Concept | VMM Operation | Purpose |\n",
    "|-----------------|---------------|---------|\n",
    "| **Reserve lanes** | `cuMemAddressReserve()` | Claim virtual address space |\n",
    "| **Build road section** | `cuMemCreate()` | Allocate physical memory on specific GPU |\n",
    "| **Connect to highway** | `cuMemMap()` | Map physical to virtual address |\n",
    "| **Open lane access** | `cuMemSetAccess()` | Grant R/W to specific GPUs |\n",
    "| **NVLink interchange** | Fabric handles | High-speed GPU interconnect |\n",
    "\n",
    "**Why This Matters:** Like lane reservations that let you control traffic flow precisely, VMM lets you control exactly which GPUs access which dataâ€”essential for security, performance, and complex multi-GPU algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multi_gpu_vmm.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void writeKernel(int* data, int n, int value) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = value;\n",
    "}\n",
    "\n",
    "__global__ void readKernel(int* data, int n, long long* sum) {\n",
    "    __shared__ long long sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? data[idx] : 0;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = 128; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(sum, sdata[0]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    printf(\"Found %d GPU(s)\\n\", deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Multi-GPU VMM requires 2+ GPUs. Demonstrating single-GPU case.\\n\");\n",
    "        \n",
    "        // Single GPU demonstration\n",
    "        cuInit(0);\n",
    "        \n",
    "        CUmemAllocationProp prop = {};\n",
    "        prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n",
    "        prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        prop.location.id = 0;\n",
    "        \n",
    "        size_t granularity;\n",
    "        cuMemGetAllocationGranularity(&granularity, &prop,\n",
    "                                      CU_MEM_ALLOC_GRANULARITY_MINIMUM);\n",
    "        \n",
    "        size_t size = 1 << 20;\n",
    "        size = ((size + granularity - 1) / granularity) * granularity;\n",
    "        \n",
    "        CUdeviceptr ptr;\n",
    "        cuMemAddressReserve(&ptr, size, granularity, 0, 0);\n",
    "        \n",
    "        CUmemGenericAllocationHandle handle;\n",
    "        cuMemCreate(&handle, size, &prop, 0);\n",
    "        cuMemMap(ptr, size, 0, handle, 0);\n",
    "        \n",
    "        CUmemAccessDesc accessDesc = {};\n",
    "        accessDesc.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        accessDesc.location.id = 0;\n",
    "        accessDesc.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "        cuMemSetAccess(ptr, size, &accessDesc, 1);\n",
    "        \n",
    "        int n = size / sizeof(int);\n",
    "        writeKernel<<<(n+255)/256, 256>>>((int*)ptr, n, 1);\n",
    "        \n",
    "        long long* d_sum;\n",
    "        cudaMalloc(&d_sum, sizeof(long long));\n",
    "        cudaMemset(d_sum, 0, sizeof(long long));\n",
    "        readKernel<<<(n+255)/256, 256>>>((int*)ptr, n, d_sum);\n",
    "        \n",
    "        long long h_sum;\n",
    "        cudaMemcpy(&h_sum, d_sum, sizeof(long long), cudaMemcpyDeviceToHost);\n",
    "        printf(\"Sum: %lld (expected %d)\\n\", h_sum, n);\n",
    "        \n",
    "        cudaFree(d_sum);\n",
    "        cuMemUnmap(ptr, size);\n",
    "        cuMemRelease(handle);\n",
    "        cuMemAddressFree(ptr, size);\n",
    "        \n",
    "        return 0;\n",
    "    }\n",
    "    \n",
    "    // Multi-GPU case\n",
    "    cuInit(0);\n",
    "    \n",
    "    // Check P2P capability\n",
    "    int canAccess;\n",
    "    cudaDeviceCanAccessPeer(&canAccess, 1, 0);\n",
    "    if (!canAccess) {\n",
    "        printf(\"GPU 1 cannot access GPU 0 memory\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    printf(\"P2P access supported between GPU 0 and GPU 1\\n\");\n",
    "    \n",
    "    // Allocate on GPU 0 with VMM\n",
    "    cudaSetDevice(0);\n",
    "    \n",
    "    CUmemAllocationProp prop = {};\n",
    "    prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n",
    "    prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "    prop.location.id = 0;\n",
    "    \n",
    "    size_t granularity;\n",
    "    cuMemGetAllocationGranularity(&granularity, &prop,\n",
    "                                  CU_MEM_ALLOC_GRANULARITY_MINIMUM);\n",
    "    \n",
    "    size_t size = 1 << 20;\n",
    "    size = ((size + granularity - 1) / granularity) * granularity;\n",
    "    \n",
    "    CUdeviceptr ptr;\n",
    "    cuMemAddressReserve(&ptr, size, granularity, 0, 0);\n",
    "    \n",
    "    CUmemGenericAllocationHandle handle;\n",
    "    cuMemCreate(&handle, size, &prop, 0);\n",
    "    cuMemMap(ptr, size, 0, handle, 0);\n",
    "    \n",
    "    // Grant access to BOTH GPUs\n",
    "    CUmemAccessDesc accessDescs[2];\n",
    "    for (int i = 0; i < 2; i++) {\n",
    "        accessDescs[i].location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        accessDescs[i].location.id = i;\n",
    "        accessDescs[i].flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "    }\n",
    "    cuMemSetAccess(ptr, size, accessDescs, 2);\n",
    "    printf(\"Granted access to GPU 0 and GPU 1\\n\");\n",
    "    \n",
    "    // GPU 0 writes\n",
    "    cudaSetDevice(0);\n",
    "    int n = size / sizeof(int);\n",
    "    writeKernel<<<(n+255)/256, 256>>>((int*)ptr, n, 42);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"GPU 0 wrote data\\n\");\n",
    "    \n",
    "    // GPU 1 reads (P2P access!)\n",
    "    cudaSetDevice(1);\n",
    "    long long* d_sum;\n",
    "    cudaMalloc(&d_sum, sizeof(long long));\n",
    "    cudaMemset(d_sum, 0, sizeof(long long));\n",
    "    readKernel<<<(n+255)/256, 256>>>((int*)ptr, n, d_sum);\n",
    "    \n",
    "    long long h_sum;\n",
    "    cudaMemcpy(&h_sum, d_sum, sizeof(long long), cudaMemcpyDeviceToHost);\n",
    "    printf(\"GPU 1 read sum: %lld (expected %lld)\\n\", h_sum, (long long)n * 42);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_sum);\n",
    "    cuMemUnmap(ptr, size);\n",
    "    cuMemRelease(handle);\n",
    "    cuMemAddressFree(ptr, size);\n",
    "    \n",
    "    printf(\"Multi-GPU VMM complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b818cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc multi_gpu_vmm.cu -o multi_gpu_vmm -lcuda && ./multi_gpu_vmm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d7319",
   "metadata": {},
   "source": [
    "## Fabric Handles for NVLink Clusters\n",
    "\n",
    "For larger clusters with NVLink/NVSwitch, fabric handles enable efficient memory sharing:\n",
    "\n",
    "```cpp\n",
    "// Export allocation as fabric handle\n",
    "CUmemFabricHandle fabricHandle;\n",
    "cuMemExportToShareableHandle(&fabricHandle, handle, \n",
    "                              CU_MEM_HANDLE_TYPE_FABRIC, 0);\n",
    "\n",
    "// Import on another GPU\n",
    "CUmemGenericAllocationHandle importedHandle;\n",
    "cuMemImportFromShareableHandle(&importedHandle, &fabricHandle,\n",
    "                                CU_MEM_HANDLE_TYPE_FABRIC);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decda932",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to practice multi-GPU VMM programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d34cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multi_gpu_vmm_exercises.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "/*\n",
    " * Multi-GPU VMM Exercises\n",
    " * \n",
    " * Exercise 1: Implement peer access using VMM\n",
    " * - Allocate memory on GPU 0 using VMM\n",
    " * - Grant GPU 1 read/write access via cuMemSetAccess\n",
    " * - Launch kernels on both GPUs accessing same memory\n",
    " * \n",
    " * Exercise 2: Ring buffer across GPUs\n",
    " * - Create shared memory region accessible by all GPUs\n",
    " * - Implement producer-consumer with GPU 0 writing, GPU 1 reading\n",
    " * \n",
    " * Exercise 3: Compare P2P methods\n",
    " * - cudaEnablePeerAccess vs VMM-based access\n",
    " * - Measure bandwidth differences\n",
    " */\n",
    "\n",
    "#define CU_CHECK(call) do { \\\n",
    "    CUresult err = call; \\\n",
    "    if (err != CUDA_SUCCESS) { \\\n",
    "        const char* errStr; \\\n",
    "        cuGetErrorString(err, &errStr); \\\n",
    "        printf(\"CUDA Driver Error: %s at %s:%d\\n\", errStr, __FILE__, __LINE__); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "#define CUDA_CHECK(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "__global__ void writeKernel(int* data, int n, int value) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = value + idx;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void readKernel(int* data, int n, long long* sum) {\n",
    "    __shared__ long long sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? data[idx] : 0;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = 128; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(sum, sdata[0]);\n",
    "}\n",
    "\n",
    "__global__ void bandwidthKernel(float* src, float* dst, size_t n) {\n",
    "    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = gridDim.x * blockDim.x;\n",
    "    for (size_t i = idx; i < n; i += stride) {\n",
    "        dst[i] = src[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Exercise 1: VMM-based Peer Access\n",
    "void exercise1_vmmPeerAccess() {\n",
    "    printf(\"=== Exercise 1: VMM-based Peer Access ===\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"This exercise requires 2+ GPUs. Showing single-GPU simulation.\\n\\n\");\n",
    "        \n",
    "        // Single GPU case - demonstrate the API calls\n",
    "        const int N = 1024;\n",
    "        const size_t size = N * sizeof(int);\n",
    "        \n",
    "        cuInit(0);\n",
    "        \n",
    "        // Get allocation properties for GPU 0\n",
    "        CUmemAllocationProp prop = {};\n",
    "        prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n",
    "        prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "        prop.location.id = 0;\n",
    "        \n",
    "        size_t granularity;\n",
    "        CU_CHECK(cuMemGetAllocationGranularity(&granularity, &prop, \n",
    "                                                CU_MEM_ALLOC_GRANULARITY_MINIMUM));\n",
    "        \n",
    "        printf(\"Allocation granularity: %zu bytes\\n\", granularity);\n",
    "        printf(\"In multi-GPU case, you would:\\n\");\n",
    "        printf(\"  1. cuMemAddressReserve for VA range\\n\");\n",
    "        printf(\"  2. cuMemCreate for physical memory on GPU 0\\n\");\n",
    "        printf(\"  3. cuMemMap to map physical to VA\\n\");\n",
    "        printf(\"  4. cuMemSetAccess to grant GPU 1 access\\n\");\n",
    "        printf(\"  5. Launch kernels on both GPUs\\n\\n\");\n",
    "        \n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // TODO Exercise 1: Multi-GPU VMM peer access\n",
    "    printf(\"Implement VMM-based peer access:\\n\");\n",
    "    \n",
    "    cuInit(0);\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const size_t size = N * sizeof(int);\n",
    "    \n",
    "    // Step 1: Get properties for GPU 0\n",
    "    CUmemAllocationProp prop = {};\n",
    "    prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n",
    "    prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "    prop.location.id = 0;\n",
    "    \n",
    "    size_t granularity;\n",
    "    CU_CHECK(cuMemGetAllocationGranularity(&granularity, &prop, \n",
    "                                            CU_MEM_ALLOC_GRANULARITY_MINIMUM));\n",
    "    \n",
    "    size_t alignedSize = ((size + granularity - 1) / granularity) * granularity;\n",
    "    \n",
    "    // TODO: Reserve VA, create physical memory, map it\n",
    "    CUdeviceptr ptr;\n",
    "    CUmemGenericAllocationHandle handle;\n",
    "    \n",
    "    // YOUR CODE HERE:\n",
    "    // CU_CHECK(cuMemAddressReserve(&ptr, alignedSize, granularity, 0, 0));\n",
    "    // CU_CHECK(cuMemCreate(&handle, alignedSize, &prop, 0));\n",
    "    // CU_CHECK(cuMemMap(ptr, alignedSize, 0, handle, 0));\n",
    "    \n",
    "    // TODO: Set access for GPU 0\n",
    "    // CUmemAccessDesc accessDesc0 = {};\n",
    "    // accessDesc0.location = prop.location;\n",
    "    // accessDesc0.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "    // CU_CHECK(cuMemSetAccess(ptr, alignedSize, &accessDesc0, 1));\n",
    "    \n",
    "    // TODO: Grant access to GPU 1\n",
    "    // CUmemAccessDesc accessDesc1 = {};\n",
    "    // accessDesc1.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n",
    "    // accessDesc1.location.id = 1;  // GPU 1\n",
    "    // accessDesc1.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;\n",
    "    // CU_CHECK(cuMemSetAccess(ptr, alignedSize, &accessDesc1, 1));\n",
    "    \n",
    "    printf(\"VMM peer access setup complete (uncomment code to run)\\n\\n\");\n",
    "    \n",
    "    // Cleanup would go here\n",
    "}\n",
    "\n",
    "// Exercise 2: Ring Buffer Pattern\n",
    "void exercise2_ringBuffer() {\n",
    "    printf(\"=== Exercise 2: Multi-GPU Ring Buffer ===\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    printf(\"Ring buffer pattern for producer-consumer:\\n\");\n",
    "    printf(\"  - GPU 0: Producer (writes data)\\n\");\n",
    "    printf(\"  - GPU 1: Consumer (reads data)\\n\");\n",
    "    printf(\"  - Shared memory region with VMM access control\\n\\n\");\n",
    "    \n",
    "    // TODO Exercise 2: Implement ring buffer\n",
    "    printf(\"Steps to implement:\\n\");\n",
    "    printf(\"  1. Allocate buffer on GPU 0 with VMM\\n\");\n",
    "    printf(\"  2. Grant GPU 1 read access via cuMemSetAccess\\n\");\n",
    "    printf(\"  3. GPU 0 writes to head, GPU 1 reads from tail\\n\");\n",
    "    printf(\"  4. Use atomic operations for head/tail pointers\\n\\n\");\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Requires 2+ GPUs for full implementation.\\n\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // YOUR CODE HERE for multi-GPU ring buffer\n",
    "}\n",
    "\n",
    "// Exercise 3: P2P Method Comparison\n",
    "void exercise3_p2pComparison() {\n",
    "    printf(\"=== Exercise 3: P2P Method Comparison ===\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Requires 2+ GPUs. Showing expected measurements.\\n\");\n",
    "        printf(\"  cudaEnablePeerAccess: Simple, automatic optimization\\n\");\n",
    "        printf(\"  VMM cuMemSetAccess: Fine-grained control, per-allocation\\n\");\n",
    "        printf(\"  Expected: Similar bandwidth when NVLink available\\n\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // TODO Exercise 3: Compare bandwidth\n",
    "    printf(\"Implement bandwidth comparison:\\n\");\n",
    "    printf(\"  Method 1: cudaEnablePeerAccess + cudaMalloc\\n\");\n",
    "    printf(\"  Method 2: VMM + cuMemSetAccess\\n\");\n",
    "    printf(\"  Measure: GPU-to-GPU memory copy bandwidth\\n\\n\");\n",
    "    \n",
    "    const size_t SIZE = 1ULL << 28;  // 256 MB\n",
    "    const int ITERATIONS = 10;\n",
    "    \n",
    "    // Check P2P capability\n",
    "    int canAccessPeer;\n",
    "    CUDA_CHECK(cudaDeviceCanAccessPeer(&canAccessPeer, 0, 1));\n",
    "    printf(\"P2P access 0->1: %s\\n\", canAccessPeer ? \"Yes\" : \"No\");\n",
    "    \n",
    "    if (!canAccessPeer) {\n",
    "        printf(\"P2P not supported between GPUs\\n\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // YOUR CODE HERE:\n",
    "    // 1. Enable peer access with cudaEnablePeerAccess\n",
    "    // 2. Allocate on GPU 0, access from GPU 1\n",
    "    // 3. Time the bandwidth\n",
    "    // 4. Repeat with VMM approach\n",
    "    // 5. Compare results\n",
    "    \n",
    "    printf(\"Implement timing comparison (uncomment code)\\n\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Multi-GPU VMM Exercises ===\\n\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    printf(\"Found %d GPU(s)\\n\\n\", deviceCount);\n",
    "    \n",
    "    exercise1_vmmPeerAccess();\n",
    "    exercise2_ringBuffer();\n",
    "    exercise3_p2pComparison();\n",
    "    \n",
    "    printf(\"=== Exercises Complete ===\\n\");\n",
    "    printf(\"For multi-GPU exercises, run on a system with 2+ GPUs.\\n\");\n",
    "    printf(\"Uncomment TODO sections to complete implementations.\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce6bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o multi_gpu_vmm_exercises multi_gpu_vmm_exercises.cu -lcuda && ./multi_gpu_vmm_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3325a",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises explore multi-GPU concepts accessible in Python/Numba:\n",
    "\n",
    "1. **Multi-GPU detection**: Use `cuda.gpus` to enumerate devices and check `cuda.gpus[i].compute_capability`\n",
    "2. **Cross-device transfers**: Implement data exchange between GPUs using `cuda.select_device()` and explicit copies\n",
    "3. **Peer access check**: Use Numba's CUDA API to check peer accessibility between GPU pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe33f3a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### ğŸ›£ï¸ Memory Management Patterns Learned\n",
    "\n",
    "| Pattern | Traditional P2P | VMM Multi-GPU | Benefit |\n",
    "|---------|----------------|---------------|---------|\n",
    "| **Selective Sharing** | All-or-nothing | Per-buffer permissions | Security & control |\n",
    "| **Dynamic Access** | Static peer enable | Runtime access changes | Flexibility |\n",
    "| **Large Clusters** | Complex setup | Fabric handles | Scalability |\n",
    "\n",
    "### Core Multi-GPU VMM Concepts\n",
    "\n",
    "1. **Fine-Grained Access** - Control exactly which GPUs access which memory regions\n",
    "2. **`cuMemSetAccess`** - Grant read/write permissions per device per allocation\n",
    "3. **VMM vs P2P** - More control than `cudaEnablePeerAccess`, works per-buffer\n",
    "4. **Fabric Handles** - Scale to large NVLink/NVSwitch clusters efficiently\n",
    "\n",
    "### ğŸ›£ï¸ The Reserved Lanes Model\n",
    "```\n",
    "Physical Alloc (GPU0) â†’ Map to VA â†’ Grant Access (GPU1) â†’ Cross-GPU Kernel\n",
    "      ğŸ—ï¸ Build           ğŸ”— Connect    ğŸ« Lane Pass         ğŸš— Traffic Flows\n",
    "```\n",
    "\n",
    "**Next:** Custom allocators for application-specific memory optimization!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
