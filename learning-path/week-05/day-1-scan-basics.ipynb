{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Remember: CUDA C++ code is the PRIMARY learning material!\")\n",
    "print(f\"CUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    print(f\"Device: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d7e40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What is Scan (Prefix Sum)?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Scan** computes running totals across an array:\n",
    "\n",
    "```\n",
    "Input:     [3, 1, 7, 0, 4, 1, 6, 3]\n",
    "\n",
    "Inclusive: [3, 4, 11, 11, 15, 16, 22, 25]  ‚Üê Includes current element\n",
    "Exclusive: [0, 3, 4, 11, 11, 15, 16, 22]   ‚Üê Excludes current element\n",
    "```\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "```\n",
    "Inclusive scan:  out[i] = in[0] ‚äï in[1] ‚äï ... ‚äï in[i]\n",
    "Exclusive scan:  out[i] = in[0] ‚äï in[1] ‚äï ... ‚äï in[i-1]  (out[0] = identity)\n",
    "\n",
    "where ‚äï is any associative operator (add, multiply, max, min, etc.)\n",
    "```\n",
    "\n",
    "### Why Scan Matters\n",
    "\n",
    "```\n",
    "Scan is a FUNDAMENTAL parallel primitive:\n",
    "‚Ä¢ Stream compaction (filtering arrays)\n",
    "‚Ä¢ Radix sort (key distribution)\n",
    "‚Ä¢ Polynomial evaluation\n",
    "‚Ä¢ Histogram computation\n",
    "‚Ä¢ Sparse matrix operations\n",
    "‚Ä¢ Tree traversal\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)\n",
    "\n",
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scan_basics.cu\n",
    "// scan_basics.cu - Sequential scan for reference\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CPU inclusive scan - O(n)\n",
    "void cpu_inclusive_scan(const int* input, int* output, int n) {\n",
    "    output[0] = input[0];\n",
    "    for (int i = 1; i < n; i++) {\n",
    "        output[i] = output[i-1] + input[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// CPU exclusive scan - O(n)\n",
    "void cpu_exclusive_scan(const int* input, int* output, int n) {\n",
    "    output[0] = 0;  // Identity element for addition\n",
    "    for (int i = 1; i < n; i++) {\n",
    "        output[i] = output[i-1] + input[i-1];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Verify scan result\n",
    "bool verify_scan(const int* result, const int* expected, int n) {\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (result[i] != expected[i]) {\n",
    "            printf(\"Mismatch at %d: got %d, expected %d\\n\", \n",
    "                   i, result[i], expected[i]);\n",
    "            return false;\n",
    "        }\n",
    "    }\n",
    "    return true;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int input[] = {3, 1, 7, 0, 4, 1, 6, 3};\n",
    "    int n = 8;\n",
    "    int inclusive[8], exclusive[8];\n",
    "    \n",
    "    cpu_inclusive_scan(input, inclusive, n);\n",
    "    cpu_exclusive_scan(input, exclusive, n);\n",
    "    \n",
    "    printf(\"Input:     \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%d \", input[i]);\n",
    "    \n",
    "    printf(\"\\nInclusive: \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%d \", inclusive[i]);\n",
    "    \n",
    "    printf(\"\\nExclusive: \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%d \", exclusive[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa350fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o scan_basics scan_basics.cu\n",
    "!./scan_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential scan implementations (CPU reference)\n",
    "def cpu_inclusive_scan(arr):\n",
    "    \"\"\"Inclusive prefix sum: out[i] = sum(arr[0:i+1])\"\"\"\n",
    "    result = np.zeros_like(arr)\n",
    "    result[0] = arr[0]\n",
    "    for i in range(1, len(arr)):\n",
    "        result[i] = result[i-1] + arr[i]\n",
    "    return result\n",
    "\n",
    "def cpu_exclusive_scan(arr):\n",
    "    \"\"\"Exclusive prefix sum: out[i] = sum(arr[0:i])\"\"\"\n",
    "    result = np.zeros_like(arr)\n",
    "    result[0] = 0  # Identity element\n",
    "    for i in range(1, len(arr)):\n",
    "        result[i] = result[i-1] + arr[i-1]\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "test_input = np.array([3, 1, 7, 0, 4, 1, 6, 3], dtype=np.int32)\n",
    "\n",
    "print(f\"Input:     {test_input}\")\n",
    "print(f\"Inclusive: {cpu_inclusive_scan(test_input)}\")\n",
    "print(f\"Exclusive: {cpu_exclusive_scan(test_input)}\")\n",
    "print(f\"\\nNote: Exclusive[i] + Input[i] = Inclusive[i]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e39d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Parallel Scan Challenge\n",
    "\n",
    "### Why Sequential Doesn't Parallelize\n",
    "\n",
    "```\n",
    "Sequential scan has DATA DEPENDENCIES:\n",
    "\n",
    "out[0] = in[0]\n",
    "out[1] = out[0] + in[1]    ‚Üê Depends on out[0]\n",
    "out[2] = out[1] + in[2]    ‚Üê Depends on out[1]\n",
    "out[3] = out[2] + in[3]    ‚Üê Depends on out[2]\n",
    "...\n",
    "\n",
    "Each step depends on the previous! Can't run in parallel.\n",
    "```\n",
    "\n",
    "### Key Insight: Associativity\n",
    "\n",
    "```\n",
    "Addition is ASSOCIATIVE:\n",
    "(a + b) + c = a + (b + c)\n",
    "\n",
    "This means we can reorganize computation!\n",
    "\n",
    "out[3] = in[0] + in[1] + in[2] + in[3]\n",
    "       = (in[0] + in[1]) + (in[2] + in[3])  ‚Üê Parallel!\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)\n",
    "\n",
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naive_parallel_scan.cu\n",
    "// naive_parallel_scan.cu - Simple but inefficient\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 256\n",
    "\n",
    "// Naive parallel scan - O(n log n) work\n",
    "__global__ void naive_inclusive_scan(int* data, int n) {\n",
    "    __shared__ int temp[BLOCK_SIZE];\n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load into shared memory\n",
    "    temp[tid] = (gid < n) ? data[gid] : 0;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Perform scan with increasing stride\n",
    "    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
    "        int val = 0;\n",
    "        if (tid >= stride) {\n",
    "            val = temp[tid - stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "        \n",
    "        temp[tid] += val;\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write back\n",
    "    if (gid < n) {\n",
    "        data[gid] = temp[tid];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int h_data[] = {3, 1, 7, 0, 4, 1, 6, 3};\n",
    "    int n = 8;\n",
    "    \n",
    "    int* d_data;\n",
    "    cudaMalloc(&d_data, n * sizeof(int));\n",
    "    cudaMemcpy(d_data, h_data, n * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    naive_inclusive_scan<<<1, 8>>>(d_data, n);\n",
    "    \n",
    "    cudaMemcpy(h_data, d_data, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Scan result: \");\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        printf(\"%d \", h_data[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o naive_parallel_scan naive_parallel_scan.cu\n",
    "!./naive_parallel_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a780f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def naive_inclusive_scan(data, n):\n",
    "    \"\"\"Naive parallel inclusive scan - O(n log n) work.\"\"\"\n",
    "    shared = cuda.shared.array(256, dtype=np.int32)\n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.blockIdx.x * cuda.blockDim.x + tid\n",
    "    \n",
    "    # Load into shared memory\n",
    "    if gid < n:\n",
    "        shared[tid] = data[gid]\n",
    "    else:\n",
    "        shared[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Scan with increasing stride\n",
    "    stride = 1\n",
    "    while stride < cuda.blockDim.x:\n",
    "        val = 0\n",
    "        if tid >= stride:\n",
    "            val = shared[tid - stride]\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        shared[tid] += val\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        stride *= 2\n",
    "    \n",
    "    # Write back\n",
    "    if gid < n:\n",
    "        data[gid] = shared[tid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789255b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test naive parallel scan\n",
    "test_data = np.array([3, 1, 7, 0, 4, 1, 6, 3], dtype=np.int32)\n",
    "expected = cpu_inclusive_scan(test_data)\n",
    "\n",
    "d_data = cuda.to_device(test_data.copy())\n",
    "naive_inclusive_scan[1, 8](d_data, len(test_data))\n",
    "result = d_data.copy_to_host()\n",
    "\n",
    "print(f\"Input:    {test_data}\")\n",
    "print(f\"Result:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {'‚úì' if np.array_equal(result, expected) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee4f38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visualizing Parallel Scan\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Input: [3, 1, 7, 0, 4, 1, 6, 3]\n",
    "\n",
    "Stride=1: Each element adds its left neighbor\n",
    "  [3, 3+1, 1+7, 7+0, 0+4, 4+1, 1+6, 6+3]\n",
    "= [3,  4,   8,   7,   4,   5,   7,   9]\n",
    "\n",
    "Stride=2: Each element adds element 2 positions left\n",
    "  [3, 4, 3+8, 4+7, 8+4, 7+5, 4+7, 5+9]\n",
    "= [3, 4, 11,  11,  12,  12,  11,  14]\n",
    "\n",
    "Stride=4: Each element adds element 4 positions left\n",
    "  [3, 4, 11, 11, 3+12, 4+12, 11+11, 11+14]\n",
    "= [3, 4, 11, 11,  15,   16,   22,    25]\n",
    "\n",
    "Done! (stride >= n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381acd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_naive_scan(arr):\n",
    "    \"\"\"Visualize each step of naive parallel scan.\"\"\"\n",
    "    data = arr.copy()\n",
    "    n = len(data)\n",
    "    \n",
    "    print(f\"Input:      {data}\")\n",
    "    print()\n",
    "    \n",
    "    stride = 1\n",
    "    step = 0\n",
    "    while stride < n:\n",
    "        new_data = data.copy()\n",
    "        for i in range(stride, n):\n",
    "            new_data[i] = data[i] + data[i - stride]\n",
    "        \n",
    "        print(f\"Stride={stride}: {new_data}\")\n",
    "        data = new_data\n",
    "        stride *= 2\n",
    "        step += 1\n",
    "    \n",
    "    print(f\"\\nTotal steps: {step} = log2({n})\")\n",
    "    return data\n",
    "\n",
    "test = np.array([3, 1, 7, 0, 4, 1, 6, 3], dtype=np.int32)\n",
    "result = visualize_naive_scan(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb529b4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Work Efficiency Analysis\n",
    "\n",
    "### Problem with Naive Approach\n",
    "\n",
    "```\n",
    "Sequential scan: O(n) work, O(n) steps\n",
    "Naive parallel:  O(n log n) work, O(log n) steps\n",
    "\n",
    "For n = 1,000,000:\n",
    "  Sequential: 1,000,000 operations\n",
    "  Naive:      20,000,000 operations (20x more!)\n",
    "\n",
    "The naive approach does TOO MUCH WORK!\n",
    "```\n",
    "\n",
    "### Why More Work is Bad\n",
    "\n",
    "```\n",
    "GPUs have finite compute resources.\n",
    "\n",
    "If we do 20x more operations:\n",
    "‚Ä¢ Need 20x more threads to maintain speed\n",
    "‚Ä¢ Or run 20x slower\n",
    "\n",
    "We want WORK-EFFICIENT algorithms:\n",
    "‚Ä¢ O(n) work total\n",
    "‚Ä¢ O(log n) parallel steps\n",
    "```\n",
    "\n",
    "### Upcoming: Better Algorithms\n",
    "\n",
    "| Algorithm | Work | Steps | Work-Efficient? |\n",
    "|-----------|------|-------|------------------|\n",
    "| Sequential | O(n) | O(n) | Yes |\n",
    "| Hillis-Steele | O(n log n) | O(log n) | No |\n",
    "| Blelloch | O(n) | O(2 log n) | Yes! |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare work done\n",
    "def analyze_work(n):\n",
    "    sequential_work = n\n",
    "    naive_parallel_work = 0\n",
    "    \n",
    "    stride = 1\n",
    "    while stride < n:\n",
    "        # Each step, (n - stride) threads do work\n",
    "        naive_parallel_work += (n - stride)\n",
    "        stride *= 2\n",
    "    \n",
    "    blelloch_work = 2 * n  # Approximately\n",
    "    \n",
    "    return sequential_work, naive_parallel_work, blelloch_work\n",
    "\n",
    "print(f\"{'N':<12} {'Sequential':<12} {'Naive':<12} {'Blelloch':<12} {'Naive/Seq':<12}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n in [8, 256, 1024, 65536, 1048576]:\n",
    "    seq, naive, blelloch = analyze_work(n)\n",
    "    ratio = naive / seq\n",
    "    print(f\"{n:<12} {seq:<12} {naive:<12} {blelloch:<12} {ratio:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56e345",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scan_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "// Error checking macro\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Exclusive Scan (Naive Parallel)\n",
    "// ============================================================\n",
    "// Exclusive scan: output[i] = sum(input[0..i-1])\n",
    "// output[0] = 0 (identity element)\n",
    "\n",
    "__global__ void naiveExclusiveScan(int* data, int n) {\n",
    "    extern __shared__ int temp[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    if (tid >= n) return;\n",
    "    \n",
    "    // Load data to shared memory\n",
    "    temp[tid] = data[tid];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // First do inclusive scan\n",
    "    for (int stride = 1; stride < n; stride *= 2) {\n",
    "        int val = 0;\n",
    "        if (tid >= stride) {\n",
    "            val = temp[tid - stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "        temp[tid] += val;\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Convert to exclusive by shifting right\n",
    "    if (tid == 0) {\n",
    "        data[tid] = 0;\n",
    "    } else {\n",
    "        data[tid] = temp[tid - 1];\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Max Scan (Running Maximum)\n",
    "// ============================================================\n",
    "// Output[i] = max(input[0..i])\n",
    "\n",
    "__global__ void maxScan(int* data, int n) {\n",
    "    extern __shared__ int temp[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    if (tid >= n) return;\n",
    "    \n",
    "    temp[tid] = data[tid];\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int stride = 1; stride < n; stride *= 2) {\n",
    "        int val = temp[tid];\n",
    "        if (tid >= stride) {\n",
    "            val = max(val, temp[tid - stride]);\n",
    "        }\n",
    "        __syncthreads();\n",
    "        temp[tid] = val;\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    data[tid] = temp[tid];\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Min-Max Scan (Track Both)\n",
    "// ============================================================\n",
    "// Track running min and max simultaneously\n",
    "\n",
    "__global__ void minMaxScan(const int* input, int* outMin, int* outMax, int n) {\n",
    "    extern __shared__ int shared[];\n",
    "    int* sMin = shared;\n",
    "    int* sMax = shared + blockDim.x;\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    if (tid >= n) return;\n",
    "    \n",
    "    sMin[tid] = input[tid];\n",
    "    sMax[tid] = input[tid];\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int stride = 1; stride < n; stride *= 2) {\n",
    "        int minVal = sMin[tid];\n",
    "        int maxVal = sMax[tid];\n",
    "        if (tid >= stride) {\n",
    "            minVal = min(minVal, sMin[tid - stride]);\n",
    "            maxVal = max(maxVal, sMax[tid - stride]);\n",
    "        }\n",
    "        __syncthreads();\n",
    "        sMin[tid] = minVal;\n",
    "        sMax[tid] = maxVal;\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    outMin[tid] = sMin[tid];\n",
    "    outMax[tid] = sMax[tid];\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Test Functions\n",
    "// ============================================================\n",
    "void testExclusiveScan() {\n",
    "    printf(\"=== Exercise 1: Exclusive Scan ===\\n\");\n",
    "    \n",
    "    int h_data[] = {3, 1, 7, 0, 4, 1, 6, 3};\n",
    "    int expected[] = {0, 3, 4, 11, 11, 15, 16, 22};\n",
    "    int n = 8;\n",
    "    \n",
    "    int* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, n * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    naiveExclusiveScan<<<1, n, n * sizeof(int)>>>(d_data, n);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    int result[8];\n",
    "    CHECK_CUDA(cudaMemcpy(result, d_data, n * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Input:    \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", h_data[i]);\n",
    "    printf(\"\\nExclusive:\");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", result[i]);\n",
    "    printf(\"\\nExpected: \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", expected[i]);\n",
    "    \n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (result[i] != expected[i]) correct = false;\n",
    "    }\n",
    "    printf(\"\\nTest %s\\n\\n\", correct ? \"PASSED ‚úì\" : \"FAILED ‚úó\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "void testMaxScan() {\n",
    "    printf(\"=== Exercise 2: Max Scan ===\\n\");\n",
    "    \n",
    "    int h_data[] = {3, 1, 7, 0, 4, 1, 6, 3};\n",
    "    int expected[] = {3, 3, 7, 7, 7, 7, 7, 7};\n",
    "    int n = 8;\n",
    "    \n",
    "    int* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, n * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    maxScan<<<1, n, n * sizeof(int)>>>(d_data, n);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    int result[8];\n",
    "    CHECK_CUDA(cudaMemcpy(result, d_data, n * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Input:   \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", h_data[i]);\n",
    "    printf(\"\\nMax Scan:\");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", result[i]);\n",
    "    printf(\"\\nExpected:\");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", expected[i]);\n",
    "    \n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (result[i] != expected[i]) correct = false;\n",
    "    }\n",
    "    printf(\"\\nTest %s\\n\\n\", correct ? \"PASSED ‚úì\" : \"FAILED ‚úó\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "void testMinMaxScan() {\n",
    "    printf(\"=== Exercise 3: Min-Max Scan ===\\n\");\n",
    "    \n",
    "    int h_data[] = {5, 2, 8, 1, 9, 3, 7, 4};\n",
    "    int expectedMin[] = {5, 2, 2, 1, 1, 1, 1, 1};\n",
    "    int expectedMax[] = {5, 5, 8, 8, 9, 9, 9, 9};\n",
    "    int n = 8;\n",
    "    \n",
    "    int *d_data, *d_min, *d_max;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_min, n * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_max, n * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, n * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    minMaxScan<<<1, n, 2 * n * sizeof(int)>>>(d_data, d_min, d_max, n);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    int resultMin[8], resultMax[8];\n",
    "    CHECK_CUDA(cudaMemcpy(resultMin, d_min, n * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    CHECK_CUDA(cudaMemcpy(resultMax, d_max, n * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Input:    \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", h_data[i]);\n",
    "    printf(\"\\nMin Scan: \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", resultMin[i]);\n",
    "    printf(\"\\nMax Scan: \");\n",
    "    for (int i = 0; i < n; i++) printf(\"%2d \", resultMax[i]);\n",
    "    \n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (resultMin[i] != expectedMin[i] || resultMax[i] != expectedMax[i]) {\n",
    "            correct = false;\n",
    "        }\n",
    "    }\n",
    "    printf(\"\\nTest %s\\n\\n\", correct ? \"PASSED ‚úì\" : \"FAILED ‚úó\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_min);\n",
    "    cudaFree(d_max);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\");\n",
    "    printf(\"‚ïë              CUDA Scan Basics Exercises                      ‚ïë\\n\");\n",
    "    printf(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Compute Capability: %d.%d\\n\\n\", prop.major, prop.minor);\n",
    "    \n",
    "    testExclusiveScan();\n",
    "    testMaxScan();\n",
    "    testMinMaxScan();\n",
    "    \n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o scan_exercises scan_exercises.cu && ./scan_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4c0e1",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Exclusive Scan Kernel\n",
    "\n",
    "Modify the naive scan to produce exclusive scan output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f61b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement exclusive scan\n",
    "# Hint: Run inclusive scan, then shift result right and prepend 0\n",
    "\n",
    "@cuda.jit\n",
    "def naive_exclusive_scan(data, n):\n",
    "    \"\"\"Naive parallel exclusive scan.\"\"\"\n",
    "    pass  # Your implementation\n",
    "\n",
    "# Test\n",
    "# Expected: [0, 3, 4, 11, 11, 15, 16, 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37024ec3",
   "metadata": {},
   "source": [
    "### Exercise 2: Max Scan\n",
    "\n",
    "Implement a scan using `max` instead of `+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdaebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement max scan\n",
    "# Input:  [3, 1, 7, 0, 4, 1, 6, 3]\n",
    "# Output: [3, 3, 7, 7, 7, 7, 7, 7]\n",
    "\n",
    "@cuda.jit\n",
    "def max_scan(data, n):\n",
    "    \"\"\"Running maximum scan.\"\"\"\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e41b6c",
   "metadata": {},
   "source": [
    "### Exercise 3: Count Work Operations\n",
    "\n",
    "Modify the visualization to count exact operations at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Count additions at each step\n",
    "def count_scan_work(n):\n",
    "    \"\"\"Count total additions in naive parallel scan.\"\"\"\n",
    "    pass  # Your implementation\n",
    "\n",
    "# Verify: For n=8, should be 3+4+5+6+7 = 25 (approx n*log2(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd0a87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| Inclusive Scan | out[i] = sum(in[0..i]) |\n",
    "| Exclusive Scan | out[i] = sum(in[0..i-1]) |\n",
    "| Associativity | (a+b)+c = a+(b+c) enables parallelism |\n",
    "| Work | Total operations performed |\n",
    "| Steps | Parallel depth (synchronization points) |\n",
    "\n",
    "### CUDA C++ Key Patterns\n",
    "\n",
    "```cpp\n",
    "// Shared memory for block-level scan\n",
    "__shared__ int temp[BLOCK_SIZE];\n",
    "\n",
    "// Load data\n",
    "temp[tid] = data[gid];\n",
    "__syncthreads();\n",
    "\n",
    "// Scan loop with doubling stride\n",
    "for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
    "    int val = (tid >= stride) ? temp[tid - stride] : 0;\n",
    "    __syncthreads();\n",
    "    temp[tid] += val;\n",
    "    __syncthreads();\n",
    "}\n",
    "```\n",
    "\n",
    "### Next: Hillis-Steele Algorithm\n",
    "Tomorrow we'll study the Hillis-Steele algorithm in depth - simple but not work-efficient."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
