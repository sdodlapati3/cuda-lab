{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81856651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c3a5d",
   "metadata": {},
   "source": [
    "# Day 4: CUDA Events - Stopwatches & Checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Hook: The Relay Race Baton Pass\n",
    "\n",
    "**Think about** a relay race. Each runner needs to:\n",
    "1. Know when the previous runner arrives (synchronization)\n",
    "2. Measure their own lap time (timing)\n",
    "3. Signal the next runner to start (coordination)\n",
    "\n",
    "```\n",
    "Without Events (everyone runs blindly):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Runner 1: [Run]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>    â”‚\n",
    "â”‚  Runner 2:         [Run]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> (started too early!)â”‚\n",
    "â”‚  Runner 3:                    [Run]â”€â”€â”€> (collision!)          â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Result: CHAOS! ğŸ”¥                                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "With Events (coordinated handoffs):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Runner 1: [Run]â”€â”€ğŸ event1                                   â”‚\n",
    "â”‚  Runner 2:         wait(event1)â”€â”€[Run]â”€â”€ğŸ event2             â”‚\n",
    "â”‚  Runner 3:                        wait(event2)â”€â”€[Run]â”€â”€ğŸ     â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Result: Perfect relay! Each runner timed precisely! â±ï¸       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**CUDA Events are your batons** - they synchronize work AND measure time with GPU precision!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Create and record** events to mark points in streams\n",
    "2. **Measure elapsed time** between events with microsecond accuracy\n",
    "3. **Synchronize streams** using `cudaStreamWaitEvent()`\n",
    "4. **Query event status** without blocking the CPU\n",
    "5. **Design DAG workflows** where some streams depend on others\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸƒ Concept Card: Stopwatches & Checkpoints\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  â±ï¸  CUDA EVENTS = STOPWATCHES + CHECKPOINTS                     â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  DUAL PURPOSE:                                                   â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  1ï¸âƒ£  STOPWATCH: Measure GPU kernel time                          â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  ğŸ start                        ğŸ stop                   â”‚  â•‘\n",
    "â•‘  â”‚    â†“                               â†“                       â”‚  â•‘\n",
    "â•‘  â”‚  [Record] â†’ [Kernel Runs] â†’ [Record] â†’ [Elapsed Time!]     â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  cudaEventElapsedTime(&ms, start, stop)  â†’ \"42.5 ms\"       â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  2ï¸âƒ£  CHECKPOINT: Coordinate between streams                      â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  Stream A: [Produce Data]â”€ğŸ event â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚  â•‘\n",
    "â•‘  â”‚                            â”‚                               â”‚  â•‘\n",
    "â•‘  â”‚  Stream B: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€wait(event)â”€[Consume Data]      â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  \"Stream B waits for Stream A to reach the checkpoint\"     â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ’¡ KEY INSIGHT: Events are recorded in streams but can be       â•‘\n",
    "â•‘     waited on by ANY stream - enabling complex dependencies!     â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  TIMING ACCURACY:                                                â•‘\n",
    "â•‘  â€¢ CPU timers:   ~1ms resolution, affected by OS scheduling      â•‘\n",
    "â•‘  â€¢ CUDA events:  ~0.5Î¼s resolution, GPU clock precision!         â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: CUDA Events Basics\n",
    "\n",
    "### What are CUDA Events?\n",
    "\n",
    "```\n",
    "EVENT = A marker in a stream that can be:\n",
    "        â€¢ Recorded (placed at a point in stream)\n",
    "        â€¢ Queried (check if reached)\n",
    "        â€¢ Waited on (block until reached)\n",
    "        â€¢ Used for timing\n",
    "\n",
    "Stream Timeline:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "[H2D Copy] â†’ [event1] â†’ [Kernel] â†’ [event2] â†’ [D2H Copy]\n",
    "              â†‘                      â†‘\n",
    "        record start           record stop\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 elapsed time!\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates event creation, recording, synchronization, and timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile event_basics.cu\n",
    "// event_basics.cu - Event creation, recording, and synchronization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void computeKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            data[tid] = sqrtf(data[tid] + 1.0f);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 22;\n",
    "    \n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // ============================================\n",
    "    // Event Creation\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    \n",
    "    // Default events (for timing and sync)\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Event with flags:\n",
    "    // cudaEventDefault      - Default behavior\n",
    "    // cudaEventBlockingSync - CPU blocks instead of spinning\n",
    "    // cudaEventDisableTiming- Faster, but can't time\n",
    "    // cudaEventInterprocess - For IPC\n",
    "    \n",
    "    cudaEvent_t fastEvent;\n",
    "    cudaEventCreateWithFlags(&fastEvent, cudaEventDisableTiming);\n",
    "    \n",
    "    // ============================================\n",
    "    // Event Recording (place marker in stream)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);  // Default stream\n",
    "    // OR: cudaEventRecord(start, stream);  // Specific stream\n",
    "    \n",
    "    computeKernel<<<256, 256>>>(d_data, N);\n",
    "    computeKernel<<<256, 256>>>(d_data, N);\n",
    "    computeKernel<<<256, 256>>>(d_data, N);\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Event Synchronization\n",
    "    // ============================================\n",
    "    \n",
    "    // Method 1: Block until event completes\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    // Method 2: Query (non-blocking)\n",
    "    cudaError_t status = cudaEventQuery(stop);\n",
    "    if (status == cudaSuccess) {\n",
    "        printf(\"Event completed!\\n\");\n",
    "    } else if (status == cudaErrorNotReady) {\n",
    "        printf(\"Event not yet reached\\n\");\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Event Timing\n",
    "    // ============================================\n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "    printf(\"Kernel time: %.3f ms\\n\", milliseconds);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaEventDestroy(fastEvent);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o event_basics event_basics.cu\n",
    "!./event_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976151dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Accurate GPU Timing\n",
    "\n",
    "### Why Events for Timing?\n",
    "\n",
    "```\n",
    "CPU Timing (WRONG for GPU):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "auto start = clock();\n",
    "kernel<<<>>>(...);        // Returns immediately!\n",
    "auto end = clock();       // Measures launch overhead only\n",
    "// WRONG: Measures ~0.001ms instead of actual kernel time\n",
    "\n",
    "GPU Event Timing (CORRECT):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "cudaEventRecord(start);\n",
    "kernel<<<>>>(...);        // Runs asynchronously\n",
    "cudaEventRecord(stop);\n",
    "cudaEventSynchronize(stop);\n",
    "cudaEventElapsedTime(&ms, start, stop);\n",
    "// CORRECT: Measures actual GPU execution time\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates proper GPU timing techniques including warmup, single kernel timing, averaging over multiple iterations, and timing individual sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1179504",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accurate_timing.cu\n",
    "// accurate_timing.cu - Proper GPU timing\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data[tid] = sinf(data[tid]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Warmup (IMPORTANT!)\n",
    "    // ============================================\n",
    "    // First kernel launch has overhead (JIT, caching, etc.)\n",
    "    kernel<<<256, 256>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // ============================================\n",
    "    // Time Single Kernel\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    kernel<<<256, 256>>>(d_data, N);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float singleTime;\n",
    "    cudaEventElapsedTime(&singleTime, start, stop);\n",
    "    printf(\"Single kernel: %.3f ms\\n\", singleTime);\n",
    "    \n",
    "    // ============================================\n",
    "    // Time Multiple Iterations (more accurate)\n",
    "    // ============================================\n",
    "    const int ITERATIONS = 100;\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        kernel<<<256, 256>>>(d_data, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float totalTime;\n",
    "    cudaEventElapsedTime(&totalTime, start, stop);\n",
    "    printf(\"Average over %d runs: %.3f ms\\n\", \n",
    "           ITERATIONS, totalTime / ITERATIONS);\n",
    "    \n",
    "    // ============================================\n",
    "    // Time Individual Sections\n",
    "    // ============================================\n",
    "    cudaEvent_t e1, e2, e3;\n",
    "    cudaEventCreate(&e1);\n",
    "    cudaEventCreate(&e2);\n",
    "    cudaEventCreate(&e3);\n",
    "    \n",
    "    float *h_data;\n",
    "    cudaMallocHost(&h_data, N * sizeof(float));\n",
    "    \n",
    "    cudaEventRecord(e1);\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(e2);\n",
    "    kernel<<<256, 256>>>(d_data, N);\n",
    "    cudaEventRecord(e3);\n",
    "    cudaEventSynchronize(e3);\n",
    "    \n",
    "    float h2dTime, kernelTime;\n",
    "    cudaEventElapsedTime(&h2dTime, e1, e2);\n",
    "    cudaEventElapsedTime(&kernelTime, e2, e3);\n",
    "    \n",
    "    printf(\"H2D transfer: %.3f ms\\n\", h2dTime);\n",
    "    printf(\"Kernel exec:  %.3f ms\\n\", kernelTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaEventDestroy(e1);\n",
    "    cudaEventDestroy(e2);\n",
    "    cudaEventDestroy(e3);\n",
    "    cudaFree(d_data);\n",
    "    cudaFreeHost(h_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27a53b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Inter-Stream Synchronization\n",
    "\n",
    "### cudaStreamWaitEvent\n",
    "\n",
    "```\n",
    "Without Event Sync:            With Event Sync:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”           â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Stream A: [Produce Data]       Stream A: [Produce]â”€â”€â”\n",
    "Stream B: [Use Data] â† RACE!   Stream B:            â””â”€â†’[Wait][Use]\n",
    "                                                    â†‘\n",
    "                                              cudaStreamWaitEvent\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "The following example demonstrates:\n",
    "- Producer-consumer pattern with event synchronization\n",
    "- Complex dependency graphs (DAG) with multiple streams\n",
    "- Proper cleanup of events and streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73708e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inter_stream_sync.cu\n",
    "// inter_stream_sync.cu - Synchronizing between streams\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void produce(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = (float)tid * 2.0f;  // Produce data\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void consume(const float* input, float* output, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        output[tid] = input[tid] + 1.0f;  // Use produced data\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    \n",
    "    float *d_produced, *d_consumed;\n",
    "    cudaMalloc(&d_produced, N * sizeof(float));\n",
    "    cudaMalloc(&d_consumed, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t streamA, streamB;\n",
    "    cudaStreamCreate(&streamA);\n",
    "    cudaStreamCreate(&streamB);\n",
    "    \n",
    "    // Event for synchronization\n",
    "    cudaEvent_t dataReady;\n",
    "    cudaEventCreate(&dataReady);\n",
    "    \n",
    "    // ============================================\n",
    "    // Stream A: Produce data\n",
    "    // ============================================\n",
    "    produce<<<256, 256, 0, streamA>>>(d_produced, N);\n",
    "    \n",
    "    // Record event when production is done\n",
    "    cudaEventRecord(dataReady, streamA);\n",
    "    \n",
    "    // ============================================\n",
    "    // Stream B: Wait for data, then consume\n",
    "    // ============================================\n",
    "    // This makes streamB wait until streamA reaches dataReady\n",
    "    cudaStreamWaitEvent(streamB, dataReady);\n",
    "    \n",
    "    consume<<<256, 256, 0, streamB>>>(d_produced, d_consumed, N);\n",
    "    \n",
    "    // Both streams can do other independent work before/after\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Producer-consumer with event sync complete!\\n\");\n",
    "    \n",
    "    // ============================================\n",
    "    // Complex Dependency Graph\n",
    "    // ============================================\n",
    "    //       A1\n",
    "    //      /  \\\n",
    "    //     B1  B2\n",
    "    //      \\  /\n",
    "    //       C1\n",
    "    \n",
    "    cudaStream_t sA, sB1, sB2, sC;\n",
    "    cudaStreamCreate(&sA);\n",
    "    cudaStreamCreate(&sB1);\n",
    "    cudaStreamCreate(&sB2);\n",
    "    cudaStreamCreate(&sC);\n",
    "    \n",
    "    cudaEvent_t afterA, afterB1, afterB2;\n",
    "    cudaEventCreate(&afterA);\n",
    "    cudaEventCreate(&afterB1);\n",
    "    cudaEventCreate(&afterB2);\n",
    "    \n",
    "    // A1: Initial work\n",
    "    produce<<<256, 256, 0, sA>>>(d_produced, N);\n",
    "    cudaEventRecord(afterA, sA);\n",
    "    \n",
    "    // B1 and B2: Parallel work after A\n",
    "    cudaStreamWaitEvent(sB1, afterA);\n",
    "    cudaStreamWaitEvent(sB2, afterA);\n",
    "    consume<<<256, 256, 0, sB1>>>(d_produced, d_consumed, N/2);\n",
    "    consume<<<256, 256, 0, sB2>>>(d_produced + N/2, d_consumed + N/2, N/2);\n",
    "    cudaEventRecord(afterB1, sB1);\n",
    "    cudaEventRecord(afterB2, sB2);\n",
    "    \n",
    "    // C1: After both B1 and B2 complete\n",
    "    cudaStreamWaitEvent(sC, afterB1);\n",
    "    cudaStreamWaitEvent(sC, afterB2);\n",
    "    produce<<<256, 256, 0, sC>>>(d_consumed, N);  // Final processing\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Complex dependency graph complete!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaEventDestroy(dataReady);\n",
    "    cudaEventDestroy(afterA);\n",
    "    cudaEventDestroy(afterB1);\n",
    "    cudaEventDestroy(afterB2);\n",
    "    cudaStreamDestroy(streamA);\n",
    "    cudaStreamDestroy(streamB);\n",
    "    cudaStreamDestroy(sA);\n",
    "    cudaStreamDestroy(sB1);\n",
    "    cudaStreamDestroy(sB2);\n",
    "    cudaStreamDestroy(sC);\n",
    "    cudaFree(d_produced);\n",
    "    cudaFree(d_consumed);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b22e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o inter_stream_sync inter_stream_sync.cu && ./inter_stream_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6acbe",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ff98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Event Timing Demo (OPTIONAL)\n",
    "\n",
    "@cuda.jit\n",
    "def heavy_kernel(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        val = data[tid]\n",
    "        for _ in range(1000):\n",
    "            val = val * 1.0001 + 0.0001\n",
    "        data[tid] = val\n",
    "\n",
    "n = 1 << 20\n",
    "\n",
    "# Create device array\n",
    "d_data = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "# Warmup\n",
    "heavy_kernel[(n+255)//256, 256](d_data)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Time with events\n",
    "stream = cuda.stream()\n",
    "start_event = stream.record()\n",
    "\n",
    "for _ in range(10):\n",
    "    heavy_kernel[(n+255)//256, 256, stream](d_data)\n",
    "\n",
    "end_event = stream.record()\n",
    "stream.synchronize()\n",
    "\n",
    "elapsed = cuda.event_elapsed_time(start_event, end_event)\n",
    "print(f\"10 kernels: {elapsed:.2f} ms\")\n",
    "print(f\"Per kernel: {elapsed/10:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c52178",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Event Flags & Advanced Usage\n",
    "\n",
    "### Event Flags\n",
    "\n",
    "```cpp\n",
    "// event_flags.cu - Different event types\n",
    "\n",
    "// cudaEventDefault - Standard event for timing and sync\n",
    "cudaEvent_t defaultEvent;\n",
    "cudaEventCreate(&defaultEvent);\n",
    "\n",
    "// cudaEventBlockingSync - CPU thread yields instead of spinning\n",
    "// Better for power efficiency, slightly higher latency\n",
    "cudaEvent_t blockingEvent;\n",
    "cudaEventCreateWithFlags(&blockingEvent, cudaEventBlockingSync);\n",
    "\n",
    "// cudaEventDisableTiming - Can't use for timing, but faster\n",
    "// Good for pure synchronization\n",
    "cudaEvent_t syncOnlyEvent;\n",
    "cudaEventCreateWithFlags(&syncOnlyEvent, cudaEventDisableTiming);\n",
    "\n",
    "// Combine flags\n",
    "cudaEvent_t fastSyncEvent;\n",
    "cudaEventCreateWithFlags(&fastSyncEvent, \n",
    "                          cudaEventBlockingSync | cudaEventDisableTiming);\n",
    "```\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    EVENT FLAGS                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Flag                 â”‚ Use Case                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Default              â”‚ Timing + sync, general use       â”‚\n",
    "â”‚ BlockingSync         â”‚ Long waits, power-sensitive      â”‚\n",
    "â”‚ DisableTiming        â”‚ Pure sync, maximum performance   â”‚\n",
    "â”‚ Both combined        â”‚ Sync-only, power-efficient       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61579d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047860fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile events_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <float.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void computeKernel(float* data, int n, int iters) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < iters; i++) {\n",
    "            val = sinf(val) * cosf(val) + 0.1f;\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Benchmark Kernel Utility\n",
    "// ============================================================\n",
    "\n",
    "struct BenchmarkResult {\n",
    "    float minMs;\n",
    "    float maxMs;\n",
    "    float avgMs;\n",
    "    float stddevMs;\n",
    "};\n",
    "\n",
    "BenchmarkResult benchmarkKernel(void (*launchFunc)(float*, int), \n",
    "                                  float* d_data, int n,\n",
    "                                  int warmupIters, int benchIters) {\n",
    "    // Warmup\n",
    "    for (int i = 0; i < warmupIters; i++) {\n",
    "        launchFunc(d_data, n);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    float* times = (float*)malloc(benchIters * sizeof(float));\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    for (int i = 0; i < benchIters; i++) {\n",
    "        cudaEventRecord(start);\n",
    "        launchFunc(d_data, n);\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        cudaEventElapsedTime(&times[i], start, stop);\n",
    "    }\n",
    "    \n",
    "    // Calculate statistics\n",
    "    BenchmarkResult result;\n",
    "    result.minMs = FLT_MAX;\n",
    "    result.maxMs = 0;\n",
    "    result.avgMs = 0;\n",
    "    \n",
    "    for (int i = 0; i < benchIters; i++) {\n",
    "        if (times[i] < result.minMs) result.minMs = times[i];\n",
    "        if (times[i] > result.maxMs) result.maxMs = times[i];\n",
    "        result.avgMs += times[i];\n",
    "    }\n",
    "    result.avgMs /= benchIters;\n",
    "    \n",
    "    // Standard deviation\n",
    "    float variance = 0;\n",
    "    for (int i = 0; i < benchIters; i++) {\n",
    "        float diff = times[i] - result.avgMs;\n",
    "        variance += diff * diff;\n",
    "    }\n",
    "    result.stddevMs = sqrtf(variance / benchIters);\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    free(times);\n",
    "    \n",
    "    return result;\n",
    "}\n",
    "\n",
    "void launchCompute(float* data, int n) {\n",
    "    computeKernel<<<(n + 255) / 256, 256>>>(data, n, 100);\n",
    "}\n",
    "\n",
    "void exercise1_benchmarkUtility() {\n",
    "    printf(\"=== Exercise 1: Benchmark Kernel Utility ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 20;\n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    BenchmarkResult result = benchmarkKernel(launchCompute, d_data, n, 5, 20);\n",
    "    \n",
    "    printf(\"Kernel Benchmark Results (20 iterations, 5 warmup):\\n\");\n",
    "    printf(\"  Min:     %.3f ms\\n\", result.minMs);\n",
    "    printf(\"  Max:     %.3f ms\\n\", result.maxMs);\n",
    "    printf(\"  Avg:     %.3f ms\\n\", result.avgMs);\n",
    "    printf(\"  Stddev:  %.3f ms\\n\", result.stddevMs);\n",
    "    printf(\"  Range:   %.3f ms (%.1f%% of avg)\\n\\n\", \n",
    "           result.maxMs - result.minMs,\n",
    "           (result.maxMs - result.minMs) / result.avgMs * 100);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: DAG Executor\n",
    "// ============================================================\n",
    "\n",
    "__global__ void kernelA(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] + 1.0f;\n",
    "}\n",
    "\n",
    "__global__ void kernelB(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] * 2.0f;\n",
    "}\n",
    "\n",
    "__global__ void kernelC(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = sinf(data[idx]);\n",
    "}\n",
    "\n",
    "__global__ void kernelD(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = sqrtf(fabsf(data[idx]));\n",
    "}\n",
    "\n",
    "__global__ void kernelE(const float* b, const float* c, const float* d, \n",
    "                         float* result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) result[idx] = b[idx] + c[idx] + d[idx];\n",
    "}\n",
    "\n",
    "void exercise2_dagExecutor() {\n",
    "    printf(\"=== Exercise 2: DAG Executor ===\\n\");\n",
    "    printf(\"DAG Structure:\\n\");\n",
    "    printf(\"     A\\n\");\n",
    "    printf(\"    /|\\\\\\n\");\n",
    "    printf(\"   B C D\\n\");\n",
    "    printf(\"    \\\\|/\\n\");\n",
    "    printf(\"     E\\n\\n\");\n",
    "    \n",
    "    const int n = 1 << 20;\n",
    "    int grid = (n + 255) / 256;\n",
    "    \n",
    "    float *d_data, *d_b, *d_c, *d_d, *d_result;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_b, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_c, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_d, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_result, n * sizeof(float)));\n",
    "    \n",
    "    // Create streams for parallel branches\n",
    "    cudaStream_t streamB, streamC, streamD, streamE;\n",
    "    cudaStreamCreate(&streamB);\n",
    "    cudaStreamCreate(&streamC);\n",
    "    cudaStreamCreate(&streamD);\n",
    "    cudaStreamCreate(&streamE);\n",
    "    \n",
    "    // Events for synchronization\n",
    "    cudaEvent_t eventA, eventB, eventC, eventD;\n",
    "    cudaEventCreate(&eventA);\n",
    "    cudaEventCreate(&eventB);\n",
    "    cudaEventCreate(&eventC);\n",
    "    cudaEventCreate(&eventD);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // A executes first (on default stream)\n",
    "    kernelA<<<grid, 256>>>(d_data, n);\n",
    "    cudaEventRecord(eventA);\n",
    "    \n",
    "    // B, C, D wait for A and run in parallel\n",
    "    cudaStreamWaitEvent(streamB, eventA);\n",
    "    cudaMemcpyAsync(d_b, d_data, n * sizeof(float), cudaMemcpyDeviceToDevice, streamB);\n",
    "    kernelB<<<grid, 256, 0, streamB>>>(d_b, n);\n",
    "    cudaEventRecord(eventB, streamB);\n",
    "    \n",
    "    cudaStreamWaitEvent(streamC, eventA);\n",
    "    cudaMemcpyAsync(d_c, d_data, n * sizeof(float), cudaMemcpyDeviceToDevice, streamC);\n",
    "    kernelC<<<grid, 256, 0, streamC>>>(d_c, n);\n",
    "    cudaEventRecord(eventC, streamC);\n",
    "    \n",
    "    cudaStreamWaitEvent(streamD, eventA);\n",
    "    cudaMemcpyAsync(d_d, d_data, n * sizeof(float), cudaMemcpyDeviceToDevice, streamD);\n",
    "    kernelD<<<grid, 256, 0, streamD>>>(d_d, n);\n",
    "    cudaEventRecord(eventD, streamD);\n",
    "    \n",
    "    // E waits for B, C, D\n",
    "    cudaStreamWaitEvent(streamE, eventB);\n",
    "    cudaStreamWaitEvent(streamE, eventC);\n",
    "    cudaStreamWaitEvent(streamE, eventD);\n",
    "    kernelE<<<grid, 256, 0, streamE>>>(d_b, d_c, d_d, d_result, n);\n",
    "    \n",
    "    cudaEventRecord(stop, streamE);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"DAG execution time: %.2f ms\\n\\n\", ms);\n",
    "    \n",
    "    cudaStreamDestroy(streamB);\n",
    "    cudaStreamDestroy(streamC);\n",
    "    cudaStreamDestroy(streamD);\n",
    "    cudaStreamDestroy(streamE);\n",
    "    cudaEventDestroy(eventA);\n",
    "    cudaEventDestroy(eventB);\n",
    "    cudaEventDestroy(eventC);\n",
    "    cudaEventDestroy(eventD);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    cudaFree(d_d);\n",
    "    cudaFree(d_result);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Event Pool\n",
    "// ============================================================\n",
    "\n",
    "class EventPool {\n",
    "private:\n",
    "    cudaEvent_t* events;\n",
    "    bool* inUse;\n",
    "    int poolSize;\n",
    "    int nextFree;\n",
    "    \n",
    "public:\n",
    "    EventPool(int size) : poolSize(size), nextFree(0) {\n",
    "        events = new cudaEvent_t[size];\n",
    "        inUse = new bool[size];\n",
    "        for (int i = 0; i < size; i++) {\n",
    "            cudaEventCreate(&events[i]);\n",
    "            inUse[i] = false;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ~EventPool() {\n",
    "        for (int i = 0; i < poolSize; i++) {\n",
    "            cudaEventDestroy(events[i]);\n",
    "        }\n",
    "        delete[] events;\n",
    "        delete[] inUse;\n",
    "    }\n",
    "    \n",
    "    cudaEvent_t acquire() {\n",
    "        // Find a free event\n",
    "        for (int i = 0; i < poolSize; i++) {\n",
    "            int idx = (nextFree + i) % poolSize;\n",
    "            if (!inUse[idx]) {\n",
    "                inUse[idx] = true;\n",
    "                nextFree = (idx + 1) % poolSize;\n",
    "                return events[idx];\n",
    "            }\n",
    "        }\n",
    "        // Pool exhausted - create a new event (shouldn't happen with proper sizing)\n",
    "        printf(\"Warning: Event pool exhausted!\\n\");\n",
    "        cudaEvent_t newEvent;\n",
    "        cudaEventCreate(&newEvent);\n",
    "        return newEvent;\n",
    "    }\n",
    "    \n",
    "    void release(cudaEvent_t event) {\n",
    "        for (int i = 0; i < poolSize; i++) {\n",
    "            if (events[i] == event) {\n",
    "                inUse[i] = false;\n",
    "                return;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "};\n",
    "\n",
    "void exercise3_eventPool() {\n",
    "    printf(\"=== Exercise 3: Event Pool ===\\n\");\n",
    "    \n",
    "    const int poolSize = 8;\n",
    "    const int numOperations = 100;\n",
    "    \n",
    "    EventPool pool(poolSize);\n",
    "    \n",
    "    const int n = 1 << 18;\n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    cudaEvent_t overallStart, overallStop;\n",
    "    cudaEventCreate(&overallStart);\n",
    "    cudaEventCreate(&overallStop);\n",
    "    \n",
    "    float totalTime = 0;\n",
    "    \n",
    "    cudaEventRecord(overallStart);\n",
    "    \n",
    "    for (int i = 0; i < numOperations; i++) {\n",
    "        cudaEvent_t start = pool.acquire();\n",
    "        cudaEvent_t stop = pool.acquire();\n",
    "        \n",
    "        cudaEventRecord(start);\n",
    "        computeKernel<<<(n + 255) / 256, 256>>>(d_data, n, 10);\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        totalTime += ms;\n",
    "        \n",
    "        pool.release(start);\n",
    "        pool.release(stop);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(overallStop);\n",
    "    cudaEventSynchronize(overallStop);\n",
    "    \n",
    "    float overallMs;\n",
    "    cudaEventElapsedTime(&overallMs, overallStart, overallStop);\n",
    "    \n",
    "    printf(\"Event Pool (size=%d):\\n\", poolSize);\n",
    "    printf(\"  Operations: %d\\n\", numOperations);\n",
    "    printf(\"  Kernel time (sum): %.2f ms\\n\", totalTime);\n",
    "    printf(\"  Overall time:      %.2f ms\\n\", overallMs);\n",
    "    printf(\"  Events reused: %d times per event\\n\\n\", numOperations * 2 / poolSize);\n",
    "    \n",
    "    cudaEventDestroy(overallStart);\n",
    "    cudaEventDestroy(overallStop);\n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘              CUDA Events Exercises                           â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\\n\", prop.name);\n",
    "    \n",
    "    exercise1_benchmarkUtility();\n",
    "    exercise2_dagExecutor();\n",
    "    exercise3_eventPool();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4330df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o events_exercises events_exercises.cu && ./events_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a891fcd",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Benchmark Kernel\n",
    "Create a timing utility that:\n",
    "- Warms up the kernel\n",
    "- Runs multiple iterations\n",
    "- Reports min/max/avg times\n",
    "\n",
    "### Exercise 2: DAG Executor\n",
    "```cpp\n",
    "// Implement a simple DAG (Directed Acyclic Graph) of kernels:\n",
    "//     A\n",
    "//    /|\\\n",
    "//   B C D\n",
    "//    \\|/\n",
    "//     E\n",
    "```\n",
    "\n",
    "### Exercise 3: Event Pool\n",
    "Implement an event pool that reuses events to avoid creation overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db6feb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                   CUDA EVENTS MASTERY                            â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  â±ï¸  Core Concept: STOPWATCHES + CHECKPOINTS                      â•‘\n",
    "â•‘     Events time kernels AND synchronize dependent streams        â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ“‹ Essential API:                                               â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  // Lifecycle                                              â”‚  â•‘\n",
    "â•‘  â”‚  cudaEventCreate(&event)                                   â”‚  â•‘\n",
    "â•‘  â”‚  cudaEventRecord(event, stream)     // Mark a point        â”‚  â•‘\n",
    "â•‘  â”‚  cudaEventDestroy(event)                                   â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  // Synchronization                                        â”‚  â•‘\n",
    "â•‘  â”‚  cudaEventSynchronize(event)        // CPU waits           â”‚  â•‘\n",
    "â•‘  â”‚  cudaStreamWaitEvent(stream, event) // Stream waits        â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  // Timing                                                 â”‚  â•‘\n",
    "â•‘  â”‚  cudaEventElapsedTime(&ms, start, stop)  // Microseconds!  â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ”€ Inter-Stream Dependencies (DAG Pattern):                     â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  Stream A: [Work]â”€ğŸâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚  â•‘\n",
    "â•‘  â”‚  Stream B: [Work]â”€ğŸâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚  â•‘\n",
    "â•‘  â”‚  Stream C:        wait(A)â”€wait(B)â”€[Depends on A & B]       â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  âš¡ Timing Best Practices:                                       â•‘\n",
    "â•‘     â€¢ Always warmup before timing                                â•‘\n",
    "â•‘     â€¢ Run multiple iterations                                    â•‘\n",
    "â•‘     â€¢ Use events NOT CPU timers for GPU work                     â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® What's Next?\n",
    "\n",
    "**Day 5: CUDA Graphs** - Compile your workflow!\n",
    "\n",
    "You've learned to orchestrate streams with events. But each kernel launch has ~5-10Î¼s CPU overhead. For workloads with MANY small kernels, this adds up!\n",
    "\n",
    "```\n",
    "Today (Events):                 Tomorrow (CUDA Graphs):\n",
    "Launch kernel 1: 5Î¼s overhead   Capture entire workflow once:\n",
    "Launch kernel 2: 5Î¼s overhead   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "Launch kernel 3: 5Î¼s overhead   â”‚  K1 â†’ K2 â†’ K3 â†’ K4 â†’ K5  â”‚\n",
    "Launch kernel 4: 5Î¼s overhead   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Launch kernel 5: 5Î¼s overhead   \n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   Launch graph: 10Î¼s TOTAL!\n",
    "Total overhead: 25Î¼s            Replay graph: 10Î¼s each time!\n",
    "```\n",
    "\n",
    "Turn your stream workflow into a \"recipe card\" that executes with minimal overhead! ğŸ“‹"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
