{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53236596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd35ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Pinned Memory\n",
    "\n",
    "### Why Pinned Memory?\n",
    "\n",
    "```\n",
    "Regular (Pageable) Memory:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "┌──────────┐     ┌──────────┐     ┌──────────┐\n",
    "│   Host   │ --> │  Pinned  │ --> │  Device  │\n",
    "│ (paged)  │     │ (staging)│     │ (GPU)    │\n",
    "└──────────┘     └──────────┘     └──────────┘\n",
    "                  Hidden copy!\n",
    "\n",
    "Pinned (Page-Locked) Memory:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "┌──────────┐ ────────────────> ┌──────────┐\n",
    "│  Pinned  │   Direct DMA!     │  Device  │\n",
    "│  (host)  │                   │  (GPU)   │\n",
    "└──────────┘                   └──────────┘\n",
    "\n",
    "Benefits:\n",
    "• Faster transfers (no staging buffer)\n",
    "• Required for async copies\n",
    "• Enables overlap with compute\n",
    "\n",
    "Drawbacks:\n",
    "• Limited resource (can't swap to disk)\n",
    "• Reduces memory for other apps\n",
    "• Slower to allocate\n",
    "```\n",
    "\n",
    "### CUDA C++ Pinned Memory (Primary)\n",
    "\n",
    "```cpp\n",
    "// pinned_memory.cu - Pinned memory allocation and transfers\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    const size_t SIZE = 1 << 26;  // 64MB\n",
    "    const size_t BYTES = SIZE * sizeof(float);\n",
    "    \n",
    "    float *h_pageable, *h_pinned;\n",
    "    float *d_data;\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Host Memory\n",
    "    // ============================================\n",
    "    \n",
    "    // Regular pageable memory\n",
    "    h_pageable = (float*)malloc(BYTES);\n",
    "    \n",
    "    // Pinned (page-locked) memory\n",
    "    cudaMallocHost(&h_pinned, BYTES);  // Or cudaHostAlloc\n",
    "    \n",
    "    // Alternative with flags:\n",
    "    // cudaHostAlloc(&h_pinned, BYTES, cudaHostAllocDefault);\n",
    "    // Flags: cudaHostAllocPortable, cudaHostAllocMapped, cudaHostAllocWriteCombined\n",
    "    \n",
    "    cudaMalloc(&d_data, BYTES);\n",
    "    \n",
    "    // Initialize\n",
    "    for (size_t i = 0; i < SIZE; i++) {\n",
    "        h_pageable[i] = 1.0f;\n",
    "        h_pinned[i] = 1.0f;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Compare Transfer Speeds\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    float ms;\n",
    "    \n",
    "    // Pageable transfer (synchronous only!)\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(d_data, h_pageable, BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Pageable H2D: %.2f ms (%.2f GB/s)\\n\", \n",
    "           ms, BYTES / ms / 1e6);\n",
    "    \n",
    "    // Pinned transfer (can be async!)\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(d_data, h_pinned, BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Pinned H2D:   %.2f ms (%.2f GB/s)\\n\", \n",
    "           ms, BYTES / ms / 1e6);\n",
    "    \n",
    "    // Cleanup\n",
    "    free(h_pageable);\n",
    "    cudaFreeHost(h_pinned);  // Must use cudaFreeHost!\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6324694",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Async Memory Copies\n",
    "\n",
    "### CUDA C++ cudaMemcpyAsync (Primary)\n",
    "\n",
    "```cpp\n",
    "// async_memcpy.cu - Asynchronous memory transfers\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        // Simulate heavy computation\n",
    "        float val = data[i];\n",
    "        for (int j = 0; j < 100; j++) {\n",
    "            val = sqrtf(val * val + 1.0f);\n",
    "        }\n",
    "        data[i] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 22;  // 4M elements\n",
    "    const size_t BYTES = N * sizeof(float);\n",
    "    \n",
    "    // MUST use pinned memory for async!\n",
    "    float *h_data;\n",
    "    cudaMallocHost(&h_data, BYTES);\n",
    "    \n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, BYTES);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // Synchronous (Blocking)\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // These block until complete:\n",
    "    cudaMemcpy(d_data, h_data, BYTES, cudaMemcpyHostToDevice);\n",
    "    processKernel<<<256, 256>>>(d_data, N);\n",
    "    cudaMemcpy(h_data, d_data, BYTES, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float syncTime;\n",
    "    cudaEventElapsedTime(&syncTime, start, stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Asynchronous (Non-Blocking)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // These return immediately to CPU:\n",
    "    cudaMemcpyAsync(d_data, h_data, BYTES, \n",
    "                    cudaMemcpyHostToDevice, stream);\n",
    "    processKernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    cudaMemcpyAsync(h_data, d_data, BYTES, \n",
    "                    cudaMemcpyDeviceToHost, stream);\n",
    "    \n",
    "    // Can do CPU work here while GPU is busy!\n",
    "    \n",
    "    cudaStreamSynchronize(stream);  // Wait when needed\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float asyncTime;\n",
    "    cudaEventElapsedTime(&asyncTime, start, stop);\n",
    "    \n",
    "    printf(\"Sync time:  %.2f ms\\n\", syncTime);\n",
    "    printf(\"Async time: %.2f ms\\n\", asyncTime);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3bfe3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Overlap Pattern\n",
    "\n",
    "### Chunked Processing for Overlap\n",
    "\n",
    "```\n",
    "Without Chunking (No Overlap):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "H2D:    ████████████████████████████████████████\n",
    "Compute:                                        ████████████████████\n",
    "D2H:                                                                ████████████\n",
    "\n",
    "With Chunking (Overlapped):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Stream0: [H2D][Compute][D2H]\n",
    "Stream1:      [H2D][Compute][D2H]\n",
    "Stream2:           [H2D][Compute][D2H]\n",
    "Stream3:                [H2D][Compute][D2H]\n",
    "\n",
    "Timeline:  ████████████████████████████████████\n",
    "           (H2D, Compute, and D2H overlap!)\n",
    "```\n",
    "\n",
    "### CUDA C++ Overlap Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// overlap_pattern.cu - Overlapping transfers with computation\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        float val = data[tid];\n",
    "        for (int i = 0; i < 500; i++) {\n",
    "            val = sinf(val) * cosf(val) + 1.0f;\n",
    "        }\n",
    "        data[tid] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 24;  // 16M elements\n",
    "    const int NUM_STREAMS = 4;\n",
    "    const int CHUNK_SIZE = N / NUM_STREAMS;\n",
    "    const size_t BYTES = N * sizeof(float);\n",
    "    const size_t CHUNK_BYTES = CHUNK_SIZE * sizeof(float);\n",
    "    \n",
    "    // Pinned host memory\n",
    "    float *h_input, *h_output;\n",
    "    cudaMallocHost(&h_input, BYTES);\n",
    "    cudaMallocHost(&h_output, BYTES);\n",
    "    \n",
    "    // Device memory (separate for each stream for true overlap)\n",
    "    float* d_data[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaMalloc(&d_data[i], CHUNK_BYTES);\n",
    "    }\n",
    "    \n",
    "    // Create streams\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Initialize input\n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // VERSION 1: No Overlap (Baseline)\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        int offset = i * CHUNK_SIZE;\n",
    "        \n",
    "        // All in default stream - sequential!\n",
    "        cudaMemcpy(d_data[i], h_input + offset, CHUNK_BYTES, \n",
    "                   cudaMemcpyHostToDevice);\n",
    "        process<<<(CHUNK_SIZE+255)/256, 256>>>(d_data[i], CHUNK_SIZE);\n",
    "        cudaMemcpy(h_output + offset, d_data[i], CHUNK_BYTES, \n",
    "                   cudaMemcpyDeviceToHost);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float noOverlapTime;\n",
    "    cudaEventElapsedTime(&noOverlapTime, start, stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // VERSION 2: With Overlap\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Issue all operations for all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        int offset = i * CHUNK_SIZE;\n",
    "        \n",
    "        cudaMemcpyAsync(d_data[i], h_input + offset, CHUNK_BYTES,\n",
    "                        cudaMemcpyHostToDevice, streams[i]);\n",
    "        \n",
    "        process<<<(CHUNK_SIZE+255)/256, 256, 0, streams[i]>>>(\n",
    "            d_data[i], CHUNK_SIZE);\n",
    "        \n",
    "        cudaMemcpyAsync(h_output + offset, d_data[i], CHUNK_BYTES,\n",
    "                        cudaMemcpyDeviceToHost, streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Synchronize all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float overlapTime;\n",
    "    cudaEventElapsedTime(&overlapTime, start, stop);\n",
    "    \n",
    "    printf(\"Without overlap: %.2f ms\\n\", noOverlapTime);\n",
    "    printf(\"With overlap:    %.2f ms\\n\", overlapTime);\n",
    "    printf(\"Speedup:         %.2fx\\n\", noOverlapTime / overlapTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    cudaFreeHost(h_input);\n",
    "    cudaFreeHost(h_output);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72208105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Async Transfer Demo (OPTIONAL)\n",
    "\n",
    "@cuda.jit\n",
    "def process(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        val = data[tid]\n",
    "        for _ in range(100):\n",
    "            val = val * 1.001 + 0.001\n",
    "        data[tid] = val\n",
    "\n",
    "n = 1 << 22\n",
    "num_streams = 4\n",
    "chunk = n // num_streams\n",
    "\n",
    "# Create streams\n",
    "streams = [cuda.stream() for _ in range(num_streams)]\n",
    "\n",
    "# Pinned host arrays\n",
    "h_input = cuda.pinned_array(n, dtype=np.float32)\n",
    "h_output = cuda.pinned_array(n, dtype=np.float32)\n",
    "h_input[:] = 1.0\n",
    "\n",
    "# Device arrays (one per stream)\n",
    "d_chunks = [cuda.device_array(chunk, dtype=np.float32) for _ in range(num_streams)]\n",
    "\n",
    "# Launch overlapped work\n",
    "start = time.time()\n",
    "\n",
    "for i in range(num_streams):\n",
    "    offset = i * chunk\n",
    "    \n",
    "    # Async copy H2D\n",
    "    d_chunks[i].copy_to_device(h_input[offset:offset+chunk], stream=streams[i])\n",
    "    \n",
    "    # Kernel\n",
    "    process[(chunk+255)//256, 256, streams[i]](d_chunks[i])\n",
    "    \n",
    "    # Async copy D2H\n",
    "    d_chunks[i].copy_to_host(h_output[offset:offset+chunk], stream=streams[i])\n",
    "\n",
    "# Sync all\n",
    "for s in streams:\n",
    "    s.synchronize()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Overlapped time: {elapsed*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ebce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Profiling Overlap with Nsight Systems\n",
    "\n",
    "```bash\n",
    "# Profile to see overlap\n",
    "nsys profile -o overlap_timeline ./overlap_pattern\n",
    "\n",
    "# View in Nsight Systems GUI\n",
    "nsys-ui overlap_timeline.nsys-rep\n",
    "```\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "```\n",
    "Nsight Systems Timeline:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "                    Without Overlap          With Overlap\n",
    "                    ─────────────────        ─────────────────\n",
    "Copy Engine H2D:    ████░░░░░░░░░░░░░        ████░░░░\n",
    "Compute:            ░░░░████░░░░░░░░░        ░░██████████\n",
    "Copy Engine D2H:    ░░░░░░░░████░░░░░        ░░░░░░░░████\n",
    "                    ↑ Sequential!            ↑ Overlapped!\n",
    "\n",
    "Look for:\n",
    "• Parallel bars in different rows = overlap achieved\n",
    "• Gaps between operations = potential improvement\n",
    "• Copy/compute at same time = good utilization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4ef94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Pinned Memory Benchmark\n",
    "Compare transfer speeds for various sizes with pageable vs pinned memory.\n",
    "\n",
    "### Exercise 2: Find Optimal Chunk Count\n",
    "Test with 2, 4, 8, 16 streams and find the sweet spot for your GPU.\n",
    "\n",
    "### Exercise 3: Double Buffering\n",
    "```cpp\n",
    "// Implement double buffering:\n",
    "// - While processing chunk N, transfer chunk N+1\n",
    "// - Use only 2 device buffers, alternate between them\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12138ea3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│              OVERLAPPING TRANSFERS                      │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Pinned Memory:                                         │\n",
    "│  • cudaMallocHost() / cudaFreeHost()                    │\n",
    "│  • Required for async transfers                         │\n",
    "│  • Faster than pageable (no staging)                    │\n",
    "│                                                         │\n",
    "│  Async Copies:                                          │\n",
    "│  • cudaMemcpyAsync(dst, src, size, kind, stream)        │\n",
    "│  • Returns immediately to CPU                           │\n",
    "│  • Operations in same stream execute in order           │\n",
    "│                                                         │\n",
    "│  Overlap Pattern:                                       │\n",
    "│  • Split data into chunks                               │\n",
    "│  • Each chunk in different stream                       │\n",
    "│  • H2D, compute, D2H can all overlap                    │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 3 - Multi-Stream Execution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
