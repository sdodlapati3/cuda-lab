{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53236596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd35ba",
   "metadata": {},
   "source": [
    "# Day 2: Overlapping Data Transfers - Double Buffering\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Hook: The Restaurant Kitchen Secret\n",
    "\n",
    "**Ever wonder** how busy restaurants serve hundreds of meals per hour?\n",
    "\n",
    "They don't wait for one dish to be completely done before starting the next. While the steak sizzles, the salad gets prepped, and dessert plates are being staged!\n",
    "\n",
    "```\n",
    "Amateur Kitchen (Sequential):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  [Prep Meal 1] ‚Üí [Cook Meal 1] ‚Üí [Plate Meal 1]               ‚îÇ\n",
    "‚îÇ                                        ‚Üí [Prep Meal 2] ‚Üí ...  ‚îÇ\n",
    "‚îÇ                                                               ‚îÇ\n",
    "‚îÇ  Customer 1 served at T=15min, Customer 2 at T=30min üò¥       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Professional Kitchen (Overlapped):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Station 1: [Prep 1][Prep 2][Prep 3][Prep 4]                  ‚îÇ\n",
    "‚îÇ  Station 2:      [Cook 1][Cook 2][Cook 3][Cook 4]             ‚îÇ\n",
    "‚îÇ  Station 3:           [Plate 1][Plate 2][Plate 3][Plate 4]    ‚îÇ\n",
    "‚îÇ                                                               ‚îÇ\n",
    "‚îÇ  All 4 customers served by T=20min! üöÄ                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Your GPU has THREE \"stations\"** (H2D engine, Compute, D2H engine) - let's use them ALL at once!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Allocate pinned memory** with `cudaMallocHost()` for async transfers\n",
    "2. **Use async copies** with `cudaMemcpyAsync()` for non-blocking transfers\n",
    "3. **Implement double buffering** to overlap compute and transfer\n",
    "4. **Measure** the performance gain from overlapping operations\n",
    "5. **Choose** optimal chunk sizes for your data pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üÉè Concept Card: Double Buffering\n",
    "\n",
    "```\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîÑ DOUBLE BUFFERING = TWO ALTERNATING WORKSPACES               ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  ANALOGY: Two Whiteboards in a Lecture                           ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  Professor uses Board A while assistant erases Board B:          ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  Time 1: [Teaching on A]  [Erasing B]                           ‚ïë\n",
    "‚ïë  Time 2: [Teaching on B]  [Erasing A]                           ‚ïë\n",
    "‚ïë  Time 3: [Teaching on A]  [Erasing B]                           ‚ïë\n",
    "‚ïë         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚ïë\n",
    "‚ïë            NEVER waiting for board to be ready!                  ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  GPU Double Buffering:                                           ‚ïë\n",
    "‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚ïë\n",
    "‚ïë  ‚îÇ  Buffer A: [Receive Data] ‚Üí [Process] ‚Üí [Send Results]    ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îÇ  Buffer B:      [Receive Data] ‚Üí [Process] ‚Üí [Send]       ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îÇ  Buffer A:           [Receive Data] ‚Üí [Process] ‚Üí ...     ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  üí° KEY INSIGHT: While computing on one buffer,                  ‚ïë\n",
    "‚ïë     load/unload the other - GPU never idles!                     ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  REQUIREMENTS:                                                   ‚ïë\n",
    "‚ïë  ‚úì Pinned memory (enables async transfers)                       ‚ïë\n",
    "‚ïë  ‚úì Multiple streams (enables concurrent operations)              ‚ïë\n",
    "‚ïë  ‚úì Independent data chunks (no dependencies)                     ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Pinned Memory\n",
    "\n",
    "### Why Pinned Memory?\n",
    "\n",
    "```\n",
    "Regular (Pageable) Memory:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Host   ‚îÇ --> ‚îÇ  Pinned  ‚îÇ --> ‚îÇ  Device  ‚îÇ\n",
    "‚îÇ (paged)  ‚îÇ     ‚îÇ (staging)‚îÇ     ‚îÇ (GPU)    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  Hidden copy!\n",
    "\n",
    "Pinned (Page-Locked) Memory:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Pinned  ‚îÇ   Direct DMA!     ‚îÇ  Device  ‚îÇ\n",
    "‚îÇ  (host)  ‚îÇ                   ‚îÇ  (GPU)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Benefits:\n",
    "‚Ä¢ Faster transfers (no staging buffer)\n",
    "‚Ä¢ Required for async copies\n",
    "‚Ä¢ Enables overlap with compute\n",
    "\n",
    "Drawbacks:\n",
    "‚Ä¢ Limited resource (can't swap to disk)\n",
    "‚Ä¢ Reduces memory for other apps\n",
    "‚Ä¢ Slower to allocate\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40580087",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pinned_memory.cu\n",
    "// pinned_memory.cu - Pinned memory allocation and transfers\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    const size_t SIZE = 1 << 26;  // 64MB\n",
    "    const size_t BYTES = SIZE * sizeof(float);\n",
    "    \n",
    "    float *h_pageable, *h_pinned;\n",
    "    float *d_data;\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Host Memory\n",
    "    // ============================================\n",
    "    \n",
    "    // Regular pageable memory\n",
    "    h_pageable = (float*)malloc(BYTES);\n",
    "    \n",
    "    // Pinned (page-locked) memory\n",
    "    cudaMallocHost(&h_pinned, BYTES);  // Or cudaHostAlloc\n",
    "    \n",
    "    // Alternative with flags:\n",
    "    // cudaHostAlloc(&h_pinned, BYTES, cudaHostAllocDefault);\n",
    "    // Flags: cudaHostAllocPortable, cudaHostAllocMapped, cudaHostAllocWriteCombined\n",
    "    \n",
    "    cudaMalloc(&d_data, BYTES);\n",
    "    \n",
    "    // Initialize\n",
    "    for (size_t i = 0; i < SIZE; i++) {\n",
    "        h_pageable[i] = 1.0f;\n",
    "        h_pinned[i] = 1.0f;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Compare Transfer Speeds\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    float ms;\n",
    "    \n",
    "    // Pageable transfer (synchronous only!)\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(d_data, h_pageable, BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Pageable H2D: %.2f ms (%.2f GB/s)\\n\", \n",
    "           ms, BYTES / ms / 1e6);\n",
    "    \n",
    "    // Pinned transfer (can be async!)\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(d_data, h_pinned, BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Pinned H2D:   %.2f ms (%.2f GB/s)\\n\", \n",
    "           ms, BYTES / ms / 1e6);\n",
    "    \n",
    "    // Cleanup\n",
    "    free(h_pageable);\n",
    "    cudaFreeHost(h_pinned);  // Must use cudaFreeHost!\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o pinned_memory pinned_memory.cu\n",
    "!./pinned_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6324694",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Async Memory Copies\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f191a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile async_memcpy.cu\n",
    "// async_memcpy.cu - Asynchronous memory transfers\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        // Simulate heavy computation\n",
    "        float val = data[i];\n",
    "        for (int j = 0; j < 100; j++) {\n",
    "            val = sqrtf(val * val + 1.0f);\n",
    "        }\n",
    "        data[i] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 22;  // 4M elements\n",
    "    const size_t BYTES = N * sizeof(float);\n",
    "    \n",
    "    // MUST use pinned memory for async!\n",
    "    float *h_data;\n",
    "    cudaMallocHost(&h_data, BYTES);\n",
    "    \n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, BYTES);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // Synchronous (Blocking)\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // These block until complete:\n",
    "    cudaMemcpy(d_data, h_data, BYTES, cudaMemcpyHostToDevice);\n",
    "    processKernel<<<256, 256>>>(d_data, N);\n",
    "    cudaMemcpy(h_data, d_data, BYTES, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float syncTime;\n",
    "    cudaEventElapsedTime(&syncTime, start, stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Asynchronous (Non-Blocking)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // These return immediately to CPU:\n",
    "    cudaMemcpyAsync(d_data, h_data, BYTES, \n",
    "                    cudaMemcpyHostToDevice, stream);\n",
    "    processKernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    cudaMemcpyAsync(h_data, d_data, BYTES, \n",
    "                    cudaMemcpyDeviceToHost, stream);\n",
    "    \n",
    "    // Can do CPU work here while GPU is busy!\n",
    "    \n",
    "    cudaStreamSynchronize(stream);  // Wait when needed\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float asyncTime;\n",
    "    cudaEventElapsedTime(&asyncTime, start, stop);\n",
    "    \n",
    "    printf(\"Sync time:  %.2f ms\\n\", syncTime);\n",
    "    printf(\"Async time: %.2f ms\\n\", asyncTime);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af64d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o async_memcpy async_memcpy.cu\n",
    "!./async_memcpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3bfe3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Overlap Pattern\n",
    "\n",
    "### Chunked Processing for Overlap\n",
    "\n",
    "```\n",
    "Without Chunking (No Overlap):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "H2D:    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "Compute:                                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "D2H:                                                                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "\n",
    "With Chunking (Overlapped):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Stream0: [H2D][Compute][D2H]\n",
    "Stream1:      [H2D][Compute][D2H]\n",
    "Stream2:           [H2D][Compute][D2H]\n",
    "Stream3:                [H2D][Compute][D2H]\n",
    "\n",
    "Timeline:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "           (H2D, Compute, and D2H overlap!)\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba923f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile overlap_pattern.cu\n",
    "// overlap_pattern.cu - Overlapping transfers with computation\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        float val = data[tid];\n",
    "        for (int i = 0; i < 500; i++) {\n",
    "            val = sinf(val) * cosf(val) + 1.0f;\n",
    "        }\n",
    "        data[tid] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 24;  // 16M elements\n",
    "    const int NUM_STREAMS = 4;\n",
    "    const int CHUNK_SIZE = N / NUM_STREAMS;\n",
    "    const size_t BYTES = N * sizeof(float);\n",
    "    const size_t CHUNK_BYTES = CHUNK_SIZE * sizeof(float);\n",
    "    \n",
    "    // Pinned host memory\n",
    "    float *h_input, *h_output;\n",
    "    cudaMallocHost(&h_input, BYTES);\n",
    "    cudaMallocHost(&h_output, BYTES);\n",
    "    \n",
    "    // Device memory (separate for each stream for true overlap)\n",
    "    float* d_data[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaMalloc(&d_data[i], CHUNK_BYTES);\n",
    "    }\n",
    "    \n",
    "    // Create streams\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Initialize input\n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // VERSION 1: No Overlap (Baseline)\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        int offset = i * CHUNK_SIZE;\n",
    "        \n",
    "        // All in default stream - sequential!\n",
    "        cudaMemcpy(d_data[i], h_input + offset, CHUNK_BYTES, \n",
    "                   cudaMemcpyHostToDevice);\n",
    "        process<<<(CHUNK_SIZE+255)/256, 256>>>(d_data[i], CHUNK_SIZE);\n",
    "        cudaMemcpy(h_output + offset, d_data[i], CHUNK_BYTES, \n",
    "                   cudaMemcpyDeviceToHost);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float noOverlapTime;\n",
    "    cudaEventElapsedTime(&noOverlapTime, start, stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // VERSION 2: With Overlap\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Issue all operations for all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        int offset = i * CHUNK_SIZE;\n",
    "        \n",
    "        cudaMemcpyAsync(d_data[i], h_input + offset, CHUNK_BYTES,\n",
    "                        cudaMemcpyHostToDevice, streams[i]);\n",
    "        \n",
    "        process<<<(CHUNK_SIZE+255)/256, 256, 0, streams[i]>>>(\n",
    "            d_data[i], CHUNK_SIZE);\n",
    "        \n",
    "        cudaMemcpyAsync(h_output + offset, d_data[i], CHUNK_BYTES,\n",
    "                        cudaMemcpyDeviceToHost, streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Synchronize all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float overlapTime;\n",
    "    cudaEventElapsedTime(&overlapTime, start, stop);\n",
    "    \n",
    "    printf(\"Without overlap: %.2f ms\\n\", noOverlapTime);\n",
    "    printf(\"With overlap:    %.2f ms\\n\", overlapTime);\n",
    "    printf(\"Speedup:         %.2fx\\n\", noOverlapTime / overlapTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    cudaFreeHost(h_input);\n",
    "    cudaFreeHost(h_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o overlap_pattern overlap_pattern.cu\n",
    "!./overlap_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87196ba",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72208105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Async Transfer Demo (OPTIONAL)\n",
    "\n",
    "@cuda.jit\n",
    "def process(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        val = data[tid]\n",
    "        for _ in range(100):\n",
    "            val = val * 1.001 + 0.001\n",
    "        data[tid] = val\n",
    "\n",
    "n = 1 << 22\n",
    "num_streams = 4\n",
    "chunk = n // num_streams\n",
    "\n",
    "# Create streams\n",
    "streams = [cuda.stream() for _ in range(num_streams)]\n",
    "\n",
    "# Pinned host arrays\n",
    "h_input = cuda.pinned_array(n, dtype=np.float32)\n",
    "h_output = cuda.pinned_array(n, dtype=np.float32)\n",
    "h_input[:] = 1.0\n",
    "\n",
    "# Device arrays (one per stream)\n",
    "d_chunks = [cuda.device_array(chunk, dtype=np.float32) for _ in range(num_streams)]\n",
    "\n",
    "# Launch overlapped work\n",
    "start = time.time()\n",
    "\n",
    "for i in range(num_streams):\n",
    "    offset = i * chunk\n",
    "    \n",
    "    # Async copy H2D\n",
    "    d_chunks[i].copy_to_device(h_input[offset:offset+chunk], stream=streams[i])\n",
    "    \n",
    "    # Kernel\n",
    "    process[(chunk+255)//256, 256, streams[i]](d_chunks[i])\n",
    "    \n",
    "    # Async copy D2H\n",
    "    d_chunks[i].copy_to_host(h_output[offset:offset+chunk], stream=streams[i])\n",
    "\n",
    "# Sync all\n",
    "for s in streams:\n",
    "    s.synchronize()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Overlapped time: {elapsed*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ebce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Profiling Overlap with Nsight Systems\n",
    "\n",
    "```bash\n",
    "# Profile to see overlap\n",
    "nsys profile -o overlap_timeline ./overlap_pattern\n",
    "\n",
    "# View in Nsight Systems GUI\n",
    "nsys-ui overlap_timeline.nsys-rep\n",
    "```\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "```\n",
    "Nsight Systems Timeline:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "                    Without Overlap          With Overlap\n",
    "                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Copy Engine H2D:    ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë\n",
    "Compute:            ‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë        ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "Copy Engine D2H:    ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà\n",
    "                    ‚Üë Sequential!            ‚Üë Overlapped!\n",
    "\n",
    "Look for:\n",
    "‚Ä¢ Parallel bars in different rows = overlap achieved\n",
    "‚Ä¢ Gaps between operations = potential improvement\n",
    "‚Ä¢ Copy/compute at same time = good utilization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4ef94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00657b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile overlap_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            val = sinf(val) * cosf(val) + 0.1f;\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Pinned Memory Benchmark\n",
    "// ============================================================\n",
    "\n",
    "void exercise1_pinnedVsPageable() {\n",
    "    printf(\"=== Exercise 1: Pinned vs Pageable Memory ===\\n\");\n",
    "    printf(\"%-12s %-15s %-15s %-10s\\n\", \"Size\", \"Pageable(ms)\", \"Pinned(ms)\", \"Speedup\");\n",
    "    printf(\"--------------------------------------------------\\n\");\n",
    "    \n",
    "    int sizes[] = {1<<20, 1<<22, 1<<24, 1<<26};  // 1MB to 64MB\n",
    "    int numSizes = sizeof(sizes) / sizeof(sizes[0]);\n",
    "    \n",
    "    for (int s = 0; s < numSizes; s++) {\n",
    "        int n = sizes[s];\n",
    "        size_t bytes = n * sizeof(float);\n",
    "        \n",
    "        float *h_pageable = (float*)malloc(bytes);\n",
    "        float *h_pinned;\n",
    "        float *d_data;\n",
    "        \n",
    "        CHECK_CUDA(cudaMallocHost(&h_pinned, bytes));\n",
    "        CHECK_CUDA(cudaMalloc(&d_data, bytes));\n",
    "        \n",
    "        cudaEvent_t start, stop;\n",
    "        cudaEventCreate(&start);\n",
    "        cudaEventCreate(&stop);\n",
    "        \n",
    "        // Pageable transfer\n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < 10; i++) {\n",
    "            cudaMemcpy(d_data, h_pageable, bytes, cudaMemcpyHostToDevice);\n",
    "            cudaMemcpy(h_pageable, d_data, bytes, cudaMemcpyDeviceToHost);\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        float pageableMs;\n",
    "        cudaEventElapsedTime(&pageableMs, start, stop);\n",
    "        \n",
    "        // Pinned transfer\n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < 10; i++) {\n",
    "            cudaMemcpy(d_data, h_pinned, bytes, cudaMemcpyHostToDevice);\n",
    "            cudaMemcpy(h_pinned, d_data, bytes, cudaMemcpyDeviceToHost);\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        float pinnedMs;\n",
    "        cudaEventElapsedTime(&pinnedMs, start, stop);\n",
    "        \n",
    "        printf(\"%-12.1f MB %-15.2f %-15.2f %.2fx\\n\", \n",
    "               bytes / (1024.0f * 1024.0f), pageableMs, pinnedMs, pageableMs / pinnedMs);\n",
    "        \n",
    "        free(h_pageable);\n",
    "        cudaFreeHost(h_pinned);\n",
    "        cudaFree(d_data);\n",
    "        cudaEventDestroy(start);\n",
    "        cudaEventDestroy(stop);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Find Optimal Chunk Count\n",
    "// ============================================================\n",
    "\n",
    "void exercise2_optimalChunks() {\n",
    "    printf(\"=== Exercise 2: Optimal Chunk Count ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 24;  // 16M elements\n",
    "    size_t bytes = n * sizeof(float);\n",
    "    \n",
    "    float *h_data;\n",
    "    float *d_data;\n",
    "    CHECK_CUDA(cudaMallocHost(&h_data, bytes));\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, bytes));\n",
    "    \n",
    "    for (int i = 0; i < n; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int chunkCounts[] = {1, 2, 4, 8, 16, 32};\n",
    "    int numTests = sizeof(chunkCounts) / sizeof(chunkCounts[0]);\n",
    "    \n",
    "    printf(\"%-10s %-15s\\n\", \"Chunks\", \"Time (ms)\");\n",
    "    printf(\"---------------------------\\n\");\n",
    "    \n",
    "    float bestTime = 1e9;\n",
    "    int bestChunks = 1;\n",
    "    \n",
    "    for (int t = 0; t < numTests; t++) {\n",
    "        int numChunks = chunkCounts[t];\n",
    "        int chunkSize = n / numChunks;\n",
    "        size_t chunkBytes = chunkSize * sizeof(float);\n",
    "        \n",
    "        cudaStream_t* streams = (cudaStream_t*)malloc(numChunks * sizeof(cudaStream_t));\n",
    "        for (int i = 0; i < numChunks; i++) {\n",
    "            cudaStreamCreate(&streams[i]);\n",
    "        }\n",
    "        \n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < numChunks; i++) {\n",
    "            int offset = i * chunkSize;\n",
    "            cudaMemcpyAsync(d_data + offset, h_data + offset, chunkBytes, \n",
    "                           cudaMemcpyHostToDevice, streams[i]);\n",
    "            processKernel<<<(chunkSize+255)/256, 256, 0, streams[i]>>>(d_data + offset, chunkSize);\n",
    "            cudaMemcpyAsync(h_data + offset, d_data + offset, chunkBytes,\n",
    "                           cudaMemcpyDeviceToHost, streams[i]);\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        printf(\"%-10d %-15.2f\", numChunks, ms);\n",
    "        \n",
    "        if (ms < bestTime) {\n",
    "            bestTime = ms;\n",
    "            bestChunks = numChunks;\n",
    "            printf(\" *\");\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "        \n",
    "        for (int i = 0; i < numChunks; i++) {\n",
    "            cudaStreamDestroy(streams[i]);\n",
    "        }\n",
    "        free(streams);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\nOptimal: %d chunks (%.2f ms)\\n\\n\", bestChunks, bestTime);\n",
    "    \n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Double Buffering\n",
    "// ============================================================\n",
    "\n",
    "void exercise3_doubleBuffering() {\n",
    "    printf(\"=== Exercise 3: Double Buffering ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 24;  // 16M elements\n",
    "    const int numChunks = 8;\n",
    "    const int chunkSize = n / numChunks;\n",
    "    size_t chunkBytes = chunkSize * sizeof(float);\n",
    "    \n",
    "    // Host memory\n",
    "    float *h_input, *h_output;\n",
    "    CHECK_CUDA(cudaMallocHost(&h_input, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMallocHost(&h_output, n * sizeof(float)));\n",
    "    \n",
    "    for (int i = 0; i < n; i++) h_input[i] = 1.0f;\n",
    "    \n",
    "    // Device memory - only 2 buffers for double buffering\n",
    "    float *d_buffer[2];\n",
    "    CHECK_CUDA(cudaMalloc(&d_buffer[0], chunkBytes));\n",
    "    CHECK_CUDA(cudaMalloc(&d_buffer[1], chunkBytes));\n",
    "    \n",
    "    cudaStream_t streams[2];\n",
    "    cudaStreamCreate(&streams[0]);\n",
    "    cudaStreamCreate(&streams[1]);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Process first chunk\n",
    "    cudaMemcpyAsync(d_buffer[0], h_input, chunkBytes, cudaMemcpyHostToDevice, streams[0]);\n",
    "    \n",
    "    for (int chunk = 0; chunk < numChunks; chunk++) {\n",
    "        int curr = chunk % 2;\n",
    "        int next = (chunk + 1) % 2;\n",
    "        \n",
    "        // Launch kernel on current chunk\n",
    "        processKernel<<<(chunkSize+255)/256, 256, 0, streams[curr]>>>(d_buffer[curr], chunkSize);\n",
    "        \n",
    "        // Start transfer of next chunk (if not last)\n",
    "        if (chunk < numChunks - 1) {\n",
    "            int nextOffset = (chunk + 1) * chunkSize;\n",
    "            cudaMemcpyAsync(d_buffer[next], h_input + nextOffset, chunkBytes,\n",
    "                           cudaMemcpyHostToDevice, streams[next]);\n",
    "        }\n",
    "        \n",
    "        // Transfer current result back\n",
    "        int currOffset = chunk * chunkSize;\n",
    "        cudaMemcpyAsync(h_output + currOffset, d_buffer[curr], chunkBytes,\n",
    "                       cudaMemcpyDeviceToHost, streams[curr]);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    printf(\"Double buffering with %d chunks: %.2f ms\\n\", numChunks, ms);\n",
    "    printf(\"Device memory used: 2 buffers (%.1f MB each)\\n\", chunkBytes / (1024.0f * 1024.0f));\n",
    "    printf(\"Total data processed: %.1f MB\\n\\n\", n * sizeof(float) / (1024.0f * 1024.0f));\n",
    "    \n",
    "    cudaStreamDestroy(streams[0]);\n",
    "    cudaStreamDestroy(streams[1]);\n",
    "    cudaFree(d_buffer[0]);\n",
    "    cudaFree(d_buffer[1]);\n",
    "    cudaFreeHost(h_input);\n",
    "    cudaFreeHost(h_output);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\");\n",
    "    printf(\"‚ïë           Overlap Transfers Exercises                        ‚ïë\\n\");\n",
    "    printf(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Async Engines: %d\\n\\n\", prop.asyncEngineCount);\n",
    "    \n",
    "    exercise1_pinnedVsPageable();\n",
    "    exercise2_optimalChunks();\n",
    "    exercise3_doubleBuffering();\n",
    "    \n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb249dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o overlap_exercises overlap_exercises.cu && ./overlap_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522c087",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Pinned Memory Benchmark\n",
    "Compare transfer speeds for various sizes with pageable vs pinned memory.\n",
    "\n",
    "### Exercise 2: Find Optimal Chunk Count\n",
    "Test with 2, 4, 8, 16 streams and find the sweet spot for your GPU.\n",
    "\n",
    "### Exercise 3: Double Buffering\n",
    "```cpp\n",
    "// Implement double buffering:\n",
    "// - While processing chunk N, transfer chunk N+1\n",
    "// - Use only 2 device buffers, alternate between them\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12138ea3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "```\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë              OVERLAPPING TRANSFERS MASTERY                       ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  üîÑ Core Concept: DOUBLE BUFFERING                               ‚ïë\n",
    "‚ïë     Work on buffer A while loading/unloading buffer B            ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  üìã Essential API:                                               ‚ïë\n",
    "‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚ïë\n",
    "‚ïë  ‚îÇ  cudaMallocHost(&ptr, size)     // Pinned memory (fast)    ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îÇ  cudaMemcpyAsync(...)           // Non-blocking copy       ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îÇ  cudaFreeHost(ptr)              // Clean up pinned mem     ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  üß© The Overlap Pipeline:                                        ‚ïë\n",
    "‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚ïë\n",
    "‚ïë  ‚îÇ  Stream 0: [H2D 0] ‚Üí [Kernel 0] ‚Üí [D2H 0]                  ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îÇ  Stream 1:    [H2D 1] ‚Üí [Kernel 1] ‚Üí [D2H 1]               ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îÇ  Stream 2:       [H2D 2] ‚Üí [Kernel 2] ‚Üí [D2H 2]            ‚îÇ  ‚ïë\n",
    "‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  ‚ö†Ô∏è  Common Pitfalls:                                            ‚ïë\n",
    "‚ïë     ‚Ä¢ Forgetting pinned memory (async fails silently!)          ‚ïë\n",
    "‚ïë     ‚Ä¢ Chunks too small (overhead dominates)                     ‚ïë\n",
    "‚ïë     ‚Ä¢ Chunks too large (no overlap opportunity)                 ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  üìä Performance Impact:                                          ‚ïë\n",
    "‚ïë     Sequential:  [H2D] + [Compute] + [D2H] = 100%              ‚ïë\n",
    "‚ïë     Overlapped:  max(H2D, Compute, D2H) ‚âà 40-60%               ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ What's Next?\n",
    "\n",
    "**Day 3: Multi-Stream Execution** - Parallel pipelines!\n",
    "\n",
    "You've learned to overlap with two buffers. Tomorrow we'll explore:\n",
    "\n",
    "```\n",
    "Today:           Tomorrow (Day 3):\n",
    "2 buffers        Multiple concurrent kernel streams\n",
    "[A][B][A][B]     [Stream 0: Kernel A]\n",
    "                 [Stream 1: Kernel B]  ‚Üê Running SIMULTANEOUSLY\n",
    "                 [Stream 2: Kernel C]\n",
    "                 [Stream 3: Kernel D]\n",
    "```\n",
    "\n",
    "When kernels don't fill the GPU, run multiple at once! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
