{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53236596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd35ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Pinned Memory\n",
    "\n",
    "### Why Pinned Memory?\n",
    "\n",
    "```\n",
    "Regular (Pageable) Memory:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Host   ‚îÇ --> ‚îÇ  Pinned  ‚îÇ --> ‚îÇ  Device  ‚îÇ\n",
    "‚îÇ (paged)  ‚îÇ     ‚îÇ (staging)‚îÇ     ‚îÇ (GPU)    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  Hidden copy!\n",
    "\n",
    "Pinned (Page-Locked) Memory:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Pinned  ‚îÇ   Direct DMA!     ‚îÇ  Device  ‚îÇ\n",
    "‚îÇ  (host)  ‚îÇ                   ‚îÇ  (GPU)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Benefits:\n",
    "‚Ä¢ Faster transfers (no staging buffer)\n",
    "‚Ä¢ Required for async copies\n",
    "‚Ä¢ Enables overlap with compute\n",
    "\n",
    "Drawbacks:\n",
    "‚Ä¢ Limited resource (can't swap to disk)\n",
    "‚Ä¢ Reduces memory for other apps\n",
    "‚Ä¢ Slower to allocate\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40580087",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pinned_memory.cu\n",
    "// pinned_memory.cu - Pinned memory allocation and transfers\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    const size_t SIZE = 1 << 26;  // 64MB\n",
    "    const size_t BYTES = SIZE * sizeof(float);\n",
    "    \n",
    "    float *h_pageable, *h_pinned;\n",
    "    float *d_data;\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Host Memory\n",
    "    // ============================================\n",
    "    \n",
    "    // Regular pageable memory\n",
    "    h_pageable = (float*)malloc(BYTES);\n",
    "    \n",
    "    // Pinned (page-locked) memory\n",
    "    cudaMallocHost(&h_pinned, BYTES);  // Or cudaHostAlloc\n",
    "    \n",
    "    // Alternative with flags:\n",
    "    // cudaHostAlloc(&h_pinned, BYTES, cudaHostAllocDefault);\n",
    "    // Flags: cudaHostAllocPortable, cudaHostAllocMapped, cudaHostAllocWriteCombined\n",
    "    \n",
    "    cudaMalloc(&d_data, BYTES);\n",
    "    \n",
    "    // Initialize\n",
    "    for (size_t i = 0; i < SIZE; i++) {\n",
    "        h_pageable[i] = 1.0f;\n",
    "        h_pinned[i] = 1.0f;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Compare Transfer Speeds\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    float ms;\n",
    "    \n",
    "    // Pageable transfer (synchronous only!)\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(d_data, h_pageable, BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Pageable H2D: %.2f ms (%.2f GB/s)\\n\", \n",
    "           ms, BYTES / ms / 1e6);\n",
    "    \n",
    "    // Pinned transfer (can be async!)\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemcpy(d_data, h_pinned, BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Pinned H2D:   %.2f ms (%.2f GB/s)\\n\", \n",
    "           ms, BYTES / ms / 1e6);\n",
    "    \n",
    "    // Cleanup\n",
    "    free(h_pageable);\n",
    "    cudaFreeHost(h_pinned);  // Must use cudaFreeHost!\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o pinned_memory pinned_memory.cu\n",
    "!./pinned_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6324694",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Async Memory Copies\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f191a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile async_memcpy.cu\n",
    "// async_memcpy.cu - Asynchronous memory transfers\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        // Simulate heavy computation\n",
    "        float val = data[i];\n",
    "        for (int j = 0; j < 100; j++) {\n",
    "            val = sqrtf(val * val + 1.0f);\n",
    "        }\n",
    "        data[i] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 22;  // 4M elements\n",
    "    const size_t BYTES = N * sizeof(float);\n",
    "    \n",
    "    // MUST use pinned memory for async!\n",
    "    float *h_data;\n",
    "    cudaMallocHost(&h_data, BYTES);\n",
    "    \n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, BYTES);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // Synchronous (Blocking)\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // These block until complete:\n",
    "    cudaMemcpy(d_data, h_data, BYTES, cudaMemcpyHostToDevice);\n",
    "    processKernel<<<256, 256>>>(d_data, N);\n",
    "    cudaMemcpy(h_data, d_data, BYTES, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float syncTime;\n",
    "    cudaEventElapsedTime(&syncTime, start, stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Asynchronous (Non-Blocking)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // These return immediately to CPU:\n",
    "    cudaMemcpyAsync(d_data, h_data, BYTES, \n",
    "                    cudaMemcpyHostToDevice, stream);\n",
    "    processKernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    cudaMemcpyAsync(h_data, d_data, BYTES, \n",
    "                    cudaMemcpyDeviceToHost, stream);\n",
    "    \n",
    "    // Can do CPU work here while GPU is busy!\n",
    "    \n",
    "    cudaStreamSynchronize(stream);  // Wait when needed\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float asyncTime;\n",
    "    cudaEventElapsedTime(&asyncTime, start, stop);\n",
    "    \n",
    "    printf(\"Sync time:  %.2f ms\\n\", syncTime);\n",
    "    printf(\"Async time: %.2f ms\\n\", asyncTime);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af64d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o async_memcpy async_memcpy.cu\n",
    "!./async_memcpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3bfe3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Overlap Pattern\n",
    "\n",
    "### Chunked Processing for Overlap\n",
    "\n",
    "```\n",
    "Without Chunking (No Overlap):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "H2D:    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "Compute:                                        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "D2H:                                                                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "\n",
    "With Chunking (Overlapped):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Stream0: [H2D][Compute][D2H]\n",
    "Stream1:      [H2D][Compute][D2H]\n",
    "Stream2:           [H2D][Compute][D2H]\n",
    "Stream3:                [H2D][Compute][D2H]\n",
    "\n",
    "Timeline:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "           (H2D, Compute, and D2H overlap!)\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba923f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile overlap_pattern.cu\n",
    "// overlap_pattern.cu - Overlapping transfers with computation\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        float val = data[tid];\n",
    "        for (int i = 0; i < 500; i++) {\n",
    "            val = sinf(val) * cosf(val) + 1.0f;\n",
    "        }\n",
    "        data[tid] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 24;  // 16M elements\n",
    "    const int NUM_STREAMS = 4;\n",
    "    const int CHUNK_SIZE = N / NUM_STREAMS;\n",
    "    const size_t BYTES = N * sizeof(float);\n",
    "    const size_t CHUNK_BYTES = CHUNK_SIZE * sizeof(float);\n",
    "    \n",
    "    // Pinned host memory\n",
    "    float *h_input, *h_output;\n",
    "    cudaMallocHost(&h_input, BYTES);\n",
    "    cudaMallocHost(&h_output, BYTES);\n",
    "    \n",
    "    // Device memory (separate for each stream for true overlap)\n",
    "    float* d_data[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaMalloc(&d_data[i], CHUNK_BYTES);\n",
    "    }\n",
    "    \n",
    "    // Create streams\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Initialize input\n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // VERSION 1: No Overlap (Baseline)\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        int offset = i * CHUNK_SIZE;\n",
    "        \n",
    "        // All in default stream - sequential!\n",
    "        cudaMemcpy(d_data[i], h_input + offset, CHUNK_BYTES, \n",
    "                   cudaMemcpyHostToDevice);\n",
    "        process<<<(CHUNK_SIZE+255)/256, 256>>>(d_data[i], CHUNK_SIZE);\n",
    "        cudaMemcpy(h_output + offset, d_data[i], CHUNK_BYTES, \n",
    "                   cudaMemcpyDeviceToHost);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float noOverlapTime;\n",
    "    cudaEventElapsedTime(&noOverlapTime, start, stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // VERSION 2: With Overlap\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Issue all operations for all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        int offset = i * CHUNK_SIZE;\n",
    "        \n",
    "        cudaMemcpyAsync(d_data[i], h_input + offset, CHUNK_BYTES,\n",
    "                        cudaMemcpyHostToDevice, streams[i]);\n",
    "        \n",
    "        process<<<(CHUNK_SIZE+255)/256, 256, 0, streams[i]>>>(\n",
    "            d_data[i], CHUNK_SIZE);\n",
    "        \n",
    "        cudaMemcpyAsync(h_output + offset, d_data[i], CHUNK_BYTES,\n",
    "                        cudaMemcpyDeviceToHost, streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Synchronize all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float overlapTime;\n",
    "    cudaEventElapsedTime(&overlapTime, start, stop);\n",
    "    \n",
    "    printf(\"Without overlap: %.2f ms\\n\", noOverlapTime);\n",
    "    printf(\"With overlap:    %.2f ms\\n\", overlapTime);\n",
    "    printf(\"Speedup:         %.2fx\\n\", noOverlapTime / overlapTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    cudaFreeHost(h_input);\n",
    "    cudaFreeHost(h_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o overlap_pattern overlap_pattern.cu\n",
    "!./overlap_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87196ba",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72208105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Async Transfer Demo (OPTIONAL)\n",
    "\n",
    "@cuda.jit\n",
    "def process(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        val = data[tid]\n",
    "        for _ in range(100):\n",
    "            val = val * 1.001 + 0.001\n",
    "        data[tid] = val\n",
    "\n",
    "n = 1 << 22\n",
    "num_streams = 4\n",
    "chunk = n // num_streams\n",
    "\n",
    "# Create streams\n",
    "streams = [cuda.stream() for _ in range(num_streams)]\n",
    "\n",
    "# Pinned host arrays\n",
    "h_input = cuda.pinned_array(n, dtype=np.float32)\n",
    "h_output = cuda.pinned_array(n, dtype=np.float32)\n",
    "h_input[:] = 1.0\n",
    "\n",
    "# Device arrays (one per stream)\n",
    "d_chunks = [cuda.device_array(chunk, dtype=np.float32) for _ in range(num_streams)]\n",
    "\n",
    "# Launch overlapped work\n",
    "start = time.time()\n",
    "\n",
    "for i in range(num_streams):\n",
    "    offset = i * chunk\n",
    "    \n",
    "    # Async copy H2D\n",
    "    d_chunks[i].copy_to_device(h_input[offset:offset+chunk], stream=streams[i])\n",
    "    \n",
    "    # Kernel\n",
    "    process[(chunk+255)//256, 256, streams[i]](d_chunks[i])\n",
    "    \n",
    "    # Async copy D2H\n",
    "    d_chunks[i].copy_to_host(h_output[offset:offset+chunk], stream=streams[i])\n",
    "\n",
    "# Sync all\n",
    "for s in streams:\n",
    "    s.synchronize()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Overlapped time: {elapsed*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ebce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Profiling Overlap with Nsight Systems\n",
    "\n",
    "```bash\n",
    "# Profile to see overlap\n",
    "nsys profile -o overlap_timeline ./overlap_pattern\n",
    "\n",
    "# View in Nsight Systems GUI\n",
    "nsys-ui overlap_timeline.nsys-rep\n",
    "```\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "```\n",
    "Nsight Systems Timeline:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "                    Without Overlap          With Overlap\n",
    "                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Copy Engine H2D:    ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë\n",
    "Compute:            ‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë        ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "Copy Engine D2H:    ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà\n",
    "                    ‚Üë Sequential!            ‚Üë Overlapped!\n",
    "\n",
    "Look for:\n",
    "‚Ä¢ Parallel bars in different rows = overlap achieved\n",
    "‚Ä¢ Gaps between operations = potential improvement\n",
    "‚Ä¢ Copy/compute at same time = good utilization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4ef94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00657b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile overlap_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            val = sinf(val) * cosf(val) + 0.1f;\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Pinned Memory Benchmark\n",
    "// ============================================================\n",
    "\n",
    "void exercise1_pinnedVsPageable() {\n",
    "    printf(\"=== Exercise 1: Pinned vs Pageable Memory ===\\n\");\n",
    "    printf(\"%-12s %-15s %-15s %-10s\\n\", \"Size\", \"Pageable(ms)\", \"Pinned(ms)\", \"Speedup\");\n",
    "    printf(\"--------------------------------------------------\\n\");\n",
    "    \n",
    "    int sizes[] = {1<<20, 1<<22, 1<<24, 1<<26};  // 1MB to 64MB\n",
    "    int numSizes = sizeof(sizes) / sizeof(sizes[0]);\n",
    "    \n",
    "    for (int s = 0; s < numSizes; s++) {\n",
    "        int n = sizes[s];\n",
    "        size_t bytes = n * sizeof(float);\n",
    "        \n",
    "        float *h_pageable = (float*)malloc(bytes);\n",
    "        float *h_pinned;\n",
    "        float *d_data;\n",
    "        \n",
    "        CHECK_CUDA(cudaMallocHost(&h_pinned, bytes));\n",
    "        CHECK_CUDA(cudaMalloc(&d_data, bytes));\n",
    "        \n",
    "        cudaEvent_t start, stop;\n",
    "        cudaEventCreate(&start);\n",
    "        cudaEventCreate(&stop);\n",
    "        \n",
    "        // Pageable transfer\n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < 10; i++) {\n",
    "            cudaMemcpy(d_data, h_pageable, bytes, cudaMemcpyHostToDevice);\n",
    "            cudaMemcpy(h_pageable, d_data, bytes, cudaMemcpyDeviceToHost);\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        float pageableMs;\n",
    "        cudaEventElapsedTime(&pageableMs, start, stop);\n",
    "        \n",
    "        // Pinned transfer\n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < 10; i++) {\n",
    "            cudaMemcpy(d_data, h_pinned, bytes, cudaMemcpyHostToDevice);\n",
    "            cudaMemcpy(h_pinned, d_data, bytes, cudaMemcpyDeviceToHost);\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        float pinnedMs;\n",
    "        cudaEventElapsedTime(&pinnedMs, start, stop);\n",
    "        \n",
    "        printf(\"%-12.1f MB %-15.2f %-15.2f %.2fx\\n\", \n",
    "               bytes / (1024.0f * 1024.0f), pageableMs, pinnedMs, pageableMs / pinnedMs);\n",
    "        \n",
    "        free(h_pageable);\n",
    "        cudaFreeHost(h_pinned);\n",
    "        cudaFree(d_data);\n",
    "        cudaEventDestroy(start);\n",
    "        cudaEventDestroy(stop);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Find Optimal Chunk Count\n",
    "// ============================================================\n",
    "\n",
    "void exercise2_optimalChunks() {\n",
    "    printf(\"=== Exercise 2: Optimal Chunk Count ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 24;  // 16M elements\n",
    "    size_t bytes = n * sizeof(float);\n",
    "    \n",
    "    float *h_data;\n",
    "    float *d_data;\n",
    "    CHECK_CUDA(cudaMallocHost(&h_data, bytes));\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, bytes));\n",
    "    \n",
    "    for (int i = 0; i < n; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int chunkCounts[] = {1, 2, 4, 8, 16, 32};\n",
    "    int numTests = sizeof(chunkCounts) / sizeof(chunkCounts[0]);\n",
    "    \n",
    "    printf(\"%-10s %-15s\\n\", \"Chunks\", \"Time (ms)\");\n",
    "    printf(\"---------------------------\\n\");\n",
    "    \n",
    "    float bestTime = 1e9;\n",
    "    int bestChunks = 1;\n",
    "    \n",
    "    for (int t = 0; t < numTests; t++) {\n",
    "        int numChunks = chunkCounts[t];\n",
    "        int chunkSize = n / numChunks;\n",
    "        size_t chunkBytes = chunkSize * sizeof(float);\n",
    "        \n",
    "        cudaStream_t* streams = (cudaStream_t*)malloc(numChunks * sizeof(cudaStream_t));\n",
    "        for (int i = 0; i < numChunks; i++) {\n",
    "            cudaStreamCreate(&streams[i]);\n",
    "        }\n",
    "        \n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < numChunks; i++) {\n",
    "            int offset = i * chunkSize;\n",
    "            cudaMemcpyAsync(d_data + offset, h_data + offset, chunkBytes, \n",
    "                           cudaMemcpyHostToDevice, streams[i]);\n",
    "            processKernel<<<(chunkSize+255)/256, 256, 0, streams[i]>>>(d_data + offset, chunkSize);\n",
    "            cudaMemcpyAsync(h_data + offset, d_data + offset, chunkBytes,\n",
    "                           cudaMemcpyDeviceToHost, streams[i]);\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        printf(\"%-10d %-15.2f\", numChunks, ms);\n",
    "        \n",
    "        if (ms < bestTime) {\n",
    "            bestTime = ms;\n",
    "            bestChunks = numChunks;\n",
    "            printf(\" *\");\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "        \n",
    "        for (int i = 0; i < numChunks; i++) {\n",
    "            cudaStreamDestroy(streams[i]);\n",
    "        }\n",
    "        free(streams);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\nOptimal: %d chunks (%.2f ms)\\n\\n\", bestChunks, bestTime);\n",
    "    \n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Double Buffering\n",
    "// ============================================================\n",
    "\n",
    "void exercise3_doubleBuffering() {\n",
    "    printf(\"=== Exercise 3: Double Buffering ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 24;  // 16M elements\n",
    "    const int numChunks = 8;\n",
    "    const int chunkSize = n / numChunks;\n",
    "    size_t chunkBytes = chunkSize * sizeof(float);\n",
    "    \n",
    "    // Host memory\n",
    "    float *h_input, *h_output;\n",
    "    CHECK_CUDA(cudaMallocHost(&h_input, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMallocHost(&h_output, n * sizeof(float)));\n",
    "    \n",
    "    for (int i = 0; i < n; i++) h_input[i] = 1.0f;\n",
    "    \n",
    "    // Device memory - only 2 buffers for double buffering\n",
    "    float *d_buffer[2];\n",
    "    CHECK_CUDA(cudaMalloc(&d_buffer[0], chunkBytes));\n",
    "    CHECK_CUDA(cudaMalloc(&d_buffer[1], chunkBytes));\n",
    "    \n",
    "    cudaStream_t streams[2];\n",
    "    cudaStreamCreate(&streams[0]);\n",
    "    cudaStreamCreate(&streams[1]);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Process first chunk\n",
    "    cudaMemcpyAsync(d_buffer[0], h_input, chunkBytes, cudaMemcpyHostToDevice, streams[0]);\n",
    "    \n",
    "    for (int chunk = 0; chunk < numChunks; chunk++) {\n",
    "        int curr = chunk % 2;\n",
    "        int next = (chunk + 1) % 2;\n",
    "        \n",
    "        // Launch kernel on current chunk\n",
    "        processKernel<<<(chunkSize+255)/256, 256, 0, streams[curr]>>>(d_buffer[curr], chunkSize);\n",
    "        \n",
    "        // Start transfer of next chunk (if not last)\n",
    "        if (chunk < numChunks - 1) {\n",
    "            int nextOffset = (chunk + 1) * chunkSize;\n",
    "            cudaMemcpyAsync(d_buffer[next], h_input + nextOffset, chunkBytes,\n",
    "                           cudaMemcpyHostToDevice, streams[next]);\n",
    "        }\n",
    "        \n",
    "        // Transfer current result back\n",
    "        int currOffset = chunk * chunkSize;\n",
    "        cudaMemcpyAsync(h_output + currOffset, d_buffer[curr], chunkBytes,\n",
    "                       cudaMemcpyDeviceToHost, streams[curr]);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    printf(\"Double buffering with %d chunks: %.2f ms\\n\", numChunks, ms);\n",
    "    printf(\"Device memory used: 2 buffers (%.1f MB each)\\n\", chunkBytes / (1024.0f * 1024.0f));\n",
    "    printf(\"Total data processed: %.1f MB\\n\\n\", n * sizeof(float) / (1024.0f * 1024.0f));\n",
    "    \n",
    "    cudaStreamDestroy(streams[0]);\n",
    "    cudaStreamDestroy(streams[1]);\n",
    "    cudaFree(d_buffer[0]);\n",
    "    cudaFree(d_buffer[1]);\n",
    "    cudaFreeHost(h_input);\n",
    "    cudaFreeHost(h_output);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\");\n",
    "    printf(\"‚ïë           Overlap Transfers Exercises                        ‚ïë\\n\");\n",
    "    printf(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Async Engines: %d\\n\\n\", prop.asyncEngineCount);\n",
    "    \n",
    "    exercise1_pinnedVsPageable();\n",
    "    exercise2_optimalChunks();\n",
    "    exercise3_doubleBuffering();\n",
    "    \n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb249dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o overlap_exercises overlap_exercises.cu && ./overlap_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522c087",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Pinned Memory Benchmark\n",
    "Compare transfer speeds for various sizes with pageable vs pinned memory.\n",
    "\n",
    "### Exercise 2: Find Optimal Chunk Count\n",
    "Test with 2, 4, 8, 16 streams and find the sweet spot for your GPU.\n",
    "\n",
    "### Exercise 3: Double Buffering\n",
    "```cpp\n",
    "// Implement double buffering:\n",
    "// - While processing chunk N, transfer chunk N+1\n",
    "// - Use only 2 device buffers, alternate between them\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12138ea3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              OVERLAPPING TRANSFERS                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Pinned Memory:                                         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ cudaMallocHost() / cudaFreeHost()                    ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Required for async transfers                         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Faster than pageable (no staging)                    ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Async Copies:                                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ cudaMemcpyAsync(dst, src, size, kind, stream)        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Returns immediately to CPU                           ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Operations in same stream execute in order           ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Overlap Pattern:                                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Split data into chunks                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Each chunk in different stream                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ H2D, compute, D2H can all overlap                    ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Next: Day 3 - Multi-Stream Execution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
