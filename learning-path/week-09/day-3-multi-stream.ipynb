{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1d233",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Concurrent Kernel Execution\n",
    "\n",
    "### When Can Kernels Run Concurrently?\n",
    "\n",
    "```\n",
    "Requirements for Concurrent Kernels:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "1. Different streams\n",
    "2. Enough GPU resources (SMs, registers, shared memory)\n",
    "3. No dependencies between kernels\n",
    "4. Device supports concurrent kernels (check capability)\n",
    "\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                      GPU SMs                            │\n",
    "├─────────────┬─────────────┬─────────────┬──────────────┤\n",
    "│    SM 0     │    SM 1     │    SM 2     │    SM 3      │\n",
    "├─────────────┼─────────────┼─────────────┼──────────────┤\n",
    "│ Kernel A    │ Kernel A    │ Kernel B    │ Kernel B     │\n",
    "│ blocks      │ blocks      │ blocks      │ blocks       │\n",
    "└─────────────┴─────────────┴─────────────┴──────────────┘\n",
    "         ↑ Multiple kernels share the GPU!\n",
    "```\n",
    "\n",
    "### CUDA C++ Concurrent Kernels (Primary)\n",
    "\n",
    "```cpp\n",
    "// concurrent_kernels.cu - Running multiple kernels at once\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Small kernel - uses few resources\n",
    "__global__ void smallKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data[tid] = sinf(data[tid]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 16;  // Small size so kernels fit together\n",
    "    const int NUM_STREAMS = 4;\n",
    "    \n",
    "    // Check concurrent kernel support\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Concurrent kernels: %s\\n\", \n",
    "           prop.concurrentKernels ? \"Yes\" : \"No\");\n",
    "    printf(\"Max concurrent kernels: ~%d (estimate)\\n\",\n",
    "           prop.multiProcessorCount);  // Rough estimate\n",
    "    \n",
    "    // Allocate\n",
    "    float* d_data[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaMalloc(&d_data[i], N * sizeof(float));\n",
    "    }\n",
    "    \n",
    "    // Create streams\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Sequential Execution (single stream)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        // All in default stream\n",
    "        smallKernel<<<64, 256>>>(d_data[i], N);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float seqTime;\n",
    "    cudaEventElapsedTime(&seqTime, start, stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Concurrent Execution (multiple streams)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        // Each in its own stream - can run concurrently!\n",
    "        smallKernel<<<64, 256, 0, streams[i]>>>(d_data[i], N);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float concTime;\n",
    "    cudaEventElapsedTime(&concTime, start, stop);\n",
    "    \n",
    "    printf(\"\\nResults:\\n\");\n",
    "    printf(\"Sequential: %.2f ms\\n\", seqTime);\n",
    "    printf(\"Concurrent: %.2f ms\\n\", concTime);\n",
    "    printf(\"Speedup:    %.2fx\\n\", seqTime / concTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2f338",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Stream Priorities\n",
    "\n",
    "### CUDA C++ Stream Priorities (Primary)\n",
    "\n",
    "```cpp\n",
    "// stream_priorities.cu - Prioritizing work\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void work(float* data, int n, const char* name) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data[tid] = sqrtf(data[tid] + 1.0f);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // ============================================\n",
    "    // Query Priority Range\n",
    "    // ============================================\n",
    "    int lowPriority, highPriority;\n",
    "    cudaDeviceGetStreamPriorityRange(&lowPriority, &highPriority);\n",
    "    \n",
    "    printf(\"Priority range: %d (low/default) to %d (high)\\n\",\n",
    "           lowPriority, highPriority);\n",
    "    // Note: LOWER number = HIGHER priority!\n",
    "    \n",
    "    // ============================================\n",
    "    // Create Prioritized Streams\n",
    "    // ============================================\n",
    "    cudaStream_t highPriorityStream, lowPriorityStream;\n",
    "    \n",
    "    cudaStreamCreateWithPriority(&highPriorityStream, \n",
    "                                  cudaStreamNonBlocking, \n",
    "                                  highPriority);\n",
    "    \n",
    "    cudaStreamCreateWithPriority(&lowPriorityStream, \n",
    "                                  cudaStreamNonBlocking, \n",
    "                                  lowPriority);\n",
    "    \n",
    "    const int N = 1 << 16;\n",
    "    float *d_high, *d_low;\n",
    "    cudaMalloc(&d_high, N * sizeof(float));\n",
    "    cudaMalloc(&d_low, N * sizeof(float));\n",
    "    \n",
    "    // Launch many low-priority kernels\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        work<<<64, 256, 0, lowPriorityStream>>>(d_low, N, \"LOW\");\n",
    "    }\n",
    "    \n",
    "    // Launch high-priority kernel (should get resources first)\n",
    "    work<<<64, 256, 0, highPriorityStream>>>(d_high, N, \"HIGH\");\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"High priority work completed!\\n\");\n",
    "    \n",
    "    cudaStreamDestroy(highPriorityStream);\n",
    "    cudaStreamDestroy(lowPriorityStream);\n",
    "    cudaFree(d_high);\n",
    "    cudaFree(d_low);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Priority Use Cases\n",
    "\n",
    "```\n",
    "When to Use Stream Priorities:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. Interactive Applications\n",
    "   └─ High: UI rendering, user input processing\n",
    "   └─ Low: Background computation, prefetching\n",
    "\n",
    "2. Real-time Systems  \n",
    "   └─ High: Critical path operations\n",
    "   └─ Low: Optional/deferrable work\n",
    "\n",
    "3. Multi-tenant GPU\n",
    "   └─ High: Latency-sensitive workloads\n",
    "   └─ Low: Throughput-oriented batch jobs\n",
    "\n",
    "Limitations:\n",
    "• Priority only affects scheduling at SM level\n",
    "• Doesn't preempt running kernels\n",
    "• Effect depends on resource availability\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2bd862",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Designing Multi-Stream Applications\n",
    "\n",
    "### Pattern: Producer-Consumer Streams\n",
    "\n",
    "```cpp\n",
    "// producer_consumer.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void produce(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = (float)tid;  // Generate data\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void consume(float* data, float* result, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        result[tid] = data[tid] * 2.0f;  // Process data\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const int NUM_CHUNKS = 4;\n",
    "    const int CHUNK = N / NUM_CHUNKS;\n",
    "    \n",
    "    float *d_data, *d_result;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMalloc(&d_result, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t producerStream, consumerStream;\n",
    "    cudaStreamCreate(&producerStream);\n",
    "    cudaStreamCreate(&consumerStream);\n",
    "    \n",
    "    cudaEvent_t chunkReady[NUM_CHUNKS];\n",
    "    for (int i = 0; i < NUM_CHUNKS; i++) {\n",
    "        cudaEventCreate(&chunkReady[i]);\n",
    "    }\n",
    "    \n",
    "    // Pipeline: Producer creates data, consumer processes it\n",
    "    for (int i = 0; i < NUM_CHUNKS; i++) {\n",
    "        // Producer generates chunk i\n",
    "        produce<<<(CHUNK+255)/256, 256, 0, producerStream>>>(\n",
    "            d_data + i*CHUNK, CHUNK);\n",
    "        \n",
    "        // Mark chunk as ready\n",
    "        cudaEventRecord(chunkReady[i], producerStream);\n",
    "        \n",
    "        // Consumer waits for chunk, then processes\n",
    "        cudaStreamWaitEvent(consumerStream, chunkReady[i]);\n",
    "        consume<<<(CHUNK+255)/256, 256, 0, consumerStream>>>(\n",
    "            d_data + i*CHUNK, d_result + i*CHUNK, CHUNK);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Producer-consumer pipeline complete!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_CHUNKS; i++) {\n",
    "        cudaEventDestroy(chunkReady[i]);\n",
    "    }\n",
    "    cudaStreamDestroy(producerStream);\n",
    "    cudaStreamDestroy(consumerStream);\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Multi-Stream Demo (OPTIONAL)\n",
    "\n",
    "@cuda.jit\n",
    "def kernel_a(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        for _ in range(500):\n",
    "            data[tid] = data[tid] * 1.0001\n",
    "\n",
    "@cuda.jit\n",
    "def kernel_b(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        for _ in range(500):\n",
    "            data[tid] = data[tid] + 0.0001\n",
    "\n",
    "n = 1 << 18\n",
    "num_streams = 4\n",
    "\n",
    "# Create streams and data\n",
    "streams = [cuda.stream() for _ in range(num_streams)]\n",
    "d_arrays = [cuda.device_array(n, dtype=np.float32) for _ in range(num_streams)]\n",
    "\n",
    "# Sequential (baseline)\n",
    "start = time.time()\n",
    "for i in range(num_streams):\n",
    "    kernel_a[(n+255)//256, 256](d_arrays[i])\n",
    "cuda.synchronize()\n",
    "seq_time = time.time() - start\n",
    "\n",
    "# Concurrent (multi-stream)\n",
    "start = time.time()\n",
    "for i in range(num_streams):\n",
    "    kernel_a[(n+255)//256, 256, streams[i]](d_arrays[i])\n",
    "for s in streams:\n",
    "    s.synchronize()\n",
    "conc_time = time.time() - start\n",
    "\n",
    "print(f\"Sequential: {seq_time*1000:.2f} ms\")\n",
    "print(f\"Concurrent: {conc_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {seq_time/conc_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eca302",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Find Concurrency Limit\n",
    "Experimentally determine how many small kernels can run concurrently on your GPU.\n",
    "\n",
    "### Exercise 2: Priority Scheduling\n",
    "Create a scenario where high-priority work completes noticeably faster.\n",
    "\n",
    "### Exercise 3: Multi-Stage Pipeline\n",
    "```cpp\n",
    "// Implement a 3-stage pipeline:\n",
    "// Stage 1: Load and preprocess\n",
    "// Stage 2: Main computation\n",
    "// Stage 3: Post-process and store\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99d2b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│            MULTI-STREAM EXECUTION                       │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Concurrent Kernels:                                    │\n",
    "│  • Different streams can run simultaneously             │\n",
    "│  • Limited by GPU resources (SMs, memory)               │\n",
    "│  • Small kernels benefit most                           │\n",
    "│                                                         │\n",
    "│  Stream Priorities:                                     │\n",
    "│  • cudaDeviceGetStreamPriorityRange()                   │\n",
    "│  • cudaStreamCreateWithPriority()                       │\n",
    "│  • Lower number = higher priority                       │\n",
    "│                                                         │\n",
    "│  Design Patterns:                                       │\n",
    "│  • Producer-consumer with events                        │\n",
    "│  • Pipeline stages in different streams                 │\n",
    "│  • Workload splitting for concurrency                   │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 4 - Events & Synchronization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
