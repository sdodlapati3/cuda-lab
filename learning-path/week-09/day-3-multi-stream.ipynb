{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1d233",
   "metadata": {},
   "source": [
    "# Day 3: Multi-Stream Execution - Parallel Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Hook: The Airport Terminal Analogy\n",
    "\n",
    "**Ever noticed** how airports handle thousands of flights?\n",
    "\n",
    "They don't use one runway for EVERYTHING - landing, takeoff, taxiing. They have:\n",
    "- Multiple runways for parallel takeoffs/landings\n",
    "- Multiple gates processing passengers simultaneously\n",
    "- Baggage handling running independently\n",
    "\n",
    "```\n",
    "One-Runway Airport (Single Stream):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  [Flight 1 Land][Taxi][Unload] â†’ [Flight 2 Land][Taxi]...     â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Throughput: 12 flights/hour ğŸ˜´                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Multi-Runway Airport (Multi-Stream):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Runway 1: [Flight 1 Land] [Flight 4 Land] [Flight 7 Land]    â”‚\n",
    "â”‚  Runway 2: [Flight 2 Land] [Flight 5 Land] [Flight 8 Land]    â”‚\n",
    "â”‚  Runway 3: [Flight 3 Land] [Flight 6 Land] [Flight 9 Land]    â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Throughput: 36 flights/hour! ğŸš€                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Your GPU has many SMs** - small kernels might only use a fraction. Fill the rest with MORE kernels!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Launch concurrent kernels** in multiple streams\n",
    "2. **Understand** when kernels can run simultaneously (resource limits)\n",
    "3. **Use stream priorities** to ensure critical work completes first\n",
    "4. **Design** multi-stream pipelines for maximum throughput\n",
    "5. **Balance** the number of streams for optimal performance\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸƒ Concept Card: Multiple Parallel Pipelines\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  ğŸš€ MULTI-STREAM = MULTIPLE PARALLEL PIPELINES                   â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ANALOGY: Grocery Store Checkout Lines                           â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  One Line (Single Stream):         Multiple Lines (Multi-Stream):â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘\n",
    "â•‘  â”‚ ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ â†’ [ğŸ’³]   â”‚        â”‚ ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ â†’ [ğŸ’³] Line 1 â”‚    â•‘\n",
    "â•‘  â”‚   Everyone waits!      â”‚        â”‚ ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ â†’ [ğŸ’³] Line 2 â”‚    â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ â†’ [ğŸ’³] Line 3 â”‚    â•‘\n",
    "â•‘                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  GPU Multi-Stream Execution:                                     â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚         SM0    SM1    SM2    SM3    SM4    SM5             â”‚  â•‘\n",
    "â•‘  â”‚  Stream0: â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ                                      â”‚  â•‘\n",
    "â•‘  â”‚  Stream1:              â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ                         â”‚  â•‘\n",
    "â•‘  â”‚  Stream2:                          â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ             â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘     Each stream gets a slice of the GPU - all run at once!       â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ’¡ KEY INSIGHT: Small kernels leave GPU resources unused.       â•‘\n",
    "â•‘     Fill those resources with concurrent work from other streams!â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  WHEN IT HELPS:                                                  â•‘\n",
    "â•‘  âœ“ Kernels don't fully occupy GPU                                â•‘\n",
    "â•‘  âœ“ Independent workloads available                               â•‘\n",
    "â•‘  âœ“ Mix of small and large kernels                                â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Concurrent Kernel Execution\n",
    "\n",
    "### When Can Kernels Run Concurrently?\n",
    "\n",
    "```\n",
    "Requirements for Concurrent Kernels:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "1. Different streams\n",
    "2. Enough GPU resources (SMs, registers, shared memory)\n",
    "3. No dependencies between kernels\n",
    "4. Device supports concurrent kernels (check capability)\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      GPU SMs                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚    SM 0     â”‚    SM 1     â”‚    SM 2     â”‚    SM 3      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Kernel A    â”‚ Kernel A    â”‚ Kernel B    â”‚ Kernel B     â”‚\n",
    "â”‚ blocks      â”‚ blocks      â”‚ blocks      â”‚ blocks       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†‘ Multiple kernels share the GPU!\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates running multiple kernels concurrently by placing them in different streams, and compares sequential vs concurrent execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile concurrent_kernels.cu\n",
    "// concurrent_kernels.cu - Running multiple kernels at once\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Small kernel - uses few resources\n",
    "__global__ void smallKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data[tid] = sinf(data[tid]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 16;  // Small size so kernels fit together\n",
    "    const int NUM_STREAMS = 4;\n",
    "    \n",
    "    // Check concurrent kernel support\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Concurrent kernels: %s\\n\", \n",
    "           prop.concurrentKernels ? \"Yes\" : \"No\");\n",
    "    printf(\"Max concurrent kernels: ~%d (estimate)\\n\",\n",
    "           prop.multiProcessorCount);  // Rough estimate\n",
    "    \n",
    "    // Allocate\n",
    "    float* d_data[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaMalloc(&d_data[i], N * sizeof(float));\n",
    "    }\n",
    "    \n",
    "    // Create streams\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Sequential Execution (single stream)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        // All in default stream\n",
    "        smallKernel<<<64, 256>>>(d_data[i], N);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float seqTime;\n",
    "    cudaEventElapsedTime(&seqTime, start, stop);\n",
    "    \n",
    "    // ============================================\n",
    "    // Concurrent Execution (multiple streams)\n",
    "    // ============================================\n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        // Each in its own stream - can run concurrently!\n",
    "        smallKernel<<<64, 256, 0, streams[i]>>>(d_data[i], N);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float concTime;\n",
    "    cudaEventElapsedTime(&concTime, start, stop);\n",
    "    \n",
    "    printf(\"\\nResults:\\n\");\n",
    "    printf(\"Sequential: %.2f ms\\n\", seqTime);\n",
    "    printf(\"Concurrent: %.2f ms\\n\", concTime);\n",
    "    printf(\"Speedup:    %.2fx\\n\", seqTime / concTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dacdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o concurrent_kernels concurrent_kernels.cu\n",
    "!./concurrent_kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2f338",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Stream Priorities\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates how to create prioritized streams where high-priority work gets GPU resources first. Note: LOWER number = HIGHER priority!\n",
    "\n",
    "### Priority Use Cases\n",
    "\n",
    "```\n",
    "When to Use Stream Priorities:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. Interactive Applications\n",
    "   â””â”€ High: UI rendering, user input processing\n",
    "   â””â”€ Low: Background computation, prefetching\n",
    "\n",
    "2. Real-time Systems  \n",
    "   â””â”€ High: Critical path operations\n",
    "   â””â”€ Low: Optional/deferrable work\n",
    "\n",
    "3. Multi-tenant GPU\n",
    "   â””â”€ High: Latency-sensitive workloads\n",
    "   â””â”€ Low: Throughput-oriented batch jobs\n",
    "\n",
    "Limitations:\n",
    "â€¢ Priority only affects scheduling at SM level\n",
    "â€¢ Doesn't preempt running kernels\n",
    "â€¢ Effect depends on resource availability\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_priorities.cu\n",
    "// stream_priorities.cu - Prioritizing work\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void work(float* data, int n, const char* name) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data[tid] = sqrtf(data[tid] + 1.0f);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // ============================================\n",
    "    // Query Priority Range\n",
    "    // ============================================\n",
    "    int lowPriority, highPriority;\n",
    "    cudaDeviceGetStreamPriorityRange(&lowPriority, &highPriority);\n",
    "    \n",
    "    printf(\"Priority range: %d (low/default) to %d (high)\\n\",\n",
    "           lowPriority, highPriority);\n",
    "    // Note: LOWER number = HIGHER priority!\n",
    "    \n",
    "    // ============================================\n",
    "    // Create Prioritized Streams\n",
    "    // ============================================\n",
    "    cudaStream_t highPriorityStream, lowPriorityStream;\n",
    "    \n",
    "    cudaStreamCreateWithPriority(&highPriorityStream, \n",
    "                                  cudaStreamNonBlocking, \n",
    "                                  highPriority);\n",
    "    \n",
    "    cudaStreamCreateWithPriority(&lowPriorityStream, \n",
    "                                  cudaStreamNonBlocking, \n",
    "                                  lowPriority);\n",
    "    \n",
    "    const int N = 1 << 16;\n",
    "    float *d_high, *d_low;\n",
    "    cudaMalloc(&d_high, N * sizeof(float));\n",
    "    cudaMalloc(&d_low, N * sizeof(float));\n",
    "    \n",
    "    // Launch many low-priority kernels\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        work<<<64, 256, 0, lowPriorityStream>>>(d_low, N, \"LOW\");\n",
    "    }\n",
    "    \n",
    "    // Launch high-priority kernel (should get resources first)\n",
    "    work<<<64, 256, 0, highPriorityStream>>>(d_high, N, \"HIGH\");\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"High priority work completed!\\n\");\n",
    "    \n",
    "    cudaStreamDestroy(highPriorityStream);\n",
    "    cudaStreamDestroy(lowPriorityStream);\n",
    "    cudaFree(d_high);\n",
    "    cudaFree(d_low);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o stream_priorities stream_priorities.cu\n",
    "!./stream_priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2bd862",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Designing Multi-Stream Applications\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "**Pattern: Producer-Consumer Streams**\n",
    "\n",
    "This example demonstrates a producer-consumer pattern where:\n",
    "- A producer stream generates data chunks\n",
    "- Events mark when each chunk is ready  \n",
    "- A consumer stream waits for each event before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile producer_consumer.cu\n",
    "// producer_consumer.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void produce(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = (float)tid;  // Generate data\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void consume(float* data, float* result, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        result[tid] = data[tid] * 2.0f;  // Process data\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const int NUM_CHUNKS = 4;\n",
    "    const int CHUNK = N / NUM_CHUNKS;\n",
    "    \n",
    "    float *d_data, *d_result;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMalloc(&d_result, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t producerStream, consumerStream;\n",
    "    cudaStreamCreate(&producerStream);\n",
    "    cudaStreamCreate(&consumerStream);\n",
    "    \n",
    "    cudaEvent_t chunkReady[NUM_CHUNKS];\n",
    "    for (int i = 0; i < NUM_CHUNKS; i++) {\n",
    "        cudaEventCreate(&chunkReady[i]);\n",
    "    }\n",
    "    \n",
    "    // Pipeline: Producer creates data, consumer processes it\n",
    "    for (int i = 0; i < NUM_CHUNKS; i++) {\n",
    "        // Producer generates chunk i\n",
    "        produce<<<(CHUNK+255)/256, 256, 0, producerStream>>>(\n",
    "            d_data + i*CHUNK, CHUNK);\n",
    "        \n",
    "        // Mark chunk as ready\n",
    "        cudaEventRecord(chunkReady[i], producerStream);\n",
    "        \n",
    "        // Consumer waits for chunk, then processes\n",
    "        cudaStreamWaitEvent(consumerStream, chunkReady[i]);\n",
    "        consume<<<(CHUNK+255)/256, 256, 0, consumerStream>>>(\n",
    "            d_data + i*CHUNK, d_result + i*CHUNK, CHUNK);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Producer-consumer pipeline complete!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_CHUNKS; i++) {\n",
    "        cudaEventDestroy(chunkReady[i]);\n",
    "    }\n",
    "    cudaStreamDestroy(producerStream);\n",
    "    cudaStreamDestroy(consumerStream);\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o producer_consumer producer_consumer.cu\n",
    "!./producer_consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c9459",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Multi-Stream Demo (OPTIONAL)\n",
    "\n",
    "@cuda.jit\n",
    "def kernel_a(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        for _ in range(500):\n",
    "            data[tid] = data[tid] * 1.0001\n",
    "\n",
    "@cuda.jit\n",
    "def kernel_b(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        for _ in range(500):\n",
    "            data[tid] = data[tid] + 0.0001\n",
    "\n",
    "n = 1 << 18\n",
    "num_streams = 4\n",
    "\n",
    "# Create streams and data\n",
    "streams = [cuda.stream() for _ in range(num_streams)]\n",
    "d_arrays = [cuda.device_array(n, dtype=np.float32) for _ in range(num_streams)]\n",
    "\n",
    "# Sequential (baseline)\n",
    "start = time.time()\n",
    "for i in range(num_streams):\n",
    "    kernel_a[(n+255)//256, 256](d_arrays[i])\n",
    "cuda.synchronize()\n",
    "seq_time = time.time() - start\n",
    "\n",
    "# Concurrent (multi-stream)\n",
    "start = time.time()\n",
    "for i in range(num_streams):\n",
    "    kernel_a[(n+255)//256, 256, streams[i]](d_arrays[i])\n",
    "for s in streams:\n",
    "    s.synchronize()\n",
    "conc_time = time.time() - start\n",
    "\n",
    "print(f\"Sequential: {seq_time*1000:.2f} ms\")\n",
    "print(f\"Concurrent: {conc_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {seq_time/conc_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eca302",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afa3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multistream_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Small kernel for concurrency testing\n",
    "__global__ void smallKernel(float* data, int n, int iters) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < iters; i++) {\n",
    "            val = sinf(val) + 0.1f;\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Longer kernel\n",
    "__global__ void longKernel(float* data, int n, int iters) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < iters; i++) {\n",
    "            val = sinf(val) * cosf(val) + 0.001f;\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Find Concurrency Limit\n",
    "// ============================================================\n",
    "\n",
    "void exercise1_concurrencyLimit() {\n",
    "    printf(\"=== Exercise 1: Find Concurrency Limit ===\\n\");\n",
    "    \n",
    "    const int n = 1024;  // Small work per kernel\n",
    "    const int iters = 100;\n",
    "    \n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int testCounts[] = {1, 2, 4, 8, 16, 32, 64, 128};\n",
    "    int numTests = sizeof(testCounts) / sizeof(testCounts[0]);\n",
    "    \n",
    "    printf(\"%-15s %-15s %-15s\\n\", \"Num Kernels\", \"Total Time\", \"Time/Kernel\");\n",
    "    printf(\"----------------------------------------------\\n\");\n",
    "    \n",
    "    float singleKernelTime = 0;\n",
    "    \n",
    "    for (int t = 0; t < numTests; t++) {\n",
    "        int numKernels = testCounts[t];\n",
    "        \n",
    "        cudaStream_t* streams = (cudaStream_t*)malloc(numKernels * sizeof(cudaStream_t));\n",
    "        for (int i = 0; i < numKernels; i++) {\n",
    "            cudaStreamCreate(&streams[i]);\n",
    "        }\n",
    "        \n",
    "        // Warmup\n",
    "        for (int i = 0; i < numKernels; i++) {\n",
    "            smallKernel<<<1, 256, 0, streams[i]>>>(d_data, n, iters);\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < numKernels; i++) {\n",
    "            smallKernel<<<1, 256, 0, streams[i]>>>(d_data, n, iters);\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        \n",
    "        if (numKernels == 1) singleKernelTime = ms;\n",
    "        \n",
    "        float idealTime = singleKernelTime;  // Fully concurrent\n",
    "        float efficiency = idealTime / ms * 100.0f;\n",
    "        \n",
    "        printf(\"%-15d %-12.3f ms %-12.3f ms (%.0f%% efficient)\\n\", \n",
    "               numKernels, ms, ms / numKernels, efficiency);\n",
    "        \n",
    "        for (int i = 0; i < numKernels; i++) {\n",
    "            cudaStreamDestroy(streams[i]);\n",
    "        }\n",
    "        free(streams);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Priority Scheduling\n",
    "// ============================================================\n",
    "\n",
    "void exercise2_priorityScheduling() {\n",
    "    printf(\"=== Exercise 2: Priority Scheduling ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 16;\n",
    "    const int longIters = 1000;\n",
    "    const int shortIters = 50;\n",
    "    \n",
    "    int leastPriority, greatestPriority;\n",
    "    cudaDeviceGetStreamPriorityRange(&leastPriority, &greatestPriority);\n",
    "    printf(\"Priority range: %d (least) to %d (greatest)\\n\", leastPriority, greatestPriority);\n",
    "    \n",
    "    if (leastPriority == greatestPriority) {\n",
    "        printf(\"Stream priorities not supported on this device!\\n\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    float *d_data1, *d_data2;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data1, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_data2, n * sizeof(float)));\n",
    "    \n",
    "    cudaStream_t lowPriorityStream, highPriorityStream;\n",
    "    cudaStreamCreateWithPriority(&lowPriorityStream, cudaStreamNonBlocking, leastPriority);\n",
    "    cudaStreamCreateWithPriority(&highPriorityStream, cudaStreamNonBlocking, greatestPriority);\n",
    "    \n",
    "    cudaEvent_t startLow, stopLow, startHigh, stopHigh;\n",
    "    cudaEventCreate(&startLow);\n",
    "    cudaEventCreate(&stopLow);\n",
    "    cudaEventCreate(&startHigh);\n",
    "    cudaEventCreate(&stopHigh);\n",
    "    \n",
    "    int grid = (n + 255) / 256;\n",
    "    \n",
    "    // Launch long-running low-priority kernel\n",
    "    cudaEventRecord(startLow, lowPriorityStream);\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        longKernel<<<grid, 256, 0, lowPriorityStream>>>(d_data1, n, longIters);\n",
    "    }\n",
    "    cudaEventRecord(stopLow, lowPriorityStream);\n",
    "    \n",
    "    // Small delay, then launch high-priority kernel\n",
    "    cudaEventRecord(startHigh, highPriorityStream);\n",
    "    smallKernel<<<grid, 256, 0, highPriorityStream>>>(d_data2, n, shortIters);\n",
    "    cudaEventRecord(stopHigh, highPriorityStream);\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    float lowMs, highMs;\n",
    "    cudaEventElapsedTime(&lowMs, startLow, stopLow);\n",
    "    cudaEventElapsedTime(&highMs, startHigh, stopHigh);\n",
    "    \n",
    "    printf(\"Low priority work:  %.2f ms\\n\", lowMs);\n",
    "    printf(\"High priority work: %.2f ms\\n\", highMs);\n",
    "    printf(\"High priority completed while low priority was running!\\n\\n\");\n",
    "    \n",
    "    cudaStreamDestroy(lowPriorityStream);\n",
    "    cudaStreamDestroy(highPriorityStream);\n",
    "    cudaEventDestroy(startLow);\n",
    "    cudaEventDestroy(stopLow);\n",
    "    cudaEventDestroy(startHigh);\n",
    "    cudaEventDestroy(stopHigh);\n",
    "    cudaFree(d_data1);\n",
    "    cudaFree(d_data2);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Multi-Stage Pipeline\n",
    "// ============================================================\n",
    "\n",
    "__global__ void stage1_preprocess(const float* input, float* output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        output[idx] = fabsf(input[idx]) + 1.0f;  // Ensure positive\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void stage2_compute(const float* input, float* output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = input[idx];\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            val = sqrtf(val) + logf(val + 1.0f);\n",
    "        }\n",
    "        output[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void stage3_postprocess(const float* input, float* output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        output[idx] = tanhf(input[idx]);  // Normalize to [-1, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise3_multiStagePipeline() {\n",
    "    printf(\"=== Exercise 3: Multi-Stage Pipeline ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 20;  // 1M elements\n",
    "    const int numChunks = 4;\n",
    "    const int chunkSize = n / numChunks;\n",
    "    \n",
    "    // Allocate buffers for pipeline stages\n",
    "    float *d_input, *d_stage1, *d_stage2, *d_output;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_stage1, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_stage2, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, n * sizeof(float)));\n",
    "    \n",
    "    // Create streams for each pipeline stage\n",
    "    cudaStream_t streams[3];\n",
    "    for (int i = 0; i < 3; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Events for synchronization between stages\n",
    "    cudaEvent_t stage1Done[numChunks], stage2Done[numChunks];\n",
    "    for (int i = 0; i < numChunks; i++) {\n",
    "        cudaEventCreate(&stage1Done[i]);\n",
    "        cudaEventCreate(&stage2Done[i]);\n",
    "    }\n",
    "    \n",
    "    int grid = (chunkSize + 255) / 256;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int chunk = 0; chunk < numChunks; chunk++) {\n",
    "        int offset = chunk * chunkSize;\n",
    "        \n",
    "        // Stage 1: Preprocess\n",
    "        if (chunk > 0) {\n",
    "            // Wait for previous stage1 to complete before reusing stage1 buffer slot\n",
    "            cudaStreamWaitEvent(streams[0], stage1Done[(chunk - 1) % 2]);\n",
    "        }\n",
    "        stage1_preprocess<<<grid, 256, 0, streams[0]>>>(\n",
    "            d_input + offset, d_stage1 + offset, chunkSize);\n",
    "        cudaEventRecord(stage1Done[chunk % 2], streams[0]);\n",
    "        \n",
    "        // Stage 2: Main computation (waits for stage 1)\n",
    "        cudaStreamWaitEvent(streams[1], stage1Done[chunk % 2]);\n",
    "        stage2_compute<<<grid, 256, 0, streams[1]>>>(\n",
    "            d_stage1 + offset, d_stage2 + offset, chunkSize);\n",
    "        cudaEventRecord(stage2Done[chunk % 2], streams[1]);\n",
    "        \n",
    "        // Stage 3: Postprocess (waits for stage 2)\n",
    "        cudaStreamWaitEvent(streams[2], stage2Done[chunk % 2]);\n",
    "        stage3_postprocess<<<grid, 256, 0, streams[2]>>>(\n",
    "            d_stage2 + offset, d_output + offset, chunkSize);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    printf(\"3-stage pipeline with %d chunks: %.2f ms\\n\", numChunks, ms);\n",
    "    printf(\"Stages: preprocess -> compute -> postprocess\\n\\n\");\n",
    "    \n",
    "    for (int i = 0; i < 3; i++) cudaStreamDestroy(streams[i]);\n",
    "    for (int i = 0; i < numChunks; i++) {\n",
    "        cudaEventDestroy(stage1Done[i]);\n",
    "        cudaEventDestroy(stage2Done[i]);\n",
    "    }\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_stage1);\n",
    "    cudaFree(d_stage2);\n",
    "    cudaFree(d_output);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘           Multi-Stream Exercises                             â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Concurrent Kernels: %s\\n\", prop.concurrentKernels ? \"Yes\" : \"No\");\n",
    "    printf(\"Max Threads/SM: %d\\n\\n\", prop.maxThreadsPerMultiProcessor);\n",
    "    \n",
    "    exercise1_concurrencyLimit();\n",
    "    exercise2_priorityScheduling();\n",
    "    exercise3_multiStagePipeline();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o multistream_exercises multistream_exercises.cu && ./multistream_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a8ee8",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Find Concurrency Limit\n",
    "Experimentally determine how many small kernels can run concurrently on your GPU.\n",
    "\n",
    "### Exercise 2: Priority Scheduling\n",
    "Create a scenario where high-priority work completes noticeably faster.\n",
    "\n",
    "### Exercise 3: Multi-Stage Pipeline\n",
    "```cpp\n",
    "// Implement a 3-stage pipeline:\n",
    "// Stage 1: Load and preprocess\n",
    "// Stage 2: Main computation\n",
    "// Stage 3: Post-process and store\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99d2b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘            MULTI-STREAM EXECUTION MASTERY                        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸš€ Core Concept: PARALLEL PIPELINES                             â•‘\n",
    "â•‘     Multiple checkout lines - everyone gets served faster!       â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ“‹ Essential API:                                               â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  cudaDeviceGetStreamPriorityRange(&low, &high)             â”‚  â•‘\n",
    "â•‘  â”‚  cudaStreamCreateWithPriority(&s, flags, priority)         â”‚  â•‘\n",
    "â•‘  â”‚  // Lower number = higher priority!                        â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ”¢ Concurrency Limits:                                          â•‘\n",
    "â•‘     â€¢ Max concurrent kernels: typically 16-128                   â•‘\n",
    "â•‘     â€¢ Limited by: SMs, registers, shared memory                  â•‘\n",
    "â•‘     â€¢ Check: deviceQuery for your GPU's limits                   â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  âš ï¸  When Multi-Stream DOESN'T Help:                             â•‘\n",
    "â•‘     â€¢ Kernel already fills GPU (no room for more)               â•‘\n",
    "â•‘     â€¢ Memory bandwidth saturated                                 â•‘\n",
    "â•‘     â€¢ Operations have dependencies                               â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ“Š Design Patterns:                                             â•‘\n",
    "â•‘     Producer-Consumer: Stream A produces, Stream B consumes      â•‘\n",
    "â•‘     Pipeline Stages:   Stage 1 â†’ Stage 2 â†’ Stage 3 in parallel   â•‘\n",
    "â•‘     Work Splitting:    Divide large task across streams          â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® What's Next?\n",
    "\n",
    "**Day 4: Events & Synchronization** - Fine-grained control!\n",
    "\n",
    "Streams work great for independent work, but what about dependencies? Tomorrow you'll learn:\n",
    "\n",
    "```\n",
    "Today:                    Tomorrow (Day 4):\n",
    "Independent streams       Streams with dependencies\n",
    "\n",
    "[Stream 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]      [Stream 1: â–ˆâ–ˆâ–ˆâ–ˆ]â”€eventâ”€â”\n",
    "[Stream 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]      [Stream 2: wait for event]â†’[â–ˆâ–ˆâ–ˆâ–ˆ]\n",
    "                                    â†‘\n",
    "                          \"Don't start until Stream 1 reaches here!\"\n",
    "```\n",
    "\n",
    "Events are your stopwatches AND synchronization checkpoints! â±ï¸"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
