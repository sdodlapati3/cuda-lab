{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9139e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What are CUDA Streams?\n",
    "\n",
    "### The Concept\n",
    "\n",
    "```\n",
    "STREAM = A sequence of operations that execute in order\n",
    "\n",
    "Without Streams (Sequential):          With Streams (Concurrent):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━           ━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "┌──────────┐                           Stream 0: ┌──────────┐\n",
    "│ H2D Copy │                                     │ H2D Copy │───┐\n",
    "└────┬─────┘                           Stream 1: └──────────┘   │\n",
    "     ↓                                           ┌──────────┐   │\n",
    "┌──────────┐                                     │ Kernel A │←──┤\n",
    "│ Kernel A │                           Stream 0: └──────────┘   │\n",
    "└────┬─────┘                                     ┌──────────┐   │\n",
    "     ↓                                           │ Kernel B │←──┘\n",
    "┌──────────┐                           Stream 1: └──────────┘\n",
    "│ D2H Copy │                                     ┌──────────┐\n",
    "└──────────┘                                     │ D2H Copy │\n",
    "                                                 └──────────┘\n",
    "Time: ████████████████████             Time: ████████████\n",
    "```\n",
    "\n",
    "### Why Streams Matter\n",
    "\n",
    "```\n",
    "GPU Hardware has multiple engines:\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Copy Engine (H2D)  │  Compute Engine  │  Copy Engine (D2H)  │\n",
    "│  ─────────────────  │  ───────────────  │  ─────────────────  │\n",
    "│  Can run while      │  Can run while   │  Can run while     │\n",
    "│  compute runs!      │  copying!        │  compute runs!     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "\n",
    "Streams let you USE all engines simultaneously!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451171e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Stream Creation and Management\n",
    "\n",
    "### CUDA C++ Stream Basics (Primary)\n",
    "\n",
    "The following code demonstrates four different methods for working with CUDA streams:\n",
    "1. **Default Stream** - Sequential operations that wait for all previous work\n",
    "2. **Non-Default Streams** - Created streams that can run concurrently\n",
    "3. **Stream with Flags** - Non-blocking streams that don't sync with default stream\n",
    "4. **Per-Thread Default Stream** - Compile-time option for thread-local default streams\n",
    "\n",
    "### Compile and Run\n",
    "```bash\n",
    "nvcc -O3 -arch=sm_75 stream_basics.cu -o stream_basics\n",
    "./stream_basics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_basics.cu\n",
    "// stream_basics.cu - Creating and using streams\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void simpleKernel(float* data, int n, int streamId) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        // Simulate some work\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            data[tid] = sqrtf(data[tid]) + 1.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;  // 1M elements\n",
    "    const int NUM_STREAMS = 4;\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 1: Default Stream (NULL stream)\n",
    "    // ============================================\n",
    "    // Operations in default stream are BLOCKING\n",
    "    // They wait for ALL previous operations to complete\n",
    "    \n",
    "    float *d_default;\n",
    "    cudaMalloc(&d_default, N * sizeof(float));\n",
    "    \n",
    "    // These execute sequentially (default stream)\n",
    "    simpleKernel<<<256, 256>>>(d_default, N, 0);  // Stream 0 (default)\n",
    "    simpleKernel<<<256, 256>>>(d_default, N, 0);  // Waits for above\n",
    "    \n",
    "    cudaDeviceSynchronize();  // Wait for all\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 2: Create Non-Default Streams\n",
    "    // ============================================\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    float* d_data[NUM_STREAMS];\n",
    "    \n",
    "    // Create streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "        cudaMalloc(&d_data[i], N * sizeof(float));\n",
    "    }\n",
    "    \n",
    "    // Launch kernels in different streams (can run concurrently!)\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        // Syntax: kernel<<<grid, block, sharedMem, stream>>>\n",
    "        simpleKernel<<<256, 256, 0, streams[i]>>>(d_data[i], N, i);\n",
    "    }\n",
    "    \n",
    "    // Synchronize all streams\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 3: Stream with Flags\n",
    "    // ============================================\n",
    "    cudaStream_t nonBlockingStream;\n",
    "    \n",
    "    // cudaStreamNonBlocking: doesn't sync with default stream\n",
    "    cudaStreamCreateWithFlags(&nonBlockingStream, cudaStreamNonBlocking);\n",
    "    \n",
    "    simpleKernel<<<256, 256, 0, nonBlockingStream>>>(d_default, N, 0);\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 4: Per-Thread Default Stream\n",
    "    // ============================================\n",
    "    // Compile with: nvcc --default-stream per-thread\n",
    "    // Makes each CPU thread have its own default stream\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    cudaStreamDestroy(nonBlockingStream);\n",
    "    cudaFree(d_default);\n",
    "    \n",
    "    printf(\"Stream basics complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effa827",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o stream_basics stream_basics.cu\n",
    "!./stream_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad0a09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Default Stream Behavior\n",
    "\n",
    "### Legacy vs Per-Thread Default Stream\n",
    "\n",
    "This example demonstrates the difference between:\n",
    "- **Legacy Default Stream** - Acts as a synchronization barrier between all streams\n",
    "- **Non-Blocking Streams** - Use `cudaStreamNonBlocking` flag to avoid synchronization with the default stream\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "Legacy Default Stream (--default-stream legacy):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Stream1:    ████████│wait│        │████████│\n",
    "Default:            │    │████████│        │\n",
    "                    ↑    ↑        ↑\n",
    "              sync points (implicit barriers)\n",
    "\n",
    "Per-Thread Default (--default-stream per-thread):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Stream1:    ████████████████\n",
    "Default:    ████████\n",
    "            (can overlap - each thread has own default)\n",
    "\n",
    "Non-Blocking Flag (cudaStreamNonBlocking):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "NonBlock:   ████████████████████████\n",
    "Default:    ████████\n",
    "            (no sync between them)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75643dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_stream_behavior.cu\n",
    "// default_stream_behavior.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void work(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data[tid] = sinf(data[tid]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 18;\n",
    "    float *d_a, *d_b;\n",
    "    cudaMalloc(&d_a, N * sizeof(float));\n",
    "    cudaMalloc(&d_b, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream1;\n",
    "    cudaStreamCreate(&stream1);\n",
    "    \n",
    "    // ============================================\n",
    "    // LEGACY DEFAULT STREAM (blocking)\n",
    "    // ============================================\n",
    "    // Compile with: nvcc file.cu (default)\n",
    "    \n",
    "    printf(\"Testing with legacy default stream...\\n\");\n",
    "    \n",
    "    work<<<256, 256, 0, stream1>>>(d_a, N);  // Stream 1\n",
    "    work<<<256, 256>>>(d_b, N);               // Default stream\n",
    "    work<<<256, 256, 0, stream1>>>(d_a, N);  // Stream 1\n",
    "    \n",
    "    // Execution order with legacy default stream:\n",
    "    // 1. First stream1 kernel starts\n",
    "    // 2. Default stream kernel WAITS for stream1 to finish\n",
    "    // 3. Second stream1 kernel WAITS for default stream\n",
    "    // Result: NO OVERLAP (all sequential)\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // ============================================\n",
    "    // NON-BLOCKING STREAM (no sync with default)\n",
    "    // ============================================\n",
    "    cudaStream_t nonBlocking;\n",
    "    cudaStreamCreateWithFlags(&nonBlocking, cudaStreamNonBlocking);\n",
    "    \n",
    "    printf(\"Testing with non-blocking stream...\\n\");\n",
    "    \n",
    "    work<<<256, 256, 0, nonBlocking>>>(d_a, N);  // Non-blocking\n",
    "    work<<<256, 256>>>(d_b, N);                   // Default stream\n",
    "    work<<<256, 256, 0, nonBlocking>>>(d_a, N);  // Non-blocking\n",
    "    \n",
    "    // Execution: All three can potentially overlap!\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(nonBlocking);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    \n",
    "    printf(\"Default stream behavior demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o default_stream_behavior default_stream_behavior.cu\n",
    "!./default_stream_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58292dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Stream Example (OPTIONAL)\n",
    "\n",
    "@cuda.jit\n",
    "def simple_work(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        val = data[tid]\n",
    "        for _ in range(100):\n",
    "            val = val * 1.001 + 0.001\n",
    "        data[tid] = val\n",
    "\n",
    "n = 1 << 20\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "\n",
    "# Create streams\n",
    "stream1 = cuda.stream()\n",
    "stream2 = cuda.stream()\n",
    "\n",
    "# Allocate data\n",
    "d_a = cuda.device_array(n, dtype=np.float32)\n",
    "d_b = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "# Launch in different streams\n",
    "simple_work[blocks, threads, stream1](d_a)\n",
    "simple_work[blocks, threads, stream2](d_b)\n",
    "\n",
    "# Synchronize\n",
    "stream1.synchronize()\n",
    "stream2.synchronize()\n",
    "\n",
    "print(\"Both kernels completed (potentially overlapped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd65cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Stream Synchronization\n",
    "\n",
    "### Synchronization Methods\n",
    "\n",
    "This example demonstrates different ways to synchronize with CUDA streams:\n",
    "1. **cudaDeviceSynchronize()** - Waits for ALL streams (heavy-weight)\n",
    "2. **cudaStreamSynchronize(stream)** - Waits for specific stream only\n",
    "3. **cudaStreamQuery(stream)** - Non-blocking check if stream is done\n",
    "4. **Callbacks** - Run host function when stream reaches point (deprecated in favor of graphs)\n",
    "\n",
    "### Synchronization Comparison\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│           Synchronization Methods                       │\n",
    "├──────────────────────┬──────────────────────────────────┤\n",
    "│ Method               │ Use Case                         │\n",
    "├──────────────────────┼──────────────────────────────────┤\n",
    "│ cudaDeviceSynchronize│ End of program, debugging        │\n",
    "│ cudaStreamSynchronize│ Wait for specific stream         │\n",
    "│ cudaStreamQuery      │ Poll without blocking            │\n",
    "│ cudaEventSynchronize │ Wait for specific point (Day 4)  │\n",
    "│ cudaStreamWaitEvent  │ Inter-stream dependency (Day 4)  │\n",
    "└──────────────────────┴──────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412844d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_sync.cu\n",
    "// stream_sync.cu - Different synchronization approaches\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] *= 2.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 1: cudaDeviceSynchronize()\n",
    "    // ============================================\n",
    "    // Waits for ALL streams on the device\n",
    "    // Heavy-weight, use sparingly\n",
    "    \n",
    "    kernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();  // Blocks until ALL work done\n",
    "    printf(\"Method 1: cudaDeviceSynchronize() - waited for all work\\n\");\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 2: cudaStreamSynchronize(stream)\n",
    "    // ============================================\n",
    "    // Waits for specific stream only\n",
    "    // Lighter weight than device sync\n",
    "    \n",
    "    kernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    cudaStreamSynchronize(stream);  // Wait for this stream only\n",
    "    printf(\"Method 2: cudaStreamSynchronize() - waited for specific stream\\n\");\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 3: cudaStreamQuery(stream)\n",
    "    // ============================================\n",
    "    // Non-blocking check if stream is done\n",
    "    \n",
    "    kernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    \n",
    "    cudaError_t status;\n",
    "    int pollCount = 0;\n",
    "    do {\n",
    "        status = cudaStreamQuery(stream);\n",
    "        pollCount++;\n",
    "        // Can do CPU work here while waiting!\n",
    "    } while (status == cudaErrorNotReady);\n",
    "    \n",
    "    if (status == cudaSuccess) {\n",
    "        printf(\"Method 3: cudaStreamQuery() - polled %d times before completion\\n\", pollCount);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 4: Callback (deprecated in favor of graphs)\n",
    "    // ============================================\n",
    "    // cudaStreamAddCallback() - runs host function when stream reaches point\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    printf(\"Stream synchronization demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba01088",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o stream_sync stream_sync.cu\n",
    "!./stream_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bca89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Multi-Stream Kernel Launch\n",
    "\n",
    "```cpp\n",
    "// Launch 8 independent kernels in 4 streams\n",
    "// Measure time with vs without streams\n",
    "\n",
    "// Your implementation:\n",
    "```\n",
    "\n",
    "### Exercise 2: Stream Query Loop\n",
    "\n",
    "```cpp\n",
    "// While GPU is working, do CPU computation\n",
    "// Use cudaStreamQuery to check completion\n",
    "\n",
    "// Your implementation:\n",
    "```\n",
    "\n",
    "### Exercise 3: Non-Blocking vs Blocking\n",
    "\n",
    "```cpp\n",
    "// Compare behavior with:\n",
    "// 1. Regular stream (cudaStreamCreate)\n",
    "// 2. Non-blocking stream (cudaStreamNonBlocking)\n",
    "\n",
    "// Your implementation:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebae4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   STREAM BASICS                         │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Stream = sequence of GPU operations                    │\n",
    "│                                                         │\n",
    "│  Key Functions:                                         │\n",
    "│  • cudaStreamCreate(&stream)                            │\n",
    "│  • cudaStreamDestroy(stream)                            │\n",
    "│  • cudaStreamSynchronize(stream)                        │\n",
    "│  • kernel<<<grid, block, smem, stream>>>()              │\n",
    "│                                                         │\n",
    "│  Default Stream Gotcha:                                 │\n",
    "│  • Legacy: blocks other streams                         │\n",
    "│  • Use cudaStreamNonBlocking for independence           │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 2 - Overlapping Data Transfers\n",
    "\n",
    "Tomorrow we'll learn to overlap H2D/D2H transfers with computation using pinned memory!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
