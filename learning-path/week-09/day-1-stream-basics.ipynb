{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9139e3",
   "metadata": {},
   "source": [
    "# Day 1: Stream Basics - The Assembly Line\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Hook: Why Work One-at-a-Time When You Can Parallel?\n",
    "\n",
    "**Think about it**: You're running a bakery. Would you:\n",
    "- A) Bake one cookie, wait, ice it, wait, box it, then start the next?\n",
    "- B) While cookies bake, ice yesterday's batch, and box what's already iced?\n",
    "\n",
    "**Option B is obvious!** But most GPU code does Option A:\n",
    "\n",
    "```\n",
    "Your typical GPU code:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Copy data    â†’    Run kernel    â†’    Copy results      â”‚\n",
    "â”‚   to GPU              â³                from GPU         â”‚\n",
    "â”‚    wait...          wait...            wait...          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Total time = copy1 + compute + copy2  â† All sequential!\n",
    "\n",
    "What streams enable:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stream 1: [Copy A] â†’ [Compute A] â†’ [Copy A back]       â”‚\n",
    "â”‚  Stream 2:      [Copy B] â†’ [Compute B] â†’ [Copy B back]  â”‚\n",
    "â”‚  Stream 3:           [Copy C] â†’ [Compute C] â†’ [Copy C]  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Operations OVERLAP - total time shrinks dramatically!\n",
    "```\n",
    "\n",
    "**Today, you'll learn to run GPU operations like an assembly line** - always keeping every part of the GPU busy!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Explain** what CUDA streams are and why they matter for performance\n",
    "2. **Create and destroy** streams using `cudaStreamCreate()` and `cudaStreamDestroy()`\n",
    "3. **Launch kernels** in specific streams to enable concurrent execution\n",
    "4. **Synchronize** streams properly using `cudaStreamSynchronize()` and `cudaDeviceSynchronize()`\n",
    "5. **Understand** the difference between blocking and non-blocking streams\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸƒ Concept Card: The Assembly Line\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  ğŸ­ CUDA STREAMS = ASSEMBLY LINE STATIONS                        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ANALOGY: Car Manufacturing Assembly Line                        â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  Station 1 (Copy Engine H2D):   Station 2 (Compute):             â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â•‘\n",
    "â•‘  â”‚  \"Receiving Dock\"       â”‚    â”‚  \"Assembly Floor\"       â”‚      â•‘\n",
    "â•‘  â”‚  Parts arrive           â”‚ â†’  â”‚  Build the car          â”‚      â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â•‘\n",
    "â•‘                                          â†“                       â•‘\n",
    "â•‘                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â•‘\n",
    "â•‘                                 â”‚  \"Shipping Dock\"        â”‚      â•‘\n",
    "â•‘                                 â”‚  Station 3 (Copy D2H)   â”‚      â•‘\n",
    "â•‘                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  WITHOUT STREAMS:                                                â•‘\n",
    "â•‘  Car 1: [Receive]â”€[Build]â”€[Ship]                                â•‘\n",
    "â•‘  Car 2:                     [Receive]â”€[Build]â”€[Ship]            â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  WITH STREAMS (multiple cars in pipeline):                       â•‘\n",
    "â•‘  Car 1: [Receive]â”€[Build]â”€[Ship]                                â•‘\n",
    "â•‘  Car 2:      [Receive]â”€[Build]â”€[Ship]                           â•‘\n",
    "â•‘  Car 3:           [Receive]â”€[Build]â”€[Ship]                      â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ’¡ KEY INSIGHT: Different \"stations\" (GPU engines) can work    â•‘\n",
    "â•‘     on different items simultaneously!                           â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: What are CUDA Streams?\n",
    "\n",
    "### The Concept\n",
    "\n",
    "```\n",
    "STREAM = A sequence of operations that execute in order\n",
    "\n",
    "Without Streams (Sequential):          With Streams (Concurrent):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”           â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           Stream 0: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ H2D Copy â”‚                                     â”‚ H2D Copy â”‚â”€â”€â”€â”\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                           Stream 1: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "     â†“                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚ Kernel A â”‚â†â”€â”€â”¤\n",
    "â”‚ Kernel A â”‚                           Stream 0: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "     â†“                                           â”‚ Kernel B â”‚â†â”€â”€â”˜\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           Stream 1: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚ D2H Copy â”‚                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚ D2H Copy â”‚\n",
    "                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Time: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             Time: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "```\n",
    "\n",
    "### Why Streams Matter\n",
    "\n",
    "```\n",
    "GPU Hardware has multiple engines:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Copy Engine (H2D)  â”‚  Compute Engine  â”‚  Copy Engine (D2H)  â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
    "â”‚  Can run while      â”‚  Can run while   â”‚  Can run while     â”‚\n",
    "â”‚  compute runs!      â”‚  copying!        â”‚  compute runs!     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Streams let you USE all engines simultaneously!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451171e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Stream Creation and Management\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "The following code demonstrates four different methods for working with CUDA streams:\n",
    "1. **Default Stream** - Sequential operations that wait for all previous work\n",
    "2. **Non-Default Streams** - Created streams that can run concurrently\n",
    "3. **Stream with Flags** - Non-blocking streams that don't sync with default stream\n",
    "4. **Per-Thread Default Stream** - Compile-time option for thread-local default streams\n",
    "\n",
    "### Compile and Run\n",
    "```bash\n",
    "nvcc -O3 -arch=sm_75 stream_basics.cu -o stream_basics\n",
    "./stream_basics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_basics.cu\n",
    "// stream_basics.cu - Creating and using streams\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void simpleKernel(float* data, int n, int streamId) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        // Simulate some work\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            data[tid] = sqrtf(data[tid]) + 1.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;  // 1M elements\n",
    "    const int NUM_STREAMS = 4;\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 1: Default Stream (NULL stream)\n",
    "    // ============================================\n",
    "    // Operations in default stream are BLOCKING\n",
    "    // They wait for ALL previous operations to complete\n",
    "    \n",
    "    float *d_default;\n",
    "    cudaMalloc(&d_default, N * sizeof(float));\n",
    "    \n",
    "    // These execute sequentially (default stream)\n",
    "    simpleKernel<<<256, 256>>>(d_default, N, 0);  // Stream 0 (default)\n",
    "    simpleKernel<<<256, 256>>>(d_default, N, 0);  // Waits for above\n",
    "    \n",
    "    cudaDeviceSynchronize();  // Wait for all\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 2: Create Non-Default Streams\n",
    "    // ============================================\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    float* d_data[NUM_STREAMS];\n",
    "    \n",
    "    // Create streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "        cudaMalloc(&d_data[i], N * sizeof(float));\n",
    "    }\n",
    "    \n",
    "    // Launch kernels in different streams (can run concurrently!)\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        // Syntax: kernel<<<grid, block, sharedMem, stream>>>\n",
    "        simpleKernel<<<256, 256, 0, streams[i]>>>(d_data[i], N, i);\n",
    "    }\n",
    "    \n",
    "    // Synchronize all streams\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 3: Stream with Flags\n",
    "    // ============================================\n",
    "    cudaStream_t nonBlockingStream;\n",
    "    \n",
    "    // cudaStreamNonBlocking: doesn't sync with default stream\n",
    "    cudaStreamCreateWithFlags(&nonBlockingStream, cudaStreamNonBlocking);\n",
    "    \n",
    "    simpleKernel<<<256, 256, 0, nonBlockingStream>>>(d_default, N, 0);\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 4: Per-Thread Default Stream\n",
    "    // ============================================\n",
    "    // Compile with: nvcc --default-stream per-thread\n",
    "    // Makes each CPU thread have its own default stream\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    cudaStreamDestroy(nonBlockingStream);\n",
    "    cudaFree(d_default);\n",
    "    \n",
    "    printf(\"Stream basics complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effa827",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o stream_basics stream_basics.cu\n",
    "!./stream_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad0a09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Default Stream Behavior\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates the difference between:\n",
    "- **Legacy Default Stream** - Acts as a synchronization barrier between all streams\n",
    "- **Non-Blocking Streams** - Use `cudaStreamNonBlocking` flag to avoid synchronization with the default stream\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "Legacy Default Stream (--default-stream legacy):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Stream1:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚waitâ”‚        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚\n",
    "Default:            â”‚    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚        â”‚\n",
    "                    â†‘    â†‘        â†‘\n",
    "              sync points (implicit barriers)\n",
    "\n",
    "Per-Thread Default (--default-stream per-thread):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Stream1:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "Default:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "            (can overlap - each thread has own default)\n",
    "\n",
    "Non-Blocking Flag (cudaStreamNonBlocking):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "NonBlock:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "Default:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "            (no sync between them)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75643dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_stream_behavior.cu\n",
    "// default_stream_behavior.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void work(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data[tid] = sinf(data[tid]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 18;\n",
    "    float *d_a, *d_b;\n",
    "    cudaMalloc(&d_a, N * sizeof(float));\n",
    "    cudaMalloc(&d_b, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream1;\n",
    "    cudaStreamCreate(&stream1);\n",
    "    \n",
    "    // ============================================\n",
    "    // LEGACY DEFAULT STREAM (blocking)\n",
    "    // ============================================\n",
    "    // Compile with: nvcc file.cu (default)\n",
    "    \n",
    "    printf(\"Testing with legacy default stream...\\n\");\n",
    "    \n",
    "    work<<<256, 256, 0, stream1>>>(d_a, N);  // Stream 1\n",
    "    work<<<256, 256>>>(d_b, N);               // Default stream\n",
    "    work<<<256, 256, 0, stream1>>>(d_a, N);  // Stream 1\n",
    "    \n",
    "    // Execution order with legacy default stream:\n",
    "    // 1. First stream1 kernel starts\n",
    "    // 2. Default stream kernel WAITS for stream1 to finish\n",
    "    // 3. Second stream1 kernel WAITS for default stream\n",
    "    // Result: NO OVERLAP (all sequential)\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // ============================================\n",
    "    // NON-BLOCKING STREAM (no sync with default)\n",
    "    // ============================================\n",
    "    cudaStream_t nonBlocking;\n",
    "    cudaStreamCreateWithFlags(&nonBlocking, cudaStreamNonBlocking);\n",
    "    \n",
    "    printf(\"Testing with non-blocking stream...\\n\");\n",
    "    \n",
    "    work<<<256, 256, 0, nonBlocking>>>(d_a, N);  // Non-blocking\n",
    "    work<<<256, 256>>>(d_b, N);                   // Default stream\n",
    "    work<<<256, 256, 0, nonBlocking>>>(d_a, N);  // Non-blocking\n",
    "    \n",
    "    // Execution: All three can potentially overlap!\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(nonBlocking);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    \n",
    "    printf(\"Default stream behavior demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o default_stream_behavior default_stream_behavior.cu\n",
    "!./default_stream_behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8fa408",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58292dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Stream Example (OPTIONAL)\n",
    "\n",
    "@cuda.jit\n",
    "def simple_work(data):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < data.shape[0]:\n",
    "        val = data[tid]\n",
    "        for _ in range(100):\n",
    "            val = val * 1.001 + 0.001\n",
    "        data[tid] = val\n",
    "\n",
    "n = 1 << 20\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "\n",
    "# Create streams\n",
    "stream1 = cuda.stream()\n",
    "stream2 = cuda.stream()\n",
    "\n",
    "# Allocate data\n",
    "d_a = cuda.device_array(n, dtype=np.float32)\n",
    "d_b = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "# Launch in different streams\n",
    "simple_work[blocks, threads, stream1](d_a)\n",
    "simple_work[blocks, threads, stream2](d_b)\n",
    "\n",
    "# Synchronize\n",
    "stream1.synchronize()\n",
    "stream2.synchronize()\n",
    "\n",
    "print(\"Both kernels completed (potentially overlapped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd65cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Stream Synchronization\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "This example demonstrates different ways to synchronize with CUDA streams:\n",
    "1. **cudaDeviceSynchronize()** - Waits for ALL streams (heavy-weight)\n",
    "2. **cudaStreamSynchronize(stream)** - Waits for specific stream only\n",
    "3. **cudaStreamQuery(stream)** - Non-blocking check if stream is done\n",
    "4. **Callbacks** - Run host function when stream reaches point (deprecated in favor of graphs)\n",
    "\n",
    "### Synchronization Comparison\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Synchronization Methods                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Method               â”‚ Use Case                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ cudaDeviceSynchronizeâ”‚ End of program, debugging        â”‚\n",
    "â”‚ cudaStreamSynchronizeâ”‚ Wait for specific stream         â”‚\n",
    "â”‚ cudaStreamQuery      â”‚ Poll without blocking            â”‚\n",
    "â”‚ cudaEventSynchronize â”‚ Wait for specific point (Day 4)  â”‚\n",
    "â”‚ cudaStreamWaitEvent  â”‚ Inter-stream dependency (Day 4)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412844d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_sync.cu\n",
    "// stream_sync.cu - Different synchronization approaches\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) data[tid] *= 2.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 1: cudaDeviceSynchronize()\n",
    "    // ============================================\n",
    "    // Waits for ALL streams on the device\n",
    "    // Heavy-weight, use sparingly\n",
    "    \n",
    "    kernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();  // Blocks until ALL work done\n",
    "    printf(\"Method 1: cudaDeviceSynchronize() - waited for all work\\n\");\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 2: cudaStreamSynchronize(stream)\n",
    "    // ============================================\n",
    "    // Waits for specific stream only\n",
    "    // Lighter weight than device sync\n",
    "    \n",
    "    kernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    cudaStreamSynchronize(stream);  // Wait for this stream only\n",
    "    printf(\"Method 2: cudaStreamSynchronize() - waited for specific stream\\n\");\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 3: cudaStreamQuery(stream)\n",
    "    // ============================================\n",
    "    // Non-blocking check if stream is done\n",
    "    \n",
    "    kernel<<<256, 256, 0, stream>>>(d_data, N);\n",
    "    \n",
    "    cudaError_t status;\n",
    "    int pollCount = 0;\n",
    "    do {\n",
    "        status = cudaStreamQuery(stream);\n",
    "        pollCount++;\n",
    "        // Can do CPU work here while waiting!\n",
    "    } while (status == cudaErrorNotReady);\n",
    "    \n",
    "    if (status == cudaSuccess) {\n",
    "        printf(\"Method 3: cudaStreamQuery() - polled %d times before completion\\n\", pollCount);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Method 4: Callback (deprecated in favor of graphs)\n",
    "    // ============================================\n",
    "    // cudaStreamAddCallback() - runs host function when stream reaches point\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    printf(\"Stream synchronization demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba01088",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o stream_sync stream_sync.cu\n",
    "!./stream_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bca89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stream_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Simple compute kernel\n",
    "__global__ void computeKernel(float* data, int n, int iterations) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < iterations; i++) {\n",
    "            val = sinf(val) * cosf(val) + 0.1f;\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Multi-Stream Kernel Launch\n",
    "// ============================================================\n",
    "\n",
    "void exercise1_multiStreamLaunch() {\n",
    "    printf(\"=== Exercise 1: Multi-Stream Kernel Launch ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 20;  // 1M elements\n",
    "    const int numKernels = 8;\n",
    "    const int numStreams = 4;\n",
    "    const int iterations = 100;\n",
    "    \n",
    "    float* d_data[numKernels];\n",
    "    cudaStream_t streams[numStreams];\n",
    "    \n",
    "    for (int i = 0; i < numStreams; i++) {\n",
    "        CHECK_CUDA(cudaStreamCreate(&streams[i]));\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < numKernels; i++) {\n",
    "        CHECK_CUDA(cudaMalloc(&d_data[i], n * sizeof(float)));\n",
    "    }\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Without streams (default stream - serialized)\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < numKernels; i++) {\n",
    "        computeKernel<<<gridSize, blockSize>>>(d_data[i], n, iterations);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float serialMs;\n",
    "    cudaEventElapsedTime(&serialMs, start, stop);\n",
    "    printf(\"Serial (default stream): %.2f ms\\n\", serialMs);\n",
    "    \n",
    "    // With multiple streams (parallel)\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < numKernels; i++) {\n",
    "        int streamIdx = i % numStreams;\n",
    "        computeKernel<<<gridSize, blockSize, 0, streams[streamIdx]>>>(d_data[i], n, iterations);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float parallelMs;\n",
    "    cudaEventElapsedTime(&parallelMs, start, stop);\n",
    "    printf(\"Parallel (%d streams):   %.2f ms\\n\", numStreams, parallelMs);\n",
    "    printf(\"Speedup: %.2fx\\n\\n\", serialMs / parallelMs);\n",
    "    \n",
    "    for (int i = 0; i < numStreams; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "    }\n",
    "    for (int i = 0; i < numKernels; i++) {\n",
    "        cudaFree(d_data[i]);\n",
    "    }\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Stream Query Loop (CPU/GPU overlap)\n",
    "// ============================================================\n",
    "\n",
    "void exercise2_streamQuery() {\n",
    "    printf(\"=== Exercise 2: Stream Query Loop ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 22;  // 4M elements\n",
    "    const int iterations = 500;\n",
    "    \n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    CHECK_CUDA(cudaStreamCreate(&stream));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // Launch kernel in stream\n",
    "    computeKernel<<<gridSize, blockSize, 0, stream>>>(d_data, n, iterations);\n",
    "    \n",
    "    // Do CPU work while GPU is computing\n",
    "    int cpuWorkIterations = 0;\n",
    "    volatile double cpuResult = 0.0;\n",
    "    \n",
    "    while (cudaStreamQuery(stream) == cudaErrorNotReady) {\n",
    "        // CPU work - simulated computation\n",
    "        for (int i = 0; i < 10000; i++) {\n",
    "            cpuResult += 0.00001;\n",
    "        }\n",
    "        cpuWorkIterations++;\n",
    "    }\n",
    "    \n",
    "    printf(\"GPU kernel completed!\\n\");\n",
    "    printf(\"CPU did %d work iterations while waiting\\n\", cpuWorkIterations);\n",
    "    printf(\"CPU result: %.4f\\n\\n\", cpuResult);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Non-Blocking vs Blocking Streams\n",
    "// ============================================================\n",
    "\n",
    "void exercise3_nonBlocking() {\n",
    "    printf(\"=== Exercise 3: Non-Blocking vs Blocking Streams ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 20;\n",
    "    const int iterations = 200;\n",
    "    \n",
    "    float* d_data1;\n",
    "    float* d_data2;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data1, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_data2, n * sizeof(float)));\n",
    "    \n",
    "    cudaStream_t blockingStream, nonBlockingStream;\n",
    "    CHECK_CUDA(cudaStreamCreate(&blockingStream));\n",
    "    CHECK_CUDA(cudaStreamCreateWithFlags(&nonBlockingStream, cudaStreamNonBlocking));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Test 1: Blocking stream + default stream\n",
    "    printf(\"Test 1: Blocking stream + default stream\\n\");\n",
    "    cudaEventRecord(start);\n",
    "    computeKernel<<<gridSize, blockSize, 0, blockingStream>>>(d_data1, n, iterations);\n",
    "    computeKernel<<<gridSize, blockSize>>>(d_data2, n, iterations);  // Default stream\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float blockingMs;\n",
    "    cudaEventElapsedTime(&blockingMs, start, stop);\n",
    "    printf(\"  Time: %.2f ms\\n\", blockingMs);\n",
    "    \n",
    "    // Test 2: Non-blocking stream + default stream\n",
    "    printf(\"Test 2: Non-blocking stream + default stream\\n\");\n",
    "    cudaEventRecord(start);\n",
    "    computeKernel<<<gridSize, blockSize, 0, nonBlockingStream>>>(d_data1, n, iterations);\n",
    "    computeKernel<<<gridSize, blockSize>>>(d_data2, n, iterations);  // Default stream\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float nonBlockingMs;\n",
    "    cudaEventElapsedTime(&nonBlockingMs, start, stop);\n",
    "    printf(\"  Time: %.2f ms\\n\", nonBlockingMs);\n",
    "    \n",
    "    printf(\"Non-blocking allows more parallelism: %.2fx faster\\n\\n\", blockingMs / nonBlockingMs);\n",
    "    \n",
    "    cudaStreamDestroy(blockingStream);\n",
    "    cudaStreamDestroy(nonBlockingStream);\n",
    "    cudaFree(d_data1);\n",
    "    cudaFree(d_data2);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘              CUDA Streams Exercises                          â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Concurrent Kernels: %s\\n\", prop.concurrentKernels ? \"Yes\" : \"No\");\n",
    "    printf(\"Async Engine Count: %d\\n\\n\", prop.asyncEngineCount);\n",
    "    \n",
    "    exercise1_multiStreamLaunch();\n",
    "    exercise2_streamQuery();\n",
    "    exercise3_nonBlocking();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65adea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o stream_exercises stream_exercises.cu && ./stream_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667cfc6",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Multi-Stream Kernel Launch\n",
    "\n",
    "```cpp\n",
    "// Launch 8 independent kernels in 4 streams\n",
    "// Measure time with vs without streams\n",
    "\n",
    "// Your implementation:\n",
    "```\n",
    "\n",
    "### Exercise 2: Stream Query Loop\n",
    "\n",
    "```cpp\n",
    "// While GPU is working, do CPU computation\n",
    "// Use cudaStreamQuery to check completion\n",
    "\n",
    "// Your implementation:\n",
    "```\n",
    "\n",
    "### Exercise 3: Non-Blocking vs Blocking\n",
    "\n",
    "```cpp\n",
    "// Compare behavior with:\n",
    "// 1. Regular stream (cudaStreamCreate)\n",
    "// 2. Non-blocking stream (cudaStreamNonBlocking)\n",
    "\n",
    "// Your implementation:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebae4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    STREAM BASICS MASTERY                         â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ­ Core Concept: ASSEMBLY LINE                                  â•‘\n",
    "â•‘     Streams let different GPU \"stations\" work simultaneously     â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ“‹ Essential API:                                               â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  cudaStreamCreate(&stream)       // Create a stream        â”‚  â•‘\n",
    "â•‘  â”‚  kernel<<<grid, block, 0, stream>>>()  // Launch in stream â”‚  â•‘\n",
    "â•‘  â”‚  cudaStreamSynchronize(stream)   // Wait for stream        â”‚  â•‘\n",
    "â•‘  â”‚  cudaStreamDestroy(stream)       // Clean up               â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  âš ï¸  Default Stream Gotcha:                                      â•‘\n",
    "â•‘     â€¢ Legacy default stream blocks OTHER streams!               â•‘\n",
    "â•‘     â€¢ Use cudaStreamNonBlocking for true independence           â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ”¢ Stream Selection Strategy:                                   â•‘\n",
    "â•‘     Few streams:  Simpler, less overhead                        â•‘\n",
    "â•‘     Many streams: More overlap potential                        â•‘\n",
    "â•‘     Sweet spot:   Often 4-8 streams                             â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  âœ… When to Use Streams:                                         â•‘\n",
    "â•‘     â€¢ Multiple independent operations                           â•‘\n",
    "â•‘     â€¢ Data can be chunked (process while transferring)          â•‘\n",
    "â•‘     â€¢ CPU needs to do work while GPU runs                       â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® What's Next?\n",
    "\n",
    "**Day 2: Overlapping Data Transfers** - The real power of streams!\n",
    "\n",
    "You've learned to create streams, but the magic happens when we **overlap** data transfers with computation. Tomorrow you'll see:\n",
    "\n",
    "```\n",
    "Without Overlap:      With Overlap (Day 2):\n",
    "[H2D][Compute][D2H]   [H2D][H2D][H2D]\n",
    "                          [Comp][Comp][Comp]\n",
    "                              [D2H][D2H][D2H]\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>\n",
    "     100% time              60% time!\n",
    "```\n",
    "\n",
    "Get ready to make your GPU ALWAYS busy! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
