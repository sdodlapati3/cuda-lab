{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aba9538",
   "metadata": {},
   "source": [
    "# Day 5: CUDA Graphs - Recipe Cards for the GPU\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Hook: The Head Chef's Secret Weapon\n",
    "\n",
    "**Ever watched** a professional kitchen during rush hour?\n",
    "\n",
    "They don't figure out how to make each dish from scratch every time. They have **recipe cards** - pre-planned sequences that get executed perfectly every time with minimal thinking.\n",
    "\n",
    "```\n",
    "Without Recipe Cards (Regular Launches):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Order 1: \"How do I make this again?\" â†’ Read recipe â†’ Cook    â”‚\n",
    "â”‚  Order 2: \"Wait, what temperature?\" â†’ Read recipe â†’ Cook      â”‚\n",
    "â”‚  Order 3: \"Which spices?\" â†’ Read recipe â†’ Cook                â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Overhead: Thinking time before EVERY dish! ğŸŒ                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "With Recipe Cards (CUDA Graphs):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Step 1: Create recipe card ONCE (capture graph)              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚  1. Prep â†’ 2. Season â†’ 3. Sear â†’ 4. Rest â†’ 5. Plate    â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Step 2: Execute recipe instantly, every time!                â”‚\n",
    "â”‚  Order 1: [Execute recipe] â† No thinking, just do!           â”‚\n",
    "â”‚  Order 2: [Execute recipe]                                    â”‚\n",
    "â”‚  Order 3: [Execute recipe]                                    â”‚\n",
    "â”‚                                                               â”‚\n",
    "â”‚  Overhead: Near-ZERO per dish! ğŸš€                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**CUDA Graphs capture your kernel workflow** and replay it with minimal CPU overhead!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Explain** why CUDA Graphs reduce launch overhead dramatically\n",
    "2. **Capture** existing stream operations into a graph\n",
    "3. **Build graphs manually** for complex dependency patterns\n",
    "4. **Update graph parameters** without rebuilding the graph\n",
    "5. **Identify** when graphs provide the most benefit\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸƒ Concept Card: Recipe Cards\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  ğŸ“‹ CUDA GRAPHS = PRE-COMPILED RECIPE CARDS                      â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  THE PROBLEM: Launch Overhead                                    â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  Each kernel launch: ~5-10Î¼s CPU overhead                  â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  100 small kernels = 500-1000Î¼s of JUST launching!         â”‚  â•‘\n",
    "â•‘  â”‚  (Often more than the actual compute time!)                â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  THE SOLUTION: CUDA Graphs                                       â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  1. CAPTURE: Record sequence of operations                 â”‚  â•‘\n",
    "â•‘  â”‚     cudaStreamBeginCapture(stream)                         â”‚  â•‘\n",
    "â•‘  â”‚     kernel1<<<...>>>(); kernel2<<<...>>>(); ...            â”‚  â•‘\n",
    "â•‘  â”‚     cudaStreamEndCapture(stream, &graph)                   â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  2. INSTANTIATE: Compile graph for execution               â”‚  â•‘\n",
    "â•‘  â”‚     cudaGraphInstantiate(&graphExec, graph, ...)           â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  3. LAUNCH: Execute entire graph at once!                  â”‚  â•‘\n",
    "â•‘  â”‚     cudaGraphLaunch(graphExec, stream)  // ~10Î¼s total!    â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ’¡ KEY INSIGHT: Capture once, replay many times!                â•‘\n",
    "â•‘     Perfect for ML training loops, physics simulations,          â•‘\n",
    "â•‘     any repetitive multi-kernel workflow!                        â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  PERFORMANCE IMPACT:                                             â•‘\n",
    "â•‘  â”‚ Kernels â”‚ Regular Launch â”‚ Graph Launch â”‚ Speedup â”‚          â•‘\n",
    "â•‘  â”‚---------|----------------|--------------|---------|          â•‘\n",
    "â•‘  â”‚    10   â”‚     50-100Î¼s   â”‚    ~10Î¼s     â”‚   5-10x â”‚          â•‘\n",
    "â•‘  â”‚   100   â”‚    500-1000Î¼s  â”‚    ~10Î¼s     â”‚ 50-100x â”‚          â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why CUDA Graphs?\n",
    "\n",
    "**Problem**: Each kernel launch has CPU overhead (~5-10Î¼s per launch)\n",
    "\n",
    "**Solution**: CUDA Graphs capture entire workflows and replay them with minimal overhead\n",
    "\n",
    "| Approach | Launch Overhead per Kernel |\n",
    "|----------|---------------------------|\n",
    "| Regular launches | ~5-10 Î¼s |\n",
    "| CUDA Graph launch | ~10 Î¼s total (for entire graph) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_graphs_basics.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel1(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] + 1.0f;\n",
    "}\n",
    "\n",
    "__global__ void kernel2(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] * 2.0f;\n",
    "}\n",
    "\n",
    "__global__ void kernel3(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] - 1.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const int blocks = (N + 255) / 256;\n",
    "    \n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMemset(d_data, 0, N * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // === METHOD 1: Stream Capture ===\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t graphExec;\n",
    "    \n",
    "    // Begin capture\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // These are captured, NOT executed yet\n",
    "    kernel1<<<blocks, 256, 0, stream>>>(d_data, N);\n",
    "    kernel2<<<blocks, 256, 0, stream>>>(d_data, N);\n",
    "    kernel3<<<blocks, 256, 0, stream>>>(d_data, N);\n",
    "    \n",
    "    // End capture\n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    \n",
    "    // Create executable graph\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Launch graph multiple times\n",
    "    printf(\"Launching graph 10 times...\\n\");\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    // Verify result: ((0+1)*2-1) = 1, applied 10 times\n",
    "    float h_result;\n",
    "    cudaMemcpy(&h_result, d_data, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Result after 10 graph launches: %.0f\\n\", h_result);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cuda_graphs_basics.cu -o cuda_graphs_basics && ./cuda_graphs_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b64dab",
   "metadata": {},
   "source": [
    "## Manual Graph Construction\n",
    "\n",
    "Build graphs programmatically with full control over node dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47633aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_graphs_manual.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void initKernel(float* data, int n, float val) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = val;\n",
    "}\n",
    "\n",
    "__global__ void squareKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] * data[idx];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Create empty graph\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphCreate(&graph, 0);\n",
    "    \n",
    "    // Add kernel nodes manually\n",
    "    cudaGraphNode_t initNode, squareNode;\n",
    "    \n",
    "    // Kernel parameters for initKernel\n",
    "    cudaKernelNodeParams initParams = {};\n",
    "    void* initArgs[] = {&d_data, (void*)&N, nullptr};\n",
    "    float initVal = 3.0f;\n",
    "    initArgs[2] = &initVal;\n",
    "    initParams.func = (void*)initKernel;\n",
    "    initParams.gridDim = dim3((N + 255) / 256);\n",
    "    initParams.blockDim = dim3(256);\n",
    "    initParams.sharedMemBytes = 0;\n",
    "    initParams.kernelParams = initArgs;\n",
    "    initParams.extra = nullptr;\n",
    "    \n",
    "    // Add init node (no dependencies)\n",
    "    cudaGraphAddKernelNode(&initNode, graph, nullptr, 0, &initParams);\n",
    "    \n",
    "    // Kernel parameters for squareKernel\n",
    "    cudaKernelNodeParams squareParams = {};\n",
    "    void* squareArgs[] = {&d_data, (void*)&N};\n",
    "    squareParams.func = (void*)squareKernel;\n",
    "    squareParams.gridDim = dim3((N + 255) / 256);\n",
    "    squareParams.blockDim = dim3(256);\n",
    "    squareParams.sharedMemBytes = 0;\n",
    "    squareParams.kernelParams = squareArgs;\n",
    "    squareParams.extra = nullptr;\n",
    "    \n",
    "    // Add square node (depends on init)\n",
    "    cudaGraphNode_t deps[] = {initNode};\n",
    "    cudaGraphAddKernelNode(&squareNode, graph, deps, 1, &squareParams);\n",
    "    \n",
    "    // Instantiate and launch\n",
    "    cudaGraphExec_t graphExec;\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float h_result;\n",
    "    cudaMemcpy(&h_result, d_data, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Result: init(3.0) then square = %.0f (expected 9)\\n\", h_result);\n",
    "    \n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cuda_graphs_manual.cu -o cuda_graphs_manual && ./cuda_graphs_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfe623",
   "metadata": {},
   "source": [
    "## Graph Update (Parameter Changes)\n",
    "\n",
    "Update graph parameters without rebuilding the entire graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0911e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_graphs_update.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void scaleKernel(float* data, int n, float scale) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = data[idx] * scale;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    float h_data[N];\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    // Capture initial graph with scale = 2.0\n",
    "    float scale = 2.0f;\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t graphExec;\n",
    "    \n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    scaleKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N, scale);\n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Launch with scale = 2.0\n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    float h_result;\n",
    "    cudaMemcpy(&h_result, d_data, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"After scale=2.0: %.1f\\n\", h_result);\n",
    "    \n",
    "    // Update graph with new scale = 3.0\n",
    "    cudaGraph_t newGraph;\n",
    "    scale = 3.0f;\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    scaleKernel<<<(N+255)/256, 256, 0, stream>>>(d_data, N, scale);\n",
    "    cudaStreamEndCapture(stream, &newGraph);\n",
    "    \n",
    "    // Update existing executable with new graph\n",
    "    cudaGraphExecUpdateResultInfo updateResult;\n",
    "    cudaGraphExecUpdate(graphExec, newGraph, &updateResult);\n",
    "    \n",
    "    if (updateResult.result == cudaGraphExecUpdateSuccess) {\n",
    "        printf(\"Graph updated successfully!\\n\");\n",
    "    }\n",
    "    \n",
    "    // Launch updated graph\n",
    "    cudaGraphLaunch(graphExec, stream);\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaMemcpy(&h_result, d_data, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"After scale=3.0: %.1f\\n\", h_result);\n",
    "    \n",
    "    cudaGraphDestroy(newGraph);\n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cuda_graphs_update.cu -o cuda_graphs_update && ./cuda_graphs_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb839e",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_graphs_benchmark.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void tinyKernel(float* data) {\n",
    "    data[threadIdx.x] += 1.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, 256 * sizeof(float));\n",
    "    cudaMemset(d_data, 0, 256 * sizeof(float));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    const int KERNELS = 100;\n",
    "    const int ITERATIONS = 1000;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark regular launches\n",
    "    cudaEventRecord(start);\n",
    "    for (int iter = 0; iter < ITERATIONS; iter++) {\n",
    "        for (int k = 0; k < KERNELS; k++) {\n",
    "            tinyKernel<<<1, 256>>>(d_data);\n",
    "        }\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float regularMs;\n",
    "    cudaEventElapsedTime(&regularMs, start, stop);\n",
    "    \n",
    "    // Capture graph\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t graphExec;\n",
    "    \n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    for (int k = 0; k < KERNELS; k++) {\n",
    "        tinyKernel<<<1, 256, 0, stream>>>(d_data);\n",
    "    }\n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Benchmark graph launches\n",
    "    cudaEventRecord(start);\n",
    "    for (int iter = 0; iter < ITERATIONS; iter++) {\n",
    "        cudaGraphLaunch(graphExec, stream);\n",
    "    }\n",
    "    cudaStreamSynchronize(stream);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float graphMs;\n",
    "    cudaEventElapsedTime(&graphMs, start, stop);\n",
    "    \n",
    "    printf(\"Launching %d tiny kernels x %d iterations:\\n\", KERNELS, ITERATIONS);\n",
    "    printf(\"Regular launches: %.2f ms\\n\", regularMs);\n",
    "    printf(\"Graph launches:   %.2f ms\\n\", graphMs);\n",
    "    printf(\"Speedup: %.1fx\\n\", regularMs / graphMs);\n",
    "    \n",
    "    cudaGraphExecDestroy(graphExec);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "    cudaFree(d_data);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cuda_graphs_benchmark.cu -o cuda_graphs_benchmark && ./cuda_graphs_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375b3cb",
   "metadata": {},
   "source": [
    "## CUDA Graph Node Types\n",
    "\n",
    "| Node Type | Purpose |\n",
    "|-----------|--------|\n",
    "| `cudaGraphAddKernelNode` | Kernel execution |\n",
    "| `cudaGraphAddMemcpyNode` | Memory copy |\n",
    "| `cudaGraphAddMemsetNode` | Memory set |\n",
    "| `cudaGraphAddHostNode` | CPU callback |\n",
    "| `cudaGraphAddChildGraphNode` | Nested graph |\n",
    "| `cudaGraphAddEventRecordNode` | Record event |\n",
    "| `cudaGraphAddEventWaitNode` | Wait for event |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc81d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to practice CUDA Graphs concepts:\n",
    "\n",
    "**Exercise 1: Stream Capture**\n",
    "- Create a graph using stream capture with multiple dependent kernels\n",
    "- Launch the graph multiple times and measure performance\n",
    "\n",
    "**Exercise 2: Graph Node Updates**\n",
    "- Build a graph manually with kernel nodes\n",
    "- Update kernel parameters without recreating the graph\n",
    "\n",
    "**Exercise 3: Nested Graphs**\n",
    "- Create a child graph for a reusable operation\n",
    "- Embed it in a parent graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbac800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_graphs_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Simple kernels for graph construction\n",
    "__global__ void initKernel(float* data, int n, float value) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = value;\n",
    "}\n",
    "\n",
    "__global__ void scaleKernel(float* data, int n, float scale) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] *= scale;\n",
    "}\n",
    "\n",
    "__global__ void addKernel(float* a, float* b, float* c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) c[idx] = a[idx] + b[idx];\n",
    "}\n",
    "\n",
    "__global__ void reductionKernel(float* input, float* output, int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? input[idx] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(output, sdata[0]);\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Stream Capture\n",
    "// TODO: Create a graph using stream capture\n",
    "// =============================================================================\n",
    "void exercise1_StreamCapture() {\n",
    "    printf(\"=== Exercise 1: Stream Capture ===\\n\");\n",
    "    \n",
    "    int n = 1024 * 1024;\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    CHECK_CUDA(cudaMalloc(&d_a, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_b, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_c, n * sizeof(float)));\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    CHECK_CUDA(cudaStreamCreate(&stream));\n",
    "    \n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t graphExec;\n",
    "    \n",
    "    // TODO: Exercise 1a - Begin stream capture\n",
    "    // Hint: cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    CHECK_CUDA(cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal));\n",
    "    \n",
    "    // TODO: Exercise 1b - Launch kernels into the stream (they get captured)\n",
    "    // Initialize arrays\n",
    "    initKernel<<<numBlocks, blockSize, 0, stream>>>(d_a, n, 1.0f);\n",
    "    initKernel<<<numBlocks, blockSize, 0, stream>>>(d_b, n, 2.0f);\n",
    "    \n",
    "    // Scale arrays\n",
    "    scaleKernel<<<numBlocks, blockSize, 0, stream>>>(d_a, n, 2.0f);\n",
    "    scaleKernel<<<numBlocks, blockSize, 0, stream>>>(d_b, n, 3.0f);\n",
    "    \n",
    "    // Add arrays\n",
    "    addKernel<<<numBlocks, blockSize, 0, stream>>>(d_a, d_b, d_c, n);\n",
    "    \n",
    "    // TODO: Exercise 1c - End capture and get graph\n",
    "    // Hint: cudaStreamEndCapture(stream, &graph);\n",
    "    CHECK_CUDA(cudaStreamEndCapture(stream, &graph));\n",
    "    \n",
    "    // Instantiate graph\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0));\n",
    "    \n",
    "    // TODO: Exercise 1d - Launch graph multiple times and time it\n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    int numIterations = 100;\n",
    "    \n",
    "    CHECK_CUDA(cudaEventRecord(start, stream));\n",
    "    for (int i = 0; i < numIterations; i++) {\n",
    "        CHECK_CUDA(cudaGraphLaunch(graphExec, stream));\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop, stream));\n",
    "    CHECK_CUDA(cudaStreamSynchronize(stream));\n",
    "    \n",
    "    float graphTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&graphTime, start, stop));\n",
    "    \n",
    "    // Compare with non-graph version\n",
    "    CHECK_CUDA(cudaEventRecord(start, stream));\n",
    "    for (int i = 0; i < numIterations; i++) {\n",
    "        initKernel<<<numBlocks, blockSize, 0, stream>>>(d_a, n, 1.0f);\n",
    "        initKernel<<<numBlocks, blockSize, 0, stream>>>(d_b, n, 2.0f);\n",
    "        scaleKernel<<<numBlocks, blockSize, 0, stream>>>(d_a, n, 2.0f);\n",
    "        scaleKernel<<<numBlocks, blockSize, 0, stream>>>(d_b, n, 3.0f);\n",
    "        addKernel<<<numBlocks, blockSize, 0, stream>>>(d_a, d_b, d_c, n);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop, stream));\n",
    "    CHECK_CUDA(cudaStreamSynchronize(stream));\n",
    "    \n",
    "    float noGraphTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&noGraphTime, start, stop));\n",
    "    \n",
    "    printf(\"Graph time for %d iterations: %.3f ms\\n\", numIterations, graphTime);\n",
    "    printf(\"No-graph time: %.3f ms\\n\", noGraphTime);\n",
    "    printf(\"Speedup: %.2fx\\n\\n\", noGraphTime / graphTime);\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaGraphExecDestroy(graphExec));\n",
    "    CHECK_CUDA(cudaGraphDestroy(graph));\n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaStreamDestroy(stream));\n",
    "    CHECK_CUDA(cudaFree(d_a));\n",
    "    CHECK_CUDA(cudaFree(d_b));\n",
    "    CHECK_CUDA(cudaFree(d_c));\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Graph Node Updates\n",
    "// TODO: Update kernel parameters without recreating the graph\n",
    "// =============================================================================\n",
    "void exercise2_GraphUpdate() {\n",
    "    printf(\"=== Exercise 2: Graph Node Updates ===\\n\");\n",
    "    \n",
    "    int n = 1024;\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    float *d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    // Create graph manually\n",
    "    cudaGraph_t graph;\n",
    "    CHECK_CUDA(cudaGraphCreate(&graph, 0));\n",
    "    \n",
    "    // Add init kernel node\n",
    "    cudaGraphNode_t initNode;\n",
    "    cudaKernelNodeParams initParams = {0};\n",
    "    \n",
    "    float initValue = 5.0f;\n",
    "    void* initArgs[] = {&d_data, &n, &initValue};\n",
    "    \n",
    "    initParams.func = (void*)initKernel;\n",
    "    initParams.gridDim = dim3(numBlocks);\n",
    "    initParams.blockDim = dim3(blockSize);\n",
    "    initParams.sharedMemBytes = 0;\n",
    "    initParams.kernelParams = initArgs;\n",
    "    initParams.extra = NULL;\n",
    "    \n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&initNode, graph, NULL, 0, &initParams));\n",
    "    \n",
    "    // Instantiate\n",
    "    cudaGraphExec_t graphExec;\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0));\n",
    "    \n",
    "    // Launch with initial value\n",
    "    CHECK_CUDA(cudaGraphLaunch(graphExec, 0));\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    float h_result;\n",
    "    CHECK_CUDA(cudaMemcpy(&h_result, d_data, sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    printf(\"Initial value: %.1f\\n\", h_result);\n",
    "    \n",
    "    // TODO: Exercise 2a - Update the kernel parameter (change init value)\n",
    "    // Hint: Update initValue and use cudaGraphExecKernelNodeSetParams\n",
    "    float newValue = 10.0f;\n",
    "    initArgs[2] = &newValue;\n",
    "    initParams.kernelParams = initArgs;\n",
    "    \n",
    "    CHECK_CUDA(cudaGraphExecKernelNodeSetParams(graphExec, initNode, &initParams));\n",
    "    \n",
    "    // Launch with updated value\n",
    "    CHECK_CUDA(cudaGraphLaunch(graphExec, 0));\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(&h_result, d_data, sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    printf(\"Updated value: %.1f\\n\", h_result);\n",
    "    printf(\"Status: %s\\n\\n\", (h_result == 10.0f) ? \"PASS\" : \"NEEDS WORK\");\n",
    "    \n",
    "    CHECK_CUDA(cudaGraphExecDestroy(graphExec));\n",
    "    CHECK_CUDA(cudaGraphDestroy(graph));\n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Nested Graphs (Child Graph)\n",
    "// TODO: Create a child graph and embed in parent\n",
    "// =============================================================================\n",
    "void exercise3_NestedGraphs() {\n",
    "    printf(\"=== Exercise 3: Nested Graphs ===\\n\");\n",
    "    \n",
    "    int n = 1024;\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    float *d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    // TODO: Exercise 3a - Create a child graph for \"double and add 1\" operation\n",
    "    cudaGraph_t childGraph;\n",
    "    CHECK_CUDA(cudaGraphCreate(&childGraph, 0));\n",
    "    \n",
    "    // Scale by 2\n",
    "    cudaGraphNode_t scaleNode;\n",
    "    cudaKernelNodeParams scaleParams = {0};\n",
    "    float scaleVal = 2.0f;\n",
    "    void* scaleArgs[] = {&d_data, &n, &scaleVal};\n",
    "    scaleParams.func = (void*)scaleKernel;\n",
    "    scaleParams.gridDim = dim3(numBlocks);\n",
    "    scaleParams.blockDim = dim3(blockSize);\n",
    "    scaleParams.kernelParams = scaleArgs;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&scaleNode, childGraph, NULL, 0, &scaleParams));\n",
    "    \n",
    "    // TODO: Exercise 3b - Create parent graph\n",
    "    cudaGraph_t parentGraph;\n",
    "    CHECK_CUDA(cudaGraphCreate(&parentGraph, 0));\n",
    "    \n",
    "    // Initialize data in parent\n",
    "    cudaGraphNode_t initNode;\n",
    "    cudaKernelNodeParams initParams = {0};\n",
    "    float initVal = 5.0f;\n",
    "    void* initArgs[] = {&d_data, &n, &initVal};\n",
    "    initParams.func = (void*)initKernel;\n",
    "    initParams.gridDim = dim3(numBlocks);\n",
    "    initParams.blockDim = dim3(blockSize);\n",
    "    initParams.kernelParams = initArgs;\n",
    "    CHECK_CUDA(cudaGraphAddKernelNode(&initNode, parentGraph, NULL, 0, &initParams));\n",
    "    \n",
    "    // TODO: Exercise 3c - Add child graph as a node (depends on init)\n",
    "    cudaGraphNode_t childNode;\n",
    "    cudaGraphNode_t dependencies[] = {initNode};\n",
    "    CHECK_CUDA(cudaGraphAddChildGraphNode(&childNode, parentGraph, dependencies, 1, childGraph));\n",
    "    \n",
    "    // Instantiate parent graph\n",
    "    cudaGraphExec_t graphExec;\n",
    "    CHECK_CUDA(cudaGraphInstantiate(&graphExec, parentGraph, NULL, NULL, 0));\n",
    "    \n",
    "    // Launch\n",
    "    CHECK_CUDA(cudaGraphLaunch(graphExec, 0));\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    float h_result;\n",
    "    CHECK_CUDA(cudaMemcpy(&h_result, d_data, sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Expected: init(5) -> scale(2) = 10\n",
    "    printf(\"Result: %.1f (expected: 10.0)\\n\", h_result);\n",
    "    printf(\"Status: %s\\n\\n\", (h_result == 10.0f) ? \"PASS\" : \"NEEDS WORK\");\n",
    "    \n",
    "    CHECK_CUDA(cudaGraphExecDestroy(graphExec));\n",
    "    CHECK_CUDA(cudaGraphDestroy(parentGraph));\n",
    "    CHECK_CUDA(cudaGraphDestroy(childGraph));\n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘              CUDA Graphs Exercises                           â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    exercise1_StreamCapture();\n",
    "    exercise2_GraphUpdate();\n",
    "    exercise3_NestedGraphs();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"Explore the code to understand CUDA Graphs patterns!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fefef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cuda_graphs_exercises cuda_graphs_exercises.cu && ./cuda_graphs_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff488e",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises explore graph-like patterns in Python:\n",
    "\n",
    "```python\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# Note: Numba doesn't directly support CUDA Graphs, but you can explore\n",
    "# similar concepts using streams and dependencies\n",
    "\n",
    "@cuda.jit\n",
    "def scale_kernel(data, scale):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < data.size:\n",
    "        data[idx] *= scale\n",
    "\n",
    "# Exercise: Create a pipeline using multiple streams\n",
    "def stream_pipeline():\n",
    "    \"\"\"\n",
    "    Create overlapping operations using streams\n",
    "    (conceptually similar to graph dependencies)\n",
    "    \"\"\"\n",
    "    n = 1024 * 1024\n",
    "    stream1 = cuda.stream()\n",
    "    stream2 = cuda.stream()\n",
    "    \n",
    "    # TODO: Use streams to overlap transfers and compute\n",
    "    # This simulates the dependency management of graphs\n",
    "    pass\n",
    "\n",
    "# For true CUDA Graphs support, use CUDA C++ or the cuda-python package\n",
    "```\n",
    "\n",
    "**Note**: For production CUDA Graphs usage, CUDA C++ provides the full API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36667002",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                   CUDA GRAPHS MASTERY                            â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ“‹ Core Concept: RECIPE CARDS                                   â•‘\n",
    "â•‘     Capture workflow once, execute many times with no overhead   â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ“‹ Essential Patterns:                                          â•‘\n",
    "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
    "â•‘  â”‚  Stream Capture (easiest):                                 â”‚  â•‘\n",
    "â•‘  â”‚    cudaStreamBeginCapture(stream, mode)                    â”‚  â•‘\n",
    "â•‘  â”‚    // ... your kernel launches ...                         â”‚  â•‘\n",
    "â•‘  â”‚    cudaStreamEndCapture(stream, &graph)                    â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  Manual Construction (full control):                       â”‚  â•‘\n",
    "â•‘  â”‚    cudaGraphCreate(&graph, 0)                              â”‚  â•‘\n",
    "â•‘  â”‚    cudaGraphAddKernelNode(&node, graph, deps, ...)         â”‚  â•‘\n",
    "â•‘  â”‚                                                            â”‚  â•‘\n",
    "â•‘  â”‚  Graph Update (change parameters):                         â”‚  â•‘\n",
    "â•‘  â”‚    cudaGraphExecKernelNodeSetParams(graphExec, node, ...)  â”‚  â•‘\n",
    "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  âœ… When to Use CUDA Graphs:                                     â•‘\n",
    "â•‘     â€¢ Many small kernels launched repeatedly                     â•‘\n",
    "â•‘     â€¢ ML training loops (same operations each iteration)         â•‘\n",
    "â•‘     â€¢ Physics simulations (same update pattern each frame)       â•‘\n",
    "â•‘     â€¢ Any workflow where launch overhead > compute time          â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  âš ï¸  When NOT to Use:                                            â•‘\n",
    "â•‘     â€¢ Dynamic workflows (different operations each time)         â•‘\n",
    "â•‘     â€¢ Few large kernels (launch overhead is negligible)          â•‘\n",
    "â•‘     â€¢ Single-run workflows (capture overhead not amortized)      â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘  ğŸ“Š Performance Potential:                                       â•‘\n",
    "â•‘     10-100x reduction in launch overhead                         â•‘\n",
    "â•‘     Most impactful for launch-bound workloads                    â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Week 9 Complete: Streams & Concurrency Mastery!\n",
    "\n",
    "```\n",
    "Week 9 Journey:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Day 1: Stream Basics        â†’ ğŸ­ Assembly line - keep GPU stations busy\n",
    "Day 2: Overlap Transfers    â†’ ğŸ”„ Double buffering - never idle!\n",
    "Day 3: Multi-Stream         â†’ ğŸš€ Parallel pipelines - concurrent kernels\n",
    "Day 4: Events               â†’ â±ï¸  Stopwatches & checkpoints - sync & time\n",
    "Day 5: CUDA Graphs          â†’ ğŸ“‹ Recipe cards - minimal launch overhead\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "You now have COMPLETE control over GPU concurrency!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® What's Next?\n",
    "\n",
    "**Week 10: Atomics and Synchronization** - Thread-safe GPU programming!\n",
    "\n",
    "With streams mastered, next we tackle what happens when threads need to **share data**:\n",
    "\n",
    "```\n",
    "Week 9 (Streams):           Week 10 (Atomics):\n",
    "Operations in parallel      Threads accessing SAME memory\n",
    "\n",
    "[Stream 1]â”€â”€â”€â”€â”€â”€â”€â”€â”€         Thread 1: read x, add 1, write x\n",
    "[Stream 2]â”€â”€â”€â”€â”€â”€â”€â”€â”€    â†’    Thread 2: read x, add 1, write x\n",
    "[Stream 3]â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â†‘ RACE CONDITION! \n",
    "                            \n",
    "                            Solution: atomicAdd(&x, 1)\n",
    "```\n",
    "\n",
    "Learn to coordinate thousands of threads safely! ğŸ”’"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
