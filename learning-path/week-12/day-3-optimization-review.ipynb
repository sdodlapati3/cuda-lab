{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a7cd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Optimization Hierarchy\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              OPTIMIZATION PRIORITY ORDER                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  1. ALGORITHM CHOICE          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ~10-100x  ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Choose right algorithm                                ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Reduce algorithmic complexity                         ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  2. MEMORY ACCESS PATTERNS    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ~5-20x    ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Coalesced access                                      ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Minimize global memory traffic                        ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Use shared memory                                     ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  3. OCCUPANCY & PARALLELISM   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        ~2-5x     ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Enough threads to hide latency                        ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Balance resources per block                           ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  4. INSTRUCTION OPTIMIZATION  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            ~1.5-3x   ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Fast math functions                                   ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Avoid divergence                                      ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  5. CONCURRENCY               ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              ~1.2-2x   ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Streams, graphs                                       ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Overlap compute/transfer                              ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a40c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Memory Optimization Techniques\n",
    "\n",
    "### 1.1 Coalesced Memory Access\n",
    "\n",
    "```cpp\n",
    "// GOOD: Coalesced - threads access consecutive memory\n",
    "__global__ void coalesced(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f;  // Thread i accesses element i\n",
    "    }\n",
    "}\n",
    "\n",
    "// BAD: Strided - threads access with stride\n",
    "__global__ void strided(float* data, int n, int stride) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int idx = tid * stride;  // Thread i accesses element i*stride\n",
    "    if (idx < n) {\n",
    "        data[idx] = data[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 1.2 Shared Memory Usage\n",
    "\n",
    "```cpp\n",
    "// Pattern: Load to shared -> Sync -> Process -> Sync -> Store\n",
    "__global__ void withShared(float* out, float* in, int n) {\n",
    "    __shared__ float smem[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + tid;\n",
    "    \n",
    "    // Load to shared memory\n",
    "    smem[tid] = (gid < n) ? in[gid] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Process using shared memory\n",
    "    float result = smem[tid];\n",
    "    if (tid > 0) result += smem[tid - 1];\n",
    "    if (tid < 255) result += smem[tid + 1];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Write result\n",
    "    if (gid < n) out[gid] = result;\n",
    "}\n",
    "```\n",
    "\n",
    "### 1.3 Bank Conflict Avoidance\n",
    "\n",
    "```cpp\n",
    "// BAD: Bank conflicts (stride of 32)\n",
    "__shared__ float smem[32][32];\n",
    "smem[threadIdx.x][0] = value;  // All access bank 0!\n",
    "\n",
    "// GOOD: Padding to avoid conflicts\n",
    "__shared__ float smem[32][33];  // +1 padding\n",
    "smem[threadIdx.x][0] = value;   // Different banks\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile memory_patterns.cu\n",
    "// memory_patterns.cu - Memory optimization examples\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// GOOD: Coalesced - threads access consecutive memory\n",
    "__global__ void coalesced(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f;  // Thread i accesses element i\n",
    "    }\n",
    "}\n",
    "\n",
    "// BAD: Strided - threads access with stride\n",
    "__global__ void strided(float* data, int n, int stride) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int idx = tid * stride;  // Thread i accesses element i*stride\n",
    "    if (idx < n) {\n",
    "        data[idx] = data[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Pattern: Load to shared -> Sync -> Process -> Sync -> Store\n",
    "__global__ void withShared(float* out, float* in, int n) {\n",
    "    __shared__ float smem[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + tid;\n",
    "    \n",
    "    // Load to shared memory\n",
    "    smem[tid] = (gid < n) ? in[gid] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Process using shared memory\n",
    "    float result = smem[tid];\n",
    "    if (tid > 0) result += smem[tid - 1];\n",
    "    if (tid < 255) result += smem[tid + 1];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Write result\n",
    "    if (gid < n) out[gid] = result;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data, *d_out;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMalloc(&d_out, N * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_data = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // Test coalesced\n",
    "    coalesced<<<numBlocks, blockSize>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Coalesced access completed\\n\");\n",
    "    \n",
    "    // Test with shared memory\n",
    "    withShared<<<numBlocks, blockSize>>>(d_out, d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Shared memory pattern completed\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_out);\n",
    "    delete[] h_data;\n",
    "    \n",
    "    printf(\"Memory patterns demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o memory_patterns memory_patterns.cu\n",
    "!./memory_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f68692",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Compute Optimization\n",
    "\n",
    "### 2.1 Instruction Throughput\n",
    "\n",
    "```cpp\n",
    "// Fast math intrinsics (less accurate, much faster)\n",
    "__device__ float fast_sin(float x) {\n",
    "    return __sinf(x);      // ~10x faster than sinf()\n",
    "}\n",
    "\n",
    "__device__ float fast_exp(float x) {\n",
    "    return __expf(x);      // ~10x faster than expf()\n",
    "}\n",
    "\n",
    "__device__ float fast_rsqrt(float x) {\n",
    "    return rsqrtf(x);      // 1/sqrt(x), very fast\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.2 Warp Divergence\n",
    "\n",
    "```cpp\n",
    "// BAD: Divergent branches\n",
    "__global__ void divergent(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid % 2 == 0) {      // Half warp does one thing\n",
    "        data[tid] = expensive_op1(data[tid]);\n",
    "    } else {                 // Other half does another\n",
    "        data[tid] = expensive_op2(data[tid]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// BETTER: Separate into different warps\n",
    "__global__ void nondivergent(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int warpId = tid / 32;\n",
    "    \n",
    "    if (warpId % 2 == 0) {   // Whole warp does one thing\n",
    "        data[tid] = expensive_op1(data[tid]);\n",
    "    } else {                 // Other warp does another\n",
    "        data[tid] = expensive_op2(data[tid]);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.3 Loop Unrolling\n",
    "\n",
    "```cpp\n",
    "// Manual unroll\n",
    "__global__ void unrolled(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    #pragma unroll 4\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "        int idx = tid + i * blockDim.x * gridDim.x;\n",
    "        if (idx < n) data[idx] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile compute_optimization.cu\n",
    "// compute_optimization.cu - Compute optimization examples\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Fast math intrinsics (less accurate, much faster)\n",
    "__device__ float fast_computation(float x) {\n",
    "    float sin_val = __sinf(x);      // ~10x faster than sinf()\n",
    "    float exp_val = __expf(x);      // ~10x faster than expf()\n",
    "    float rsqrt_val = rsqrtf(x);    // 1/sqrt(x), very fast\n",
    "    return sin_val + exp_val * rsqrt_val;\n",
    "}\n",
    "\n",
    "// BETTER: Non-divergent - whole warps take same path\n",
    "__global__ void nondivergent(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int warpId = tid / 32;\n",
    "    \n",
    "    if (tid < n) {\n",
    "        if (warpId % 2 == 0) {   // Whole warp does one thing\n",
    "            data[tid] = __sinf(data[tid]);\n",
    "        } else {                 // Other warp does another\n",
    "            data[tid] = __cosf(data[tid]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Manual unroll\n",
    "__global__ void unrolled(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    #pragma unroll 4\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "        int idx = tid + i * blockDim.x * gridDim.x;\n",
    "        if (idx < n) data[idx] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_data = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f + (float)i / N;\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // Test non-divergent\n",
    "    nondivergent<<<numBlocks, blockSize>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Non-divergent kernel completed\\n\");\n",
    "    \n",
    "    // Test unrolled\n",
    "    unrolled<<<numBlocks / 4, blockSize>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Unrolled kernel completed\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    delete[] h_data;\n",
    "    \n",
    "    printf(\"Compute optimization demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o compute_optimization compute_optimization.cu\n",
    "!./compute_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544893c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Occupancy Optimization\n",
    "\n",
    "### 3.1 Resource Balancing\n",
    "\n",
    "```cpp\n",
    "// Query optimal block size\n",
    "int minGridSize, optBlockSize;\n",
    "cudaOccupancyMaxPotentialBlockSize(\n",
    "    &minGridSize, &optBlockSize, \n",
    "    myKernel, 0, 0);\n",
    "\n",
    "printf(\"Optimal block size: %d\\n\", optBlockSize);\n",
    "printf(\"Min grid size: %d\\n\", minGridSize);\n",
    "```\n",
    "\n",
    "### 3.2 Register Pressure\n",
    "\n",
    "```cpp\n",
    "// Limit registers to increase occupancy\n",
    "__global__ __launch_bounds__(256, 4)  // 256 threads, 4 blocks/SM\n",
    "void limitedRegisters(float* data) {\n",
    "    // Kernel code\n",
    "}\n",
    "\n",
    "// Compile with: nvcc -maxrregcount=32 kernel.cu\n",
    "```\n",
    "\n",
    "### 3.3 Shared Memory Configuration\n",
    "\n",
    "```cpp\n",
    "// Prefer more shared memory over L1 cache\n",
    "cudaFuncSetCacheConfig(myKernel, cudaFuncCachePreferShared);\n",
    "\n",
    "// Options:\n",
    "// cudaFuncCachePreferNone   - No preference\n",
    "// cudaFuncCachePreferShared - Prefer shared memory\n",
    "// cudaFuncCachePreferL1     - Prefer L1 cache\n",
    "// cudaFuncCachePreferEqual  - Equal split\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile occupancy_demo.cu\n",
    "// occupancy_demo.cu - Occupancy optimization examples\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Limit registers to increase occupancy\n",
    "__global__ __launch_bounds__(256, 4)  // 256 threads, 4 blocks/SM\n",
    "void limitedRegisters(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void myKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = sqrtf(data[tid]) * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Query optimal block size\n",
    "    int minGridSize, optBlockSize;\n",
    "    cudaOccupancyMaxPotentialBlockSize(\n",
    "        &minGridSize, &optBlockSize, \n",
    "        myKernel, 0, 0);\n",
    "    \n",
    "    printf(\"Optimal block size: %d\\n\", optBlockSize);\n",
    "    printf(\"Min grid size: %d\\n\", minGridSize);\n",
    "    \n",
    "    // Calculate occupancy\n",
    "    int numBlocks;\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "        &numBlocks, myKernel, optBlockSize, 0);\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    \n",
    "    int maxBlocksPerSM = prop.maxThreadsPerMultiProcessor / optBlockSize;\n",
    "    float occupancy = (float)numBlocks / maxBlocksPerSM;\n",
    "    printf(\"Max blocks per SM: %d\\n\", maxBlocksPerSM);\n",
    "    printf(\"Achieved occupancy: %.1f%%\\n\", occupancy * 100);\n",
    "    \n",
    "    // Set cache preference\n",
    "    cudaFuncSetCacheConfig(myKernel, cudaFuncCachePreferShared);\n",
    "    printf(\"Set cache preference: PreferShared\\n\");\n",
    "    \n",
    "    // Launch kernel with optimal parameters\n",
    "    int numBlocksTotal = (N + optBlockSize - 1) / optBlockSize;\n",
    "    myKernel<<<numBlocksTotal, optBlockSize>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Kernel completed with optimal configuration!\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o occupancy_demo occupancy_demo.cu\n",
    "!./occupancy_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2baf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Concurrency Optimization\n",
    "\n",
    "### 4.1 Stream Overlap Pattern\n",
    "\n",
    "```cpp\n",
    "// Chunk and overlap pattern\n",
    "const int NUM_STREAMS = 4;\n",
    "cudaStream_t streams[NUM_STREAMS];\n",
    "\n",
    "for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "    cudaStreamCreate(&streams[i]);\n",
    "}\n",
    "\n",
    "int chunkSize = N / NUM_STREAMS;\n",
    "\n",
    "for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "    int offset = i * chunkSize;\n",
    "    \n",
    "    // H2D for chunk i\n",
    "    cudaMemcpyAsync(d_in + offset, h_in + offset,\n",
    "                    chunkSize * sizeof(float),\n",
    "                    cudaMemcpyHostToDevice, streams[i]);\n",
    "    \n",
    "    // Compute chunk i\n",
    "    kernel<<<blocks, threads, 0, streams[i]>>>(\n",
    "        d_out + offset, d_in + offset, chunkSize);\n",
    "    \n",
    "    // D2H for chunk i\n",
    "    cudaMemcpyAsync(h_out + offset, d_out + offset,\n",
    "                    chunkSize * sizeof(float),\n",
    "                    cudaMemcpyDeviceToHost, streams[i]);\n",
    "}\n",
    "```\n",
    "\n",
    "### 4.2 CUDA Graphs for Repeated Patterns\n",
    "\n",
    "```cpp\n",
    "// Capture repeating pattern\n",
    "cudaGraph_t graph;\n",
    "cudaGraphExec_t instance;\n",
    "\n",
    "cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "\n",
    "// Pattern to repeat\n",
    "kernelA<<<...>>>(...);\n",
    "kernelB<<<...>>>(...);\n",
    "kernelC<<<...>>>(...);\n",
    "\n",
    "cudaStreamEndCapture(stream, &graph);\n",
    "cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);\n",
    "\n",
    "// Execute efficiently many times\n",
    "for (int iter = 0; iter < 1000; iter++) {\n",
    "    cudaGraphLaunch(instance, stream);\n",
    "}\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile concurrency_demo.cu\n",
    "// concurrency_demo.cu - Stream overlap and CUDA Graphs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = sqrtf(data[tid]) * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 22;  // 4M elements\n",
    "    const int NUM_STREAMS = 4;\n",
    "    size_t bytes = N * sizeof(float);\n",
    "    int chunkSize = N / NUM_STREAMS;\n",
    "    \n",
    "    // Allocate pinned host memory\n",
    "    float *h_in, *h_out;\n",
    "    cudaMallocHost(&h_in, bytes);\n",
    "    cudaMallocHost(&h_out, bytes);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < N; i++) h_in[i] = (float)i;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, bytes);\n",
    "    cudaMalloc(&d_out, bytes);\n",
    "    \n",
    "    // Create streams\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int blocks = (chunkSize + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // ============================================\n",
    "    // Stream Overlap Pattern\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        int offset = i * chunkSize;\n",
    "        \n",
    "        // H2D for chunk i\n",
    "        cudaMemcpyAsync(d_in + offset, h_in + offset,\n",
    "                        chunkSize * sizeof(float),\n",
    "                        cudaMemcpyHostToDevice, streams[i]);\n",
    "        \n",
    "        // Compute chunk i\n",
    "        kernel<<<blocks, blockSize, 0, streams[i]>>>(\n",
    "            d_in + offset, chunkSize);\n",
    "        \n",
    "        // D2H for chunk i\n",
    "        cudaMemcpyAsync(h_out + offset, d_in + offset,\n",
    "                        chunkSize * sizeof(float),\n",
    "                        cudaMemcpyDeviceToHost, streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Sync all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Stream overlap: %.2f ms\\n\", ms);\n",
    "    \n",
    "    // ============================================\n",
    "    // CUDA Graph for Repeated Pattern\n",
    "    // ============================================\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t instance;\n",
    "    \n",
    "    // Reset data\n",
    "    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Pattern to repeat: 3 kernel passes\n",
    "    kernel<<<(N + 255) / 256, 256, 0, streams[0]>>>(d_in, N);\n",
    "    kernel<<<(N + 255) / 256, 256, 0, streams[0]>>>(d_in, N);\n",
    "    kernel<<<(N + 255) / 256, 256, 0, streams[0]>>>(d_in, N);\n",
    "    \n",
    "    cudaStreamEndCapture(streams[0], &graph);\n",
    "    cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Execute graph 10 times\n",
    "    for (int iter = 0; iter < 10; iter++) {\n",
    "        cudaGraphLaunch(instance, streams[0]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"CUDA Graph (10 iterations x 3 kernels): %.2f ms\\n\", ms);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(instance);\n",
    "    cudaGraphDestroy(graph);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    cudaFreeHost(h_in);\n",
    "    cudaFreeHost(h_out);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    printf(\"Concurrency demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o concurrency_demo concurrency_demo.cu\n",
    "!./concurrency_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243256e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimization Checklist\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              CUDA OPTIMIZATION CHECKLIST                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  ‚ñ° MEMORY ACCESS                                            ‚îÇ\n",
    "‚îÇ    ‚ñ° Coalesced global memory access                         ‚îÇ\n",
    "‚îÇ    ‚ñ° Minimize global memory transactions                    ‚îÇ\n",
    "‚îÇ    ‚ñ° Use shared memory for reused data                      ‚îÇ\n",
    "‚îÇ    ‚ñ° Avoid bank conflicts in shared memory                  ‚îÇ\n",
    "‚îÇ    ‚ñ° Use __ldg() for read-only data                         ‚îÇ\n",
    "‚îÇ    ‚ñ° Use pinned memory for host-device transfers            ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  ‚ñ° COMPUTE                                                  ‚îÇ\n",
    "‚îÇ    ‚ñ° Use fast math where precision allows                   ‚îÇ\n",
    "‚îÇ    ‚ñ° Minimize warp divergence                               ‚îÇ\n",
    "‚îÇ    ‚ñ° Use appropriate data types (float vs double)           ‚îÇ\n",
    "‚îÇ    ‚ñ° Unroll loops where beneficial                          ‚îÇ\n",
    "‚îÇ    ‚ñ° Use warp-level primitives                              ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  ‚ñ° OCCUPANCY                                                ‚îÇ\n",
    "‚îÇ    ‚ñ° Use occupancy calculator for block size                ‚îÇ\n",
    "‚îÇ    ‚ñ° Balance registers vs occupancy                         ‚îÇ\n",
    "‚îÇ    ‚ñ° Balance shared memory vs occupancy                     ‚îÇ\n",
    "‚îÇ    ‚ñ° Ensure enough blocks to saturate GPU                   ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  ‚ñ° CONCURRENCY                                              ‚îÇ\n",
    "‚îÇ    ‚ñ° Overlap compute and memory transfers                   ‚îÇ\n",
    "‚îÇ    ‚ñ° Use multiple streams for independent work              ‚îÇ\n",
    "‚îÇ    ‚ñ° Use CUDA Graphs for repeated patterns                  ‚îÇ\n",
    "‚îÇ    ‚ñ° Enable peer access for multi-GPU                       ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  ‚ñ° PROFILING                                                ‚îÇ\n",
    "‚îÇ    ‚ñ° Profile with Nsight Compute                            ‚îÇ\n",
    "‚îÇ    ‚ñ° Check roofline position                                ‚îÇ\n",
    "‚îÇ    ‚ñ° Identify bottlenecks before optimizing                 ‚îÇ\n",
    "‚îÇ    ‚ñ° Measure improvement after each change                  ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d81c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Optimized Example: Matrix Transpose\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// optimized_transpose.cu - Fully optimized matrix transpose\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "// Naive transpose (for comparison)\n",
    "__global__ void transposeNaive(float* out, float* in, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        out[x * height + y] = in[y * width + x];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized transpose with shared memory and bank conflict avoidance\n",
    "__global__ void transposeOptimized(float* out, float* in, int width, int height) {\n",
    "    // +1 padding to avoid bank conflicts\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Load tile (coalesced read)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Transposed coordinates\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Store transposed tile (coalesced write)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int WIDTH = 4096, HEIGHT = 4096;\n",
    "    size_t bytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, bytes);\n",
    "    cudaMalloc(&d_out, bytes);\n",
    "    \n",
    "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 grid((WIDTH + TILE_DIM - 1) / TILE_DIM,\n",
    "              (HEIGHT + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    // Warmup\n",
    "    transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int RUNS = 100;\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float bandwidth = 2.0f * bytes / (ms / RUNS / 1000.0f) / 1e9;\n",
    "    printf(\"Optimized Transpose: %.2f GB/s\\n\", bandwidth);\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e03de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile optimized_transpose.cu\n",
    "// optimized_transpose.cu - Fully optimized matrix transpose\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "// Naive transpose (for comparison)\n",
    "__global__ void transposeNaive(float* out, float* in, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        out[x * height + y] = in[y * width + x];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized transpose with shared memory and bank conflict avoidance\n",
    "__global__ void transposeOptimized(float* out, float* in, int width, int height) {\n",
    "    // +1 padding to avoid bank conflicts\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Load tile (coalesced read)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Transposed coordinates\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Store transposed tile (coalesced write)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int WIDTH = 4096, HEIGHT = 4096;\n",
    "    size_t bytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, bytes);\n",
    "    cudaMalloc(&d_out, bytes);\n",
    "    \n",
    "    // Initialize input\n",
    "    float* h_in = new float[WIDTH * HEIGHT];\n",
    "    for (int i = 0; i < WIDTH * HEIGHT; i++) h_in[i] = (float)i;\n",
    "    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 grid((WIDTH + TILE_DIM - 1) / TILE_DIM,\n",
    "              (HEIGHT + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    // Warmup\n",
    "    transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int RUNS = 100;\n",
    "    float ms;\n",
    "    \n",
    "    // Benchmark naive\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        transposeNaive<<<grid, dim3(32, 32)>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float naiveBW = 2.0f * bytes / (ms / RUNS / 1000.0f) / 1e9;\n",
    "    printf(\"Naive Transpose:     %.2f GB/s\\n\", naiveBW);\n",
    "    \n",
    "    // Benchmark optimized\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float optBW = 2.0f * bytes / (ms / RUNS / 1000.0f) / 1e9;\n",
    "    printf(\"Optimized Transpose: %.2f GB/s\\n\", optBW);\n",
    "    printf(\"Speedup: %.2fx\\n\", optBW / naiveBW);\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    delete[] h_in;\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o optimized_transpose optimized_transpose.cu\n",
    "!./optimized_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a62d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to review and practice optimization techniques:\n",
    "\n",
    "**Exercise 1: Memory Coalescing**\n",
    "- Fix a kernel with poor memory access patterns\n",
    "- Measure the performance improvement\n",
    "\n",
    "**Exercise 2: Shared Memory Optimization**\n",
    "- Convert a naive matrix operation to use shared memory\n",
    "- Handle bank conflicts appropriately\n",
    "\n",
    "**Exercise 3: Occupancy Analysis**\n",
    "- Experiment with different block sizes\n",
    "- Find the optimal configuration for your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c650c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile optimization_review_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Memory Coalescing\n",
    "// The \"bad\" kernel has strided access; fix it in the \"good\" kernel\n",
    "// =============================================================================\n",
    "\n",
    "// BAD: Strided memory access (each thread reads columns)\n",
    "__global__ void transposeNaive(float* out, float* in, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        // Reading: row-major with stride (bad for reads when x varies slowly)\n",
    "        // Writing: column-major (bad for writes)\n",
    "        out[x * height + y] = in[y * width + x];\n",
    "    }\n",
    "}\n",
    "\n",
    "// TODO: Exercise 1 - Fix the memory access pattern using shared memory\n",
    "// Use a tile of shared memory to enable coalesced reads and writes\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "__global__ void transposeOptimized(float* out, float* in, int width, int height) {\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];  // +1 to avoid bank conflicts\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // TODO: Exercise 1a - Load tile with coalesced reads\n",
    "    // Each thread loads multiple elements (TILE_DIM / BLOCK_ROWS)\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // TODO: Exercise 1b - Write tile with coalesced writes\n",
    "    // Note: indices are swapped for transpose\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Shared Memory - Matrix Row Sum\n",
    "// =============================================================================\n",
    "\n",
    "// Naive: Each thread reads entire row (lots of global memory traffic)\n",
    "__global__ void rowSumNaive(float* out, float* matrix, int rows, int cols) {\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < rows) {\n",
    "        float sum = 0.0f;\n",
    "        for (int c = 0; c < cols; c++) {\n",
    "            sum += matrix[row * cols + c];\n",
    "        }\n",
    "        out[row] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// TODO: Exercise 2 - Use shared memory for collaborative loading\n",
    "__global__ void rowSumOptimized(float* out, float* matrix, int rows, int cols) {\n",
    "    extern __shared__ float sdata[];\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // TODO: Exercise 2a - Each thread loads multiple columns collaboratively\n",
    "    float sum = 0.0f;\n",
    "    for (int c = tid; c < cols; c += blockDim.x) {\n",
    "        sum += matrix[row * cols + c];\n",
    "    }\n",
    "    \n",
    "    // TODO: Exercise 2b - Store partial sum to shared memory\n",
    "    sdata[tid] = sum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // TODO: Exercise 2c - Parallel reduction in shared memory\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] += sdata[tid + s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write result\n",
    "    if (tid == 0) {\n",
    "        out[row] = sdata[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Occupancy Optimization\n",
    "// =============================================================================\n",
    "\n",
    "// Vector add with configurable block size\n",
    "__global__ void vectorAdd(float* c, float* a, float* b, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void testOccupancy() {\n",
    "    printf(\"=== Exercise 3: Occupancy Analysis ===\\n\");\n",
    "    \n",
    "    int n = 10 * 1024 * 1024;  // 10M elements\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    CHECK_CUDA(cudaMalloc(&d_a, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_b, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_c, n * sizeof(float)));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    // Test different block sizes\n",
    "    int blockSizes[] = {32, 64, 128, 256, 512, 1024};\n",
    "    int numSizes = sizeof(blockSizes) / sizeof(blockSizes[0]);\n",
    "    \n",
    "    printf(\"\\n%-12s %-12s %-12s %-15s\\n\", \"Block Size\", \"Num Blocks\", \"Occupancy\", \"Time (ms)\");\n",
    "    printf(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\");\n",
    "    \n",
    "    for (int i = 0; i < numSizes; i++) {\n",
    "        int blockSize = blockSizes[i];\n",
    "        int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "        \n",
    "        // Calculate theoretical occupancy\n",
    "        int maxActiveBlocks;\n",
    "        CHECK_CUDA(cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "            &maxActiveBlocks, vectorAdd, blockSize, 0));\n",
    "        \n",
    "        cudaDeviceProp prop;\n",
    "        CHECK_CUDA(cudaGetDeviceProperties(&prop, 0));\n",
    "        \n",
    "        float occupancy = (float)(maxActiveBlocks * blockSize) / prop.maxThreadsPerMultiProcessor;\n",
    "        \n",
    "        // Warmup\n",
    "        vectorAdd<<<numBlocks, blockSize>>>(d_c, d_a, d_b, n);\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());\n",
    "        \n",
    "        // Time it\n",
    "        CHECK_CUDA(cudaEventRecord(start));\n",
    "        for (int j = 0; j < 100; j++) {\n",
    "            vectorAdd<<<numBlocks, blockSize>>>(d_c, d_a, d_b, n);\n",
    "        }\n",
    "        CHECK_CUDA(cudaEventRecord(stop));\n",
    "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "        \n",
    "        float ms;\n",
    "        CHECK_CUDA(cudaEventElapsedTime(&ms, start, stop));\n",
    "        \n",
    "        printf(\"%-12d %-12d %-12.1f%% %-15.3f\\n\", \n",
    "               blockSize, numBlocks, occupancy * 100, ms / 100);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    // Use occupancy calculator to find optimal\n",
    "    int minGridSize, optBlockSize;\n",
    "    CHECK_CUDA(cudaOccupancyMaxPotentialBlockSize(\n",
    "        &minGridSize, &optBlockSize, vectorAdd, 0, 0));\n",
    "    \n",
    "    printf(\"Suggested optimal block size: %d\\n\", optBlockSize);\n",
    "    printf(\"Suggested minimum grid size: %d\\n\\n\", minGridSize);\n",
    "    \n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaFree(d_a));\n",
    "    CHECK_CUDA(cudaFree(d_b));\n",
    "    CHECK_CUDA(cudaFree(d_c));\n",
    "}\n",
    "\n",
    "// Test Exercise 1\n",
    "void testTranspose() {\n",
    "    printf(\"=== Exercise 1: Memory Coalescing (Transpose) ===\\n\");\n",
    "    \n",
    "    int width = 4096, height = 4096;\n",
    "    size_t size = width * height * sizeof(float);\n",
    "    \n",
    "    float *h_in = (float*)malloc(size);\n",
    "    float *h_out = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < width * height; i++) h_in[i] = (float)i;\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    CHECK_CUDA(cudaMalloc(&d_in, size));\n",
    "    CHECK_CUDA(cudaMalloc(&d_out, size));\n",
    "    CHECK_CUDA(cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 grid((width + TILE_DIM - 1) / TILE_DIM, (height + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    // Naive\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        transposeNaive<<<grid, block>>>(d_out, d_in, width, height);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    float naiveTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&naiveTime, start, stop));\n",
    "    \n",
    "    // Optimized\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        transposeOptimized<<<grid, block>>>(d_out, d_in, width, height);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    float optTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&optTime, start, stop));\n",
    "    \n",
    "    printf(\"Naive transpose: %.3f ms\\n\", naiveTime / 10);\n",
    "    printf(\"Optimized transpose: %.3f ms\\n\", optTime / 10);\n",
    "    printf(\"Speedup: %.2fx\\n\\n\", naiveTime / optTime);\n",
    "    \n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaFree(d_in));\n",
    "    CHECK_CUDA(cudaFree(d_out));\n",
    "    free(h_in);\n",
    "    free(h_out);\n",
    "}\n",
    "\n",
    "// Test Exercise 2\n",
    "void testRowSum() {\n",
    "    printf(\"=== Exercise 2: Shared Memory (Row Sum) ===\\n\");\n",
    "    \n",
    "    int rows = 4096, cols = 4096;\n",
    "    size_t matrixSize = rows * cols * sizeof(float);\n",
    "    size_t outSize = rows * sizeof(float);\n",
    "    \n",
    "    float *h_matrix = (float*)malloc(matrixSize);\n",
    "    float *h_out = (float*)malloc(outSize);\n",
    "    \n",
    "    for (int i = 0; i < rows * cols; i++) h_matrix[i] = 1.0f;\n",
    "    \n",
    "    float *d_matrix, *d_out;\n",
    "    CHECK_CUDA(cudaMalloc(&d_matrix, matrixSize));\n",
    "    CHECK_CUDA(cudaMalloc(&d_out, outSize));\n",
    "    CHECK_CUDA(cudaMemcpy(d_matrix, h_matrix, matrixSize, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    // Naive\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        rowSumNaive<<<(rows + 255) / 256, 256>>>(d_out, d_matrix, rows, cols);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    float naiveTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&naiveTime, start, stop));\n",
    "    \n",
    "    // Optimized\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        rowSumOptimized<<<rows, 256, 256 * sizeof(float)>>>(d_out, d_matrix, rows, cols);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    float optTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&optTime, start, stop));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_out, d_out, outSize, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Naive row sum: %.3f ms\\n\", naiveTime / 10);\n",
    "    printf(\"Optimized row sum: %.3f ms\\n\", optTime / 10);\n",
    "    printf(\"Speedup: %.2fx\\n\", naiveTime / optTime);\n",
    "    printf(\"Sample result (expected %d): %.0f\\n\\n\", cols, h_out[0]);\n",
    "    \n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaFree(d_matrix));\n",
    "    CHECK_CUDA(cudaFree(d_out));\n",
    "    free(h_matrix);\n",
    "    free(h_out);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"\\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\");\n",
    "    printf(\"‚ïë           Optimization Review Exercises                      ‚ïë\\n\");\n",
    "    printf(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\\n\\n\");\n",
    "    \n",
    "    testTranspose();\n",
    "    testRowSum();\n",
    "    testOccupancy();\n",
    "    \n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    printf(\"Study the optimized versions and try profiling with Nsight!\\n\");\n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o optimization_review_exercises optimization_review_exercises.cu && ./optimization_review_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a79e6",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises practice optimization concepts using Numba:\n",
    "\n",
    "```python\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# Exercise 1: Compare coalesced vs non-coalesced access\n",
    "@cuda.jit\n",
    "def coalesced_copy(out, inp):\n",
    "    \"\"\"Good: Coalesced access - consecutive threads access consecutive memory\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < inp.size:\n",
    "        out[idx] = inp[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def strided_copy(out, inp, stride):\n",
    "    \"\"\"Bad: Strided access - threads access memory with gaps\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    strided_idx = (idx * stride) % inp.size\n",
    "    if idx < inp.size:\n",
    "        out[idx] = inp[strided_idx]\n",
    "\n",
    "# Exercise 2: Shared memory tile\n",
    "@cuda.jit\n",
    "def tiled_operation(out, inp):\n",
    "    \"\"\"Use shared memory for data reuse\"\"\"\n",
    "    tile = cuda.shared.array(256, dtype=numba.float32)\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    idx = bx * cuda.blockDim.x + tx\n",
    "    \n",
    "    # Load to shared memory\n",
    "    if idx < inp.size:\n",
    "        tile[tx] = inp[idx]\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Process with neighbors (data reuse from shared memory)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "**Tip**: Profile with `cuda.profiling()` context manager to measure kernel times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7911356",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              OPTIMIZATION SUMMARY                           ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  1. Profile First                                           ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Identify bottleneck before optimizing                 ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Don't guess, measure                                  ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  2. Memory is Usually the Bottleneck                        ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Coalescing is critical                                ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Shared memory for data reuse                          ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Minimize transfers                                    ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  3. Occupancy Matters (to a point)                          ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Need enough parallelism to hide latency               ‚îÇ\n",
    "‚îÇ     ‚Ä¢ But higher isn't always better                        ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  4. Concurrency for Free Performance                        ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Overlap compute and transfers                         ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Use streams and graphs                                ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  5. Iterate and Measure                                     ‚îÇ\n",
    "‚îÇ     ‚Ä¢ One optimization at a time                            ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Verify improvement after each change                  ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Next: Day 4 - Capstone Project"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
