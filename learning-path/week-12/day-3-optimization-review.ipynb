{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a7cd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Optimization Hierarchy\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              OPTIMIZATION PRIORITY ORDER                    │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. ALGORITHM CHOICE          ███████████████████ ~10-100x  │\n",
    "│     • Choose right algorithm                                │\n",
    "│     • Reduce algorithmic complexity                         │\n",
    "│                                                             │\n",
    "│  2. MEMORY ACCESS PATTERNS    ████████████████    ~5-20x    │\n",
    "│     • Coalesced access                                      │\n",
    "│     • Minimize global memory traffic                        │\n",
    "│     • Use shared memory                                     │\n",
    "│                                                             │\n",
    "│  3. OCCUPANCY & PARALLELISM   ████████████        ~2-5x     │\n",
    "│     • Enough threads to hide latency                        │\n",
    "│     • Balance resources per block                           │\n",
    "│                                                             │\n",
    "│  4. INSTRUCTION OPTIMIZATION  ████████            ~1.5-3x   │\n",
    "│     • Fast math functions                                   │\n",
    "│     • Avoid divergence                                      │\n",
    "│                                                             │\n",
    "│  5. CONCURRENCY               ██████              ~1.2-2x   │\n",
    "│     • Streams, graphs                                       │\n",
    "│     • Overlap compute/transfer                              │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a40c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Memory Optimization Techniques\n",
    "\n",
    "### 1.1 Coalesced Memory Access\n",
    "\n",
    "```cpp\n",
    "// GOOD: Coalesced - threads access consecutive memory\n",
    "__global__ void coalesced(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f;  // Thread i accesses element i\n",
    "    }\n",
    "}\n",
    "\n",
    "// BAD: Strided - threads access with stride\n",
    "__global__ void strided(float* data, int n, int stride) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int idx = tid * stride;  // Thread i accesses element i*stride\n",
    "    if (idx < n) {\n",
    "        data[idx] = data[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 1.2 Shared Memory Usage\n",
    "\n",
    "```cpp\n",
    "// Pattern: Load to shared -> Sync -> Process -> Sync -> Store\n",
    "__global__ void withShared(float* out, float* in, int n) {\n",
    "    __shared__ float smem[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + tid;\n",
    "    \n",
    "    // Load to shared memory\n",
    "    smem[tid] = (gid < n) ? in[gid] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Process using shared memory\n",
    "    float result = smem[tid];\n",
    "    if (tid > 0) result += smem[tid - 1];\n",
    "    if (tid < 255) result += smem[tid + 1];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Write result\n",
    "    if (gid < n) out[gid] = result;\n",
    "}\n",
    "```\n",
    "\n",
    "### 1.3 Bank Conflict Avoidance\n",
    "\n",
    "```cpp\n",
    "// BAD: Bank conflicts (stride of 32)\n",
    "__shared__ float smem[32][32];\n",
    "smem[threadIdx.x][0] = value;  // All access bank 0!\n",
    "\n",
    "// GOOD: Padding to avoid conflicts\n",
    "__shared__ float smem[32][33];  // +1 padding\n",
    "smem[threadIdx.x][0] = value;   // Different banks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f68692",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Compute Optimization\n",
    "\n",
    "### 2.1 Instruction Throughput\n",
    "\n",
    "```cpp\n",
    "// Fast math intrinsics (less accurate, much faster)\n",
    "__device__ float fast_sin(float x) {\n",
    "    return __sinf(x);      // ~10x faster than sinf()\n",
    "}\n",
    "\n",
    "__device__ float fast_exp(float x) {\n",
    "    return __expf(x);      // ~10x faster than expf()\n",
    "}\n",
    "\n",
    "__device__ float fast_rsqrt(float x) {\n",
    "    return rsqrtf(x);      // 1/sqrt(x), very fast\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.2 Warp Divergence\n",
    "\n",
    "```cpp\n",
    "// BAD: Divergent branches\n",
    "__global__ void divergent(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid % 2 == 0) {      // Half warp does one thing\n",
    "        data[tid] = expensive_op1(data[tid]);\n",
    "    } else {                 // Other half does another\n",
    "        data[tid] = expensive_op2(data[tid]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// BETTER: Separate into different warps\n",
    "__global__ void nondivergent(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int warpId = tid / 32;\n",
    "    \n",
    "    if (warpId % 2 == 0) {   // Whole warp does one thing\n",
    "        data[tid] = expensive_op1(data[tid]);\n",
    "    } else {                 // Other warp does another\n",
    "        data[tid] = expensive_op2(data[tid]);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.3 Loop Unrolling\n",
    "\n",
    "```cpp\n",
    "// Manual unroll\n",
    "__global__ void unrolled(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    #pragma unroll 4\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "        int idx = tid + i * blockDim.x * gridDim.x;\n",
    "        if (idx < n) data[idx] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544893c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Occupancy Optimization\n",
    "\n",
    "### 3.1 Resource Balancing\n",
    "\n",
    "```cpp\n",
    "// Query optimal block size\n",
    "int minGridSize, optBlockSize;\n",
    "cudaOccupancyMaxPotentialBlockSize(\n",
    "    &minGridSize, &optBlockSize, \n",
    "    myKernel, 0, 0);\n",
    "\n",
    "printf(\"Optimal block size: %d\\n\", optBlockSize);\n",
    "printf(\"Min grid size: %d\\n\", minGridSize);\n",
    "```\n",
    "\n",
    "### 3.2 Register Pressure\n",
    "\n",
    "```cpp\n",
    "// Limit registers to increase occupancy\n",
    "__global__ __launch_bounds__(256, 4)  // 256 threads, 4 blocks/SM\n",
    "void limitedRegisters(float* data) {\n",
    "    // Kernel code\n",
    "}\n",
    "\n",
    "// Compile with: nvcc -maxrregcount=32 kernel.cu\n",
    "```\n",
    "\n",
    "### 3.3 Shared Memory Configuration\n",
    "\n",
    "```cpp\n",
    "// Prefer more shared memory over L1 cache\n",
    "cudaFuncSetCacheConfig(myKernel, cudaFuncCachePreferShared);\n",
    "\n",
    "// Options:\n",
    "// cudaFuncCachePreferNone   - No preference\n",
    "// cudaFuncCachePreferShared - Prefer shared memory\n",
    "// cudaFuncCachePreferL1     - Prefer L1 cache\n",
    "// cudaFuncCachePreferEqual  - Equal split\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2baf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Concurrency Optimization\n",
    "\n",
    "### 4.1 Stream Overlap Pattern\n",
    "\n",
    "```cpp\n",
    "// Chunk and overlap pattern\n",
    "const int NUM_STREAMS = 4;\n",
    "cudaStream_t streams[NUM_STREAMS];\n",
    "\n",
    "for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "    cudaStreamCreate(&streams[i]);\n",
    "}\n",
    "\n",
    "int chunkSize = N / NUM_STREAMS;\n",
    "\n",
    "for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "    int offset = i * chunkSize;\n",
    "    \n",
    "    // H2D for chunk i\n",
    "    cudaMemcpyAsync(d_in + offset, h_in + offset,\n",
    "                    chunkSize * sizeof(float),\n",
    "                    cudaMemcpyHostToDevice, streams[i]);\n",
    "    \n",
    "    // Compute chunk i\n",
    "    kernel<<<blocks, threads, 0, streams[i]>>>(\n",
    "        d_out + offset, d_in + offset, chunkSize);\n",
    "    \n",
    "    // D2H for chunk i\n",
    "    cudaMemcpyAsync(h_out + offset, d_out + offset,\n",
    "                    chunkSize * sizeof(float),\n",
    "                    cudaMemcpyDeviceToHost, streams[i]);\n",
    "}\n",
    "```\n",
    "\n",
    "### 4.2 CUDA Graphs for Repeated Patterns\n",
    "\n",
    "```cpp\n",
    "// Capture repeating pattern\n",
    "cudaGraph_t graph;\n",
    "cudaGraphExec_t instance;\n",
    "\n",
    "cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "\n",
    "// Pattern to repeat\n",
    "kernelA<<<...>>>(...);\n",
    "kernelB<<<...>>>(...);\n",
    "kernelC<<<...>>>(...);\n",
    "\n",
    "cudaStreamEndCapture(stream, &graph);\n",
    "cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);\n",
    "\n",
    "// Execute efficiently many times\n",
    "for (int iter = 0; iter < 1000; iter++) {\n",
    "    cudaGraphLaunch(instance, stream);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243256e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimization Checklist\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              CUDA OPTIMIZATION CHECKLIST                    │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  □ MEMORY ACCESS                                            │\n",
    "│    □ Coalesced global memory access                         │\n",
    "│    □ Minimize global memory transactions                    │\n",
    "│    □ Use shared memory for reused data                      │\n",
    "│    □ Avoid bank conflicts in shared memory                  │\n",
    "│    □ Use __ldg() for read-only data                         │\n",
    "│    □ Use pinned memory for host-device transfers            │\n",
    "│                                                             │\n",
    "│  □ COMPUTE                                                  │\n",
    "│    □ Use fast math where precision allows                   │\n",
    "│    □ Minimize warp divergence                               │\n",
    "│    □ Use appropriate data types (float vs double)           │\n",
    "│    □ Unroll loops where beneficial                          │\n",
    "│    □ Use warp-level primitives                              │\n",
    "│                                                             │\n",
    "│  □ OCCUPANCY                                                │\n",
    "│    □ Use occupancy calculator for block size                │\n",
    "│    □ Balance registers vs occupancy                         │\n",
    "│    □ Balance shared memory vs occupancy                     │\n",
    "│    □ Ensure enough blocks to saturate GPU                   │\n",
    "│                                                             │\n",
    "│  □ CONCURRENCY                                              │\n",
    "│    □ Overlap compute and memory transfers                   │\n",
    "│    □ Use multiple streams for independent work              │\n",
    "│    □ Use CUDA Graphs for repeated patterns                  │\n",
    "│    □ Enable peer access for multi-GPU                       │\n",
    "│                                                             │\n",
    "│  □ PROFILING                                                │\n",
    "│    □ Profile with Nsight Compute                            │\n",
    "│    □ Check roofline position                                │\n",
    "│    □ Identify bottlenecks before optimizing                 │\n",
    "│    □ Measure improvement after each change                  │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d81c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Optimized Example: Matrix Transpose\n",
    "\n",
    "```cpp\n",
    "// optimized_transpose.cu - Fully optimized matrix transpose\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "// Naive transpose (for comparison)\n",
    "__global__ void transposeNaive(float* out, float* in, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        out[x * height + y] = in[y * width + x];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized transpose with shared memory and bank conflict avoidance\n",
    "__global__ void transposeOptimized(float* out, float* in, int width, int height) {\n",
    "    // +1 padding to avoid bank conflicts\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Load tile (coalesced read)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Transposed coordinates\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Store transposed tile (coalesced write)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int WIDTH = 4096, HEIGHT = 4096;\n",
    "    size_t bytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, bytes);\n",
    "    cudaMalloc(&d_out, bytes);\n",
    "    \n",
    "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 grid((WIDTH + TILE_DIM - 1) / TILE_DIM,\n",
    "              (HEIGHT + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    // Warmup\n",
    "    transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int RUNS = 100;\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float bandwidth = 2.0f * bytes / (ms / RUNS / 1000.0f) / 1e9;\n",
    "    printf(\"Optimized Transpose: %.2f GB/s\\n\", bandwidth);\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7911356",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              OPTIMIZATION SUMMARY                           │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. Profile First                                           │\n",
    "│     • Identify bottleneck before optimizing                 │\n",
    "│     • Don't guess, measure                                  │\n",
    "│                                                             │\n",
    "│  2. Memory is Usually the Bottleneck                        │\n",
    "│     • Coalescing is critical                                │\n",
    "│     • Shared memory for data reuse                          │\n",
    "│     • Minimize transfers                                    │\n",
    "│                                                             │\n",
    "│  3. Occupancy Matters (to a point)                          │\n",
    "│     • Need enough parallelism to hide latency               │\n",
    "│     • But higher isn't always better                        │\n",
    "│                                                             │\n",
    "│  4. Concurrency for Free Performance                        │\n",
    "│     • Overlap compute and transfers                         │\n",
    "│     • Use streams and graphs                                │\n",
    "│                                                             │\n",
    "│  5. Iterate and Measure                                     │\n",
    "│     • One optimization at a time                            │\n",
    "│     • Verify improvement after each change                  │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 4 - Capstone Project"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
