{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c5a13",
   "metadata": {},
   "source": [
    "# Day 3: Optimization Review - The Full Performance Tuning Checklist\n",
    "\n",
    "## ğŸ¯ The Hook: The Performance Detective's Handbook!\n",
    "\n",
    "**You're a performance detective.** Your job: investigate slow code, identify the culprits, and fix them. But where do you even start?\n",
    "\n",
    "After 11 weeks of learning individual techniques, it's time to see **the big picture**. Think of today as getting your **detective's handbook** - a systematic approach to GPU optimization:\n",
    "\n",
    "```\n",
    "ğŸ” Case File: \"The Slow Kernel Mystery\"\n",
    "   Suspect #1: Memory bandwidth bottleneck\n",
    "   Suspect #2: Low occupancy\n",
    "   Suspect #3: Thread divergence\n",
    "   Suspect #4: Instruction inefficiency\n",
    "   \n",
    "   Your tools: Profiler, roofline analysis, checklist\n",
    "```\n",
    "\n",
    "Every optimization you've learned has its place. Today you'll learn **when to apply what** and **in what order**. Because applying the wrong optimization is worse than none at all!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Apply the optimization hierarchy** - Know which optimizations give the biggest gains\n",
    "2. **Diagnose performance bottlenecks** - Memory-bound vs compute-bound analysis\n",
    "3. **Use a systematic checklist** - Step-by-step approach to optimization\n",
    "4. **Recall techniques from all weeks** - Connect memory, parallelism, and concurrency optimizations\n",
    "5. **Make data-driven decisions** - Profile first, optimize second, verify always\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ´ Concept Card: Full Performance Tuning Checklist\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚            FULL PERFORMANCE TUNING CHECKLIST                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ”¢ OPTIMIZATION PRIORITY (BIGGEST IMPACT FIRST):               â”‚\n",
    "â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•               â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  [10-100x] 1. ALGORITHM CHOICE                                  â”‚\n",
    "â”‚            â–¡ Right algorithm for the problem?                   â”‚\n",
    "â”‚            â–¡ Reduced computational complexity?                  â”‚\n",
    "â”‚            â–¡ GPU-friendly algorithm variant?                    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  [5-20x]   2. MEMORY ACCESS PATTERNS                            â”‚\n",
    "â”‚            â–¡ Coalesced global memory access? (Week 2)           â”‚\n",
    "â”‚            â–¡ Shared memory for data reuse? (Week 2-3)           â”‚\n",
    "â”‚            â–¡ No bank conflicts? (Week 3)                        â”‚\n",
    "â”‚            â–¡ Texture/constant memory for read-only? (Week 2)    â”‚\n",
    "â”‚            â–¡ Minimized host-device transfers? (Week 9)          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  [2-5x]    3. OCCUPANCY & PARALLELISM                           â”‚\n",
    "â”‚            â–¡ Enough threads to hide latency? (Week 7)           â”‚\n",
    "â”‚            â–¡ Balanced register/shared usage? (Week 7)           â”‚\n",
    "â”‚            â–¡ Block size tuned (32-256)? (Week 1)                â”‚\n",
    "â”‚            â–¡ Grid size covers all data? (Week 1)                â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  [1.5-3x]  4. INSTRUCTION OPTIMIZATION                          â”‚\n",
    "â”‚            â–¡ Fast math where precision allows? (Week 7)         â”‚\n",
    "â”‚            â–¡ Minimized warp divergence? (Week 4)                â”‚\n",
    "â”‚            â–¡ Loop unrolling applied? (Week 4)                   â”‚\n",
    "â”‚            â–¡ Intrinsics used? (__fmaf, __sinf)                  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  [1.2-2x]  5. CONCURRENCY                                       â”‚\n",
    "â”‚            â–¡ Overlapped compute/transfer? (Week 9)              â”‚\n",
    "â”‚            â–¡ Multiple streams utilized? (Week 9)                â”‚\n",
    "â”‚            â–¡ CUDA graphs for repeated work? (Week 10)           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸ” DIAGNOSTIC DECISION TREE:                                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Is kernel slow?                                                â”‚\n",
    "â”‚       â”‚                                                         â”‚\n",
    "â”‚       â–¼                                                         â”‚\n",
    "â”‚  Profile with Nsight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
    "â”‚       â”‚                                â”‚                        â”‚\n",
    "â”‚       â–¼                                â–¼                        â”‚\n",
    "â”‚  Memory-bound?              Compute-bound?                      â”‚\n",
    "â”‚  â€¢ Low compute intensity    â€¢ High FLOP/byte                    â”‚\n",
    "â”‚  â€¢ Fix: coalescing,         â€¢ Fix: fast math,                   â”‚\n",
    "â”‚    shared memory,             instruction mix,                  â”‚\n",
    "â”‚    reduce traffic             algorithmic change                â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸ’¡ KEY INSIGHT:                                                â”‚\n",
    "â”‚  \"Premature optimization is the root of all evil\" - Knuth       â”‚\n",
    "â”‚  ALWAYS profile first! The bottleneck is rarely where you think.â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a7cd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Optimization Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              OPTIMIZATION PRIORITY ORDER                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  1. ALGORITHM CHOICE          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ~10-100x  â”‚\n",
    "â”‚     â€¢ Choose right algorithm                                â”‚\n",
    "â”‚     â€¢ Reduce algorithmic complexity                         â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  2. MEMORY ACCESS PATTERNS    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    ~5-20x    â”‚\n",
    "â”‚     â€¢ Coalesced access                                      â”‚\n",
    "â”‚     â€¢ Minimize global memory traffic                        â”‚\n",
    "â”‚     â€¢ Use shared memory                                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  3. OCCUPANCY & PARALLELISM   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        ~2-5x     â”‚\n",
    "â”‚     â€¢ Enough threads to hide latency                        â”‚\n",
    "â”‚     â€¢ Balance resources per block                           â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  4. INSTRUCTION OPTIMIZATION  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            ~1.5-3x   â”‚\n",
    "â”‚     â€¢ Fast math functions                                   â”‚\n",
    "â”‚     â€¢ Avoid divergence                                      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  5. CONCURRENCY               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              ~1.2-2x   â”‚\n",
    "â”‚     â€¢ Streams, graphs                                       â”‚\n",
    "â”‚     â€¢ Overlap compute/transfer                              â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a40c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Memory Optimization Techniques\n",
    "\n",
    "### 1.1 Coalesced Memory Access\n",
    "\n",
    "```cpp\n",
    "// GOOD: Coalesced - threads access consecutive memory\n",
    "__global__ void coalesced(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f;  // Thread i accesses element i\n",
    "    }\n",
    "}\n",
    "\n",
    "// BAD: Strided - threads access with stride\n",
    "__global__ void strided(float* data, int n, int stride) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int idx = tid * stride;  // Thread i accesses element i*stride\n",
    "    if (idx < n) {\n",
    "        data[idx] = data[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 1.2 Shared Memory Usage\n",
    "\n",
    "```cpp\n",
    "// Pattern: Load to shared -> Sync -> Process -> Sync -> Store\n",
    "__global__ void withShared(float* out, float* in, int n) {\n",
    "    __shared__ float smem[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + tid;\n",
    "    \n",
    "    // Load to shared memory\n",
    "    smem[tid] = (gid < n) ? in[gid] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Process using shared memory\n",
    "    float result = smem[tid];\n",
    "    if (tid > 0) result += smem[tid - 1];\n",
    "    if (tid < 255) result += smem[tid + 1];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Write result\n",
    "    if (gid < n) out[gid] = result;\n",
    "}\n",
    "```\n",
    "\n",
    "### 1.3 Bank Conflict Avoidance\n",
    "\n",
    "```cpp\n",
    "// BAD: Bank conflicts (stride of 32)\n",
    "__shared__ float smem[32][32];\n",
    "smem[threadIdx.x][0] = value;  // All access bank 0!\n",
    "\n",
    "// GOOD: Padding to avoid conflicts\n",
    "__shared__ float smem[32][33];  // +1 padding\n",
    "smem[threadIdx.x][0] = value;   // Different banks\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile memory_patterns.cu\n",
    "// memory_patterns.cu - Memory optimization examples\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// GOOD: Coalesced - threads access consecutive memory\n",
    "__global__ void coalesced(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f;  // Thread i accesses element i\n",
    "    }\n",
    "}\n",
    "\n",
    "// BAD: Strided - threads access with stride\n",
    "__global__ void strided(float* data, int n, int stride) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int idx = tid * stride;  // Thread i accesses element i*stride\n",
    "    if (idx < n) {\n",
    "        data[idx] = data[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Pattern: Load to shared -> Sync -> Process -> Sync -> Store\n",
    "__global__ void withShared(float* out, float* in, int n) {\n",
    "    __shared__ float smem[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + tid;\n",
    "    \n",
    "    // Load to shared memory\n",
    "    smem[tid] = (gid < n) ? in[gid] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Process using shared memory\n",
    "    float result = smem[tid];\n",
    "    if (tid > 0) result += smem[tid - 1];\n",
    "    if (tid < 255) result += smem[tid + 1];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Write result\n",
    "    if (gid < n) out[gid] = result;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data, *d_out;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMalloc(&d_out, N * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_data = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // Test coalesced\n",
    "    coalesced<<<numBlocks, blockSize>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Coalesced access completed\\n\");\n",
    "    \n",
    "    // Test with shared memory\n",
    "    withShared<<<numBlocks, blockSize>>>(d_out, d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Shared memory pattern completed\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_out);\n",
    "    delete[] h_data;\n",
    "    \n",
    "    printf(\"Memory patterns demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o memory_patterns memory_patterns.cu\n",
    "!./memory_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f68692",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Compute Optimization\n",
    "\n",
    "### 2.1 Instruction Throughput\n",
    "\n",
    "```cpp\n",
    "// Fast math intrinsics (less accurate, much faster)\n",
    "__device__ float fast_sin(float x) {\n",
    "    return __sinf(x);      // ~10x faster than sinf()\n",
    "}\n",
    "\n",
    "__device__ float fast_exp(float x) {\n",
    "    return __expf(x);      // ~10x faster than expf()\n",
    "}\n",
    "\n",
    "__device__ float fast_rsqrt(float x) {\n",
    "    return rsqrtf(x);      // 1/sqrt(x), very fast\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.2 Warp Divergence\n",
    "\n",
    "```cpp\n",
    "// BAD: Divergent branches\n",
    "__global__ void divergent(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid % 2 == 0) {      // Half warp does one thing\n",
    "        data[tid] = expensive_op1(data[tid]);\n",
    "    } else {                 // Other half does another\n",
    "        data[tid] = expensive_op2(data[tid]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// BETTER: Separate into different warps\n",
    "__global__ void nondivergent(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int warpId = tid / 32;\n",
    "    \n",
    "    if (warpId % 2 == 0) {   // Whole warp does one thing\n",
    "        data[tid] = expensive_op1(data[tid]);\n",
    "    } else {                 // Other warp does another\n",
    "        data[tid] = expensive_op2(data[tid]);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.3 Loop Unrolling\n",
    "\n",
    "```cpp\n",
    "// Manual unroll\n",
    "__global__ void unrolled(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    #pragma unroll 4\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "        int idx = tid + i * blockDim.x * gridDim.x;\n",
    "        if (idx < n) data[idx] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile compute_optimization.cu\n",
    "// compute_optimization.cu - Compute optimization examples\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Fast math intrinsics (less accurate, much faster)\n",
    "__device__ float fast_computation(float x) {\n",
    "    float sin_val = __sinf(x);      // ~10x faster than sinf()\n",
    "    float exp_val = __expf(x);      // ~10x faster than expf()\n",
    "    float rsqrt_val = rsqrtf(x);    // 1/sqrt(x), very fast\n",
    "    return sin_val + exp_val * rsqrt_val;\n",
    "}\n",
    "\n",
    "// BETTER: Non-divergent - whole warps take same path\n",
    "__global__ void nondivergent(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int warpId = tid / 32;\n",
    "    \n",
    "    if (tid < n) {\n",
    "        if (warpId % 2 == 0) {   // Whole warp does one thing\n",
    "            data[tid] = __sinf(data[tid]);\n",
    "        } else {                 // Other warp does another\n",
    "            data[tid] = __cosf(data[tid]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Manual unroll\n",
    "__global__ void unrolled(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    #pragma unroll 4\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "        int idx = tid + i * blockDim.x * gridDim.x;\n",
    "        if (idx < n) data[idx] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_data = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f + (float)i / N;\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // Test non-divergent\n",
    "    nondivergent<<<numBlocks, blockSize>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Non-divergent kernel completed\\n\");\n",
    "    \n",
    "    // Test unrolled\n",
    "    unrolled<<<numBlocks / 4, blockSize>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Unrolled kernel completed\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    delete[] h_data;\n",
    "    \n",
    "    printf(\"Compute optimization demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o compute_optimization compute_optimization.cu\n",
    "!./compute_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544893c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Occupancy Optimization\n",
    "\n",
    "### 3.1 Resource Balancing\n",
    "\n",
    "```cpp\n",
    "// Query optimal block size\n",
    "int minGridSize, optBlockSize;\n",
    "cudaOccupancyMaxPotentialBlockSize(\n",
    "    &minGridSize, &optBlockSize, \n",
    "    myKernel, 0, 0);\n",
    "\n",
    "printf(\"Optimal block size: %d\\n\", optBlockSize);\n",
    "printf(\"Min grid size: %d\\n\", minGridSize);\n",
    "```\n",
    "\n",
    "### 3.2 Register Pressure\n",
    "\n",
    "```cpp\n",
    "// Limit registers to increase occupancy\n",
    "__global__ __launch_bounds__(256, 4)  // 256 threads, 4 blocks/SM\n",
    "void limitedRegisters(float* data) {\n",
    "    // Kernel code\n",
    "}\n",
    "\n",
    "// Compile with: nvcc -maxrregcount=32 kernel.cu\n",
    "```\n",
    "\n",
    "### 3.3 Shared Memory Configuration\n",
    "\n",
    "```cpp\n",
    "// Prefer more shared memory over L1 cache\n",
    "cudaFuncSetCacheConfig(myKernel, cudaFuncCachePreferShared);\n",
    "\n",
    "// Options:\n",
    "// cudaFuncCachePreferNone   - No preference\n",
    "// cudaFuncCachePreferShared - Prefer shared memory\n",
    "// cudaFuncCachePreferL1     - Prefer L1 cache\n",
    "// cudaFuncCachePreferEqual  - Equal split\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile occupancy_demo.cu\n",
    "// occupancy_demo.cu - Occupancy optimization examples\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Limit registers to increase occupancy\n",
    "__global__ __launch_bounds__(256, 4)  // 256 threads, 4 blocks/SM\n",
    "void limitedRegisters(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void myKernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = sqrtf(data[tid]) * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Query optimal block size\n",
    "    int minGridSize, optBlockSize;\n",
    "    cudaOccupancyMaxPotentialBlockSize(\n",
    "        &minGridSize, &optBlockSize, \n",
    "        myKernel, 0, 0);\n",
    "    \n",
    "    printf(\"Optimal block size: %d\\n\", optBlockSize);\n",
    "    printf(\"Min grid size: %d\\n\", minGridSize);\n",
    "    \n",
    "    // Calculate occupancy\n",
    "    int numBlocks;\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "        &numBlocks, myKernel, optBlockSize, 0);\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    \n",
    "    int maxBlocksPerSM = prop.maxThreadsPerMultiProcessor / optBlockSize;\n",
    "    float occupancy = (float)numBlocks / maxBlocksPerSM;\n",
    "    printf(\"Max blocks per SM: %d\\n\", maxBlocksPerSM);\n",
    "    printf(\"Achieved occupancy: %.1f%%\\n\", occupancy * 100);\n",
    "    \n",
    "    // Set cache preference\n",
    "    cudaFuncSetCacheConfig(myKernel, cudaFuncCachePreferShared);\n",
    "    printf(\"Set cache preference: PreferShared\\n\");\n",
    "    \n",
    "    // Launch kernel with optimal parameters\n",
    "    int numBlocksTotal = (N + optBlockSize - 1) / optBlockSize;\n",
    "    myKernel<<<numBlocksTotal, optBlockSize>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Kernel completed with optimal configuration!\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o occupancy_demo occupancy_demo.cu\n",
    "!./occupancy_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2baf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Concurrency Optimization\n",
    "\n",
    "### 4.1 Stream Overlap Pattern\n",
    "\n",
    "```cpp\n",
    "// Chunk and overlap pattern\n",
    "const int NUM_STREAMS = 4;\n",
    "cudaStream_t streams[NUM_STREAMS];\n",
    "\n",
    "for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "    cudaStreamCreate(&streams[i]);\n",
    "}\n",
    "\n",
    "int chunkSize = N / NUM_STREAMS;\n",
    "\n",
    "for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "    int offset = i * chunkSize;\n",
    "    \n",
    "    // H2D for chunk i\n",
    "    cudaMemcpyAsync(d_in + offset, h_in + offset,\n",
    "                    chunkSize * sizeof(float),\n",
    "                    cudaMemcpyHostToDevice, streams[i]);\n",
    "    \n",
    "    // Compute chunk i\n",
    "    kernel<<<blocks, threads, 0, streams[i]>>>(\n",
    "        d_out + offset, d_in + offset, chunkSize);\n",
    "    \n",
    "    // D2H for chunk i\n",
    "    cudaMemcpyAsync(h_out + offset, d_out + offset,\n",
    "                    chunkSize * sizeof(float),\n",
    "                    cudaMemcpyDeviceToHost, streams[i]);\n",
    "}\n",
    "```\n",
    "\n",
    "### 4.2 CUDA Graphs for Repeated Patterns\n",
    "\n",
    "```cpp\n",
    "// Capture repeating pattern\n",
    "cudaGraph_t graph;\n",
    "cudaGraphExec_t instance;\n",
    "\n",
    "cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "\n",
    "// Pattern to repeat\n",
    "kernelA<<<...>>>(...);\n",
    "kernelB<<<...>>>(...);\n",
    "kernelC<<<...>>>(...);\n",
    "\n",
    "cudaStreamEndCapture(stream, &graph);\n",
    "cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);\n",
    "\n",
    "// Execute efficiently many times\n",
    "for (int iter = 0; iter < 1000; iter++) {\n",
    "    cudaGraphLaunch(instance, stream);\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile concurrency_demo.cu\n",
    "// concurrency_demo.cu - Stream overlap and CUDA Graphs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void kernel(float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = sqrtf(data[tid]) * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 22;  // 4M elements\n",
    "    const int NUM_STREAMS = 4;\n",
    "    size_t bytes = N * sizeof(float);\n",
    "    int chunkSize = N / NUM_STREAMS;\n",
    "    \n",
    "    // Allocate pinned host memory\n",
    "    float *h_in, *h_out;\n",
    "    cudaMallocHost(&h_in, bytes);\n",
    "    cudaMallocHost(&h_out, bytes);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < N; i++) h_in[i] = (float)i;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, bytes);\n",
    "    cudaMalloc(&d_out, bytes);\n",
    "    \n",
    "    // Create streams\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int blocks = (chunkSize + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // ============================================\n",
    "    // Stream Overlap Pattern\n",
    "    // ============================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        int offset = i * chunkSize;\n",
    "        \n",
    "        // H2D for chunk i\n",
    "        cudaMemcpyAsync(d_in + offset, h_in + offset,\n",
    "                        chunkSize * sizeof(float),\n",
    "                        cudaMemcpyHostToDevice, streams[i]);\n",
    "        \n",
    "        // Compute chunk i\n",
    "        kernel<<<blocks, blockSize, 0, streams[i]>>>(\n",
    "            d_in + offset, chunkSize);\n",
    "        \n",
    "        // D2H for chunk i\n",
    "        cudaMemcpyAsync(h_out + offset, d_in + offset,\n",
    "                        chunkSize * sizeof(float),\n",
    "                        cudaMemcpyDeviceToHost, streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Sync all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Stream overlap: %.2f ms\\n\", ms);\n",
    "    \n",
    "    // ============================================\n",
    "    // CUDA Graph for Repeated Pattern\n",
    "    // ============================================\n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t instance;\n",
    "    \n",
    "    // Reset data\n",
    "    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Pattern to repeat: 3 kernel passes\n",
    "    kernel<<<(N + 255) / 256, 256, 0, streams[0]>>>(d_in, N);\n",
    "    kernel<<<(N + 255) / 256, 256, 0, streams[0]>>>(d_in, N);\n",
    "    kernel<<<(N + 255) / 256, 256, 0, streams[0]>>>(d_in, N);\n",
    "    \n",
    "    cudaStreamEndCapture(streams[0], &graph);\n",
    "    cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Execute graph 10 times\n",
    "    for (int iter = 0; iter < 10; iter++) {\n",
    "        cudaGraphLaunch(instance, streams[0]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"CUDA Graph (10 iterations x 3 kernels): %.2f ms\\n\", ms);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaGraphExecDestroy(instance);\n",
    "    cudaGraphDestroy(graph);\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    cudaFreeHost(h_in);\n",
    "    cudaFreeHost(h_out);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    printf(\"Concurrency demo complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o concurrency_demo concurrency_demo.cu\n",
    "!./concurrency_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243256e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimization Checklist\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              CUDA OPTIMIZATION CHECKLIST                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â–¡ MEMORY ACCESS                                            â”‚\n",
    "â”‚    â–¡ Coalesced global memory access                         â”‚\n",
    "â”‚    â–¡ Minimize global memory transactions                    â”‚\n",
    "â”‚    â–¡ Use shared memory for reused data                      â”‚\n",
    "â”‚    â–¡ Avoid bank conflicts in shared memory                  â”‚\n",
    "â”‚    â–¡ Use __ldg() for read-only data                         â”‚\n",
    "â”‚    â–¡ Use pinned memory for host-device transfers            â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â–¡ COMPUTE                                                  â”‚\n",
    "â”‚    â–¡ Use fast math where precision allows                   â”‚\n",
    "â”‚    â–¡ Minimize warp divergence                               â”‚\n",
    "â”‚    â–¡ Use appropriate data types (float vs double)           â”‚\n",
    "â”‚    â–¡ Unroll loops where beneficial                          â”‚\n",
    "â”‚    â–¡ Use warp-level primitives                              â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â–¡ OCCUPANCY                                                â”‚\n",
    "â”‚    â–¡ Use occupancy calculator for block size                â”‚\n",
    "â”‚    â–¡ Balance registers vs occupancy                         â”‚\n",
    "â”‚    â–¡ Balance shared memory vs occupancy                     â”‚\n",
    "â”‚    â–¡ Ensure enough blocks to saturate GPU                   â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â–¡ CONCURRENCY                                              â”‚\n",
    "â”‚    â–¡ Overlap compute and memory transfers                   â”‚\n",
    "â”‚    â–¡ Use multiple streams for independent work              â”‚\n",
    "â”‚    â–¡ Use CUDA Graphs for repeated patterns                  â”‚\n",
    "â”‚    â–¡ Enable peer access for multi-GPU                       â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â–¡ PROFILING                                                â”‚\n",
    "â”‚    â–¡ Profile with Nsight Compute                            â”‚\n",
    "â”‚    â–¡ Check roofline position                                â”‚\n",
    "â”‚    â–¡ Identify bottlenecks before optimizing                 â”‚\n",
    "â”‚    â–¡ Measure improvement after each change                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d81c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Optimized Example: Matrix Transpose\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// optimized_transpose.cu - Fully optimized matrix transpose\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "// Naive transpose (for comparison)\n",
    "__global__ void transposeNaive(float* out, float* in, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        out[x * height + y] = in[y * width + x];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized transpose with shared memory and bank conflict avoidance\n",
    "__global__ void transposeOptimized(float* out, float* in, int width, int height) {\n",
    "    // +1 padding to avoid bank conflicts\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Load tile (coalesced read)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Transposed coordinates\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Store transposed tile (coalesced write)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int WIDTH = 4096, HEIGHT = 4096;\n",
    "    size_t bytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, bytes);\n",
    "    cudaMalloc(&d_out, bytes);\n",
    "    \n",
    "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 grid((WIDTH + TILE_DIM - 1) / TILE_DIM,\n",
    "              (HEIGHT + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    // Warmup\n",
    "    transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int RUNS = 100;\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float bandwidth = 2.0f * bytes / (ms / RUNS / 1000.0f) / 1e9;\n",
    "    printf(\"Optimized Transpose: %.2f GB/s\\n\", bandwidth);\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e03de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile optimized_transpose.cu\n",
    "// optimized_transpose.cu - Fully optimized matrix transpose\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "// Naive transpose (for comparison)\n",
    "__global__ void transposeNaive(float* out, float* in, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        out[x * height + y] = in[y * width + x];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized transpose with shared memory and bank conflict avoidance\n",
    "__global__ void transposeOptimized(float* out, float* in, int width, int height) {\n",
    "    // +1 padding to avoid bank conflicts\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Load tile (coalesced read)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Transposed coordinates\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Store transposed tile (coalesced write)\n",
    "    #pragma unroll\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int WIDTH = 4096, HEIGHT = 4096;\n",
    "    size_t bytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, bytes);\n",
    "    cudaMalloc(&d_out, bytes);\n",
    "    \n",
    "    // Initialize input\n",
    "    float* h_in = new float[WIDTH * HEIGHT];\n",
    "    for (int i = 0; i < WIDTH * HEIGHT; i++) h_in[i] = (float)i;\n",
    "    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 grid((WIDTH + TILE_DIM - 1) / TILE_DIM,\n",
    "              (HEIGHT + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    // Warmup\n",
    "    transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int RUNS = 100;\n",
    "    float ms;\n",
    "    \n",
    "    // Benchmark naive\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        transposeNaive<<<grid, dim3(32, 32)>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float naiveBW = 2.0f * bytes / (ms / RUNS / 1000.0f) / 1e9;\n",
    "    printf(\"Naive Transpose:     %.2f GB/s\\n\", naiveBW);\n",
    "    \n",
    "    // Benchmark optimized\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        transposeOptimized<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float optBW = 2.0f * bytes / (ms / RUNS / 1000.0f) / 1e9;\n",
    "    printf(\"Optimized Transpose: %.2f GB/s\\n\", optBW);\n",
    "    printf(\"Speedup: %.2fx\\n\", optBW / naiveBW);\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    delete[] h_in;\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o optimized_transpose optimized_transpose.cu\n",
    "!./optimized_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a62d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to review and practice optimization techniques:\n",
    "\n",
    "**Exercise 1: Memory Coalescing**\n",
    "- Fix a kernel with poor memory access patterns\n",
    "- Measure the performance improvement\n",
    "\n",
    "**Exercise 2: Shared Memory Optimization**\n",
    "- Convert a naive matrix operation to use shared memory\n",
    "- Handle bank conflicts appropriately\n",
    "\n",
    "**Exercise 3: Occupancy Analysis**\n",
    "- Experiment with different block sizes\n",
    "- Find the optimal configuration for your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c650c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile optimization_review_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Memory Coalescing\n",
    "// The \"bad\" kernel has strided access; fix it in the \"good\" kernel\n",
    "// =============================================================================\n",
    "\n",
    "// BAD: Strided memory access (each thread reads columns)\n",
    "__global__ void transposeNaive(float* out, float* in, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        // Reading: row-major with stride (bad for reads when x varies slowly)\n",
    "        // Writing: column-major (bad for writes)\n",
    "        out[x * height + y] = in[y * width + x];\n",
    "    }\n",
    "}\n",
    "\n",
    "// TODO: Exercise 1 - Fix the memory access pattern using shared memory\n",
    "// Use a tile of shared memory to enable coalesced reads and writes\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "__global__ void transposeOptimized(float* out, float* in, int width, int height) {\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];  // +1 to avoid bank conflicts\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // TODO: Exercise 1a - Load tile with coalesced reads\n",
    "    // Each thread loads multiple elements (TILE_DIM / BLOCK_ROWS)\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // TODO: Exercise 1b - Write tile with coalesced writes\n",
    "    // Note: indices are swapped for transpose\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Shared Memory - Matrix Row Sum\n",
    "// =============================================================================\n",
    "\n",
    "// Naive: Each thread reads entire row (lots of global memory traffic)\n",
    "__global__ void rowSumNaive(float* out, float* matrix, int rows, int cols) {\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < rows) {\n",
    "        float sum = 0.0f;\n",
    "        for (int c = 0; c < cols; c++) {\n",
    "            sum += matrix[row * cols + c];\n",
    "        }\n",
    "        out[row] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// TODO: Exercise 2 - Use shared memory for collaborative loading\n",
    "__global__ void rowSumOptimized(float* out, float* matrix, int rows, int cols) {\n",
    "    extern __shared__ float sdata[];\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // TODO: Exercise 2a - Each thread loads multiple columns collaboratively\n",
    "    float sum = 0.0f;\n",
    "    for (int c = tid; c < cols; c += blockDim.x) {\n",
    "        sum += matrix[row * cols + c];\n",
    "    }\n",
    "    \n",
    "    // TODO: Exercise 2b - Store partial sum to shared memory\n",
    "    sdata[tid] = sum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // TODO: Exercise 2c - Parallel reduction in shared memory\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] += sdata[tid + s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write result\n",
    "    if (tid == 0) {\n",
    "        out[row] = sdata[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Occupancy Optimization\n",
    "// =============================================================================\n",
    "\n",
    "// Vector add with configurable block size\n",
    "__global__ void vectorAdd(float* c, float* a, float* b, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void testOccupancy() {\n",
    "    printf(\"=== Exercise 3: Occupancy Analysis ===\\n\");\n",
    "    \n",
    "    int n = 10 * 1024 * 1024;  // 10M elements\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    CHECK_CUDA(cudaMalloc(&d_a, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_b, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_c, n * sizeof(float)));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    // Test different block sizes\n",
    "    int blockSizes[] = {32, 64, 128, 256, 512, 1024};\n",
    "    int numSizes = sizeof(blockSizes) / sizeof(blockSizes[0]);\n",
    "    \n",
    "    printf(\"\\n%-12s %-12s %-12s %-15s\\n\", \"Block Size\", \"Num Blocks\", \"Occupancy\", \"Time (ms)\");\n",
    "    printf(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\");\n",
    "    \n",
    "    for (int i = 0; i < numSizes; i++) {\n",
    "        int blockSize = blockSizes[i];\n",
    "        int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "        \n",
    "        // Calculate theoretical occupancy\n",
    "        int maxActiveBlocks;\n",
    "        CHECK_CUDA(cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "            &maxActiveBlocks, vectorAdd, blockSize, 0));\n",
    "        \n",
    "        cudaDeviceProp prop;\n",
    "        CHECK_CUDA(cudaGetDeviceProperties(&prop, 0));\n",
    "        \n",
    "        float occupancy = (float)(maxActiveBlocks * blockSize) / prop.maxThreadsPerMultiProcessor;\n",
    "        \n",
    "        // Warmup\n",
    "        vectorAdd<<<numBlocks, blockSize>>>(d_c, d_a, d_b, n);\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());\n",
    "        \n",
    "        // Time it\n",
    "        CHECK_CUDA(cudaEventRecord(start));\n",
    "        for (int j = 0; j < 100; j++) {\n",
    "            vectorAdd<<<numBlocks, blockSize>>>(d_c, d_a, d_b, n);\n",
    "        }\n",
    "        CHECK_CUDA(cudaEventRecord(stop));\n",
    "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "        \n",
    "        float ms;\n",
    "        CHECK_CUDA(cudaEventElapsedTime(&ms, start, stop));\n",
    "        \n",
    "        printf(\"%-12d %-12d %-12.1f%% %-15.3f\\n\", \n",
    "               blockSize, numBlocks, occupancy * 100, ms / 100);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    // Use occupancy calculator to find optimal\n",
    "    int minGridSize, optBlockSize;\n",
    "    CHECK_CUDA(cudaOccupancyMaxPotentialBlockSize(\n",
    "        &minGridSize, &optBlockSize, vectorAdd, 0, 0));\n",
    "    \n",
    "    printf(\"Suggested optimal block size: %d\\n\", optBlockSize);\n",
    "    printf(\"Suggested minimum grid size: %d\\n\\n\", minGridSize);\n",
    "    \n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaFree(d_a));\n",
    "    CHECK_CUDA(cudaFree(d_b));\n",
    "    CHECK_CUDA(cudaFree(d_c));\n",
    "}\n",
    "\n",
    "// Test Exercise 1\n",
    "void testTranspose() {\n",
    "    printf(\"=== Exercise 1: Memory Coalescing (Transpose) ===\\n\");\n",
    "    \n",
    "    int width = 4096, height = 4096;\n",
    "    size_t size = width * height * sizeof(float);\n",
    "    \n",
    "    float *h_in = (float*)malloc(size);\n",
    "    float *h_out = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < width * height; i++) h_in[i] = (float)i;\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    CHECK_CUDA(cudaMalloc(&d_in, size));\n",
    "    CHECK_CUDA(cudaMalloc(&d_out, size));\n",
    "    CHECK_CUDA(cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 grid((width + TILE_DIM - 1) / TILE_DIM, (height + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    // Naive\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        transposeNaive<<<grid, block>>>(d_out, d_in, width, height);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    float naiveTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&naiveTime, start, stop));\n",
    "    \n",
    "    // Optimized\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        transposeOptimized<<<grid, block>>>(d_out, d_in, width, height);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    float optTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&optTime, start, stop));\n",
    "    \n",
    "    printf(\"Naive transpose: %.3f ms\\n\", naiveTime / 10);\n",
    "    printf(\"Optimized transpose: %.3f ms\\n\", optTime / 10);\n",
    "    printf(\"Speedup: %.2fx\\n\\n\", naiveTime / optTime);\n",
    "    \n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaFree(d_in));\n",
    "    CHECK_CUDA(cudaFree(d_out));\n",
    "    free(h_in);\n",
    "    free(h_out);\n",
    "}\n",
    "\n",
    "// Test Exercise 2\n",
    "void testRowSum() {\n",
    "    printf(\"=== Exercise 2: Shared Memory (Row Sum) ===\\n\");\n",
    "    \n",
    "    int rows = 4096, cols = 4096;\n",
    "    size_t matrixSize = rows * cols * sizeof(float);\n",
    "    size_t outSize = rows * sizeof(float);\n",
    "    \n",
    "    float *h_matrix = (float*)malloc(matrixSize);\n",
    "    float *h_out = (float*)malloc(outSize);\n",
    "    \n",
    "    for (int i = 0; i < rows * cols; i++) h_matrix[i] = 1.0f;\n",
    "    \n",
    "    float *d_matrix, *d_out;\n",
    "    CHECK_CUDA(cudaMalloc(&d_matrix, matrixSize));\n",
    "    CHECK_CUDA(cudaMalloc(&d_out, outSize));\n",
    "    CHECK_CUDA(cudaMemcpy(d_matrix, h_matrix, matrixSize, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    // Naive\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        rowSumNaive<<<(rows + 255) / 256, 256>>>(d_out, d_matrix, rows, cols);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    float naiveTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&naiveTime, start, stop));\n",
    "    \n",
    "    // Optimized\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        rowSumOptimized<<<rows, 256, 256 * sizeof(float)>>>(d_out, d_matrix, rows, cols);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    float optTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&optTime, start, stop));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_out, d_out, outSize, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Naive row sum: %.3f ms\\n\", naiveTime / 10);\n",
    "    printf(\"Optimized row sum: %.3f ms\\n\", optTime / 10);\n",
    "    printf(\"Speedup: %.2fx\\n\", naiveTime / optTime);\n",
    "    printf(\"Sample result (expected %d): %.0f\\n\\n\", cols, h_out[0]);\n",
    "    \n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaFree(d_matrix));\n",
    "    CHECK_CUDA(cudaFree(d_out));\n",
    "    free(h_matrix);\n",
    "    free(h_out);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘           Optimization Review Exercises                      â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    testTranspose();\n",
    "    testRowSum();\n",
    "    testOccupancy();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"Study the optimized versions and try profiling with Nsight!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o optimization_review_exercises optimization_review_exercises.cu && ./optimization_review_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a79e6",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises practice optimization concepts using Numba:\n",
    "\n",
    "```python\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# Exercise 1: Compare coalesced vs non-coalesced access\n",
    "@cuda.jit\n",
    "def coalesced_copy(out, inp):\n",
    "    \"\"\"Good: Coalesced access - consecutive threads access consecutive memory\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < inp.size:\n",
    "        out[idx] = inp[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def strided_copy(out, inp, stride):\n",
    "    \"\"\"Bad: Strided access - threads access memory with gaps\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    strided_idx = (idx * stride) % inp.size\n",
    "    if idx < inp.size:\n",
    "        out[idx] = inp[strided_idx]\n",
    "\n",
    "# Exercise 2: Shared memory tile\n",
    "@cuda.jit\n",
    "def tiled_operation(out, inp):\n",
    "    \"\"\"Use shared memory for data reuse\"\"\"\n",
    "    tile = cuda.shared.array(256, dtype=numba.float32)\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    idx = bx * cuda.blockDim.x + tx\n",
    "    \n",
    "    # Load to shared memory\n",
    "    if idx < inp.size:\n",
    "        tile[tx] = inp[idx]\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Process with neighbors (data reuse from shared memory)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "**Tip**: Profile with `cuda.profiling()` context manager to measure kernel times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7911356",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   OPTIMIZATION MASTERY                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ“‹ THE DETECTIVE'S CREED:                                      â”‚\n",
    "â”‚  \"Profile first. Optimize second. Verify always.\"              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸ¯ OPTIMIZATION HIERARCHY (MEMORIZE THIS!):                    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. Algorithm choice         â†’ 10-100x gains                    â”‚\n",
    "â”‚  2. Memory access patterns   â†’ 5-20x gains                      â”‚\n",
    "â”‚  3. Occupancy & parallelism  â†’ 2-5x gains                       â”‚\n",
    "â”‚  4. Instruction optimization â†’ 1.5-3x gains                     â”‚\n",
    "â”‚  5. Concurrency              â†’ 1.2-2x gains                     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  âš ï¸ Work TOP-DOWN! Don't micro-optimize before fixing memory!   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸ“š CURRICULUM CALLBACK - TECHNIQUES BY WEEK:                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Week 1-2: Thread indexing, memory basics, coalescing           â”‚\n",
    "â”‚  Week 3:   Shared memory, bank conflicts, synchronization       â”‚\n",
    "â”‚  Week 4:   Reduction, warp-level primitives                     â”‚\n",
    "â”‚  Week 5:   Scan algorithms (Hillis-Steele, Blelloch)            â”‚\n",
    "â”‚  Week 6:   Matrix operations, tiling                            â”‚\n",
    "â”‚  Week 7:   Occupancy, register pressure, memory optimization    â”‚\n",
    "â”‚  Week 8:   Profiling with Nsight, roofline analysis             â”‚\n",
    "â”‚  Week 9:   Streams, async operations, overlap                   â”‚\n",
    "â”‚  Week 10:  CUDA Graphs                                          â”‚\n",
    "â”‚  Week 11:  Cooperative groups, dynamic parallelism              â”‚\n",
    "â”‚  Week 12:  Multi-GPU, YOU ARE HERE!                             â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  âš¡ QUICK DIAGNOSTIC GUIDE:                                     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Symptom: Low GPU utilization                                   â”‚\n",
    "â”‚  Likely causes: Not enough threads, occupancy too low           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Symptom: Memory throughput < peak                              â”‚\n",
    "â”‚  Likely causes: Non-coalesced access, bank conflicts            â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Symptom: High instruction count                                â”‚\n",
    "â”‚  Likely causes: Warp divergence, no unrolling                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Symptom: GPU idle during transfers                             â”‚\n",
    "â”‚  Likely causes: Missing streams, no overlap                     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸ† PERFORMANCE CULTURE:                                        â”‚\n",
    "â”‚  â€¢ One optimization at a time                                   â”‚\n",
    "â”‚  â€¢ Measure before AND after                                     â”‚\n",
    "â”‚  â€¢ Keep notes - document what worked                            â”‚\n",
    "â”‚  â€¢ Regression test - ensure correctness after changes           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Tomorrow is the **Capstone Project**! ğŸ“ You'll bring together everything from this 12-week journey to build a complete GPU-accelerated image convolution system.\n",
    "\n",
    "All your skills will be tested:\n",
    "- Thread indexing for 2D image processing\n",
    "- Shared memory for tiled convolution\n",
    "- Streams for processing multiple images\n",
    "- Graphs for repeated filter applications\n",
    "- Multi-GPU for large images\n",
    "\n",
    "**Get ready to show what you've learned!** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
