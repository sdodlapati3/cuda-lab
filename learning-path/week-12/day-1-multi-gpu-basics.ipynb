{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"⚠️  Multi-GPU code requires multiple physical GPUs!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad21f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Device Query and Selection\n",
    "\n",
    "### CUDA C++ Device Management (Primary)\n",
    "\n",
    "```cpp\n",
    "// device_query.cu - Enumerate and select GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    // ============================================\n",
    "    // Query Available GPUs\n",
    "    // ============================================\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    printf(\"Found %d CUDA device(s)\\n\", deviceCount);\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, i);\n",
    "        \n",
    "        printf(\"\\nDevice %d: %s\\n\", i, prop.name);\n",
    "        printf(\"  Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"  Global Memory: %.2f GB\\n\", \n",
    "               prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));\n",
    "        printf(\"  SM Count: %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"  Max Threads per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "        printf(\"  Memory Clock: %.2f GHz\\n\", prop.memoryClockRate / 1e6);\n",
    "        printf(\"  Memory Bus Width: %d bits\\n\", prop.memoryBusWidth);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Select a Device\n",
    "    // ============================================\n",
    "    int selectedDevice = 0;  // Use first GPU\n",
    "    cudaSetDevice(selectedDevice);\n",
    "    \n",
    "    printf(\"\\nSelected device %d for computation\\n\", selectedDevice);\n",
    "    \n",
    "    // ============================================\n",
    "    // Query Current Device\n",
    "    // ============================================\n",
    "    int currentDevice;\n",
    "    cudaGetDevice(&currentDevice);\n",
    "    printf(\"Current device: %d\\n\", currentDevice);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3713a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Peer-to-Peer Access\n",
    "\n",
    "### Direct GPU-to-GPU Communication\n",
    "\n",
    "```\n",
    "Without P2P:                   With P2P:\n",
    "━━━━━━━━━━━━                   ━━━━━━━━━\n",
    "\n",
    "GPU 0 ─────> Host ─────> GPU 1    GPU 0 ════════> GPU 1\n",
    "       copy       copy                  direct copy\n",
    "\n",
    "• 2x latency                   • 1x latency\n",
    "• Host memory bottleneck       • PCIe/NVLink speed\n",
    "• CPU involved                 • GPU-to-GPU direct\n",
    "```\n",
    "\n",
    "### CUDA C++ P2P Setup (Primary)\n",
    "\n",
    "```cpp\n",
    "// p2p_access.cu - Enable peer access between GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need at least 2 GPUs for P2P demo\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    int gpu0 = 0, gpu1 = 1;\n",
    "    \n",
    "    // ============================================\n",
    "    // Check P2P Capability\n",
    "    // ============================================\n",
    "    int canAccessPeer0to1, canAccessPeer1to0;\n",
    "    \n",
    "    cudaDeviceCanAccessPeer(&canAccessPeer0to1, gpu0, gpu1);\n",
    "    cudaDeviceCanAccessPeer(&canAccessPeer1to0, gpu1, gpu0);\n",
    "    \n",
    "    printf(\"P2P GPU %d -> GPU %d: %s\\n\", \n",
    "           gpu0, gpu1, canAccessPeer0to1 ? \"YES\" : \"NO\");\n",
    "    printf(\"P2P GPU %d -> GPU %d: %s\\n\", \n",
    "           gpu1, gpu0, canAccessPeer1to0 ? \"YES\" : \"NO\");\n",
    "    \n",
    "    if (!canAccessPeer0to1 || !canAccessPeer1to0) {\n",
    "        printf(\"P2P not supported between these GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Enable P2P Access\n",
    "    // ============================================\n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaDeviceEnablePeerAccess(gpu1, 0);  // Enable 0 -> 1\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaDeviceEnablePeerAccess(gpu0, 0);  // Enable 1 -> 0\n",
    "    \n",
    "    printf(\"P2P access enabled between GPU %d and GPU %d\\n\", gpu0, gpu1);\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Memory on Each GPU\n",
    "    // ============================================\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data0, *d_data1;\n",
    "    \n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaMalloc(&d_data0, N * sizeof(float));\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaMalloc(&d_data1, N * sizeof(float));\n",
    "    \n",
    "    // ============================================\n",
    "    // Direct P2P Copy\n",
    "    // ============================================\n",
    "    cudaMemcpyPeer(d_data1, gpu1, d_data0, gpu0, N * sizeof(float));\n",
    "    printf(\"Copied data directly from GPU %d to GPU %d\\n\", gpu0, gpu1);\n",
    "    \n",
    "    // ============================================\n",
    "    // Cleanup\n",
    "    // ============================================\n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaDeviceDisablePeerAccess(gpu1);\n",
    "    cudaFree(d_data0);\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaDeviceDisablePeerAccess(gpu0);\n",
    "    cudaFree(d_data1);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc081a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Running Kernels on Multiple GPUs\n",
    "\n",
    "### CUDA C++ Multi-GPU Kernel Execution (Primary)\n",
    "\n",
    "```cpp\n",
    "// multi_gpu_kernel.cu - Execute kernels on multiple GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n, int gpuId) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f + (float)gpuId;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need at least 2 GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const int NUM_GPUS = 2;\n",
    "    const int N_PER_GPU = N / NUM_GPUS;\n",
    "    \n",
    "    // Host data\n",
    "    float* h_data;\n",
    "    cudaMallocHost(&h_data, N * sizeof(float));\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    // Device data and streams\n",
    "    float* d_data[NUM_GPUS];\n",
    "    cudaStream_t streams[NUM_GPUS];\n",
    "    \n",
    "    // ============================================\n",
    "    // Setup Each GPU\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaMalloc(&d_data[gpu], N_PER_GPU * sizeof(float));\n",
    "        cudaStreamCreate(&streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Copy Data to Each GPU (Async)\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemcpyAsync(d_data[gpu], h_data + offset,\n",
    "                        N_PER_GPU * sizeof(float),\n",
    "                        cudaMemcpyHostToDevice, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch Kernels on Each GPU\n",
    "    // ============================================\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N_PER_GPU + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        process<<<numBlocks, blockSize, 0, streams[gpu]>>>(\n",
    "            d_data[gpu], N_PER_GPU, gpu);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Copy Results Back (Async)\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemcpyAsync(h_data + offset, d_data[gpu],\n",
    "                        N_PER_GPU * sizeof(float),\n",
    "                        cudaMemcpyDeviceToHost, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Synchronize All GPUs\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamSynchronize(streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Verify\n",
    "    printf(\"GPU 0 result[0] = %.1f (expected 2.0)\\n\", h_data[0]);\n",
    "    printf(\"GPU 1 result[0] = %.1f (expected 3.0)\\n\", h_data[N_PER_GPU]);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaFree(d_data[gpu]);\n",
    "        cudaStreamDestroy(streams[gpu]);\n",
    "    }\n",
    "    cudaFreeHost(h_data);\n",
    "    \n",
    "    printf(\"Multi-GPU computation complete!\\n\");\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e8e3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Unified Memory for Multi-GPU\n",
    "\n",
    "### Automatic Data Migration\n",
    "\n",
    "```cpp\n",
    "// unified_multi_gpu.cu - Unified Memory with multiple GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int start, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[start + tid] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need 2+ GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const int NUM_GPUS = 2;\n",
    "    const int N_PER_GPU = N / NUM_GPUS;\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Unified Memory\n",
    "    // ============================================\n",
    "    float* data;\n",
    "    cudaMallocManaged(&data, N * sizeof(float));\n",
    "    \n",
    "    // Initialize on host\n",
    "    for (int i = 0; i < N; i++) data[i] = 1.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // Give Hints About Data Location\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        size_t size = N_PER_GPU * sizeof(float);\n",
    "        \n",
    "        // Hint: This data is preferred on this GPU\n",
    "        cudaMemAdvise(data + offset, size, \n",
    "                      cudaMemAdviseSetPreferredLocation, gpu);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Prefetch Data to GPUs\n",
    "    // ============================================\n",
    "    cudaStream_t streams[NUM_GPUS];\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamCreate(&streams[gpu]);\n",
    "        \n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemPrefetchAsync(data + offset, \n",
    "                             N_PER_GPU * sizeof(float),\n",
    "                             gpu, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch Kernels\n",
    "    // ============================================\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N_PER_GPU + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        \n",
    "        process<<<numBlocks, blockSize, 0, streams[gpu]>>>(\n",
    "            data, offset, N_PER_GPU);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Prefetch Back to Host\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemPrefetchAsync(data + offset,\n",
    "                             N_PER_GPU * sizeof(float),\n",
    "                             cudaCpuDeviceId, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Sync all\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamSynchronize(streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Verify on host\n",
    "    printf(\"data[0] = %.1f (expected 2.0)\\n\", data[0]);\n",
    "    printf(\"data[%d] = %.1f (expected 2.0)\\n\", N_PER_GPU, data[N_PER_GPU]);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamDestroy(streams[gpu]);\n",
    "    }\n",
    "    cudaFree(data);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3167496",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Device Query\n",
    "Write code to find the GPU with the most memory.\n",
    "\n",
    "### Exercise 2: P2P Bandwidth\n",
    "Measure P2P copy bandwidth between two GPUs.\n",
    "\n",
    "### Exercise 3: Multi-GPU Vector Add\n",
    "Implement vector addition split across 2 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca086248",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                MULTI-GPU BASICS                         │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Device Management:                                     │\n",
    "│  • cudaGetDeviceCount(&count)                           │\n",
    "│  • cudaSetDevice(id)                                    │\n",
    "│  • cudaGetDevice(&id)                                   │\n",
    "│                                                         │\n",
    "│  Peer Access:                                           │\n",
    "│  • cudaDeviceCanAccessPeer(&can, dev, peer)             │\n",
    "│  • cudaDeviceEnablePeerAccess(peer, 0)                  │\n",
    "│  • cudaMemcpyPeer(dst, dstDev, src, srcDev, size)       │\n",
    "│                                                         │\n",
    "│  Pattern:                                               │\n",
    "│  1. Query/select devices                                │\n",
    "│  2. Enable P2P if available                             │\n",
    "│  3. Allocate memory on each GPU                         │\n",
    "│  4. Distribute data                                     │\n",
    "│  5. Launch kernels                                      │\n",
    "│  6. Collect results                                     │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 2 - Multi-GPU Patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
