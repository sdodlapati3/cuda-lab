{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"‚ö†Ô∏è  Multi-GPU code requires multiple physical GPUs!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad21f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Device Query and Selection\n",
    "\n",
    "### üî∑ CUDA C++ Device Management (Primary)\n",
    "\n",
    "```cpp\n",
    "// device_query.cu - Enumerate and select GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    // ============================================\n",
    "    // Query Available GPUs\n",
    "    // ============================================\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    printf(\"Found %d CUDA device(s)\\n\", deviceCount);\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, i);\n",
    "        \n",
    "        printf(\"\\nDevice %d: %s\\n\", i, prop.name);\n",
    "        printf(\"  Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"  Global Memory: %.2f GB\\n\", \n",
    "               prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));\n",
    "        printf(\"  SM Count: %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"  Max Threads per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "        printf(\"  Memory Clock: %.2f GHz\\n\", prop.memoryClockRate / 1e6);\n",
    "        printf(\"  Memory Bus Width: %d bits\\n\", prop.memoryBusWidth);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Select a Device\n",
    "    // ============================================\n",
    "    int selectedDevice = 0;  // Use first GPU\n",
    "    cudaSetDevice(selectedDevice);\n",
    "    \n",
    "    printf(\"\\nSelected device %d for computation\\n\", selectedDevice);\n",
    "    \n",
    "    // ============================================\n",
    "    // Query Current Device\n",
    "    // ============================================\n",
    "    int currentDevice;\n",
    "    cudaGetDevice(&currentDevice);\n",
    "    printf(\"Current device: %d\\n\", currentDevice);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e47c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile device_query.cu\n",
    "// device_query.cu - Enumerate and select GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    // ============================================\n",
    "    // Query Available GPUs\n",
    "    // ============================================\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    printf(\"Found %d CUDA device(s)\\n\", deviceCount);\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, i);\n",
    "        \n",
    "        printf(\"\\nDevice %d: %s\\n\", i, prop.name);\n",
    "        printf(\"  Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"  Global Memory: %.2f GB\\n\", \n",
    "               prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));\n",
    "        printf(\"  SM Count: %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"  Max Threads per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "        printf(\"  Memory Clock: %.2f GHz\\n\", prop.memoryClockRate / 1e6);\n",
    "        printf(\"  Memory Bus Width: %d bits\\n\", prop.memoryBusWidth);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Select a Device\n",
    "    // ============================================\n",
    "    int selectedDevice = 0;  // Use first GPU\n",
    "    cudaSetDevice(selectedDevice);\n",
    "    \n",
    "    printf(\"\\nSelected device %d for computation\\n\", selectedDevice);\n",
    "    \n",
    "    // ============================================\n",
    "    // Query Current Device\n",
    "    // ============================================\n",
    "    int currentDevice;\n",
    "    cudaGetDevice(&currentDevice);\n",
    "    printf(\"Current device: %d\\n\", currentDevice);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695792e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o device_query device_query.cu\n",
    "!./device_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3713a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Peer-to-Peer Access\n",
    "\n",
    "### Direct GPU-to-GPU Communication\n",
    "\n",
    "```\n",
    "Without P2P:                   With P2P:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "GPU 0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Host ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> GPU 1    GPU 0 ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê> GPU 1\n",
    "       copy       copy                  direct copy\n",
    "\n",
    "‚Ä¢ 2x latency                   ‚Ä¢ 1x latency\n",
    "‚Ä¢ Host memory bottleneck       ‚Ä¢ PCIe/NVLink speed\n",
    "‚Ä¢ CPU involved                 ‚Ä¢ GPU-to-GPU direct\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ P2P Setup (Primary)\n",
    "\n",
    "```cpp\n",
    "// p2p_access.cu - Enable peer access between GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need at least 2 GPUs for P2P demo\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    int gpu0 = 0, gpu1 = 1;\n",
    "    \n",
    "    // ============================================\n",
    "    // Check P2P Capability\n",
    "    // ============================================\n",
    "    int canAccessPeer0to1, canAccessPeer1to0;\n",
    "    \n",
    "    cudaDeviceCanAccessPeer(&canAccessPeer0to1, gpu0, gpu1);\n",
    "    cudaDeviceCanAccessPeer(&canAccessPeer1to0, gpu1, gpu0);\n",
    "    \n",
    "    printf(\"P2P GPU %d -> GPU %d: %s\\n\", \n",
    "           gpu0, gpu1, canAccessPeer0to1 ? \"YES\" : \"NO\");\n",
    "    printf(\"P2P GPU %d -> GPU %d: %s\\n\", \n",
    "           gpu1, gpu0, canAccessPeer1to0 ? \"YES\" : \"NO\");\n",
    "    \n",
    "    if (!canAccessPeer0to1 || !canAccessPeer1to0) {\n",
    "        printf(\"P2P not supported between these GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Enable P2P Access\n",
    "    // ============================================\n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaDeviceEnablePeerAccess(gpu1, 0);  // Enable 0 -> 1\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaDeviceEnablePeerAccess(gpu0, 0);  // Enable 1 -> 0\n",
    "    \n",
    "    printf(\"P2P access enabled between GPU %d and GPU %d\\n\", gpu0, gpu1);\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Memory on Each GPU\n",
    "    // ============================================\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data0, *d_data1;\n",
    "    \n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaMalloc(&d_data0, N * sizeof(float));\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaMalloc(&d_data1, N * sizeof(float));\n",
    "    \n",
    "    // ============================================\n",
    "    // Direct P2P Copy\n",
    "    // ============================================\n",
    "    cudaMemcpyPeer(d_data1, gpu1, d_data0, gpu0, N * sizeof(float));\n",
    "    printf(\"Copied data directly from GPU %d to GPU %d\\n\", gpu0, gpu1);\n",
    "    \n",
    "    // ============================================\n",
    "    // Cleanup\n",
    "    // ============================================\n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaDeviceDisablePeerAccess(gpu1);\n",
    "    cudaFree(d_data0);\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaDeviceDisablePeerAccess(gpu0);\n",
    "    cudaFree(d_data1);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile p2p_access.cu\n",
    "// p2p_access.cu - Enable peer access between GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need at least 2 GPUs for P2P demo\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    int gpu0 = 0, gpu1 = 1;\n",
    "    \n",
    "    // ============================================\n",
    "    // Check P2P Capability\n",
    "    // ============================================\n",
    "    int canAccessPeer0to1, canAccessPeer1to0;\n",
    "    \n",
    "    cudaDeviceCanAccessPeer(&canAccessPeer0to1, gpu0, gpu1);\n",
    "    cudaDeviceCanAccessPeer(&canAccessPeer1to0, gpu1, gpu0);\n",
    "    \n",
    "    printf(\"P2P GPU %d -> GPU %d: %s\\n\", \n",
    "           gpu0, gpu1, canAccessPeer0to1 ? \"YES\" : \"NO\");\n",
    "    printf(\"P2P GPU %d -> GPU %d: %s\\n\", \n",
    "           gpu1, gpu0, canAccessPeer1to0 ? \"YES\" : \"NO\");\n",
    "    \n",
    "    if (!canAccessPeer0to1 || !canAccessPeer1to0) {\n",
    "        printf(\"P2P not supported between these GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Enable P2P Access\n",
    "    // ============================================\n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaDeviceEnablePeerAccess(gpu1, 0);  // Enable 0 -> 1\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaDeviceEnablePeerAccess(gpu0, 0);  // Enable 1 -> 0\n",
    "    \n",
    "    printf(\"P2P access enabled between GPU %d and GPU %d\\n\", gpu0, gpu1);\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Memory on Each GPU\n",
    "    // ============================================\n",
    "    const int N = 1 << 20;\n",
    "    float *d_data0, *d_data1;\n",
    "    \n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaMalloc(&d_data0, N * sizeof(float));\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaMalloc(&d_data1, N * sizeof(float));\n",
    "    \n",
    "    // ============================================\n",
    "    // Direct P2P Copy\n",
    "    // ============================================\n",
    "    cudaMemcpyPeer(d_data1, gpu1, d_data0, gpu0, N * sizeof(float));\n",
    "    printf(\"Copied data directly from GPU %d to GPU %d\\n\", gpu0, gpu1);\n",
    "    \n",
    "    // ============================================\n",
    "    // Cleanup\n",
    "    // ============================================\n",
    "    cudaSetDevice(gpu0);\n",
    "    cudaDeviceDisablePeerAccess(gpu1);\n",
    "    cudaFree(d_data0);\n",
    "    \n",
    "    cudaSetDevice(gpu1);\n",
    "    cudaDeviceDisablePeerAccess(gpu0);\n",
    "    cudaFree(d_data1);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o p2p_access p2p_access.cu\n",
    "!./p2p_access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc081a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Running Kernels on Multiple GPUs\n",
    "\n",
    "### üî∑ CUDA C++ Multi-GPU Kernel Execution (Primary)\n",
    "\n",
    "```cpp\n",
    "// multi_gpu_kernel.cu - Execute kernels on multiple GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n, int gpuId) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f + (float)gpuId;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need at least 2 GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const int NUM_GPUS = 2;\n",
    "    const int N_PER_GPU = N / NUM_GPUS;\n",
    "    \n",
    "    // Host data\n",
    "    float* h_data;\n",
    "    cudaMallocHost(&h_data, N * sizeof(float));\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    // Device data and streams\n",
    "    float* d_data[NUM_GPUS];\n",
    "    cudaStream_t streams[NUM_GPUS];\n",
    "    \n",
    "    // ============================================\n",
    "    // Setup Each GPU\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaMalloc(&d_data[gpu], N_PER_GPU * sizeof(float));\n",
    "        cudaStreamCreate(&streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Copy Data to Each GPU (Async)\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemcpyAsync(d_data[gpu], h_data + offset,\n",
    "                        N_PER_GPU * sizeof(float),\n",
    "                        cudaMemcpyHostToDevice, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch Kernels on Each GPU\n",
    "    // ============================================\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N_PER_GPU + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        process<<<numBlocks, blockSize, 0, streams[gpu]>>>(\n",
    "            d_data[gpu], N_PER_GPU, gpu);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Copy Results Back (Async)\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemcpyAsync(h_data + offset, d_data[gpu],\n",
    "                        N_PER_GPU * sizeof(float),\n",
    "                        cudaMemcpyDeviceToHost, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Synchronize All GPUs\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamSynchronize(streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Verify\n",
    "    printf(\"GPU 0 result[0] = %.1f (expected 2.0)\\n\", h_data[0]);\n",
    "    printf(\"GPU 1 result[0] = %.1f (expected 3.0)\\n\", h_data[N_PER_GPU]);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaFree(d_data[gpu]);\n",
    "        cudaStreamDestroy(streams[gpu]);\n",
    "    }\n",
    "    cudaFreeHost(h_data);\n",
    "    \n",
    "    printf(\"Multi-GPU computation complete!\\n\");\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b76c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multi_gpu_kernel.cu\n",
    "// multi_gpu_kernel.cu - Execute kernels on multiple GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int n, int gpuId) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] = data[tid] * 2.0f + (float)gpuId;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need at least 2 GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const int NUM_GPUS = 2;\n",
    "    const int N_PER_GPU = N / NUM_GPUS;\n",
    "    \n",
    "    // Host data\n",
    "    float* h_data;\n",
    "    cudaMallocHost(&h_data, N * sizeof(float));\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    // Device data and streams\n",
    "    float* d_data[NUM_GPUS];\n",
    "    cudaStream_t streams[NUM_GPUS];\n",
    "    \n",
    "    // ============================================\n",
    "    // Setup Each GPU\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaMalloc(&d_data[gpu], N_PER_GPU * sizeof(float));\n",
    "        cudaStreamCreate(&streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Copy Data to Each GPU (Async)\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemcpyAsync(d_data[gpu], h_data + offset,\n",
    "                        N_PER_GPU * sizeof(float),\n",
    "                        cudaMemcpyHostToDevice, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch Kernels on Each GPU\n",
    "    // ============================================\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N_PER_GPU + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        process<<<numBlocks, blockSize, 0, streams[gpu]>>>(\n",
    "            d_data[gpu], N_PER_GPU, gpu);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Copy Results Back (Async)\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemcpyAsync(h_data + offset, d_data[gpu],\n",
    "                        N_PER_GPU * sizeof(float),\n",
    "                        cudaMemcpyDeviceToHost, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Synchronize All GPUs\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamSynchronize(streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Verify\n",
    "    printf(\"GPU 0 result[0] = %.1f (expected 2.0)\\n\", h_data[0]);\n",
    "    printf(\"GPU 1 result[0] = %.1f (expected 3.0)\\n\", h_data[N_PER_GPU]);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaFree(d_data[gpu]);\n",
    "        cudaStreamDestroy(streams[gpu]);\n",
    "    }\n",
    "    cudaFreeHost(h_data);\n",
    "    \n",
    "    printf(\"Multi-GPU computation complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o multi_gpu_kernel multi_gpu_kernel.cu\n",
    "!./multi_gpu_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e8e3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Unified Memory for Multi-GPU\n",
    "\n",
    "### Automatic Data Migration\n",
    "\n",
    "```cpp\n",
    "// unified_multi_gpu.cu - Unified Memory with multiple GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int start, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[start + tid] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need 2+ GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const int NUM_GPUS = 2;\n",
    "    const int N_PER_GPU = N / NUM_GPUS;\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Unified Memory\n",
    "    // ============================================\n",
    "    float* data;\n",
    "    cudaMallocManaged(&data, N * sizeof(float));\n",
    "    \n",
    "    // Initialize on host\n",
    "    for (int i = 0; i < N; i++) data[i] = 1.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // Give Hints About Data Location\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        size_t size = N_PER_GPU * sizeof(float);\n",
    "        \n",
    "        // Hint: This data is preferred on this GPU\n",
    "        cudaMemAdvise(data + offset, size, \n",
    "                      cudaMemAdviseSetPreferredLocation, gpu);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Prefetch Data to GPUs\n",
    "    // ============================================\n",
    "    cudaStream_t streams[NUM_GPUS];\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamCreate(&streams[gpu]);\n",
    "        \n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemPrefetchAsync(data + offset, \n",
    "                             N_PER_GPU * sizeof(float),\n",
    "                             gpu, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch Kernels\n",
    "    // ============================================\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N_PER_GPU + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        \n",
    "        process<<<numBlocks, blockSize, 0, streams[gpu]>>>(\n",
    "            data, offset, N_PER_GPU);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Prefetch Back to Host\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemPrefetchAsync(data + offset,\n",
    "                             N_PER_GPU * sizeof(float),\n",
    "                             cudaCpuDeviceId, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Sync all\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamSynchronize(streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Verify on host\n",
    "    printf(\"data[0] = %.1f (expected 2.0)\\n\", data[0]);\n",
    "    printf(\"data[%d] = %.1f (expected 2.0)\\n\", N_PER_GPU, data[N_PER_GPU]);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamDestroy(streams[gpu]);\n",
    "    }\n",
    "    cudaFree(data);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bcb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile unified_multi_gpu.cu\n",
    "// unified_multi_gpu.cu - Unified Memory with multiple GPUs\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void process(float* data, int start, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[start + tid] *= 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need 2+ GPUs\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    const int N = 1 << 20;\n",
    "    const int NUM_GPUS = 2;\n",
    "    const int N_PER_GPU = N / NUM_GPUS;\n",
    "    \n",
    "    // ============================================\n",
    "    // Allocate Unified Memory\n",
    "    // ============================================\n",
    "    float* data;\n",
    "    cudaMallocManaged(&data, N * sizeof(float));\n",
    "    \n",
    "    // Initialize on host\n",
    "    for (int i = 0; i < N; i++) data[i] = 1.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // Give Hints About Data Location\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        size_t size = N_PER_GPU * sizeof(float);\n",
    "        \n",
    "        // Hint: This data is preferred on this GPU\n",
    "        cudaMemAdvise(data + offset, size, \n",
    "                      cudaMemAdviseSetPreferredLocation, gpu);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Prefetch Data to GPUs\n",
    "    // ============================================\n",
    "    cudaStream_t streams[NUM_GPUS];\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamCreate(&streams[gpu]);\n",
    "        \n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemPrefetchAsync(data + offset, \n",
    "                             N_PER_GPU * sizeof(float),\n",
    "                             gpu, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Launch Kernels\n",
    "    // ============================================\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N_PER_GPU + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        \n",
    "        process<<<numBlocks, blockSize, 0, streams[gpu]>>>(\n",
    "            data, offset, N_PER_GPU);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Prefetch Back to Host\n",
    "    // ============================================\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        int offset = gpu * N_PER_GPU;\n",
    "        cudaMemPrefetchAsync(data + offset,\n",
    "                             N_PER_GPU * sizeof(float),\n",
    "                             cudaCpuDeviceId, streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Sync all\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamSynchronize(streams[gpu]);\n",
    "    }\n",
    "    \n",
    "    // Verify on host\n",
    "    printf(\"data[0] = %.1f (expected 2.0)\\n\", data[0]);\n",
    "    printf(\"data[%d] = %.1f (expected 2.0)\\n\", N_PER_GPU, data[N_PER_GPU]);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int gpu = 0; gpu < NUM_GPUS; gpu++) {\n",
    "        cudaSetDevice(gpu);\n",
    "        cudaStreamDestroy(streams[gpu]);\n",
    "    }\n",
    "    cudaFree(data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe95c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o unified_multi_gpu unified_multi_gpu.cu\n",
    "!./unified_multi_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3167496",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multi_gpu_basics_exercises.cu\n",
    "// multi_gpu_basics_exercises.cu - Multi-GPU Basics Exercises\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "               cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 1: Find GPU with Most Memory\n",
    "// ============================================\n",
    "void findBestGPU() {\n",
    "    printf(\"\\n=== Exercise 1: Device Query - Find Best GPU ===\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CHECK_CUDA(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    printf(\"Found %d GPU(s)\\n\\n\", deviceCount);\n",
    "    \n",
    "    int bestDevice = 0;\n",
    "    size_t maxMemory = 0;\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        cudaDeviceProp prop;\n",
    "        CHECK_CUDA(cudaGetDeviceProperties(&prop, i));\n",
    "        \n",
    "        printf(\"GPU %d: %s\\n\", i, prop.name);\n",
    "        printf(\"  Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"  Total Memory: %.2f GB\\n\", prop.totalGlobalMem / (1024.0*1024.0*1024.0));\n",
    "        printf(\"  SM Count: %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"  Max Threads/Block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "        printf(\"  Memory Clock: %.0f MHz\\n\", prop.memoryClockRate / 1000.0);\n",
    "        printf(\"  Memory Bus Width: %d bits\\n\", prop.memoryBusWidth);\n",
    "        \n",
    "        // Calculate theoretical bandwidth\n",
    "        float bandwidth = 2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) / 1.0e6;\n",
    "        printf(\"  Theoretical Bandwidth: %.1f GB/s\\n\\n\", bandwidth);\n",
    "        \n",
    "        if (prop.totalGlobalMem > maxMemory) {\n",
    "            maxMemory = prop.totalGlobalMem;\n",
    "            bestDevice = i;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    cudaDeviceProp bestProp;\n",
    "    CHECK_CUDA(cudaGetDeviceProperties(&bestProp, bestDevice));\n",
    "    printf(\"‚úì Best GPU (most memory): GPU %d (%s) with %.2f GB\\n\",\n",
    "           bestDevice, bestProp.name, maxMemory / (1024.0*1024.0*1024.0));\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 2: P2P Bandwidth Measurement\n",
    "// ============================================\n",
    "void measureP2PBandwidth() {\n",
    "    printf(\"\\n=== Exercise 2: P2P Bandwidth Measurement ===\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CHECK_CUDA(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    if (deviceCount < 2) {\n",
    "        printf(\"Need at least 2 GPUs for P2P test. Found: %d\\n\", deviceCount);\n",
    "        printf(\"Simulating with single GPU (host staging)...\\n\\n\");\n",
    "        \n",
    "        // Single GPU simulation via host\n",
    "        const size_t size = 256 * 1024 * 1024;  // 256 MB\n",
    "        const int iterations = 10;\n",
    "        \n",
    "        float *d_src, *d_dst;\n",
    "        float *h_buffer;\n",
    "        CHECK_CUDA(cudaMalloc(&d_src, size));\n",
    "        CHECK_CUDA(cudaMalloc(&d_dst, size));\n",
    "        CHECK_CUDA(cudaMallocHost(&h_buffer, size));  // Pinned memory\n",
    "        \n",
    "        cudaEvent_t start, stop;\n",
    "        cudaEventCreate(&start);\n",
    "        cudaEventCreate(&stop);\n",
    "        \n",
    "        // Warm up\n",
    "        CHECK_CUDA(cudaMemcpy(h_buffer, d_src, size, cudaMemcpyDeviceToHost));\n",
    "        CHECK_CUDA(cudaMemcpy(d_dst, h_buffer, size, cudaMemcpyHostToDevice));\n",
    "        \n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < iterations; i++) {\n",
    "            CHECK_CUDA(cudaMemcpy(d_dst, d_src, size, cudaMemcpyDeviceToDevice));\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        float bandwidth = (float)size * iterations / (ms / 1000.0) / (1024*1024*1024);\n",
    "        \n",
    "        printf(\"Device-to-Device copy (same GPU):\\n\");\n",
    "        printf(\"  Size: %.0f MB\\n\", size / (1024.0 * 1024.0));\n",
    "        printf(\"  Time: %.2f ms (%d iterations)\\n\", ms, iterations);\n",
    "        printf(\"  Bandwidth: %.2f GB/s\\n\", bandwidth);\n",
    "        \n",
    "        cudaFree(d_src);\n",
    "        cudaFree(d_dst);\n",
    "        cudaFreeHost(h_buffer);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Multi-GPU P2P test\n",
    "    int gpu0 = 0, gpu1 = 1;\n",
    "    \n",
    "    // Check P2P capability\n",
    "    int canAccess01, canAccess10;\n",
    "    CHECK_CUDA(cudaDeviceCanAccessPeer(&canAccess01, gpu0, gpu1));\n",
    "    CHECK_CUDA(cudaDeviceCanAccessPeer(&canAccess10, gpu1, gpu0));\n",
    "    \n",
    "    printf(\"P2P Access: GPU %d -> GPU %d: %s\\n\", \n",
    "           gpu0, gpu1, canAccess01 ? \"YES\" : \"NO\");\n",
    "    printf(\"P2P Access: GPU %d -> GPU %d: %s\\n\", \n",
    "           gpu1, gpu0, canAccess10 ? \"YES\" : \"NO\");\n",
    "    \n",
    "    if (canAccess01 && canAccess10) {\n",
    "        CHECK_CUDA(cudaSetDevice(gpu0));\n",
    "        CHECK_CUDA(cudaDeviceEnablePeerAccess(gpu1, 0));\n",
    "        CHECK_CUDA(cudaSetDevice(gpu1));\n",
    "        CHECK_CUDA(cudaDeviceEnablePeerAccess(gpu0, 0));\n",
    "        printf(\"P2P enabled between GPUs\\n\\n\");\n",
    "    }\n",
    "    \n",
    "    const size_t size = 256 * 1024 * 1024;  // 256 MB\n",
    "    const int iterations = 10;\n",
    "    \n",
    "    // Allocate on each GPU\n",
    "    float *d_gpu0, *d_gpu1;\n",
    "    CHECK_CUDA(cudaSetDevice(gpu0));\n",
    "    CHECK_CUDA(cudaMalloc(&d_gpu0, size));\n",
    "    CHECK_CUDA(cudaSetDevice(gpu1));\n",
    "    CHECK_CUDA(cudaMalloc(&d_gpu1, size));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaSetDevice(gpu0));\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Measure GPU0 -> GPU1\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        CHECK_CUDA(cudaMemcpyPeer(d_gpu1, gpu1, d_gpu0, gpu0, size));\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    float bw01 = (float)size * iterations / (ms / 1000.0) / (1024*1024*1024);\n",
    "    \n",
    "    printf(\"GPU %d -> GPU %d:\\n\", gpu0, gpu1);\n",
    "    printf(\"  Bandwidth: %.2f GB/s\\n\", bw01);\n",
    "    \n",
    "    // Measure GPU1 -> GPU0\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        CHECK_CUDA(cudaMemcpyPeer(d_gpu0, gpu0, d_gpu1, gpu1, size));\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    float bw10 = (float)size * iterations / (ms / 1000.0) / (1024*1024*1024);\n",
    "    \n",
    "    printf(\"GPU %d -> GPU %d:\\n\", gpu1, gpu0);\n",
    "    printf(\"  Bandwidth: %.2f GB/s\\n\", bw10);\n",
    "    printf(\"\\nBidirectional: %.2f GB/s\\n\", bw01 + bw10);\n",
    "    \n",
    "    cudaFree(d_gpu0);\n",
    "    cudaFree(d_gpu1);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 3: Multi-GPU Vector Addition\n",
    "// ============================================\n",
    "__global__ void vectorAddKernel(float* a, float* b, float* c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiGPUVectorAdd() {\n",
    "    printf(\"\\n=== Exercise 3: Multi-GPU Vector Addition ===\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CHECK_CUDA(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    // Use up to 2 GPUs\n",
    "    int numGPUs = (deviceCount >= 2) ? 2 : 1;\n",
    "    printf(\"Using %d GPU(s)\\n\", numGPUs);\n",
    "    \n",
    "    const int N = 16 * 1024 * 1024;  // 16M elements\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    // Host arrays\n",
    "    float *h_a, *h_b, *h_c;\n",
    "    CHECK_CUDA(cudaMallocHost(&h_a, size));\n",
    "    CHECK_CUDA(cudaMallocHost(&h_b, size));\n",
    "    CHECK_CUDA(cudaMallocHost(&h_c, size));\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_a[i] = (float)i;\n",
    "        h_b[i] = (float)(i * 2);\n",
    "    }\n",
    "    \n",
    "    // Calculate chunk sizes\n",
    "    int chunkSize = N / numGPUs;\n",
    "    int remainder = N % numGPUs;\n",
    "    \n",
    "    // Device pointers and streams\n",
    "    float *d_a[2], *d_b[2], *d_c[2];\n",
    "    cudaStream_t streams[2];\n",
    "    int offsets[2], sizes[2];\n",
    "    \n",
    "    // Setup each GPU\n",
    "    for (int i = 0; i < numGPUs; i++) {\n",
    "        offsets[i] = i * chunkSize;\n",
    "        sizes[i] = chunkSize + (i == numGPUs - 1 ? remainder : 0);\n",
    "        \n",
    "        CHECK_CUDA(cudaSetDevice(i));\n",
    "        CHECK_CUDA(cudaStreamCreate(&streams[i]));\n",
    "        CHECK_CUDA(cudaMalloc(&d_a[i], sizes[i] * sizeof(float)));\n",
    "        CHECK_CUDA(cudaMalloc(&d_b[i], sizes[i] * sizeof(float)));\n",
    "        CHECK_CUDA(cudaMalloc(&d_c[i], sizes[i] * sizeof(float)));\n",
    "    }\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaSetDevice(0));\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Copy to GPUs (async)\n",
    "    for (int i = 0; i < numGPUs; i++) {\n",
    "        CHECK_CUDA(cudaSetDevice(i));\n",
    "        CHECK_CUDA(cudaMemcpyAsync(d_a[i], h_a + offsets[i], \n",
    "                                   sizes[i] * sizeof(float),\n",
    "                                   cudaMemcpyHostToDevice, streams[i]));\n",
    "        CHECK_CUDA(cudaMemcpyAsync(d_b[i], h_b + offsets[i],\n",
    "                                   sizes[i] * sizeof(float),\n",
    "                                   cudaMemcpyHostToDevice, streams[i]));\n",
    "    }\n",
    "    \n",
    "    // Launch kernels\n",
    "    int blockSize = 256;\n",
    "    for (int i = 0; i < numGPUs; i++) {\n",
    "        CHECK_CUDA(cudaSetDevice(i));\n",
    "        int gridSize = (sizes[i] + blockSize - 1) / blockSize;\n",
    "        vectorAddKernel<<<gridSize, blockSize, 0, streams[i]>>>(\n",
    "            d_a[i], d_b[i], d_c[i], sizes[i]);\n",
    "    }\n",
    "    \n",
    "    // Copy results back (async)\n",
    "    for (int i = 0; i < numGPUs; i++) {\n",
    "        CHECK_CUDA(cudaSetDevice(i));\n",
    "        CHECK_CUDA(cudaMemcpyAsync(h_c + offsets[i], d_c[i],\n",
    "                                   sizes[i] * sizeof(float),\n",
    "                                   cudaMemcpyDeviceToHost, streams[i]));\n",
    "    }\n",
    "    \n",
    "    // Synchronize all\n",
    "    for (int i = 0; i < numGPUs; i++) {\n",
    "        CHECK_CUDA(cudaSetDevice(i));\n",
    "        CHECK_CUDA(cudaStreamSynchronize(streams[i]));\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        float expected = h_a[i] + h_b[i];\n",
    "        if (fabs(h_c[i] - expected) > 1e-5) {\n",
    "            printf(\"Mismatch at %d: %.2f vs %.2f\\n\", i, h_c[i], expected);\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"\\nVector size: %d elements (%.0f MB)\\n\", N, size / (1024.0 * 1024.0));\n",
    "    for (int i = 0; i < numGPUs; i++) {\n",
    "        printf(\"GPU %d processed: %d elements\\n\", i, sizes[i]);\n",
    "    }\n",
    "    printf(\"\\nTotal time: %.3f ms\\n\", ms);\n",
    "    printf(\"Throughput: %.2f GB/s\\n\", 3.0 * size / (ms / 1000.0) / (1024*1024*1024));\n",
    "    printf(\"Result: %s\\n\", correct ? \"CORRECT ‚úì\" : \"INCORRECT ‚úó\");\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < numGPUs; i++) {\n",
    "        CHECK_CUDA(cudaSetDevice(i));\n",
    "        cudaFree(d_a[i]);\n",
    "        cudaFree(d_b[i]);\n",
    "        cudaFree(d_c[i]);\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "    }\n",
    "    cudaFreeHost(h_a);\n",
    "    cudaFreeHost(h_b);\n",
    "    cudaFreeHost(h_c);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Main\n",
    "// ============================================\n",
    "int main() {\n",
    "    printf(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\");\n",
    "    printf(\"‚ïë            MULTI-GPU BASICS - EXERCISES                       ‚ïë\\n\");\n",
    "    printf(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\\n\");\n",
    "    \n",
    "    findBestGPU();\n",
    "    measureP2PBandwidth();\n",
    "    multiGPUVectorAdd();\n",
    "    \n",
    "    printf(\"\\n‚úì All exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o multi_gpu_basics_exercises multi_gpu_basics_exercises.cu && ./multi_gpu_basics_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197190ac",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Device Query\n",
    "Write code to find the GPU with the most memory.\n",
    "\n",
    "### Exercise 2: P2P Bandwidth\n",
    "Measure P2P copy bandwidth between two GPUs.\n",
    "\n",
    "### Exercise 3: Multi-GPU Vector Add\n",
    "Implement vector addition split across 2 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca086248",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                MULTI-GPU BASICS                         ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Device Management:                                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ cudaGetDeviceCount(&count)                           ‚îÇ\n",
    "‚îÇ  ‚Ä¢ cudaSetDevice(id)                                    ‚îÇ\n",
    "‚îÇ  ‚Ä¢ cudaGetDevice(&id)                                   ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Peer Access:                                           ‚îÇ\n",
    "‚îÇ  ‚Ä¢ cudaDeviceCanAccessPeer(&can, dev, peer)             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ cudaDeviceEnablePeerAccess(peer, 0)                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ cudaMemcpyPeer(dst, dstDev, src, srcDev, size)       ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Pattern:                                               ‚îÇ\n",
    "‚îÇ  1. Query/select devices                                ‚îÇ\n",
    "‚îÇ  2. Enable P2P if available                             ‚îÇ\n",
    "‚îÇ  3. Allocate memory on each GPU                         ‚îÇ\n",
    "‚îÇ  4. Distribute data                                     ‚îÇ\n",
    "‚îÇ  5. Launch kernels                                      ‚îÇ\n",
    "‚îÇ  6. Collect results                                     ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Next: Day 2 - Multi-GPU Patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
