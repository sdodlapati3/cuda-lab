{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce799b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f220b7",
   "metadata": {},
   "source": [
    "# Day 4: Capstone Project - Putting It All Together\n",
    "\n",
    "## ğŸ¯ The Hook: Your GPU Masterpiece!\n",
    "\n",
    "**You've spent 12 weeks learning to harness the GPU.** Thread indexing, memory hierarchies, parallel patterns, streams, graphs, multi-GPU programming... dozens of techniques, hundreds of code examples.\n",
    "\n",
    "**Now it's YOUR turn to build something real.**\n",
    "\n",
    "Think of this capstone like a musician's final recital. You've practiced scales (thread indexing), learned chords (memory patterns), studied composition (optimization), and played with an orchestra (multi-GPU). Today, you perform your masterpiece.\n",
    "\n",
    "**The project: GPU-Accelerated Image Convolution**\n",
    "\n",
    "```\n",
    "INPUT IMAGE          â†’    GPU PROCESSING    â†’    OUTPUT IMAGE\n",
    "[Raw pixels]              [Your kernels!]        [Filtered result]\n",
    "\n",
    "                    Skills demonstrated:\n",
    "                    âœ“ 2D thread indexing\n",
    "                    âœ“ Tiled shared memory\n",
    "                    âœ“ Constant memory filters\n",
    "                    âœ“ Streams for batches\n",
    "                    âœ“ CUDA graphs for pipelines\n",
    "                    âœ“ Multi-GPU for large images\n",
    "```\n",
    "\n",
    "**This is the culmination of everything you've learned. Let's build something amazing!** ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this capstone, you will:\n",
    "\n",
    "1. **Integrate multiple CUDA techniques** - Combine memory optimization, parallelism, and concurrency\n",
    "2. **Build a production-quality system** - Complete end-to-end GPU pipeline\n",
    "3. **Demonstrate performance gains** - Show significant speedup over CPU\n",
    "4. **Apply the optimization checklist** - Use systematic approach from Day 3\n",
    "5. **Celebrate completion** - You've mastered GPU programming fundamentals!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ´ Concept Card: Putting It All Together\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 PUTTING IT ALL TOGETHER                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ¼ YOUR CUDA SYMPHONY - 12 WEEKS OF LEARNING:                  â”‚\n",
    "â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  WEEK 1-2: THE FOUNDATION                                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
    "â”‚  â”‚ Thread Indexing â€¢ Memory Basics         â”‚                    â”‚\n",
    "â”‚  â”‚ Coalescing â€¢ Error Handling             â”‚  â† Basic skills    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â”‚                    â†“                                            â”‚\n",
    "â”‚  WEEK 3-4: MEMORY MASTERY                                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
    "â”‚  â”‚ Shared Memory â€¢ Bank Conflicts          â”‚                    â”‚\n",
    "â”‚  â”‚ Reduction â€¢ Warp Primitives             â”‚  â† Core patterns   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â”‚                    â†“                                            â”‚\n",
    "â”‚  WEEK 5-6: PARALLEL PATTERNS                                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
    "â”‚  â”‚ Scan Algorithms â€¢ Matrix Operations     â”‚                    â”‚\n",
    "â”‚  â”‚ Tiling â€¢ cuBLAS Integration             â”‚  â† Algorithms      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â”‚                    â†“                                            â”‚\n",
    "â”‚  WEEK 7-8: PERFORMANCE                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
    "â”‚  â”‚ Occupancy â€¢ Profiling â€¢ Roofline        â”‚                    â”‚\n",
    "â”‚  â”‚ Memory Optimization â€¢ Tuning            â”‚  â† Analysis        â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â”‚                    â†“                                            â”‚\n",
    "â”‚  WEEK 9-10: CONCURRENCY                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
    "â”‚  â”‚ Streams â€¢ Async Execution               â”‚                    â”‚\n",
    "â”‚  â”‚ CUDA Graphs â€¢ Pipelines                 â”‚  â† Advanced exec   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â”‚                    â†“                                            â”‚\n",
    "â”‚  WEEK 11-12: MASTERY                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
    "â”‚  â”‚ Cooperative Groups â€¢ Dynamic Parallelism â”‚                   â”‚\n",
    "â”‚  â”‚ Multi-GPU â€¢ Optimization Review         â”‚  â† Expert level    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â”‚                    â†“                                            â”‚\n",
    "â”‚             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                   â”‚\n",
    "â”‚             â•‘   CAPSTONE    â•‘  â† YOU ARE HERE!                  â”‚\n",
    "â”‚             â•‘   PROJECT     â•‘                                   â”‚\n",
    "â”‚             â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸ¯ CAPSTONE PROJECT: IMAGE CONVOLUTION                         â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Week 1-2 skills: 2D grid indexing for image pixels             â”‚\n",
    "â”‚  Week 3 skills:   Shared memory for tiled convolution           â”‚\n",
    "â”‚  Week 4 skills:   Warp-level operations for edge cases          â”‚\n",
    "â”‚  Week 6 skills:   Tiled algorithm from matrix operations        â”‚\n",
    "â”‚  Week 7-8 skills: Occupancy tuning and profiling                â”‚\n",
    "â”‚  Week 9 skills:   Streams for multi-image processing            â”‚\n",
    "â”‚  Week 10 skills:  Graphs for filter pipelines                   â”‚\n",
    "â”‚  Week 12 skills:  Multi-GPU for large images                    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸ’¡ FINAL INSIGHT:                                              â”‚\n",
    "â”‚  Every technique you learned has a place. The art of GPU        â”‚\n",
    "â”‚  programming is knowing WHEN to apply WHAT. You now have        â”‚\n",
    "â”‚  both the knowledge and the judgment. Go build amazing things!  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c263e8f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capstone Project: GPU-Accelerated Image Convolution\n",
    "\n",
    "### Project Requirements\n",
    "\n",
    "Build a complete image convolution system that:\n",
    "1. Loads images from disk\n",
    "2. Applies various filters (blur, sharpen, edge detect)\n",
    "3. Uses optimized CUDA kernels\n",
    "4. Demonstrates performance gains over CPU\n",
    "5. Uses streams for overlapped processing of multiple images\n",
    "\n",
    "### Skills Demonstrated\n",
    "\n",
    "```\n",
    "âœ“ Week 1-2: Thread indexing, 2D grids\n",
    "âœ“ Week 3-4: Shared memory, memory coalescing\n",
    "âœ“ Week 5:   Scan for histogram normalization\n",
    "âœ“ Week 6:   2D tiled algorithms\n",
    "âœ“ Week 7:   Occupancy and memory optimization\n",
    "âœ“ Week 8:   Profiling and roofline analysis\n",
    "âœ“ Week 9:   Streams for multi-image processing\n",
    "âœ“ Week 10:  CUDA Graphs for repeated filters\n",
    "âœ“ Week 11:  Cooperative groups for reductions\n",
    "âœ“ Week 12:  Multi-GPU for large images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8dc55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Core Convolution Kernel\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// convolution.cu - Complete image convolution system\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "#define FILTER_RADIUS 2\n",
    "#define FILTER_SIZE (2 * FILTER_RADIUS + 1)\n",
    "\n",
    "// Constant memory for filter coefficients\n",
    "__constant__ float c_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "\n",
    "// ============================================================\n",
    "// Naive Convolution (Baseline)\n",
    "// ============================================================\n",
    "__global__ void convNaive(\n",
    "    float* output, const float* input,\n",
    "    int width, int height\n",
    ") {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x >= width || y >= height) return;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    for (int fy = -FILTER_RADIUS; fy <= FILTER_RADIUS; fy++) {\n",
    "        for (int fx = -FILTER_RADIUS; fx <= FILTER_RADIUS; fx++) {\n",
    "            int ix = min(max(x + fx, 0), width - 1);\n",
    "            int iy = min(max(y + fy, 0), height - 1);\n",
    "            \n",
    "            int fidx = (fy + FILTER_RADIUS) * FILTER_SIZE + \n",
    "                       (fx + FILTER_RADIUS);\n",
    "            \n",
    "            sum += input[iy * width + ix] * c_filter[fidx];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output[y * width + x] = sum;\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Optimized Convolution with Shared Memory\n",
    "// ============================================================\n",
    "__global__ void convShared(\n",
    "    float* output, const float* input,\n",
    "    int width, int height\n",
    ") {\n",
    "    // Shared memory tile with apron\n",
    "    const int TILE_W = BLOCK_SIZE + 2 * FILTER_RADIUS;\n",
    "    __shared__ float smem[TILE_W][TILE_W];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int x = blockIdx.x * BLOCK_SIZE + tx;\n",
    "    int y = blockIdx.y * BLOCK_SIZE + ty;\n",
    "    \n",
    "    // Calculate source positions including apron\n",
    "    int srcX = x - FILTER_RADIUS;\n",
    "    int srcY = y - FILTER_RADIUS;\n",
    "    \n",
    "    // Load main tile element\n",
    "    if (srcX >= 0 && srcX < width && srcY >= 0 && srcY < height) {\n",
    "        smem[ty][tx] = input[srcY * width + srcX];\n",
    "    } else {\n",
    "        smem[ty][tx] = 0.0f;\n",
    "    }\n",
    "    \n",
    "    // Load additional elements for apron (right and bottom edges)\n",
    "    if (tx < 2 * FILTER_RADIUS) {\n",
    "        int ax = srcX + BLOCK_SIZE;\n",
    "        if (ax >= 0 && ax < width && srcY >= 0 && srcY < height) {\n",
    "            smem[ty][tx + BLOCK_SIZE] = input[srcY * width + ax];\n",
    "        } else {\n",
    "            smem[ty][tx + BLOCK_SIZE] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (ty < 2 * FILTER_RADIUS) {\n",
    "        int ay = srcY + BLOCK_SIZE;\n",
    "        if (srcX >= 0 && srcX < width && ay >= 0 && ay < height) {\n",
    "            smem[ty + BLOCK_SIZE][tx] = input[ay * width + srcX];\n",
    "        } else {\n",
    "            smem[ty + BLOCK_SIZE][tx] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (tx < 2 * FILTER_RADIUS && ty < 2 * FILTER_RADIUS) {\n",
    "        int ax = srcX + BLOCK_SIZE;\n",
    "        int ay = srcY + BLOCK_SIZE;\n",
    "        if (ax >= 0 && ax < width && ay >= 0 && ay < height) {\n",
    "            smem[ty + BLOCK_SIZE][tx + BLOCK_SIZE] = input[ay * width + ax];\n",
    "        } else {\n",
    "            smem[ty + BLOCK_SIZE][tx + BLOCK_SIZE] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute convolution from shared memory\n",
    "    if (x < width && y < height) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int fy = 0; fy < FILTER_SIZE; fy++) {\n",
    "            #pragma unroll\n",
    "            for (int fx = 0; fx < FILTER_SIZE; fx++) {\n",
    "                sum += smem[ty + fy][tx + fx] * \n",
    "                       c_filter[fy * FILTER_SIZE + fx];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output[y * width + x] = sum;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile convolution.cu\n",
    "// convolution.cu - Complete image convolution system\n",
    "#include <stdio.h>\n",
    "#include <string.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "#define FILTER_RADIUS 2\n",
    "#define FILTER_SIZE (2 * FILTER_RADIUS + 1)\n",
    "\n",
    "// Constant memory for filter coefficients\n",
    "__constant__ float c_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "\n",
    "// ============================================================\n",
    "// Naive Convolution (Baseline)\n",
    "// ============================================================\n",
    "__global__ void convNaive(\n",
    "    float* output, const float* input,\n",
    "    int width, int height\n",
    ") {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x >= width || y >= height) return;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    for (int fy = -FILTER_RADIUS; fy <= FILTER_RADIUS; fy++) {\n",
    "        for (int fx = -FILTER_RADIUS; fx <= FILTER_RADIUS; fx++) {\n",
    "            int ix = min(max(x + fx, 0), width - 1);\n",
    "            int iy = min(max(y + fy, 0), height - 1);\n",
    "            \n",
    "            int fidx = (fy + FILTER_RADIUS) * FILTER_SIZE + \n",
    "                       (fx + FILTER_RADIUS);\n",
    "            \n",
    "            sum += input[iy * width + ix] * c_filter[fidx];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output[y * width + x] = sum;\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Optimized Convolution with Shared Memory\n",
    "// ============================================================\n",
    "__global__ void convShared(\n",
    "    float* output, const float* input,\n",
    "    int width, int height\n",
    ") {\n",
    "    // Shared memory tile with apron\n",
    "    const int TILE_W = BLOCK_SIZE + 2 * FILTER_RADIUS;\n",
    "    __shared__ float smem[TILE_W][TILE_W];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int x = blockIdx.x * BLOCK_SIZE + tx;\n",
    "    int y = blockIdx.y * BLOCK_SIZE + ty;\n",
    "    \n",
    "    // Calculate source positions including apron\n",
    "    int srcX = x - FILTER_RADIUS;\n",
    "    int srcY = y - FILTER_RADIUS;\n",
    "    \n",
    "    // Load main tile element\n",
    "    if (srcX >= 0 && srcX < width && srcY >= 0 && srcY < height) {\n",
    "        smem[ty][tx] = input[srcY * width + srcX];\n",
    "    } else {\n",
    "        smem[ty][tx] = 0.0f;\n",
    "    }\n",
    "    \n",
    "    // Load additional elements for apron (right and bottom edges)\n",
    "    if (tx < 2 * FILTER_RADIUS) {\n",
    "        int ax = srcX + BLOCK_SIZE;\n",
    "        if (ax >= 0 && ax < width && srcY >= 0 && srcY < height) {\n",
    "            smem[ty][tx + BLOCK_SIZE] = input[srcY * width + ax];\n",
    "        } else {\n",
    "            smem[ty][tx + BLOCK_SIZE] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (ty < 2 * FILTER_RADIUS) {\n",
    "        int ay = srcY + BLOCK_SIZE;\n",
    "        if (srcX >= 0 && srcX < width && ay >= 0 && ay < height) {\n",
    "            smem[ty + BLOCK_SIZE][tx] = input[ay * width + srcX];\n",
    "        } else {\n",
    "            smem[ty + BLOCK_SIZE][tx] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (tx < 2 * FILTER_RADIUS && ty < 2 * FILTER_RADIUS) {\n",
    "        int ax = srcX + BLOCK_SIZE;\n",
    "        int ay = srcY + BLOCK_SIZE;\n",
    "        if (ax >= 0 && ax < width && ay >= 0 && ay < height) {\n",
    "            smem[ty + BLOCK_SIZE][tx + BLOCK_SIZE] = input[ay * width + ax];\n",
    "        } else {\n",
    "            smem[ty + BLOCK_SIZE][tx + BLOCK_SIZE] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute convolution from shared memory\n",
    "    if (x < width && y < height) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int fy = 0; fy < FILTER_SIZE; fy++) {\n",
    "            #pragma unroll\n",
    "            for (int fx = 0; fx < FILTER_SIZE; fx++) {\n",
    "                sum += smem[ty + fy][tx + fx] * \n",
    "                       c_filter[fy * FILTER_SIZE + fx];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output[y * width + x] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Filter Definitions\n",
    "// ============================================================\n",
    "\n",
    "void createGaussianBlur(float* filter) {\n",
    "    float kernel[] = {\n",
    "        1.0f/256, 4.0f/256,  7.0f/256,  4.0f/256, 1.0f/256,\n",
    "        4.0f/256, 16.0f/256, 26.0f/256, 16.0f/256, 4.0f/256,\n",
    "        7.0f/256, 26.0f/256, 41.0f/256, 26.0f/256, 7.0f/256,\n",
    "        4.0f/256, 16.0f/256, 26.0f/256, 16.0f/256, 4.0f/256,\n",
    "        1.0f/256, 4.0f/256,  7.0f/256,  4.0f/256, 1.0f/256\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "void createSharpen(float* filter) {\n",
    "    float kernel[] = {\n",
    "         0,  0, -1,  0,  0,\n",
    "         0, -1, -1, -1,  0,\n",
    "        -1, -1, 13, -1, -1,\n",
    "         0, -1, -1, -1,  0,\n",
    "         0,  0, -1,  0,  0\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "void createEdgeDetect(float* filter) {\n",
    "    float kernel[] = {\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, 24, -1, -1,\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, -1, -1, -1\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Main Program\n",
    "// ============================================================\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    const int WIDTH = 4096;\n",
    "    const int HEIGHT = 4096;\n",
    "    size_t imageBytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory (pinned for async transfers)\n",
    "    float *h_input, *h_output;\n",
    "    cudaMallocHost(&h_input, imageBytes);\n",
    "    cudaMallocHost(&h_output, imageBytes);\n",
    "    \n",
    "    // Initialize with test pattern\n",
    "    for (int i = 0; i < WIDTH * HEIGHT; i++) {\n",
    "        h_input[i] = (float)(i % 256) / 255.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, imageBytes);\n",
    "    cudaMalloc(&d_output, imageBytes);\n",
    "    \n",
    "    // Create and upload Gaussian blur filter\n",
    "    float h_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "    createGaussianBlur(h_filter);\n",
    "    cudaMemcpyToSymbol(c_filter, h_filter, \n",
    "                       FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "    \n",
    "    // Copy input to device\n",
    "    cudaMemcpy(d_input, h_input, imageBytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Setup kernel launch parameters\n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((WIDTH + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (HEIGHT + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    // Warmup\n",
    "    convShared<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // =========================================\n",
    "    // Benchmark: Naive vs Optimized\n",
    "    // =========================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int RUNS = 100;\n",
    "    float ms;\n",
    "    \n",
    "    // Naive kernel\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        convNaive<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float naiveTime = ms / RUNS;\n",
    "    printf(\"Naive convolution:     %.2f ms\\n\", naiveTime);\n",
    "    \n",
    "    // Optimized kernel\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        convShared<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float optTime = ms / RUNS;\n",
    "    printf(\"Optimized convolution: %.2f ms\\n\", optTime);\n",
    "    printf(\"Speedup: %.2fx\\n\", naiveTime / optTime);\n",
    "    \n",
    "    // Calculate effective bandwidth\n",
    "    float bandwidth = 2.0f * imageBytes / (optTime / 1000.0f) / 1e9;\n",
    "    printf(\"Effective bandwidth:   %.2f GB/s\\n\", bandwidth);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFreeHost(h_input);\n",
    "    cudaFreeHost(h_output);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    printf(\"\\nCapstone project complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o convolution convolution.cu\n",
    "!./convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b491a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Filter Definitions and Host Code\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// ============================================================\n",
    "// Filter Definitions\n",
    "// ============================================================\n",
    "\n",
    "void createGaussianBlur(float* filter) {\n",
    "    float kernel[] = {\n",
    "        1.0f/256, 4.0f/256,  7.0f/256,  4.0f/256, 1.0f/256,\n",
    "        4.0f/256, 16.0f/256, 26.0f/256, 16.0f/256, 4.0f/256,\n",
    "        7.0f/256, 26.0f/256, 41.0f/256, 26.0f/256, 7.0f/256,\n",
    "        4.0f/256, 16.0f/256, 26.0f/256, 16.0f/256, 4.0f/256,\n",
    "        1.0f/256, 4.0f/256,  7.0f/256,  4.0f/256, 1.0f/256\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "void createSharpen(float* filter) {\n",
    "    float kernel[] = {\n",
    "         0,  0, -1,  0,  0,\n",
    "         0, -1, -1, -1,  0,\n",
    "        -1, -1, 13, -1, -1,\n",
    "         0, -1, -1, -1,  0,\n",
    "         0,  0, -1,  0,  0\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "void createEdgeDetect(float* filter) {\n",
    "    float kernel[] = {\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, 24, -1, -1,\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, -1, -1, -1\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Stream-based Multi-Image Processing\n",
    "// ============================================================\n",
    "\n",
    "void processMultipleImages(\n",
    "    float** h_inputs, float** h_outputs,\n",
    "    int numImages, int width, int height\n",
    ") {\n",
    "    const int NUM_STREAMS = 4;\n",
    "    size_t imageBytes = width * height * sizeof(float);\n",
    "    \n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    float* d_input[NUM_STREAMS];\n",
    "    float* d_output[NUM_STREAMS];\n",
    "    \n",
    "    // Create streams and allocate device memory\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "        cudaMalloc(&d_input[i], imageBytes);\n",
    "        cudaMalloc(&d_output[i], imageBytes);\n",
    "    }\n",
    "    \n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((width + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    // Process images with overlapping\n",
    "    for (int img = 0; img < numImages; img++) {\n",
    "        int streamIdx = img % NUM_STREAMS;\n",
    "        \n",
    "        // H2D\n",
    "        cudaMemcpyAsync(d_input[streamIdx], h_inputs[img],\n",
    "                        imageBytes, cudaMemcpyHostToDevice,\n",
    "                        streams[streamIdx]);\n",
    "        \n",
    "        // Compute\n",
    "        convShared<<<grid, block, 0, streams[streamIdx]>>>(\n",
    "            d_output[streamIdx], d_input[streamIdx], width, height);\n",
    "        \n",
    "        // D2H\n",
    "        cudaMemcpyAsync(h_outputs[img], d_output[streamIdx],\n",
    "                        imageBytes, cudaMemcpyDeviceToHost,\n",
    "                        streams[streamIdx]);\n",
    "    }\n",
    "    \n",
    "    // Sync all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_input[i]);\n",
    "        cudaFree(d_output[i]);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd440032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multi_image_conv.cu\n",
    "// multi_image_conv.cu - Stream-based multi-image processing\n",
    "#include <stdio.h>\n",
    "#include <string.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "#define FILTER_RADIUS 2\n",
    "#define FILTER_SIZE (2 * FILTER_RADIUS + 1)\n",
    "\n",
    "__constant__ float c_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "\n",
    "__global__ void convShared(\n",
    "    float* output, const float* input,\n",
    "    int width, int height\n",
    ") {\n",
    "    const int TILE_W = BLOCK_SIZE + 2 * FILTER_RADIUS;\n",
    "    __shared__ float smem[TILE_W][TILE_W];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int x = blockIdx.x * BLOCK_SIZE + tx;\n",
    "    int y = blockIdx.y * BLOCK_SIZE + ty;\n",
    "    \n",
    "    int srcX = x - FILTER_RADIUS;\n",
    "    int srcY = y - FILTER_RADIUS;\n",
    "    \n",
    "    if (srcX >= 0 && srcX < width && srcY >= 0 && srcY < height) {\n",
    "        smem[ty][tx] = input[srcY * width + srcX];\n",
    "    } else {\n",
    "        smem[ty][tx] = 0.0f;\n",
    "    }\n",
    "    \n",
    "    if (tx < 2 * FILTER_RADIUS) {\n",
    "        int ax = srcX + BLOCK_SIZE;\n",
    "        if (ax >= 0 && ax < width && srcY >= 0 && srcY < height) {\n",
    "            smem[ty][tx + BLOCK_SIZE] = input[srcY * width + ax];\n",
    "        } else {\n",
    "            smem[ty][tx + BLOCK_SIZE] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (ty < 2 * FILTER_RADIUS) {\n",
    "        int ay = srcY + BLOCK_SIZE;\n",
    "        if (srcX >= 0 && srcX < width && ay >= 0 && ay < height) {\n",
    "            smem[ty + BLOCK_SIZE][tx] = input[ay * width + srcX];\n",
    "        } else {\n",
    "            smem[ty + BLOCK_SIZE][tx] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (tx < 2 * FILTER_RADIUS && ty < 2 * FILTER_RADIUS) {\n",
    "        int ax = srcX + BLOCK_SIZE;\n",
    "        int ay = srcY + BLOCK_SIZE;\n",
    "        if (ax >= 0 && ax < width && ay >= 0 && ay < height) {\n",
    "            smem[ty + BLOCK_SIZE][tx + BLOCK_SIZE] = input[ay * width + ax];\n",
    "        } else {\n",
    "            smem[ty + BLOCK_SIZE][tx + BLOCK_SIZE] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int fy = 0; fy < FILTER_SIZE; fy++) {\n",
    "            #pragma unroll\n",
    "            for (int fx = 0; fx < FILTER_SIZE; fx++) {\n",
    "                sum += smem[ty + fy][tx + fx] * \n",
    "                       c_filter[fy * FILTER_SIZE + fx];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output[y * width + x] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "void createGaussianBlur(float* filter) {\n",
    "    float kernel[] = {\n",
    "        1.0f/256, 4.0f/256,  7.0f/256,  4.0f/256, 1.0f/256,\n",
    "        4.0f/256, 16.0f/256, 26.0f/256, 16.0f/256, 4.0f/256,\n",
    "        7.0f/256, 26.0f/256, 41.0f/256, 26.0f/256, 7.0f/256,\n",
    "        4.0f/256, 16.0f/256, 26.0f/256, 16.0f/256, 4.0f/256,\n",
    "        1.0f/256, 4.0f/256,  7.0f/256,  4.0f/256, 1.0f/256\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int WIDTH = 2048;\n",
    "    const int HEIGHT = 2048;\n",
    "    const int NUM_IMAGES = 8;\n",
    "    const int NUM_STREAMS = 4;\n",
    "    size_t imageBytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    printf(\"Processing %d images of size %dx%d using %d streams\\n\",\n",
    "           NUM_IMAGES, WIDTH, HEIGHT, NUM_STREAMS);\n",
    "    \n",
    "    // Allocate host memory for images\n",
    "    float* h_inputs[NUM_IMAGES];\n",
    "    float* h_outputs[NUM_IMAGES];\n",
    "    for (int i = 0; i < NUM_IMAGES; i++) {\n",
    "        cudaMallocHost(&h_inputs[i], imageBytes);\n",
    "        cudaMallocHost(&h_outputs[i], imageBytes);\n",
    "        // Initialize with different patterns\n",
    "        for (int j = 0; j < WIDTH * HEIGHT; j++) {\n",
    "            h_inputs[i][j] = (float)((i + j) % 256) / 255.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Upload filter\n",
    "    float h_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "    createGaussianBlur(h_filter);\n",
    "    cudaMemcpyToSymbol(c_filter, h_filter, \n",
    "                       FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "    \n",
    "    // Create streams and allocate device memory\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    float* d_input[NUM_STREAMS];\n",
    "    float* d_output[NUM_STREAMS];\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "        cudaMalloc(&d_input[i], imageBytes);\n",
    "        cudaMalloc(&d_output[i], imageBytes);\n",
    "    }\n",
    "    \n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((WIDTH + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (HEIGHT + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Process images with overlapping streams\n",
    "    for (int img = 0; img < NUM_IMAGES; img++) {\n",
    "        int streamIdx = img % NUM_STREAMS;\n",
    "        \n",
    "        // H2D\n",
    "        cudaMemcpyAsync(d_input[streamIdx], h_inputs[img],\n",
    "                        imageBytes, cudaMemcpyHostToDevice,\n",
    "                        streams[streamIdx]);\n",
    "        \n",
    "        // Compute\n",
    "        convShared<<<grid, block, 0, streams[streamIdx]>>>(\n",
    "            d_output[streamIdx], d_input[streamIdx], WIDTH, HEIGHT);\n",
    "        \n",
    "        // D2H\n",
    "        cudaMemcpyAsync(h_outputs[img], d_output[streamIdx],\n",
    "                        imageBytes, cudaMemcpyDeviceToHost,\n",
    "                        streams[streamIdx]);\n",
    "    }\n",
    "    \n",
    "    // Sync all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"Total time for %d images: %.2f ms\\n\", NUM_IMAGES, ms);\n",
    "    printf(\"Average time per image: %.2f ms\\n\", ms / NUM_IMAGES);\n",
    "    printf(\"Throughput: %.2f images/sec\\n\", NUM_IMAGES / (ms / 1000.0f));\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_input[i]);\n",
    "        cudaFree(d_output[i]);\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < NUM_IMAGES; i++) {\n",
    "        cudaFreeHost(h_inputs[i]);\n",
    "        cudaFreeHost(h_outputs[i]);\n",
    "    }\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    printf(\"Multi-image processing complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o multi_image_conv multi_image_conv.cu\n",
    "!./multi_image_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9faa4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: CUDA Graph for Repeated Filters\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// ============================================================\n",
    "// CUDA Graph for Multi-Pass Filtering\n",
    "// ============================================================\n",
    "\n",
    "void multiPassFilter(\n",
    "    float* d_output, float* d_input,\n",
    "    float* d_temp,\n",
    "    int width, int height,\n",
    "    int numPasses\n",
    ") {\n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((width + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t instance;\n",
    "    \n",
    "    // Capture graph for one pass\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Forward pass: input -> temp\n",
    "    convShared<<<grid, block, 0, stream>>>(\n",
    "        d_temp, d_input, width, height);\n",
    "    \n",
    "    // Backward pass: temp -> output\n",
    "    convShared<<<grid, block, 0, stream>>>(\n",
    "        d_output, d_temp, width, height);\n",
    "    \n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Execute graph multiple times\n",
    "    for (int pass = 0; pass < numPasses; pass++) {\n",
    "        cudaGraphLaunch(instance, stream);\n",
    "    }\n",
    "    \n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaGraphExecDestroy(instance);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e7df6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Main Program and Benchmarking\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// ============================================================\n",
    "// Main Program\n",
    "// ============================================================\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    const int WIDTH = 4096;\n",
    "    const int HEIGHT = 4096;\n",
    "    size_t imageBytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory (pinned for async transfers)\n",
    "    float *h_input, *h_output;\n",
    "    cudaMallocHost(&h_input, imageBytes);\n",
    "    cudaMallocHost(&h_output, imageBytes);\n",
    "    \n",
    "    // Initialize with test pattern\n",
    "    for (int i = 0; i < WIDTH * HEIGHT; i++) {\n",
    "        h_input[i] = (float)(i % 256) / 255.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, imageBytes);\n",
    "    cudaMalloc(&d_output, imageBytes);\n",
    "    \n",
    "    // Create and upload Gaussian blur filter\n",
    "    float h_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "    createGaussianBlur(h_filter);\n",
    "    cudaMemcpyToSymbol(c_filter, h_filter, \n",
    "                       FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "    \n",
    "    // Copy input to device\n",
    "    cudaMemcpy(d_input, h_input, imageBytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Setup kernel launch parameters\n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((WIDTH + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (HEIGHT + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    // Warmup\n",
    "    convShared<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // =========================================\n",
    "    // Benchmark: Naive vs Optimized\n",
    "    // =========================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int RUNS = 100;\n",
    "    float ms;\n",
    "    \n",
    "    // Naive kernel\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        convNaive<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float naiveTime = ms / RUNS;\n",
    "    printf(\"Naive convolution:     %.2f ms\\n\", naiveTime);\n",
    "    \n",
    "    // Optimized kernel\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        convShared<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float optTime = ms / RUNS;\n",
    "    printf(\"Optimized convolution: %.2f ms\\n\", optTime);\n",
    "    printf(\"Speedup: %.2fx\\n\", naiveTime / optTime);\n",
    "    \n",
    "    // Calculate effective bandwidth\n",
    "    float bandwidth = 2.0f * imageBytes / (optTime / 1000.0f) / 1e9;\n",
    "    printf(\"Effective bandwidth:   %.2f GB/s\\n\", bandwidth);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFreeHost(h_input);\n",
    "    cudaFreeHost(h_output);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    printf(\"\\nCapstone project complete!\\n\");\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "// Compile: nvcc -O3 -arch=sm_70 convolution.cu -o convolution\n",
    "// Profile: ncu --set full ./convolution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112beb2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Capstone Project Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "This comprehensive capstone project integrates all the CUDA concepts you've learned throughout the curriculum. You will implement a complete **Image Processing Pipeline** with multiple stages.\n",
    "\n",
    "**Project Overview:**\n",
    "Build an optimized image processing pipeline that includes:\n",
    "1. **Grayscale Conversion** - RGB to grayscale with memory coalescing\n",
    "2. **Gaussian Blur** - Shared memory tiled convolution\n",
    "3. **Edge Detection** - Sobel filter implementation\n",
    "4. **Histogram Equalization** - Parallel histogram and prefix scan\n",
    "5. **Pipeline Integration** - Use streams for overlapping operations\n",
    "\n",
    "**Grading Criteria:**\n",
    "- âœ… Correctness of each kernel\n",
    "- âœ… Proper memory management (no leaks)\n",
    "- âœ… Use of shared memory where appropriate\n",
    "- âœ… Memory coalescing optimization\n",
    "- âœ… Stream-based pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile capstone_project.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// Image dimensions (simulated)\n",
    "#define IMG_WIDTH 1024\n",
    "#define IMG_HEIGHT 1024\n",
    "#define IMG_CHANNELS 3\n",
    "\n",
    "// =============================================================================\n",
    "// STAGE 1: Grayscale Conversion\n",
    "// TODO: Implement RGB to grayscale with coalesced memory access\n",
    "// Formula: gray = 0.299*R + 0.587*G + 0.114*B\n",
    "// =============================================================================\n",
    "__global__ void rgbToGrayscale(unsigned char* gray, unsigned char* rgb, \n",
    "                                int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        int grayIdx = y * width + x;\n",
    "        int rgbIdx = grayIdx * 3;\n",
    "        \n",
    "        // TODO: Implement grayscale conversion\n",
    "        // Hint: Use the luminance formula\n",
    "        unsigned char r = rgb[rgbIdx + 0];\n",
    "        unsigned char g = rgb[rgbIdx + 1];\n",
    "        unsigned char b = rgb[rgbIdx + 2];\n",
    "        \n",
    "        gray[grayIdx] = (unsigned char)(0.299f * r + 0.587f * g + 0.114f * b);\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// STAGE 2: Gaussian Blur (3x3)\n",
    "// TODO: Implement using shared memory for the tile\n",
    "// =============================================================================\n",
    "#define BLUR_TILE_SIZE 16\n",
    "#define BLUR_RADIUS 1\n",
    "\n",
    "__constant__ float d_gaussianKernel[9] = {\n",
    "    1.0f/16, 2.0f/16, 1.0f/16,\n",
    "    2.0f/16, 4.0f/16, 2.0f/16,\n",
    "    1.0f/16, 2.0f/16, 1.0f/16\n",
    "};\n",
    "\n",
    "__global__ void gaussianBlur(unsigned char* out, unsigned char* in,\n",
    "                              int width, int height) {\n",
    "    // Shared memory with halo for boundary\n",
    "    __shared__ float tile[BLUR_TILE_SIZE + 2][BLUR_TILE_SIZE + 2];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int x = blockIdx.x * BLUR_TILE_SIZE + tx;\n",
    "    int y = blockIdx.y * BLUR_TILE_SIZE + ty;\n",
    "    \n",
    "    // TODO: Exercise - Load tile with halo into shared memory\n",
    "    // Handle boundary conditions (clamp to edge)\n",
    "    int loadX = x - BLUR_RADIUS;\n",
    "    int loadY = y - BLUR_RADIUS;\n",
    "    \n",
    "    // Each thread loads one element of the tile\n",
    "    // Some threads also load halo elements\n",
    "    if (tx < BLUR_TILE_SIZE + 2 && ty < BLUR_TILE_SIZE + 2) {\n",
    "        int srcX = min(max(loadX + tx, 0), width - 1);\n",
    "        int srcY = min(max(loadY + ty, 0), height - 1);\n",
    "        tile[ty][tx] = (float)in[srcY * width + srcX];\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // TODO: Apply convolution\n",
    "    if (x < width && y < height && tx < BLUR_TILE_SIZE && ty < BLUR_TILE_SIZE) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        for (int ky = 0; ky < 3; ky++) {\n",
    "            for (int kx = 0; kx < 3; kx++) {\n",
    "                sum += tile[ty + ky][tx + kx] * d_gaussianKernel[ky * 3 + kx];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        out[y * width + x] = (unsigned char)min(max(sum, 0.0f), 255.0f);\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// STAGE 3: Sobel Edge Detection\n",
    "// TODO: Implement Sobel operator\n",
    "// =============================================================================\n",
    "__constant__ int d_sobelX[9] = {-1, 0, 1, -2, 0, 2, -1, 0, 1};\n",
    "__constant__ int d_sobelY[9] = {-1, -2, -1, 0, 0, 0, 1, 2, 1};\n",
    "\n",
    "__global__ void sobelEdgeDetection(unsigned char* out, unsigned char* in,\n",
    "                                    int width, int height) {\n",
    "    __shared__ float tile[18][18];  // 16 + 2 halo on each side\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int x = blockIdx.x * blockDim.x + tx;\n",
    "    int y = blockIdx.y * blockDim.y + ty;\n",
    "    \n",
    "    // Load tile with halo\n",
    "    int loadX = (int)(blockIdx.x * blockDim.x) - 1 + tx;\n",
    "    int loadY = (int)(blockIdx.y * blockDim.y) - 1 + ty;\n",
    "    \n",
    "    if (tx < 18 && ty < 18) {\n",
    "        int srcX = min(max(loadX, 0), width - 1);\n",
    "        int srcY = min(max(loadY, 0), height - 1);\n",
    "        tile[ty][tx] = (float)in[srcY * width + srcX];\n",
    "    }\n",
    "    \n",
    "    // Load extra elements if needed (for larger halo)\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        // TODO: Compute Gx and Gy using Sobel operators\n",
    "        float gx = 0.0f, gy = 0.0f;\n",
    "        \n",
    "        for (int ky = 0; ky < 3; ky++) {\n",
    "            for (int kx = 0; kx < 3; kx++) {\n",
    "                float val = tile[ty + ky][tx + kx];\n",
    "                gx += val * d_sobelX[ky * 3 + kx];\n",
    "                gy += val * d_sobelY[ky * 3 + kx];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Gradient magnitude\n",
    "        float mag = sqrtf(gx * gx + gy * gy);\n",
    "        out[y * width + x] = (unsigned char)min(mag, 255.0f);\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// STAGE 4: Histogram Computation\n",
    "// TODO: Implement parallel histogram with atomics\n",
    "// =============================================================================\n",
    "#define NUM_BINS 256\n",
    "\n",
    "__global__ void computeHistogram(int* histogram, unsigned char* image,\n",
    "                                  int width, int height) {\n",
    "    __shared__ int localHist[NUM_BINS];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int numPixels = width * height;\n",
    "    \n",
    "    // Initialize local histogram\n",
    "    for (int i = tid; i < NUM_BINS; i += blockDim.x) {\n",
    "        localHist[i] = 0;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // TODO: Accumulate to local histogram\n",
    "    if (idx < numPixels) {\n",
    "        atomicAdd(&localHist[image[idx]], 1);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // TODO: Merge to global histogram\n",
    "    for (int i = tid; i < NUM_BINS; i += blockDim.x) {\n",
    "        if (localHist[i] > 0) {\n",
    "            atomicAdd(&histogram[i], localHist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// STAGE 5: Histogram Equalization (CDF computation + mapping)\n",
    "// =============================================================================\n",
    "__global__ void computeCDF(float* cdf, int* histogram, int numPixels) {\n",
    "    __shared__ float temp[NUM_BINS];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Load and normalize\n",
    "    temp[tid] = (float)histogram[tid] / numPixels;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Inclusive scan (Blelloch-style up-sweep/down-sweep simplified)\n",
    "    for (int stride = 1; stride < NUM_BINS; stride *= 2) {\n",
    "        float val = 0;\n",
    "        if (tid >= stride) {\n",
    "            val = temp[tid - stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "        if (tid >= stride) {\n",
    "            temp[tid] += val;\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    cdf[tid] = temp[tid];\n",
    "}\n",
    "\n",
    "__global__ void applyEqualization(unsigned char* out, unsigned char* in,\n",
    "                                   float* cdf, float cdfMin,\n",
    "                                   int width, int height) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int numPixels = width * height;\n",
    "    \n",
    "    if (idx < numPixels) {\n",
    "        unsigned char val = in[idx];\n",
    "        float equalized = (cdf[val] - cdfMin) / (1.0f - cdfMin) * 255.0f;\n",
    "        out[idx] = (unsigned char)min(max(equalized, 0.0f), 255.0f);\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// FULL PIPELINE with Streams\n",
    "// =============================================================================\n",
    "void runPipeline(unsigned char* h_rgb, unsigned char* h_output,\n",
    "                 int width, int height) {\n",
    "    printf(\"Running full image processing pipeline...\\n\");\n",
    "    \n",
    "    size_t rgbSize = width * height * 3;\n",
    "    size_t graySize = width * height;\n",
    "    \n",
    "    // Device memory\n",
    "    unsigned char *d_rgb, *d_gray, *d_blurred, *d_edges, *d_equalized;\n",
    "    int *d_histogram;\n",
    "    float *d_cdf;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_rgb, rgbSize));\n",
    "    CHECK_CUDA(cudaMalloc(&d_gray, graySize));\n",
    "    CHECK_CUDA(cudaMalloc(&d_blurred, graySize));\n",
    "    CHECK_CUDA(cudaMalloc(&d_edges, graySize));\n",
    "    CHECK_CUDA(cudaMalloc(&d_equalized, graySize));\n",
    "    CHECK_CUDA(cudaMalloc(&d_histogram, NUM_BINS * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_cdf, NUM_BINS * sizeof(float)));\n",
    "    \n",
    "    // Create stream\n",
    "    cudaStream_t stream;\n",
    "    CHECK_CUDA(cudaStreamCreate(&stream));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    // Copy input\n",
    "    CHECK_CUDA(cudaMemcpyAsync(d_rgb, h_rgb, rgbSize, cudaMemcpyHostToDevice, stream));\n",
    "    CHECK_CUDA(cudaMemsetAsync(d_histogram, 0, NUM_BINS * sizeof(int), stream));\n",
    "    \n",
    "    CHECK_CUDA(cudaEventRecord(start, stream));\n",
    "    \n",
    "    // Stage 1: Grayscale\n",
    "    dim3 block2D(16, 16);\n",
    "    dim3 grid2D((width + 15) / 16, (height + 15) / 16);\n",
    "    rgbToGrayscale<<<grid2D, block2D, 0, stream>>>(d_gray, d_rgb, width, height);\n",
    "    \n",
    "    // Stage 2: Gaussian Blur\n",
    "    dim3 blurGrid((width + BLUR_TILE_SIZE - 1) / BLUR_TILE_SIZE,\n",
    "                  (height + BLUR_TILE_SIZE - 1) / BLUR_TILE_SIZE);\n",
    "    dim3 blurBlock(BLUR_TILE_SIZE + 2, BLUR_TILE_SIZE + 2);\n",
    "    gaussianBlur<<<blurGrid, blurBlock, 0, stream>>>(d_blurred, d_gray, width, height);\n",
    "    \n",
    "    // Stage 3: Edge Detection\n",
    "    sobelEdgeDetection<<<grid2D, block2D, 0, stream>>>(d_edges, d_blurred, width, height);\n",
    "    \n",
    "    // Stage 4: Histogram\n",
    "    int histBlocks = (width * height + 255) / 256;\n",
    "    computeHistogram<<<histBlocks, 256, 0, stream>>>(d_histogram, d_blurred, width, height);\n",
    "    \n",
    "    // Stage 5: CDF and Equalization\n",
    "    computeCDF<<<1, NUM_BINS, 0, stream>>>(d_cdf, d_histogram, width * height);\n",
    "    \n",
    "    // Get cdfMin (simplified - normally you'd find this on GPU)\n",
    "    float h_cdf[NUM_BINS];\n",
    "    CHECK_CUDA(cudaMemcpyAsync(h_cdf, d_cdf, NUM_BINS * sizeof(float), \n",
    "                                cudaMemcpyDeviceToHost, stream));\n",
    "    CHECK_CUDA(cudaStreamSynchronize(stream));\n",
    "    \n",
    "    float cdfMin = 0.0f;\n",
    "    for (int i = 0; i < NUM_BINS; i++) {\n",
    "        if (h_cdf[i] > 0) {\n",
    "            cdfMin = h_cdf[i];\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    int eqBlocks = (width * height + 255) / 256;\n",
    "    applyEqualization<<<eqBlocks, 256, 0, stream>>>(d_equalized, d_blurred, \n",
    "                                                     d_cdf, cdfMin, width, height);\n",
    "    \n",
    "    CHECK_CUDA(cudaEventRecord(stop, stream));\n",
    "    \n",
    "    // Copy results\n",
    "    CHECK_CUDA(cudaMemcpyAsync(h_output, d_edges, graySize, cudaMemcpyDeviceToHost, stream));\n",
    "    CHECK_CUDA(cudaStreamSynchronize(stream));\n",
    "    \n",
    "    float pipelineTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&pipelineTime, start, stop));\n",
    "    \n",
    "    printf(\"Pipeline completed in %.3f ms\\n\", pipelineTime);\n",
    "    printf(\"Throughput: %.2f Mpixels/sec\\n\", \n",
    "           (width * height) / (pipelineTime * 1000.0f));\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaStreamDestroy(stream));\n",
    "    CHECK_CUDA(cudaFree(d_rgb));\n",
    "    CHECK_CUDA(cudaFree(d_gray));\n",
    "    CHECK_CUDA(cudaFree(d_blurred));\n",
    "    CHECK_CUDA(cudaFree(d_edges));\n",
    "    CHECK_CUDA(cudaFree(d_equalized));\n",
    "    CHECK_CUDA(cudaFree(d_histogram));\n",
    "    CHECK_CUDA(cudaFree(d_cdf));\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Test and Verification\n",
    "// =============================================================================\n",
    "void verifyStages() {\n",
    "    printf(\"\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘            Capstone: Image Processing Pipeline               â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    int width = IMG_WIDTH;\n",
    "    int height = IMG_HEIGHT;\n",
    "    \n",
    "    // Create test image (gradient pattern)\n",
    "    unsigned char* h_rgb = (unsigned char*)malloc(width * height * 3);\n",
    "    unsigned char* h_output = (unsigned char*)malloc(width * height);\n",
    "    \n",
    "    for (int y = 0; y < height; y++) {\n",
    "        for (int x = 0; x < width; x++) {\n",
    "            int idx = (y * width + x) * 3;\n",
    "            h_rgb[idx + 0] = (x * 255) / width;      // R gradient\n",
    "            h_rgb[idx + 1] = (y * 255) / height;     // G gradient\n",
    "            h_rgb[idx + 2] = 128;                     // B constant\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"Test image: %dx%d RGB\\n\\n\", width, height);\n",
    "    \n",
    "    // Run individual stage tests\n",
    "    printf(\"=== Testing Individual Stages ===\\n\");\n",
    "    \n",
    "    // Test grayscale\n",
    "    unsigned char *d_rgb, *d_gray;\n",
    "    CHECK_CUDA(cudaMalloc(&d_rgb, width * height * 3));\n",
    "    CHECK_CUDA(cudaMalloc(&d_gray, width * height));\n",
    "    CHECK_CUDA(cudaMemcpy(d_rgb, h_rgb, width * height * 3, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    dim3 block(16, 16);\n",
    "    dim3 grid((width + 15) / 16, (height + 15) / 16);\n",
    "    rgbToGrayscale<<<grid, block>>>(d_gray, d_rgb, width, height);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ Grayscale conversion: OK\\n\");\n",
    "    \n",
    "    // Test blur\n",
    "    unsigned char* d_blurred;\n",
    "    CHECK_CUDA(cudaMalloc(&d_blurred, width * height));\n",
    "    dim3 blurGrid((width + BLUR_TILE_SIZE - 1) / BLUR_TILE_SIZE,\n",
    "                  (height + BLUR_TILE_SIZE - 1) / BLUR_TILE_SIZE);\n",
    "    dim3 blurBlock(BLUR_TILE_SIZE + 2, BLUR_TILE_SIZE + 2);\n",
    "    gaussianBlur<<<blurGrid, blurBlock>>>(d_blurred, d_gray, width, height);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ Gaussian blur: OK\\n\");\n",
    "    \n",
    "    // Test edge detection\n",
    "    unsigned char* d_edges;\n",
    "    CHECK_CUDA(cudaMalloc(&d_edges, width * height));\n",
    "    sobelEdgeDetection<<<grid, block>>>(d_edges, d_blurred, width, height);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ Edge detection: OK\\n\");\n",
    "    \n",
    "    // Test histogram\n",
    "    int* d_histogram;\n",
    "    CHECK_CUDA(cudaMalloc(&d_histogram, NUM_BINS * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_histogram, 0, NUM_BINS * sizeof(int)));\n",
    "    int histBlocks = (width * height + 255) / 256;\n",
    "    computeHistogram<<<histBlocks, 256>>>(d_histogram, d_gray, width, height);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    int h_histogram[NUM_BINS];\n",
    "    CHECK_CUDA(cudaMemcpy(h_histogram, d_histogram, NUM_BINS * sizeof(int), \n",
    "                          cudaMemcpyDeviceToHost));\n",
    "    int totalCount = 0;\n",
    "    for (int i = 0; i < NUM_BINS; i++) totalCount += h_histogram[i];\n",
    "    printf(\"âœ“ Histogram (total count: %d, expected: %d): %s\\n\", \n",
    "           totalCount, width * height, \n",
    "           (totalCount == width * height) ? \"OK\" : \"MISMATCH\");\n",
    "    \n",
    "    cudaFree(d_rgb);\n",
    "    cudaFree(d_gray);\n",
    "    cudaFree(d_blurred);\n",
    "    cudaFree(d_edges);\n",
    "    cudaFree(d_histogram);\n",
    "    \n",
    "    printf(\"\\n=== Running Full Pipeline ===\\n\");\n",
    "    runPipeline(h_rgb, h_output, width, height);\n",
    "    \n",
    "    printf(\"\\n=== Capstone Summary ===\\n\");\n",
    "    printf(\"All stages implemented and tested!\\n\");\n",
    "    printf(\"Next steps:\\n\");\n",
    "    printf(\"  1. Profile with Nsight Compute\\n\");\n",
    "    printf(\"  2. Optimize memory access patterns\\n\");\n",
    "    printf(\"  3. Experiment with different tile sizes\\n\");\n",
    "    printf(\"  4. Add multi-stream overlapping\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    free(h_rgb);\n",
    "    free(h_output);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    verifyStages();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e41d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o capstone_project capstone_project.cu && ./capstone_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934741f",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Capstone Exercises (Optional)\n",
    "\n",
    "For those preferring Python, here's a Numba-based version of the image processing pipeline:\n",
    "\n",
    "```python\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Exercise: Implement the pipeline stages in Numba\n",
    "\n",
    "@cuda.jit\n",
    "def rgb_to_grayscale(gray, rgb, width, height):\n",
    "    \"\"\"Convert RGB image to grayscale\"\"\"\n",
    "    x, y = cuda.grid(2)\n",
    "    if x < width and y < height:\n",
    "        idx = y * width + x\n",
    "        r = rgb[idx, 0]\n",
    "        g = rgb[idx, 1]\n",
    "        b = rgb[idx, 2]\n",
    "        gray[idx] = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "\n",
    "@cuda.jit\n",
    "def gaussian_blur_3x3(out, inp, width, height):\n",
    "    \"\"\"Apply 3x3 Gaussian blur using shared memory\"\"\"\n",
    "    # TODO: Implement with shared memory tile\n",
    "    tile = cuda.shared.array((18, 18), dtype=numba.float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    x = cuda.blockIdx.x * 16 + tx\n",
    "    y = cuda.blockIdx.y * 16 + ty\n",
    "    \n",
    "    # Load tile with halo...\n",
    "    # Apply convolution...\n",
    "\n",
    "@cuda.jit\n",
    "def sobel_edge_detection(out, inp, width, height):\n",
    "    \"\"\"Sobel edge detection\"\"\"\n",
    "    x, y = cuda.grid(2)\n",
    "    if 0 < x < width - 1 and 0 < y < height - 1:\n",
    "        # Compute gradients\n",
    "        gx = (-1 * inp[(y-1) * width + (x-1)] + 1 * inp[(y-1) * width + (x+1)] +\n",
    "              -2 * inp[y * width + (x-1)] + 2 * inp[y * width + (x+1)] +\n",
    "              -1 * inp[(y+1) * width + (x-1)] + 1 * inp[(y+1) * width + (x+1)])\n",
    "        \n",
    "        gy = (-1 * inp[(y-1) * width + (x-1)] - 2 * inp[(y-1) * width + x] - 1 * inp[(y-1) * width + (x+1)] +\n",
    "               1 * inp[(y+1) * width + (x-1)] + 2 * inp[(y+1) * width + x] + 1 * inp[(y+1) * width + (x+1)])\n",
    "        \n",
    "        mag = math.sqrt(gx * gx + gy * gy)\n",
    "        out[y * width + x] = min(mag, 255.0)\n",
    "\n",
    "# Full pipeline\n",
    "def run_pipeline(rgb_image):\n",
    "    width, height = rgb_image.shape[1], rgb_image.shape[0]\n",
    "    \n",
    "    # Allocate device memory\n",
    "    d_rgb = cuda.to_device(rgb_image)\n",
    "    d_gray = cuda.device_array(width * height, dtype=np.float32)\n",
    "    d_edges = cuda.device_array(width * height, dtype=np.float32)\n",
    "    \n",
    "    # Launch kernels\n",
    "    block = (16, 16)\n",
    "    grid = ((width + 15) // 16, (height + 15) // 16)\n",
    "    \n",
    "    rgb_to_grayscale[grid, block](d_gray, d_rgb, width, height)\n",
    "    sobel_edge_detection[grid, block](d_edges, d_gray, width, height)\n",
    "    \n",
    "    return d_edges.copy_to_host()\n",
    "```\n",
    "\n",
    "**Challenge**: Extend with histogram equalization and stream-based overlapping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4a553",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capstone Extensions (Optional Challenges)\n",
    "\n",
    "### Extension 1: Multi-GPU Large Image Processing\n",
    "- Split large images across GPUs\n",
    "- Handle halo exchange for overlapping regions\n",
    "\n",
    "### Extension 2: Real-time Video Filtering\n",
    "- Process video frames in a pipeline\n",
    "- Use streams for frame-level parallelism\n",
    "\n",
    "### Extension 3: Separable Filters\n",
    "- Implement separable convolution (2 1D passes)\n",
    "- Compare performance with 2D convolution\n",
    "\n",
    "### Extension 4: cuDNN Comparison\n",
    "- Compare your implementation with cuDNN\n",
    "- Analyze where cuDNN does better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140e931",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Curriculum Complete!\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                                                                  â•‘\n",
    "â•‘              ğŸ“ CONGRATULATIONS, GPU PROGRAMMER! ğŸ“              â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   You have completed the 12-week CUDA curriculum!               â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   ğŸ† SKILLS MASTERED:                                            â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Foundation (Weeks 1-2):                                        â•‘\n",
    "â•‘   âœ“ GPU Architecture & Programming Model                        â•‘\n",
    "â•‘   âœ“ Thread Indexing & Memory Hierarchies                        â•‘\n",
    "â•‘   âœ“ Coalescing & Error Handling                                  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Core Patterns (Weeks 3-4):                                     â•‘\n",
    "â•‘   âœ“ Shared Memory & Synchronization                              â•‘\n",
    "â•‘   âœ“ Bank Conflict Avoidance                                      â•‘\n",
    "â•‘   âœ“ Reduction & Warp Primitives                                  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Algorithms (Weeks 5-6):                                        â•‘\n",
    "â•‘   âœ“ Parallel Scan (Hillis-Steele, Blelloch)                      â•‘\n",
    "â•‘   âœ“ Matrix Operations & Tiling                                   â•‘\n",
    "â•‘   âœ“ cuBLAS Integration                                           â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Performance (Weeks 7-8):                                       â•‘\n",
    "â•‘   âœ“ Occupancy & Performance Analysis                             â•‘\n",
    "â•‘   âœ“ Profiling with Nsight                                        â•‘\n",
    "â•‘   âœ“ Roofline Analysis                                            â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Concurrency (Weeks 9-10):                                      â•‘\n",
    "â•‘   âœ“ Streams & Asynchronous Execution                             â•‘\n",
    "â•‘   âœ“ CUDA Graphs                                                  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Mastery (Weeks 11-12):                                         â•‘\n",
    "â•‘   âœ“ Cooperative Groups & Dynamic Parallelism                     â•‘\n",
    "â•‘   âœ“ Multi-GPU Programming                                        â•‘\n",
    "â•‘   âœ“ Comprehensive Optimization                                   â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   ğŸš€ NEXT STEPS ON YOUR GPU JOURNEY:                             â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Immediate:                                                     â•‘\n",
    "â•‘   â€¢ Build real projects with GPU acceleration                    â•‘\n",
    "â•‘   â€¢ Profile your existing code for GPU opportunities             â•‘\n",
    "â•‘   â€¢ Contribute to open-source GPU projects                       â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Advanced Topics:                                               â•‘\n",
    "â•‘   â€¢ Tensor Cores and mixed precision (FP16, BF16)                â•‘\n",
    "â•‘   â€¢ CUDA libraries (cuDNN, cuFFT, cuSPARSE)                      â•‘\n",
    "â•‘   â€¢ GPU-accelerated deep learning frameworks                     â•‘\n",
    "â•‘   â€¢ Distributed GPU computing (NCCL, MPI+CUDA)                   â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   Career Development:                                            â•‘\n",
    "â•‘   â€¢ NVIDIA Deep Learning Institute certifications                â•‘\n",
    "â•‘   â€¢ GPU computing competitions (Kaggle, MLPerf)                  â•‘\n",
    "â•‘   â€¢ HPC and scientific computing communities                     â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   ğŸ’¡ REMEMBER THE CORE PRINCIPLES:                               â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   \"Profile first, optimize second, verify always.\"              â•‘\n",
    "â•‘   \"Memory is usually the bottleneck.\"                           â•‘\n",
    "â•‘   \"Enough parallelism hides latency.\"                           â•‘\n",
    "â•‘   \"The best optimization is the right algorithm.\"               â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘   You now have the knowledge to harness thousands of GPU cores.  â•‘\n",
    "â•‘   Use this power to solve problems that matter!                  â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "**Thank you for completing this curriculum! Now go build something amazing.** ğŸš€ğŸ“"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
