{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce799b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c263e8f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capstone Project: GPU-Accelerated Image Convolution\n",
    "\n",
    "### Project Requirements\n",
    "\n",
    "Build a complete image convolution system that:\n",
    "1. Loads images from disk\n",
    "2. Applies various filters (blur, sharpen, edge detect)\n",
    "3. Uses optimized CUDA kernels\n",
    "4. Demonstrates performance gains over CPU\n",
    "5. Uses streams for overlapped processing of multiple images\n",
    "\n",
    "### Skills Demonstrated\n",
    "\n",
    "```\n",
    "âœ“ Week 1-2: Thread indexing, 2D grids\n",
    "âœ“ Week 3-4: Shared memory, memory coalescing\n",
    "âœ“ Week 5:   Scan for histogram normalization\n",
    "âœ“ Week 6:   2D tiled algorithms\n",
    "âœ“ Week 7:   Occupancy and memory optimization\n",
    "âœ“ Week 8:   Profiling and roofline analysis\n",
    "âœ“ Week 9:   Streams for multi-image processing\n",
    "âœ“ Week 10:  CUDA Graphs for repeated filters\n",
    "âœ“ Week 11:  Cooperative groups for reductions\n",
    "âœ“ Week 12:  Multi-GPU for large images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8dc55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Core Convolution Kernel\n",
    "\n",
    "### CUDA C++ Implementation (Primary)\n",
    "\n",
    "```cpp\n",
    "// convolution.cu - Complete image convolution system\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "#define FILTER_RADIUS 2\n",
    "#define FILTER_SIZE (2 * FILTER_RADIUS + 1)\n",
    "\n",
    "// Constant memory for filter coefficients\n",
    "__constant__ float c_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "\n",
    "// ============================================================\n",
    "// Naive Convolution (Baseline)\n",
    "// ============================================================\n",
    "__global__ void convNaive(\n",
    "    float* output, const float* input,\n",
    "    int width, int height\n",
    ") {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x >= width || y >= height) return;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    for (int fy = -FILTER_RADIUS; fy <= FILTER_RADIUS; fy++) {\n",
    "        for (int fx = -FILTER_RADIUS; fx <= FILTER_RADIUS; fx++) {\n",
    "            int ix = min(max(x + fx, 0), width - 1);\n",
    "            int iy = min(max(y + fy, 0), height - 1);\n",
    "            \n",
    "            int fidx = (fy + FILTER_RADIUS) * FILTER_SIZE + \n",
    "                       (fx + FILTER_RADIUS);\n",
    "            \n",
    "            sum += input[iy * width + ix] * c_filter[fidx];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output[y * width + x] = sum;\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Optimized Convolution with Shared Memory\n",
    "// ============================================================\n",
    "__global__ void convShared(\n",
    "    float* output, const float* input,\n",
    "    int width, int height\n",
    ") {\n",
    "    // Shared memory tile with apron\n",
    "    const int TILE_W = BLOCK_SIZE + 2 * FILTER_RADIUS;\n",
    "    __shared__ float smem[TILE_W][TILE_W];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int x = blockIdx.x * BLOCK_SIZE + tx;\n",
    "    int y = blockIdx.y * BLOCK_SIZE + ty;\n",
    "    \n",
    "    // Calculate source positions including apron\n",
    "    int srcX = x - FILTER_RADIUS;\n",
    "    int srcY = y - FILTER_RADIUS;\n",
    "    \n",
    "    // Load main tile element\n",
    "    if (srcX >= 0 && srcX < width && srcY >= 0 && srcY < height) {\n",
    "        smem[ty][tx] = input[srcY * width + srcX];\n",
    "    } else {\n",
    "        smem[ty][tx] = 0.0f;\n",
    "    }\n",
    "    \n",
    "    // Load additional elements for apron (right and bottom edges)\n",
    "    if (tx < 2 * FILTER_RADIUS) {\n",
    "        int ax = srcX + BLOCK_SIZE;\n",
    "        if (ax >= 0 && ax < width && srcY >= 0 && srcY < height) {\n",
    "            smem[ty][tx + BLOCK_SIZE] = input[srcY * width + ax];\n",
    "        } else {\n",
    "            smem[ty][tx + BLOCK_SIZE] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (ty < 2 * FILTER_RADIUS) {\n",
    "        int ay = srcY + BLOCK_SIZE;\n",
    "        if (srcX >= 0 && srcX < width && ay >= 0 && ay < height) {\n",
    "            smem[ty + BLOCK_SIZE][tx] = input[ay * width + srcX];\n",
    "        } else {\n",
    "            smem[ty + BLOCK_SIZE][tx] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (tx < 2 * FILTER_RADIUS && ty < 2 * FILTER_RADIUS) {\n",
    "        int ax = srcX + BLOCK_SIZE;\n",
    "        int ay = srcY + BLOCK_SIZE;\n",
    "        if (ax >= 0 && ax < width && ay >= 0 && ay < height) {\n",
    "            smem[ty + BLOCK_SIZE][tx + BLOCK_SIZE] = input[ay * width + ax];\n",
    "        } else {\n",
    "            smem[ty + BLOCK_SIZE][tx + BLOCK_SIZE] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute convolution from shared memory\n",
    "    if (x < width && y < height) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int fy = 0; fy < FILTER_SIZE; fy++) {\n",
    "            #pragma unroll\n",
    "            for (int fx = 0; fx < FILTER_SIZE; fx++) {\n",
    "                sum += smem[ty + fy][tx + fx] * \n",
    "                       c_filter[fy * FILTER_SIZE + fx];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output[y * width + x] = sum;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b491a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Filter Definitions and Host Code\n",
    "\n",
    "```cpp\n",
    "// ============================================================\n",
    "// Filter Definitions\n",
    "// ============================================================\n",
    "\n",
    "void createGaussianBlur(float* filter) {\n",
    "    float kernel[] = {\n",
    "        1.0f/256, 4.0f/256,  7.0f/256,  4.0f/256, 1.0f/256,\n",
    "        4.0f/256, 16.0f/256, 26.0f/256, 16.0f/256, 4.0f/256,\n",
    "        7.0f/256, 26.0f/256, 41.0f/256, 26.0f/256, 7.0f/256,\n",
    "        4.0f/256, 16.0f/256, 26.0f/256, 16.0f/256, 4.0f/256,\n",
    "        1.0f/256, 4.0f/256,  7.0f/256,  4.0f/256, 1.0f/256\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "void createSharpen(float* filter) {\n",
    "    float kernel[] = {\n",
    "         0,  0, -1,  0,  0,\n",
    "         0, -1, -1, -1,  0,\n",
    "        -1, -1, 13, -1, -1,\n",
    "         0, -1, -1, -1,  0,\n",
    "         0,  0, -1,  0,  0\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "void createEdgeDetect(float* filter) {\n",
    "    float kernel[] = {\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, 24, -1, -1,\n",
    "        -1, -1, -1, -1, -1,\n",
    "        -1, -1, -1, -1, -1\n",
    "    };\n",
    "    memcpy(filter, kernel, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Stream-based Multi-Image Processing\n",
    "// ============================================================\n",
    "\n",
    "void processMultipleImages(\n",
    "    float** h_inputs, float** h_outputs,\n",
    "    int numImages, int width, int height\n",
    ") {\n",
    "    const int NUM_STREAMS = 4;\n",
    "    size_t imageBytes = width * height * sizeof(float);\n",
    "    \n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    float* d_input[NUM_STREAMS];\n",
    "    float* d_output[NUM_STREAMS];\n",
    "    \n",
    "    // Create streams and allocate device memory\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "        cudaMalloc(&d_input[i], imageBytes);\n",
    "        cudaMalloc(&d_output[i], imageBytes);\n",
    "    }\n",
    "    \n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((width + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    // Process images with overlapping\n",
    "    for (int img = 0; img < numImages; img++) {\n",
    "        int streamIdx = img % NUM_STREAMS;\n",
    "        \n",
    "        // H2D\n",
    "        cudaMemcpyAsync(d_input[streamIdx], h_inputs[img],\n",
    "                        imageBytes, cudaMemcpyHostToDevice,\n",
    "                        streams[streamIdx]);\n",
    "        \n",
    "        // Compute\n",
    "        convShared<<<grid, block, 0, streams[streamIdx]>>>(\n",
    "            d_output[streamIdx], d_input[streamIdx], width, height);\n",
    "        \n",
    "        // D2H\n",
    "        cudaMemcpyAsync(h_outputs[img], d_output[streamIdx],\n",
    "                        imageBytes, cudaMemcpyDeviceToHost,\n",
    "                        streams[streamIdx]);\n",
    "    }\n",
    "    \n",
    "    // Sync all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamSynchronize(streams[i]);\n",
    "    }\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        cudaStreamDestroy(streams[i]);\n",
    "        cudaFree(d_input[i]);\n",
    "        cudaFree(d_output[i]);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9faa4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: CUDA Graph for Repeated Filters\n",
    "\n",
    "```cpp\n",
    "// ============================================================\n",
    "// CUDA Graph for Multi-Pass Filtering\n",
    "// ============================================================\n",
    "\n",
    "void multiPassFilter(\n",
    "    float* d_output, float* d_input,\n",
    "    float* d_temp,\n",
    "    int width, int height,\n",
    "    int numPasses\n",
    ") {\n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((width + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    \n",
    "    cudaGraph_t graph;\n",
    "    cudaGraphExec_t instance;\n",
    "    \n",
    "    // Capture graph for one pass\n",
    "    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);\n",
    "    \n",
    "    // Forward pass: input -> temp\n",
    "    convShared<<<grid, block, 0, stream>>>(\n",
    "        d_temp, d_input, width, height);\n",
    "    \n",
    "    // Backward pass: temp -> output\n",
    "    convShared<<<grid, block, 0, stream>>>(\n",
    "        d_output, d_temp, width, height);\n",
    "    \n",
    "    cudaStreamEndCapture(stream, &graph);\n",
    "    cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);\n",
    "    \n",
    "    // Execute graph multiple times\n",
    "    for (int pass = 0; pass < numPasses; pass++) {\n",
    "        cudaGraphLaunch(instance, stream);\n",
    "    }\n",
    "    \n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    cudaGraphExecDestroy(instance);\n",
    "    cudaGraphDestroy(graph);\n",
    "    cudaStreamDestroy(stream);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e7df6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Main Program and Benchmarking\n",
    "\n",
    "```cpp\n",
    "// ============================================================\n",
    "// Main Program\n",
    "// ============================================================\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    const int WIDTH = 4096;\n",
    "    const int HEIGHT = 4096;\n",
    "    size_t imageBytes = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory (pinned for async transfers)\n",
    "    float *h_input, *h_output;\n",
    "    cudaMallocHost(&h_input, imageBytes);\n",
    "    cudaMallocHost(&h_output, imageBytes);\n",
    "    \n",
    "    // Initialize with test pattern\n",
    "    for (int i = 0; i < WIDTH * HEIGHT; i++) {\n",
    "        h_input[i] = (float)(i % 256) / 255.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, imageBytes);\n",
    "    cudaMalloc(&d_output, imageBytes);\n",
    "    \n",
    "    // Create and upload Gaussian blur filter\n",
    "    float h_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "    createGaussianBlur(h_filter);\n",
    "    cudaMemcpyToSymbol(c_filter, h_filter, \n",
    "                       FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "    \n",
    "    // Copy input to device\n",
    "    cudaMemcpy(d_input, h_input, imageBytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Setup kernel launch parameters\n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((WIDTH + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (HEIGHT + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    // Warmup\n",
    "    convShared<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // =========================================\n",
    "    // Benchmark: Naive vs Optimized\n",
    "    // =========================================\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    const int RUNS = 100;\n",
    "    float ms;\n",
    "    \n",
    "    // Naive kernel\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        convNaive<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float naiveTime = ms / RUNS;\n",
    "    printf(\"Naive convolution:     %.2f ms\\n\", naiveTime);\n",
    "    \n",
    "    // Optimized kernel\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        convShared<<<grid, block>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    float optTime = ms / RUNS;\n",
    "    printf(\"Optimized convolution: %.2f ms\\n\", optTime);\n",
    "    printf(\"Speedup: %.2fx\\n\", naiveTime / optTime);\n",
    "    \n",
    "    // Calculate effective bandwidth\n",
    "    float bandwidth = 2.0f * imageBytes / (optTime / 1000.0f) / 1e9;\n",
    "    printf(\"Effective bandwidth:   %.2f GB/s\\n\", bandwidth);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFreeHost(h_input);\n",
    "    cudaFreeHost(h_output);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    printf(\"\\nCapstone project complete!\\n\");\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "// Compile: nvcc -O3 -arch=sm_70 convolution.cu -o convolution\n",
    "// Profile: ncu --set full ./convolution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4a553",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capstone Extensions (Optional Challenges)\n",
    "\n",
    "### Extension 1: Multi-GPU Large Image Processing\n",
    "- Split large images across GPUs\n",
    "- Handle halo exchange for overlapping regions\n",
    "\n",
    "### Extension 2: Real-time Video Filtering\n",
    "- Process video frames in a pipeline\n",
    "- Use streams for frame-level parallelism\n",
    "\n",
    "### Extension 3: Separable Filters\n",
    "- Implement separable convolution (2 1D passes)\n",
    "- Compare performance with 2D convolution\n",
    "\n",
    "### Extension 4: cuDNN Comparison\n",
    "- Compare your implementation with cuDNN\n",
    "- Analyze where cuDNN does better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140e931",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Curriculum Complete!\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                                                              â•‘\n",
    "â•‘          ğŸ“ CONGRATULATIONS! ğŸ“                              â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘   You have completed the 12-week CUDA curriculum!           â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘   Skills Mastered:                                          â•‘\n",
    "â•‘   âœ“ GPU Architecture & Programming Model                    â•‘\n",
    "â•‘   âœ“ Thread Indexing & Memory Hierarchies                    â•‘\n",
    "â•‘   âœ“ Shared Memory & Synchronization                         â•‘\n",
    "â•‘   âœ“ Parallel Patterns (Reduce, Scan, Stencil)               â•‘\n",
    "â•‘   âœ“ Memory Optimization & Coalescing                        â•‘\n",
    "â•‘   âœ“ Occupancy & Performance Analysis                        â•‘\n",
    "â•‘   âœ“ Streams & Asynchronous Execution                        â•‘\n",
    "â•‘   âœ“ CUDA Graphs                                             â•‘\n",
    "â•‘   âœ“ Cooperative Groups & Dynamic Parallelism                â•‘\n",
    "â•‘   âœ“ Multi-GPU Programming                                   â•‘\n",
    "â•‘   âœ“ Profiling with Nsight                                   â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘   Next Steps:                                                â•‘\n",
    "â•‘   â€¢ Build real projects with GPU acceleration               â•‘\n",
    "â•‘   â€¢ Explore CUDA libraries (cuBLAS, cuDNN, Thrust)          â•‘\n",
    "â•‘   â€¢ Learn about tensor cores and mixed precision            â•‘\n",
    "â•‘   â€¢ Contribute to open-source GPU projects                  â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
