{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aecc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43b9e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: cuBLAS Math Modes\n",
    "\n",
    "### Tensor Core Configuration\n",
    "\n",
    "```\n",
    "cuBLAS Math Modes:\n",
    "\n",
    "CUBLAS_DEFAULT_MATH\n",
    "‚îú‚îÄ‚îÄ Uses CUDA cores only (FP32)\n",
    "‚îú‚îÄ‚îÄ Maximum precision\n",
    "‚îî‚îÄ‚îÄ Baseline performance\n",
    "\n",
    "CUBLAS_TENSOR_OP_MATH (deprecated)\n",
    "‚îú‚îÄ‚îÄ Allow Tensor Cores\n",
    "‚îú‚îÄ‚îÄ FP16 compute internally\n",
    "‚îî‚îÄ‚îÄ Higher performance\n",
    "\n",
    "CUBLAS_TF32_TENSOR_OP_MATH (Ampere+)\n",
    "‚îú‚îÄ‚îÄ TF32 for FP32 inputs\n",
    "‚îú‚îÄ‚îÄ 8x FP32 performance\n",
    "‚îî‚îÄ‚îÄ Slight precision loss\n",
    "\n",
    "cublasSetMathMode(handle, mode);\n",
    "```\n",
    "\n",
    "### Compute Types\n",
    "\n",
    "```\n",
    "cublasGemmEx parameters:\n",
    "\n",
    "  computeType = CUBLAS_COMPUTE_16F   // FP16 Tensor Cores\n",
    "  computeType = CUBLAS_COMPUTE_32F   // FP32 (default)\n",
    "  computeType = CUBLAS_COMPUTE_32F_FAST_16F  // TF32\n",
    "  computeType = CUBLAS_COMPUTE_32F_FAST_TF32 // Ampere TF32\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23074d5b",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_tensor.cu\n",
    "// cublas_tensor.cu - cuBLAS with Tensor Cores\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "#define CHECK_CUBLAS(call) { \\\n",
    "    cublasStatus_t status = call; \\\n",
    "    if (status != CUBLAS_STATUS_SUCCESS) { \\\n",
    "        printf(\"cuBLAS error at line %d: %d\\n\", __LINE__, status); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "void benchmarkGemm(\n",
    "    cublasHandle_t handle,\n",
    "    int M, int N, int K,\n",
    "    cublasComputeType_t computeType,\n",
    "    const char* name\n",
    ") {\n",
    "    // Allocate FP32 matrices\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, M * K * sizeof(float));\n",
    "    cudaMalloc(&d_B, K * N * sizeof(float));\n",
    "    cudaMalloc(&d_C, M * N * sizeof(float));\n",
    "    \n",
    "    // Initialize with random values\n",
    "    float *h_A = new float[M * K];\n",
    "    float *h_B = new float[K * N];\n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = (rand() % 100) / 100.0f;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = (rand() % 100) / 100.0f;\n",
    "    cudaMemcpy(d_A, h_A, M * K * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, K * N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    float alpha = 1.0f, beta = 0.0f;\n",
    "    \n",
    "    // Warmup\n",
    "    CHECK_CUBLAS(cublasGemmEx(\n",
    "        handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "        N, M, K,  // Note: cuBLAS is column-major\n",
    "        &alpha,\n",
    "        d_B, CUDA_R_32F, N,\n",
    "        d_A, CUDA_R_32F, K,\n",
    "        &beta,\n",
    "        d_C, CUDA_R_32F, N,\n",
    "        computeType, CUBLAS_GEMM_DEFAULT_TENSOR_OP\n",
    "    ));\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int iterations = 100;\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        cublasGemmEx(\n",
    "            handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "            N, M, K,\n",
    "            &alpha,\n",
    "            d_B, CUDA_R_32F, N,\n",
    "            d_A, CUDA_R_32F, K,\n",
    "            &beta,\n",
    "            d_C, CUDA_R_32F, N,\n",
    "            computeType, CUBLAS_GEMM_DEFAULT_TENSOR_OP\n",
    "        );\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    double flops = 2.0 * M * N * K * iterations;\n",
    "    double tflops = flops / (ms * 1e9);\n",
    "    \n",
    "    printf(\"%s:\\n\", name);\n",
    "    printf(\"  Time: %.2f ms (%d iterations)\\n\", ms, iterations);\n",
    "    printf(\"  Performance: %.2f TFLOPS\\n\\n\", tflops);\n",
    "    \n",
    "    // Cleanup\n",
    "    delete[] h_A;\n",
    "    delete[] h_B;\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== cuBLAS Tensor Core Benchmark ===\\n\\n\");\n",
    "    \n",
    "    cublasHandle_t handle;\n",
    "    CHECK_CUBLAS(cublasCreate(&handle));\n",
    "    \n",
    "    int M = 4096, N = 4096, K = 4096;\n",
    "    printf(\"Matrix size: %d x %d x %d\\n\\n\", M, N, K);\n",
    "    \n",
    "    // Test different compute types\n",
    "    benchmarkGemm(handle, M, N, K, CUBLAS_COMPUTE_32F, \"FP32 (CUDA Cores)\");\n",
    "    benchmarkGemm(handle, M, N, K, CUBLAS_COMPUTE_32F_FAST_16F, \"FP32 with FP16 Tensor Cores\");\n",
    "    \n",
    "    cublasDestroy(handle);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b02a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -lcublas -o cublas_tensor cublas_tensor.cu\n",
    "!./cublas_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea75fd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Half Precision cuBLAS\n",
    "\n",
    "### cublasHgemm\n",
    "\n",
    "```cpp\n",
    "// Native FP16 GEMM\n",
    "cublasHgemm(\n",
    "    handle,\n",
    "    CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "    N, M, K,\n",
    "    &alpha_h,  // half precision scalars\n",
    "    B_h, N,\n",
    "    A_h, K,\n",
    "    &beta_h,\n",
    "    C_h, N\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3a827",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bef928",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_hgemm.cu\n",
    "// cublas_hgemm.cu - Half precision GEMM\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "#define CHECK_CUBLAS(call) { \\\n",
    "    cublasStatus_t status = call; \\\n",
    "    if (status != CUBLAS_STATUS_SUCCESS) { \\\n",
    "        printf(\"cuBLAS error: %d\\n\", status); exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "__global__ void float2half_kernel(float* in, half* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = __float2half(in[idx]);\n",
    "}\n",
    "\n",
    "__global__ void half2float_kernel(half* in, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = __half2float(in[idx]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== cuBLAS Half Precision GEMM ===\\n\\n\");\n",
    "    \n",
    "    cublasHandle_t handle;\n",
    "    CHECK_CUBLAS(cublasCreate(&handle));\n",
    "    \n",
    "    const int M = 4096, N = 4096, K = 4096;\n",
    "    printf(\"Matrix size: %d x %d x %d\\n\\n\", M, N, K);\n",
    "    \n",
    "    // Allocate\n",
    "    float *h_A = new float[M * K];\n",
    "    float *h_B = new float[K * N];\n",
    "    float *h_C = new float[M * N];\n",
    "    \n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = (rand() % 100) / 100.0f;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = (rand() % 100) / 100.0f;\n",
    "    \n",
    "    float *d_A_f, *d_B_f, *d_C_f;\n",
    "    half *d_A_h, *d_B_h, *d_C_h;\n",
    "    \n",
    "    cudaMalloc(&d_A_f, M * K * sizeof(float));\n",
    "    cudaMalloc(&d_B_f, K * N * sizeof(float));\n",
    "    cudaMalloc(&d_C_f, M * N * sizeof(float));\n",
    "    cudaMalloc(&d_A_h, M * K * sizeof(half));\n",
    "    cudaMalloc(&d_B_h, K * N * sizeof(half));\n",
    "    cudaMalloc(&d_C_h, M * N * sizeof(half));\n",
    "    \n",
    "    cudaMemcpy(d_A_f, h_A, M * K * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B_f, h_B, K * N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Convert to half\n",
    "    float2half_kernel<<<(M*K+255)/256, 256>>>(d_A_f, d_A_h, M*K);\n",
    "    float2half_kernel<<<(K*N+255)/256, 256>>>(d_B_f, d_B_h, K*N);\n",
    "    \n",
    "    half alpha_h = __float2half(1.0f);\n",
    "    half beta_h = __float2half(0.0f);\n",
    "    \n",
    "    // Warmup\n",
    "    CHECK_CUBLAS(cublasHgemm(\n",
    "        handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "        N, M, K,\n",
    "        &alpha_h,\n",
    "        d_B_h, N,\n",
    "        d_A_h, K,\n",
    "        &beta_h,\n",
    "        d_C_h, N\n",
    "    ));\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int iterations = 100;\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        cublasHgemm(\n",
    "            handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "            N, M, K,\n",
    "            &alpha_h,\n",
    "            d_B_h, N,\n",
    "            d_A_h, K,\n",
    "            &beta_h,\n",
    "            d_C_h, N\n",
    "        );\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    double flops = 2.0 * M * N * K * iterations;\n",
    "    double tflops = flops / (ms * 1e9);\n",
    "    \n",
    "    printf(\"cublasHgemm (FP16 Tensor Cores):\\n\");\n",
    "    printf(\"  Time: %.2f ms (%d iterations)\\n\", ms, iterations);\n",
    "    printf(\"  Performance: %.2f TFLOPS\\n\\n\", tflops);\n",
    "    \n",
    "    // Memory comparison\n",
    "    printf(\"Memory Usage:\\n\");\n",
    "    printf(\"  FP32: %.1f MB per matrix\\n\", M * K * 4.0 / 1e6);\n",
    "    printf(\"  FP16: %.1f MB per matrix\\n\", M * K * 2.0 / 1e6);\n",
    "    printf(\"  Savings: 50%%\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    delete[] h_A; delete[] h_B; delete[] h_C;\n",
    "    cudaFree(d_A_f); cudaFree(d_B_f); cudaFree(d_C_f);\n",
    "    cudaFree(d_A_h); cudaFree(d_B_h); cudaFree(d_C_h);\n",
    "    cublasDestroy(handle);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f447a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -lcublas -o cublas_hgemm cublas_hgemm.cu\n",
    "!./cublas_hgemm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b0f47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Batched Operations\n",
    "\n",
    "### cublasGemmStridedBatchedEx\n",
    "\n",
    "```\n",
    "Batched GEMM: Multiple matrix multiplies in one call\n",
    "\n",
    "C[i] = alpha * A[i] * B[i] + beta * C[i]\n",
    "\n",
    "Use cases:\n",
    "  - Transformer attention heads\n",
    "  - Batch normalization\n",
    "  - Multi-sample inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ab807",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbda3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_batched.cu\n",
    "// cublas_batched.cu - Batched GEMM for attention\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "#define CHECK_CUBLAS(call) { \\\n",
    "    cublasStatus_t status = call; \\\n",
    "    if (status != CUBLAS_STATUS_SUCCESS) { \\\n",
    "        printf(\"cuBLAS error: %d\\n\", status); exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Batched GEMM for Attention ===\\n\\n\");\n",
    "    \n",
    "    cublasHandle_t handle;\n",
    "    CHECK_CUBLAS(cublasCreate(&handle));\n",
    "    \n",
    "    // Typical attention dimensions\n",
    "    int batch_size = 32;\n",
    "    int num_heads = 8;\n",
    "    int seq_len = 512;\n",
    "    int head_dim = 64;\n",
    "    \n",
    "    // Q * K^T: (batch*heads, seq, head_dim) x (batch*heads, head_dim, seq)\n",
    "    int batch_count = batch_size * num_heads;\n",
    "    int M = seq_len, N = seq_len, K = head_dim;\n",
    "    \n",
    "    printf(\"Attention configuration:\\n\");\n",
    "    printf(\"  Batch size: %d\\n\", batch_size);\n",
    "    printf(\"  Num heads: %d\\n\", num_heads);\n",
    "    printf(\"  Sequence length: %d\\n\", seq_len);\n",
    "    printf(\"  Head dimension: %d\\n\", head_dim);\n",
    "    printf(\"  Total batched GEMMs: %d\\n\\n\", batch_count);\n",
    "    \n",
    "    // Allocate (Q, K, attention_scores)\n",
    "    half *d_Q, *d_K, *d_scores;\n",
    "    cudaMalloc(&d_Q, batch_count * M * K * sizeof(half));\n",
    "    cudaMalloc(&d_K, batch_count * K * N * sizeof(half));\n",
    "    cudaMalloc(&d_scores, batch_count * M * N * sizeof(half));\n",
    "    \n",
    "    // Initialize with dummy data\n",
    "    cudaMemset(d_Q, 0, batch_count * M * K * sizeof(half));\n",
    "    cudaMemset(d_K, 0, batch_count * K * N * sizeof(half));\n",
    "    \n",
    "    half alpha = __float2half(1.0f / sqrtf(head_dim));  // Scale factor\n",
    "    half beta = __float2half(0.0f);\n",
    "    \n",
    "    // Strides for batched GEMM\n",
    "    long long int strideQ = M * K;\n",
    "    long long int strideK = K * N;\n",
    "    long long int strideScores = M * N;\n",
    "    \n",
    "    // Warmup\n",
    "    CHECK_CUBLAS(cublasGemmStridedBatchedEx(\n",
    "        handle,\n",
    "        CUBLAS_OP_T, CUBLAS_OP_N,  // K^T * Q\n",
    "        N, M, K,\n",
    "        &alpha,\n",
    "        d_K, CUDA_R_16F, K, strideK,\n",
    "        d_Q, CUDA_R_16F, K, strideQ,\n",
    "        &beta,\n",
    "        d_scores, CUDA_R_16F, N, strideScores,\n",
    "        batch_count,\n",
    "        CUBLAS_COMPUTE_16F,\n",
    "        CUBLAS_GEMM_DEFAULT_TENSOR_OP\n",
    "    ));\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int iterations = 100;\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        cublasGemmStridedBatchedEx(\n",
    "            handle,\n",
    "            CUBLAS_OP_T, CUBLAS_OP_N,\n",
    "            N, M, K,\n",
    "            &alpha,\n",
    "            d_K, CUDA_R_16F, K, strideK,\n",
    "            d_Q, CUDA_R_16F, K, strideQ,\n",
    "            &beta,\n",
    "            d_scores, CUDA_R_16F, N, strideScores,\n",
    "            batch_count,\n",
    "            CUBLAS_COMPUTE_16F,\n",
    "            CUBLAS_GEMM_DEFAULT_TENSOR_OP\n",
    "        );\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    double flops = 2.0 * M * N * K * batch_count * iterations;\n",
    "    double tflops = flops / (ms * 1e9);\n",
    "    \n",
    "    printf(\"Q*K^T Performance:\\n\");\n",
    "    printf(\"  Time: %.2f ms (%d iterations)\\n\", ms, iterations);\n",
    "    printf(\"  Performance: %.2f TFLOPS\\n\", tflops);\n",
    "    printf(\"  Per-iteration: %.3f ms\\n\", ms / iterations);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_Q);\n",
    "    cudaFree(d_K);\n",
    "    cudaFree(d_scores);\n",
    "    cublasDestroy(handle);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a264b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -lcublas -o cublas_batched cublas_batched.cu\n",
    "!./cublas_batched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e9f80",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b04e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent using CuPy (if available)\n",
    "def explain_cublas_functions():\n",
    "    print(\"cuBLAS Function Reference\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"GEMM Functions:\")\n",
    "    print(\"  cublasSgemm    - Single precision (FP32)\")\n",
    "    print(\"  cublasDgemm    - Double precision (FP64)\")\n",
    "    print(\"  cublasHgemm    - Half precision (FP16)\")\n",
    "    print(\"  cublasGemmEx   - Extended (any type)\")\n",
    "    print()\n",
    "    print(\"Batched GEMM:\")\n",
    "    print(\"  cublasSgemmBatched         - Array of pointers\")\n",
    "    print(\"  cublasSgemmStridedBatched  - Strided memory\")\n",
    "    print(\"  cublasGemmStridedBatchedEx - Extended batched\")\n",
    "    print()\n",
    "    print(\"Tensor Core Compute Types:\")\n",
    "    print(\"  CUBLAS_COMPUTE_16F         - FP16\")\n",
    "    print(\"  CUBLAS_COMPUTE_32F         - FP32\")\n",
    "    print(\"  CUBLAS_COMPUTE_32F_FAST_16F - FP16 Tensor Cores\")\n",
    "    print(\"  CUBLAS_COMPUTE_32F_FAST_TF32 - TF32 (Ampere+)\")\n",
    "\n",
    "explain_cublas_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd3c28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_tensor_exercises.cu\n",
    "// CUDA C++ cuBLAS Tensor Core Exercises\n",
    "// Week 13, Day 4: cuBLAS with Tensor Cores\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Exercise 1: Basic cublasGemmEx with Tensor Cores\n",
    "// TODO: Set up cuBLAS handle and enable Tensor Core math mode\n",
    "\n",
    "// Exercise 2: Batched GEMM for transformer attention\n",
    "// TODO: Use cublasGemmBatchedEx for Q*K^T and attention*V\n",
    "\n",
    "// Exercise 3: Compare Tensor Core vs standard GEMM performance\n",
    "// TODO: Benchmark CUBLAS_TENSOR_OP_MATH vs CUBLAS_DEFAULT_MATH\n",
    "\n",
    "int main() {\n",
    "    printf(\"cuBLAS Tensor Core Exercises - Implement your solutions above\\n\");\n",
    "    printf(\"Compile with: nvcc -arch=sm_70 cublas_tensor_exercises.cu -lcublas -o cublas_tensor_exercises\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 cublas_tensor_exercises.cu -lcublas -o cublas_tensor_exercises && ./cublas_tensor_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8200d11d",
   "metadata": {},
   "source": [
    "### üî∂ Python/CuPy Exercises (Alternative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a128950",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### cuBLAS Tensor Core Usage\n",
    "\n",
    "```cpp\n",
    "// Enable Tensor Cores\n",
    "cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH);\n",
    "\n",
    "// Or use GemmEx with appropriate compute type\n",
    "cublasGemmEx(..., CUBLAS_COMPUTE_32F_FAST_16F, ...);\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. Use `cublasGemmEx` for flexibility\n",
    "2. Choose compute type based on accuracy needs\n",
    "3. Use batched GEMM for attention/transformers\n",
    "4. Ensure matrix dimensions are Tensor Core friendly\n",
    "\n",
    "### Week 13 Complete!\n",
    "You've learned Tensor Cores, WMMA, mixed precision, and cuBLAS integration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
