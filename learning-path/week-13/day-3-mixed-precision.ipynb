{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5264c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310b513",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Mixed Precision Fundamentals\n",
    "\n",
    "### Why Mixed Precision?\n",
    "\n",
    "```\n",
    "Full FP32 Training:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Weights: FP32    Gradients: FP32       â”‚\n",
    "â”‚  Activations: FP32   Loss: FP32         â”‚\n",
    "â”‚  Memory: 100%    Speed: 1x              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Mixed Precision Training:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Master Weights: FP32 (kept for update) â”‚\n",
    "â”‚  Forward/Backward: FP16 (fast compute)  â”‚\n",
    "â”‚  Loss Scaling: prevent underflow        â”‚\n",
    "â”‚  Memory: ~50%    Speed: 2-8x            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Problem: Gradient Underflow\n",
    "\n",
    "```\n",
    "FP16 representable range: ~6e-8 to 65504\n",
    "\n",
    "Small gradients (common in deep networks):\n",
    "  gradient = 1e-7  â†’  FP16 = 0 (underflow!)\n",
    "\n",
    "Solution: Loss Scaling\n",
    "  1. Scale loss by large factor (e.g., 1024)\n",
    "  2. Gradients also scale by 1024\n",
    "  3. Unscale before weight update\n",
    "  \n",
    "  gradient = 1e-7 * 1024 = 1.024e-4  âœ“ Representable!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4038ba4",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c520b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mixed_precision.cu\n",
    "// mixed_precision.cu - Mixed precision training primitives\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <curand_kernel.h>\n",
    "\n",
    "// Convert FP32 weights to FP16 for forward pass\n",
    "__global__ void fp32_to_fp16(float* fp32, half* fp16, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        fp16[idx] = __float2half(fp32[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Convert FP16 gradients to FP32 and unscale\n",
    "__global__ void fp16_to_fp32_unscale(\n",
    "    half* fp16_grad, float* fp32_grad, float scale, int n\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        fp32_grad[idx] = __half2float(fp16_grad[idx]) / scale;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Check for inf/nan in gradients (for dynamic loss scaling)\n",
    "__global__ void check_overflow(half* grads, int* has_overflow, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = __half2float(grads[idx]);\n",
    "        if (isinf(val) || isnan(val)) {\n",
    "            atomicExch(has_overflow, 1);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// FP16 forward pass (example: linear layer + ReLU)\n",
    "__global__ void linear_relu_fp16(\n",
    "    half* input, half* weight, half* bias, half* output,\n",
    "    int batch, int in_features, int out_features\n",
    ") {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;  // batch\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;  // out_features\n",
    "    \n",
    "    if (row < batch && col < out_features) {\n",
    "        float sum = __half2float(bias[col]);\n",
    "        \n",
    "        for (int k = 0; k < in_features; k++) {\n",
    "            sum += __half2float(input[row * in_features + k]) *\n",
    "                   __half2float(weight[k * out_features + col]);\n",
    "        }\n",
    "        \n",
    "        // ReLU\n",
    "        sum = fmaxf(sum, 0.0f);\n",
    "        output[row * out_features + col] = __float2half(sum);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Scale loss for FP16 backward pass\n",
    "__global__ void scale_loss(half* loss, half* scaled_loss, float scale, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = __half2float(loss[idx]) * scale;\n",
    "        scaled_loss[idx] = __float2half(val);\n",
    "    }\n",
    "}\n",
    "\n",
    "// SGD update with unscaled gradients\n",
    "__global__ void sgd_update(\n",
    "    float* weights, float* gradients, float lr, int n\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        weights[idx] -= lr * gradients[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Mixed Precision Training Primitives ===\\n\\n\");\n",
    "    \n",
    "    // Demonstrate loss scaling effect\n",
    "    float small_gradient = 1e-7f;\n",
    "    half fp16_direct = __float2half(small_gradient);\n",
    "    \n",
    "    float scale = 1024.0f;\n",
    "    float scaled_gradient = small_gradient * scale;\n",
    "    half fp16_scaled = __float2half(scaled_gradient);\n",
    "    float unscaled = __half2float(fp16_scaled) / scale;\n",
    "    \n",
    "    printf(\"Gradient underflow example:\\n\");\n",
    "    printf(\"  Original gradient: %e\\n\", small_gradient);\n",
    "    printf(\"  Direct FP16:       %e (underflow!)\\n\", __half2float(fp16_direct));\n",
    "    printf(\"  Scaled (x1024):    %e\\n\", scaled_gradient);\n",
    "    printf(\"  After unscale:     %e (preserved!)\\n\\n\", unscaled);\n",
    "    \n",
    "    // Memory savings\n",
    "    int params = 100000000;  // 100M parameters\n",
    "    printf(\"Memory savings for %dM parameters:\\n\", params/1000000);\n",
    "    printf(\"  FP32 only:      %.1f GB\\n\", params * 4.0 / 1e9);\n",
    "    printf(\"  Mixed (FP16):   %.1f GB\\n\", params * 2.0 / 1e9);\n",
    "    printf(\"  Savings:        %.1f%%\\n\", 50.0);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf580a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o mixed_precision mixed_precision.cu\n",
    "!./mixed_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11931e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Dynamic Loss Scaling\n",
    "\n",
    "### Automatic Scale Adjustment\n",
    "\n",
    "```\n",
    "Dynamic Loss Scaling Algorithm:\n",
    "\n",
    "1. Start with large scale (e.g., 65536)\n",
    "2. For each training step:\n",
    "   a. Scale loss\n",
    "   b. Backward pass in FP16\n",
    "   c. Check gradients for inf/nan\n",
    "   d. If overflow:\n",
    "      - Skip weight update\n",
    "      - Reduce scale by factor (e.g., /2)\n",
    "   e. If no overflow:\n",
    "      - Unscale gradients\n",
    "      - Update weights\n",
    "      - Optionally increase scale\n",
    "\n",
    "Benefits:\n",
    "  - Automatically finds optimal scale\n",
    "  - Handles varying gradient magnitudes\n",
    "  - Robust across different models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d121d06",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_scaling.cu\n",
    "// dynamic_scaling.cu - Dynamic loss scaling\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "class DynamicLossScaler {\n",
    "public:\n",
    "    float scale;\n",
    "    float scale_factor;\n",
    "    int scale_window;\n",
    "    int steps_since_increase;\n",
    "    \n",
    "    DynamicLossScaler(\n",
    "        float init_scale = 65536.0f,\n",
    "        float factor = 2.0f,\n",
    "        int window = 2000\n",
    "    ) : scale(init_scale), scale_factor(factor), \n",
    "        scale_window(window), steps_since_increase(0) {}\n",
    "    \n",
    "    void update(bool overflow) {\n",
    "        if (overflow) {\n",
    "            // Reduce scale\n",
    "            scale /= scale_factor;\n",
    "            steps_since_increase = 0;\n",
    "            printf(\"  Overflow detected! Scale reduced to %.1f\\n\", scale);\n",
    "        } else {\n",
    "            steps_since_increase++;\n",
    "            if (steps_since_increase >= scale_window) {\n",
    "                // Try increasing scale\n",
    "                scale *= scale_factor;\n",
    "                steps_since_increase = 0;\n",
    "                printf(\"  Increasing scale to %.1f\\n\", scale);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "};\n",
    "\n",
    "// Check for overflow in gradients\n",
    "__global__ void check_grads(half* grads, int* overflow_flag, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = __half2float(grads[idx]);\n",
    "        if (isinf(val) || isnan(val)) {\n",
    "            atomicExch(overflow_flag, 1);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Simulate training step with scaled gradients\n",
    "__global__ void simulate_backward(\n",
    "    half* grad_out, float scale, int n, float base_grad\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float grad = base_grad * scale;  // Scaled gradient\n",
    "        grad_out[idx] = __float2half(grad);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Dynamic Loss Scaling Demo ===\\n\\n\");\n",
    "    \n",
    "    const int n = 1000;\n",
    "    DynamicLossScaler scaler;\n",
    "    \n",
    "    half* d_grads;\n",
    "    int* d_overflow;\n",
    "    int h_overflow;\n",
    "    \n",
    "    cudaMalloc(&d_grads, n * sizeof(half));\n",
    "    cudaMalloc(&d_overflow, sizeof(int));\n",
    "    \n",
    "    // Simulate training steps with different gradient magnitudes\n",
    "    float gradient_sizes[] = {1e-4f, 1e-5f, 1e-6f, 1e-7f, 1.0f, 100.0f};\n",
    "    int num_steps = sizeof(gradient_sizes) / sizeof(float);\n",
    "    \n",
    "    for (int step = 0; step < num_steps; step++) {\n",
    "        float base_grad = gradient_sizes[step];\n",
    "        printf(\"Step %d: Base gradient = %e, Scale = %.1f\\n\", \n",
    "               step, base_grad, scaler.scale);\n",
    "        \n",
    "        // Simulate backward with current scale\n",
    "        cudaMemset(d_overflow, 0, sizeof(int));\n",
    "        simulate_backward<<<(n+255)/256, 256>>>(d_grads, scaler.scale, n, base_grad);\n",
    "        check_grads<<<(n+255)/256, 256>>>(d_grads, d_overflow, n);\n",
    "        \n",
    "        cudaMemcpy(&h_overflow, d_overflow, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "        \n",
    "        scaler.update(h_overflow == 1);\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_grads);\n",
    "    cudaFree(d_overflow);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02beceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o dynamic_scaling dynamic_scaling.cu\n",
    "!./dynamic_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755b318",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Operations That Need FP32\n",
    "\n",
    "### Numerically Sensitive Operations\n",
    "\n",
    "```\n",
    "Keep in FP32:\n",
    "â”œâ”€â”€ Softmax (large exponentials)\n",
    "â”œâ”€â”€ LayerNorm / BatchNorm (variance computation)\n",
    "â”œâ”€â”€ Loss functions (cross-entropy, MSE)\n",
    "â”œâ”€â”€ Weight updates (accumulation)\n",
    "â””â”€â”€ Reduction operations (sum, mean)\n",
    "\n",
    "Can use FP16:\n",
    "â”œâ”€â”€ Linear layers (matmul) â† Tensor Cores!\n",
    "â”œâ”€â”€ Convolutions â† Tensor Cores!\n",
    "â”œâ”€â”€ Element-wise operations (ReLU, GELU)\n",
    "â””â”€â”€ Pooling\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b320649",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a08315",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile safe_ops.cu\n",
    "// safe_ops.cu - FP32-safe operations\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <float.h>\n",
    "\n",
    "// Softmax: Must use FP32 internally to avoid overflow\n",
    "__global__ void softmax_fp16_safe(\n",
    "    half* input, half* output, int batch, int dim\n",
    ") {\n",
    "    int b = blockIdx.x;\n",
    "    if (b >= batch) return;\n",
    "    \n",
    "    // Find max (for numerical stability)\n",
    "    float max_val = -FLT_MAX;\n",
    "    for (int i = threadIdx.x; i < dim; i += blockDim.x) {\n",
    "        float val = __half2float(input[b * dim + i]);\n",
    "        max_val = fmaxf(max_val, val);\n",
    "    }\n",
    "    \n",
    "    // Warp reduction for max\n",
    "    for (int offset = 16; offset > 0; offset >>= 1) {\n",
    "        max_val = fmaxf(max_val, __shfl_down_sync(0xffffffff, max_val, offset));\n",
    "    }\n",
    "    max_val = __shfl_sync(0xffffffff, max_val, 0);\n",
    "    \n",
    "    // Compute exp and sum (in FP32!)\n",
    "    float sum = 0.0f;\n",
    "    for (int i = threadIdx.x; i < dim; i += blockDim.x) {\n",
    "        float val = __half2float(input[b * dim + i]);\n",
    "        sum += expf(val - max_val);  // FP32 exp\n",
    "    }\n",
    "    \n",
    "    // Warp reduction for sum\n",
    "    for (int offset = 16; offset > 0; offset >>= 1) {\n",
    "        sum += __shfl_down_sync(0xffffffff, sum, offset);\n",
    "    }\n",
    "    sum = __shfl_sync(0xffffffff, sum, 0);\n",
    "    \n",
    "    // Output (can be FP16)\n",
    "    for (int i = threadIdx.x; i < dim; i += blockDim.x) {\n",
    "        float val = __half2float(input[b * dim + i]);\n",
    "        float softmax = expf(val - max_val) / sum;\n",
    "        output[b * dim + i] = __float2half(softmax);\n",
    "    }\n",
    "}\n",
    "\n",
    "// LayerNorm: Variance needs FP32 precision\n",
    "__global__ void layernorm_fp16_safe(\n",
    "    half* input, half* output, half* gamma, half* beta,\n",
    "    int batch, int dim, float eps\n",
    ") {\n",
    "    int b = blockIdx.x;\n",
    "    if (b >= batch) return;\n",
    "    \n",
    "    // Compute mean (FP32)\n",
    "    float sum = 0.0f;\n",
    "    for (int i = threadIdx.x; i < dim; i += blockDim.x) {\n",
    "        sum += __half2float(input[b * dim + i]);\n",
    "    }\n",
    "    for (int offset = 16; offset > 0; offset >>= 1) {\n",
    "        sum += __shfl_down_sync(0xffffffff, sum, offset);\n",
    "    }\n",
    "    float mean = __shfl_sync(0xffffffff, sum, 0) / dim;\n",
    "    \n",
    "    // Compute variance (FP32)\n",
    "    float var_sum = 0.0f;\n",
    "    for (int i = threadIdx.x; i < dim; i += blockDim.x) {\n",
    "        float val = __half2float(input[b * dim + i]) - mean;\n",
    "        var_sum += val * val;\n",
    "    }\n",
    "    for (int offset = 16; offset > 0; offset >>= 1) {\n",
    "        var_sum += __shfl_down_sync(0xffffffff, var_sum, offset);\n",
    "    }\n",
    "    float var = __shfl_sync(0xffffffff, var_sum, 0) / dim;\n",
    "    float rstd = rsqrtf(var + eps);  // FP32 rsqrt\n",
    "    \n",
    "    // Normalize and output (can be FP16)\n",
    "    for (int i = threadIdx.x; i < dim; i += blockDim.x) {\n",
    "        float val = __half2float(input[b * dim + i]);\n",
    "        float norm = (val - mean) * rstd;\n",
    "        float g = __half2float(gamma[i]);\n",
    "        float b_val = __half2float(beta[i]);\n",
    "        output[b * dim + i] = __float2half(norm * g + b_val);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== FP32-Safe Operations for Mixed Precision ===\\n\\n\");\n",
    "    \n",
    "    printf(\"Operations requiring FP32 internally:\\n\");\n",
    "    printf(\"  âœ“ Softmax (exp can overflow)\\n\");\n",
    "    printf(\"  âœ“ LayerNorm (variance precision)\\n\");\n",
    "    printf(\"  âœ“ BatchNorm (statistics)\\n\");\n",
    "    printf(\"  âœ“ Cross-entropy loss\\n\");\n",
    "    printf(\"  âœ“ Gradient accumulation\\n\");\n",
    "    printf(\"\\n\");\n",
    "    printf(\"Safe for FP16 throughout:\\n\");\n",
    "    printf(\"  âœ“ Matrix multiply (Tensor Cores)\\n\");\n",
    "    printf(\"  âœ“ Convolution (Tensor Cores)\\n\");\n",
    "    printf(\"  âœ“ ReLU, GELU activations\\n\");\n",
    "    printf(\"  âœ“ Pooling\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9505063",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o safe_ops safe_ops.cu\n",
    "!./safe_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ab872",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25405d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python demonstration: precision issues in softmax\n",
    "import numpy as np\n",
    "\n",
    "def softmax_precision_demo():\n",
    "    \"\"\"Show why softmax needs FP32.\"\"\"\n",
    "    print(\"Softmax Precision Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Large values that cause overflow in FP16\n",
    "    x_fp32 = np.array([1000.0, 1001.0, 1002.0], dtype=np.float32)\n",
    "    x_fp16 = x_fp32.astype(np.float16)\n",
    "    \n",
    "    print(f\"\\nInput values: {x_fp32}\")\n",
    "    \n",
    "    # FP16 exp overflow\n",
    "    print(f\"\\nFP16 exp results:\")\n",
    "    print(f\"  exp(1000) in FP16: {np.exp(np.float16(1000.0))}\")\n",
    "    \n",
    "    # Safe softmax with max subtraction\n",
    "    x_shifted = x_fp32 - x_fp32.max()\n",
    "    print(f\"\\nAfter subtracting max: {x_shifted}\")\n",
    "    print(f\"  exp values: {np.exp(x_shifted)}\")\n",
    "    print(f\"  softmax: {np.exp(x_shifted) / np.exp(x_shifted).sum()}\")\n",
    "\n",
    "softmax_precision_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42894fb8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5735158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mixed_precision_exercises.cu\n",
    "// CUDA C++ Mixed Precision Exercises\n",
    "// Week 13, Day 3: Mixed Precision Training\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <mma.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "// Exercise 1: FP32 to FP16 conversion kernel\n",
    "// TODO: Implement efficient conversion with proper rounding\n",
    "\n",
    "// Exercise 2: Loss scaling implementation\n",
    "// TODO: Scale gradients to prevent underflow\n",
    "\n",
    "// Exercise 3: Mixed precision GEMM\n",
    "// TODO: FP16 inputs, FP32 accumulation, FP16 output\n",
    "\n",
    "int main() {\n",
    "    printf(\"Mixed Precision Exercises - Implement your solutions above\\n\");\n",
    "    printf(\"Compile with: nvcc -arch=sm_70 mixed_precision_exercises.cu -o mixed_precision_exercises\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91eabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 mixed_precision_exercises.cu -o mixed_precision_exercises && ./mixed_precision_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49def027",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/CuPy Exercises (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbfa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise plan\n",
    "def mixed_precision_gemm_plan():\n",
    "    print(\"Mixed Precision GEMM Implementation\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"Forward Pass:\")\n",
    "    print(\"  1. Convert FP32 weights to FP16\")\n",
    "    print(\"  2. WMMA matmul with FP16 inputs\")\n",
    "    print(\"  3. Accumulate in FP32\")\n",
    "    print(\"  4. Output in FP16 for next layer\")\n",
    "    print()\n",
    "    print(\"Backward Pass:\")\n",
    "    print(\"  1. Scale loss\")\n",
    "    print(\"  2. Compute gradients in FP16\")\n",
    "    print(\"  3. Check for overflow\")\n",
    "    print(\"  4. Unscale gradients\")\n",
    "    print(\"  5. Update FP32 master weights\")\n",
    "\n",
    "mixed_precision_gemm_plan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd325a20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Mixed Precision Recipe\n",
    "\n",
    "```\n",
    "1. Master weights in FP32\n",
    "2. Forward/backward in FP16 (Tensor Cores)\n",
    "3. Loss scaling to prevent underflow\n",
    "4. FP32 for sensitive operations\n",
    "5. Dynamic scaling for robustness\n",
    "```\n",
    "\n",
    "### Performance Guidelines\n",
    "\n",
    "| Technique | Memory Savings | Speedup |\n",
    "|-----------|---------------|--------|\n",
    "| Pure FP32 | 0% | 1x |\n",
    "| FP16 compute only | ~20% | 2-4x |\n",
    "| Full mixed precision | ~50% | 2-8x |\n",
    "\n",
    "### Tomorrow: cuBLAS with Tensor Cores\n",
    "Use library-level Tensor Core acceleration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
