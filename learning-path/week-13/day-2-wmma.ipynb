{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487d71a",
   "metadata": {},
   "source": [
    "# Day 2: WMMA - The Matrix Magic Boxes\n",
    "\n",
    "> **\"Imagine having magic boxes that can swallow matrix tiles and instantly output results. That's WMMA fragments!\"**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "1. **Master** WMMA fragment types and their roles (matrix_a, matrix_b, accumulator)\n",
    "2. **Implement** tiled matrix multiply with shared memory\n",
    "3. **Optimize** memory access patterns for Tensor Core efficiency\n",
    "4. **Handle** matrix padding and layout considerations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¨ Concept Card: Matrix Magic Boxes\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“¦ WMMA = MATRIX MAGIC BOXES                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Three Types of Magic Boxes:                                    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚ ğŸ“¥ Box A    â”‚  â”‚ ğŸ“¥ Box B    â”‚  â”‚ ğŸ“¤ Box C    â”‚              â”‚\n",
    "â”‚  â”‚ (matrix_a)  â”‚  â”‚ (matrix_b)  â”‚  â”‚ (accumulator)â”‚              â”‚\n",
    "â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚              â”‚\n",
    "â”‚  â”‚ Left input  â”‚  â”‚ Right input â”‚  â”‚ Result box  â”‚              â”‚\n",
    "â”‚  â”‚ 16Ã—16 FP16  â”‚  â”‚ 16Ã—16 FP16  â”‚  â”‚ 16Ã—16 FP32  â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚        â”‚                â”‚                â–²                      â”‚\n",
    "â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚                         â”‚                                       â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                  â”‚\n",
    "â”‚                    â”‚ MMA_SYNC â”‚  â† The Magic Spell!             â”‚\n",
    "â”‚                    â”‚ D=AÃ—B+C â”‚                                  â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  The Magic Process:                                             â”‚\n",
    "â”‚  1. load_matrix_sync  â†’ Pour ingredients into boxes             â”‚\n",
    "â”‚  2. fill_fragment     â†’ Pre-fill accumulator (usually 0)        â”‚\n",
    "â”‚  3. mma_sync          â†’ Cast the multiplication spell           â”‚\n",
    "â”‚  4. store_matrix_sync â†’ Extract the magical result              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ’¡ Key Insight:                                                â”‚\n",
    "â”‚  â€¢ Fragments are DISTRIBUTED across 32 threads in a warp        â”‚\n",
    "â”‚  â€¢ Each thread holds a piece - like 32 magicians sharing work!  â”‚\n",
    "â”‚  â€¢ Sync operations ensure all pieces move together              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: WMMA API Deep Dive\n",
    "\n",
    "### Fragment Types and Layouts\n",
    "\n",
    "```\n",
    "WMMA operates on matrix \"fragments\" distributed across warp:\n",
    "\n",
    "fragment<matrix_a, M, N, K, type, layout>  // Left operand\n",
    "fragment<matrix_b, M, N, K, type, layout>  // Right operand  \n",
    "fragment<accumulator, M, N, K, acc_type>   // Result/accumulator\n",
    "\n",
    "Layouts:\n",
    "  row_major: Elements in row-contiguous order\n",
    "  col_major: Elements in column-contiguous order\n",
    "\n",
    "Shapes supported (MÃ—NÃ—K):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Shape    â”‚  Input  â”‚  Accum   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  16Ã—16Ã—16  â”‚  FP16   â”‚  FP32    â”‚\n",
    "â”‚  32Ã—8Ã—16   â”‚  FP16   â”‚  FP32    â”‚\n",
    "â”‚  8Ã—32Ã—16   â”‚  FP16   â”‚  FP32    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### WMMA Operations\n",
    "\n",
    "```cpp\n",
    "// Load from memory into fragment\n",
    "wmma::load_matrix_sync(frag, ptr, stride);  \n",
    "\n",
    "// Initialize accumulator\n",
    "wmma::fill_fragment(frag, value);\n",
    "\n",
    "// Matrix multiply-accumulate: D = A Ã— B + C\n",
    "wmma::mma_sync(d_frag, a_frag, b_frag, c_frag);\n",
    "\n",
    "// Store fragment to memory\n",
    "wmma::store_matrix_sync(ptr, frag, stride, layout);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716a3fe",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8582f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wmma_tiled.cu\n",
    "// wmma_tiled.cu - Optimized WMMA with shared memory tiling\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <mma.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "// WMMA tile dimensions\n",
    "const int WMMA_M = 16;\n",
    "const int WMMA_N = 16;\n",
    "const int WMMA_K = 16;\n",
    "\n",
    "// Block tile dimensions (multiple WMMA tiles per block)\n",
    "const int BLOCK_ROW_TILES = 4;  // 4 WMMA tiles in M direction\n",
    "const int BLOCK_COL_TILES = 4;  // 4 WMMA tiles in N direction\n",
    "const int BLOCK_K_TILES = 2;    // 2 WMMA tiles in K direction per step\n",
    "\n",
    "const int BLOCK_M = BLOCK_ROW_TILES * WMMA_M;  // 64\n",
    "const int BLOCK_N = BLOCK_COL_TILES * WMMA_N;  // 64\n",
    "const int BLOCK_K = BLOCK_K_TILES * WMMA_K;    // 32\n",
    "\n",
    "__global__ void wmma_matmul_tiled(\n",
    "    half* __restrict__ A,\n",
    "    half* __restrict__ B, \n",
    "    float* __restrict__ C,\n",
    "    int M, int N, int K\n",
    ") {\n",
    "    // Shared memory for A and B tiles\n",
    "    __shared__ half sA[BLOCK_M][BLOCK_K];\n",
    "    __shared__ half sB[BLOCK_K][BLOCK_N];\n",
    "    \n",
    "    // Warp and lane indices\n",
    "    int warpId = threadIdx.x / 32;\n",
    "    int laneId = threadIdx.x % 32;\n",
    "    \n",
    "    // Each block has BLOCK_ROW_TILES * BLOCK_COL_TILES warps\n",
    "    // Map warp to its WMMA tile within the block\n",
    "    int warpRow = warpId / BLOCK_COL_TILES;\n",
    "    int warpCol = warpId % BLOCK_COL_TILES;\n",
    "    \n",
    "    // Block position in output matrix\n",
    "    int blockRow = blockIdx.y * BLOCK_M;\n",
    "    int blockCol = blockIdx.x * BLOCK_N;\n",
    "    \n",
    "    // Declare accumulator fragments (one per warp)\n",
    "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;\n",
    "    wmma::fill_fragment(c_frag, 0.0f);\n",
    "    \n",
    "    // Loop over K dimension\n",
    "    for (int k = 0; k < K; k += BLOCK_K) {\n",
    "        // Collaborative loading of A tile into shared memory\n",
    "        // Each thread loads multiple elements\n",
    "        for (int i = threadIdx.x; i < BLOCK_M * BLOCK_K; i += blockDim.x) {\n",
    "            int row = i / BLOCK_K;\n",
    "            int col = i % BLOCK_K;\n",
    "            int gRow = blockRow + row;\n",
    "            int gCol = k + col;\n",
    "            sA[row][col] = (gRow < M && gCol < K) ? A[gRow * K + gCol] : __float2half(0.0f);\n",
    "        }\n",
    "        \n",
    "        // Collaborative loading of B tile into shared memory\n",
    "        for (int i = threadIdx.x; i < BLOCK_K * BLOCK_N; i += blockDim.x) {\n",
    "            int row = i / BLOCK_N;\n",
    "            int col = i % BLOCK_N;\n",
    "            int gRow = k + row;\n",
    "            int gCol = blockCol + col;\n",
    "            sB[row][col] = (gRow < K && gCol < N) ? B[gRow * N + gCol] : __float2half(0.0f);\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        // Each warp computes its WMMA tiles\n",
    "        #pragma unroll\n",
    "        for (int kTile = 0; kTile < BLOCK_K_TILES; kTile++) {\n",
    "            wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n",
    "            wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b_frag;\n",
    "            \n",
    "            // Load from shared memory\n",
    "            wmma::load_matrix_sync(a_frag, &sA[warpRow * WMMA_M][kTile * WMMA_K], BLOCK_K);\n",
    "            wmma::load_matrix_sync(b_frag, &sB[kTile * WMMA_K][warpCol * WMMA_N], BLOCK_N);\n",
    "            \n",
    "            // Multiply-accumulate\n",
    "            wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Store result\n",
    "    int cRow = blockRow + warpRow * WMMA_M;\n",
    "    int cCol = blockCol + warpCol * WMMA_N;\n",
    "    if (cRow < M && cCol < N) {\n",
    "        wmma::store_matrix_sync(&C[cRow * N + cCol], c_frag, N, wmma::mem_row_major);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Standard CUDA matmul for comparison\n",
    "__global__ void cuda_matmul(float* A, float* B, float* C, int M, int N, int K) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            sum += A[row * K + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void float2half_kernel(float* in, half* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = __float2half(in[idx]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int M = 1024, N = 1024, K = 1024;\n",
    "    \n",
    "    printf(\"=== WMMA Tiled Matrix Multiply ===\\n\");\n",
    "    printf(\"Matrix size: %d x %d x %d\\n\\n\", M, N, K);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_A = new float[M * K];\n",
    "    float *h_B = new float[K * N];\n",
    "    float *h_C_wmma = new float[M * N];\n",
    "    float *h_C_cuda = new float[M * N];\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = (rand() % 10) / 10.0f;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = (rand() % 10) / 10.0f;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_A, *d_B, *d_C_wmma, *d_C_cuda;\n",
    "    half *d_A_half, *d_B_half;\n",
    "    \n",
    "    cudaMalloc(&d_A, M * K * sizeof(float));\n",
    "    cudaMalloc(&d_B, K * N * sizeof(float));\n",
    "    cudaMalloc(&d_A_half, M * K * sizeof(half));\n",
    "    cudaMalloc(&d_B_half, K * N * sizeof(half));\n",
    "    cudaMalloc(&d_C_wmma, M * N * sizeof(float));\n",
    "    cudaMalloc(&d_C_cuda, M * N * sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_A, h_A, M * K * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, K * N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Convert to half\n",
    "    float2half_kernel<<<(M*K+255)/256, 256>>>(d_A, d_A_half, M*K);\n",
    "    float2half_kernel<<<(K*N+255)/256, 256>>>(d_B, d_B_half, K*N);\n",
    "    \n",
    "    // WMMA kernel config\n",
    "    // Each block: BLOCK_ROW_TILES * BLOCK_COL_TILES warps = 16 warps = 512 threads\n",
    "    dim3 wmma_block(BLOCK_ROW_TILES * BLOCK_COL_TILES * 32);  // 512 threads\n",
    "    dim3 wmma_grid((N + BLOCK_N - 1) / BLOCK_N, (M + BLOCK_M - 1) / BLOCK_M);\n",
    "    \n",
    "    // CUDA kernel config\n",
    "    dim3 cuda_block(16, 16);\n",
    "    dim3 cuda_grid((N + 15) / 16, (M + 15) / 16);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    float wmma_time, cuda_time;\n",
    "    \n",
    "    // Benchmark WMMA\n",
    "    wmma_matmul_tiled<<<wmma_grid, wmma_block>>>(d_A_half, d_B_half, d_C_wmma, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        wmma_matmul_tiled<<<wmma_grid, wmma_block>>>(d_A_half, d_B_half, d_C_wmma, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&wmma_time, start, stop);\n",
    "    \n",
    "    // Benchmark CUDA\n",
    "    cuda_matmul<<<cuda_grid, cuda_block>>>(d_A, d_B, d_C_cuda, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        cuda_matmul<<<cuda_grid, cuda_block>>>(d_A, d_B, d_C_cuda, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&cuda_time, start, stop);\n",
    "    \n",
    "    // Results\n",
    "    printf(\"Performance Results (100 iterations):\\n\");\n",
    "    printf(\"  WMMA Tensor Core: %.2f ms (%.2f TFLOPS)\\n\", \n",
    "           wmma_time, (2.0 * M * N * K * 100) / (wmma_time * 1e9));\n",
    "    printf(\"  CUDA Naive:       %.2f ms (%.2f TFLOPS)\\n\",\n",
    "           cuda_time, (2.0 * M * N * K * 100) / (cuda_time * 1e9));\n",
    "    printf(\"  Speedup:          %.2fx\\n\", cuda_time / wmma_time);\n",
    "    \n",
    "    // Cleanup\n",
    "    delete[] h_A; delete[] h_B; delete[] h_C_wmma; delete[] h_C_cuda;\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_A_half); cudaFree(d_B_half);\n",
    "    cudaFree(d_C_wmma); cudaFree(d_C_cuda);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o wmma_tiled wmma_tiled.cu\n",
    "!./wmma_tiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c9e3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Layout Optimization\n",
    "\n",
    "### Layout Considerations\n",
    "\n",
    "```\n",
    "Tensor Cores support both row-major and column-major:\n",
    "\n",
    "Row-Major (C-style):\n",
    "  A[i][j] stored at A + i*cols + j\n",
    "  Memory: [row0][row1][row2]...\n",
    "\n",
    "Column-Major (Fortran-style):\n",
    "  A[i][j] stored at A + j*rows + i  \n",
    "  Memory: [col0][col1][col2]...\n",
    "\n",
    "For C = A Ã— B:\n",
    "  - A: row_major efficient (access rows)\n",
    "  - B: col_major efficient (access cols)\n",
    "  - OR: B transposed in row_major\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a839be",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wmma_layouts.cu\n",
    "// wmma_layouts.cu - Different memory layouts\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <mma.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "const int WMMA_M = 16;\n",
    "const int WMMA_N = 16;\n",
    "const int WMMA_K = 16;\n",
    "\n",
    "// A: row-major, B: row-major\n",
    "__global__ void wmma_row_row(\n",
    "    half* A, half* B, float* C, int M, int N, int K\n",
    ") {\n",
    "    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n",
    "    int warpN = blockIdx.y;\n",
    "    \n",
    "    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a;\n",
    "    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b;\n",
    "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c;\n",
    "    wmma::fill_fragment(c, 0.0f);\n",
    "    \n",
    "    for (int k = 0; k < K; k += WMMA_K) {\n",
    "        wmma::load_matrix_sync(a, A + warpM*WMMA_M*K + k, K);\n",
    "        wmma::load_matrix_sync(b, B + k*N + warpN*WMMA_N, N);\n",
    "        wmma::mma_sync(c, a, b, c);\n",
    "    }\n",
    "    \n",
    "    wmma::store_matrix_sync(C + warpM*WMMA_M*N + warpN*WMMA_N, c, N, wmma::mem_row_major);\n",
    "}\n",
    "\n",
    "// A: row-major, B: col-major (transposed storage)\n",
    "__global__ void wmma_row_col(\n",
    "    half* A, half* B_col, float* C, int M, int N, int K\n",
    ") {\n",
    "    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n",
    "    int warpN = blockIdx.y;\n",
    "    \n",
    "    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a;\n",
    "    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> b;\n",
    "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c;\n",
    "    wmma::fill_fragment(c, 0.0f);\n",
    "    \n",
    "    for (int k = 0; k < K; k += WMMA_K) {\n",
    "        wmma::load_matrix_sync(a, A + warpM*WMMA_M*K + k, K);\n",
    "        // B_col is stored column-major: B_col[k][n] at B_col + n*K + k\n",
    "        wmma::load_matrix_sync(b, B_col + warpN*WMMA_N*K + k, K);\n",
    "        wmma::mma_sync(c, a, b, c);\n",
    "    }\n",
    "    \n",
    "    wmma::store_matrix_sync(C + warpM*WMMA_M*N + warpN*WMMA_N, c, N, wmma::mem_row_major);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== WMMA Memory Layouts ===\\n\");\n",
    "    printf(\"Both row-row and row-col layouts are valid.\\n\");\n",
    "    printf(\"Choose based on your input data format.\\n\");\n",
    "    printf(\"\\nFor best performance:\\n\");\n",
    "    printf(\"  - A in row-major (sequential row access)\\n\");\n",
    "    printf(\"  - B in col-major (sequential col access)\\n\");\n",
    "    printf(\"  - Or: transpose B before kernel\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o wmma_layouts wmma_layouts.cu\n",
    "!./wmma_layouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb94eb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Handling Non-Aligned Sizes\n",
    "\n",
    "### Padding Strategy\n",
    "\n",
    "```\n",
    "When M, N, K are not multiples of 16:\n",
    "\n",
    "Option 1: Pad matrices\n",
    "  M_padded = ((M + 15) / 16) * 16\n",
    "  Allocate larger, zero-pad edges\n",
    "\n",
    "Option 2: Handle edge tiles specially\n",
    "  Full WMMA for interior tiles\n",
    "  Scalar code for edge elements\n",
    "\n",
    "Option 3: Clamp in load/store\n",
    "  Load zeros for out-of-bounds\n",
    "  Skip storing out-of-bounds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefc0ca",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1081e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wmma_padding.cu\n",
    "// wmma_padding.cu - Handle non-aligned matrix sizes\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <mma.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "const int WMMA_M = 16;\n",
    "const int WMMA_N = 16;\n",
    "const int WMMA_K = 16;\n",
    "\n",
    "// Pad value to multiple of 16\n",
    "__host__ __device__ int pad16(int val) {\n",
    "    return ((val + 15) / 16) * 16;\n",
    "}\n",
    "\n",
    "// Create padded copy of matrix\n",
    "__global__ void pad_matrix(\n",
    "    half* src, half* dst,\n",
    "    int srcRows, int srcCols,\n",
    "    int dstRows, int dstCols\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = dstRows * dstCols;\n",
    "    \n",
    "    for (int i = idx; i < total; i += blockDim.x * gridDim.x) {\n",
    "        int row = i / dstCols;\n",
    "        int col = i % dstCols;\n",
    "        \n",
    "        if (row < srcRows && col < srcCols) {\n",
    "            dst[i] = src[row * srcCols + col];\n",
    "        } else {\n",
    "            dst[i] = __float2half(0.0f);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Extract result from padded matrix\n",
    "__global__ void unpad_matrix(\n",
    "    float* src, float* dst,\n",
    "    int srcRows, int srcCols,\n",
    "    int dstRows, int dstCols\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = dstRows * dstCols;\n",
    "    \n",
    "    for (int i = idx; i < total; i += blockDim.x * gridDim.x) {\n",
    "        int row = i / dstCols;\n",
    "        int col = i % dstCols;\n",
    "        dst[i] = src[row * srcCols + col];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Non-aligned sizes\n",
    "    int M = 100, N = 200, K = 150;\n",
    "    \n",
    "    // Padded sizes\n",
    "    int M_pad = pad16(M);  // 112\n",
    "    int N_pad = pad16(N);  // 208\n",
    "    int K_pad = pad16(K);  // 160\n",
    "    \n",
    "    printf(\"Original sizes: %d x %d x %d\\n\", M, N, K);\n",
    "    printf(\"Padded sizes:   %d x %d x %d\\n\", M_pad, N_pad, K_pad);\n",
    "    printf(\"\\nPadding overhead:\\n\");\n",
    "    printf(\"  A: %.1f%% extra elements\\n\", \n",
    "           100.0 * (M_pad*K_pad - M*K) / (M*K));\n",
    "    printf(\"  B: %.1f%% extra elements\\n\",\n",
    "           100.0 * (K_pad*N_pad - K*N) / (K*N));\n",
    "    printf(\"  C: %.1f%% extra elements\\n\",\n",
    "           100.0 * (M_pad*N_pad - M*N) / (M*N));\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eac17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o wmma_padding wmma_padding.cu\n",
    "!./wmma_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81dc753",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03da0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wmma_exercises.cu\n",
    "// CUDA C++ WMMA Exercises\n",
    "// Week 13, Day 2: WMMA API\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <mma.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "// Exercise 1: Basic WMMA matrix multiply\n",
    "// TODO: Implement a simple 16x16x16 matrix multiply using WMMA\n",
    "\n",
    "// Exercise 2: Tiled WMMA for larger matrices\n",
    "// TODO: Extend to handle matrices larger than 16x16\n",
    "\n",
    "// Exercise 3: Batched WMMA\n",
    "// TODO: Process multiple matrix pairs in parallel\n",
    "\n",
    "int main() {\n",
    "    printf(\"WMMA Exercises - Implement your solutions above\\n\");\n",
    "    printf(\"Compile with: nvcc -arch=sm_70 wmma_exercises.cu -o wmma_exercises\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d85a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 wmma_exercises.cu -o wmma_exercises && ./wmma_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6ce2c",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/CuPy Exercises (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Plan your batched WMMA implementation\n",
    "\n",
    "def plan_batched_wmma():\n",
    "    print(\"Batched WMMA Implementation Plan\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"Option 1: Loop in host\")\n",
    "    print(\"  for b in range(batch):\")\n",
    "    print(\"    wmma_kernel(A[b], B[b], C[b])\")\n",
    "    print(\"  Pros: Simple, reuse existing kernel\")\n",
    "    print(\"  Cons: Kernel launch overhead\")\n",
    "    print()\n",
    "    print(\"Option 2: Batch dimension in grid\")\n",
    "    print(\"  grid = (tiles_n, tiles_m, batch)\")\n",
    "    print(\"  batch_idx = blockIdx.z\")\n",
    "    print(\"  Pros: Single launch\")\n",
    "    print(\"  Cons: Limited by max grid dimensions\")\n",
    "    print()\n",
    "    print(\"Option 3: Strided batching\")\n",
    "    print(\"  Multiple matrices per block\")\n",
    "    print(\"  Pros: High occupancy\")\n",
    "    print(\"  Cons: Complex implementation\")\n",
    "\n",
    "plan_batched_wmma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92654b28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### WMMA Magic Box Reference\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“¦ MATRIX MAGIC MASTERED!                                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  The Four Spells:                                               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚  load_matrix_sync  â†’ Fill the magic box from memory     â”‚    â”‚\n",
    "â”‚  â”‚  fill_fragment     â†’ Initialize with starting value     â”‚    â”‚\n",
    "â”‚  â”‚  mma_sync          â†’ Execute matrix multiplication      â”‚    â”‚\n",
    "â”‚  â”‚  store_matrix_sync â†’ Save results back to memory        â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Fragment Cheat Sheet:                                          â”‚\n",
    "â”‚  â€¢ matrix_a:    Left input  (16Ã—K, row_major typical)           â”‚\n",
    "â”‚  â€¢ matrix_b:    Right input (KÃ—16, col_major typical)           â”‚\n",
    "â”‚  â€¢ accumulator: Result (16Ã—16, FP32 for precision)              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### WMMA API Quick Reference\n",
    "\n",
    "```cpp\n",
    "// Fragment types\n",
    "fragment<matrix_a, M, N, K, half, layout> a_frag;\n",
    "fragment<matrix_b, M, N, K, half, layout> b_frag;\n",
    "fragment<accumulator, M, N, K, float> c_frag;\n",
    "\n",
    "// Operations\n",
    "load_matrix_sync(frag, ptr, stride);\n",
    "fill_fragment(frag, value);\n",
    "mma_sync(d, a, b, c);  // d = a Ã— b + c\n",
    "store_matrix_sync(ptr, frag, stride, layout);\n",
    "```\n",
    "\n",
    "### Optimization Checklist\n",
    "\n",
    "- [ ] Use shared memory for tile loading\n",
    "- [ ] Ensure coalesced global memory access\n",
    "- [ ] Pad matrices to multiples of 16\n",
    "- [ ] Match layout to your data format\n",
    "- [ ] Unroll K-dimension loop for performance\n",
    "\n",
    "### Performance Pattern\n",
    "\n",
    "| Optimization | Impact | Effort |\n",
    "|-------------|--------|--------|\n",
    "| Basic WMMA | Baseline | Low |\n",
    "| Shared memory tiling | 2-3Ã— | Medium |\n",
    "| Double buffering | 1.3-1.5Ã— | Medium |\n",
    "| Warp-level scheduling | 1.2-1.5Ã— | High |\n",
    "\n",
    "### Tomorrow: Mixed Precision Training\n",
    "We'll apply Tensor Cores to neural network training with loss scaling."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
