{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487d71a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: WMMA API Deep Dive\n",
    "\n",
    "### Fragment Types and Layouts\n",
    "\n",
    "```\n",
    "WMMA operates on matrix \"fragments\" distributed across warp:\n",
    "\n",
    "fragment<matrix_a, M, N, K, type, layout>  // Left operand\n",
    "fragment<matrix_b, M, N, K, type, layout>  // Right operand  \n",
    "fragment<accumulator, M, N, K, acc_type>   // Result/accumulator\n",
    "\n",
    "Layouts:\n",
    "  row_major: Elements in row-contiguous order\n",
    "  col_major: Elements in column-contiguous order\n",
    "\n",
    "Shapes supported (M√óN√óK):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Shape    ‚îÇ  Input  ‚îÇ  Accum   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  16√ó16√ó16  ‚îÇ  FP16   ‚îÇ  FP32    ‚îÇ\n",
    "‚îÇ  32√ó8√ó16   ‚îÇ  FP16   ‚îÇ  FP32    ‚îÇ\n",
    "‚îÇ  8√ó32√ó16   ‚îÇ  FP16   ‚îÇ  FP32    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### WMMA Operations\n",
    "\n",
    "```cpp\n",
    "// Load from memory into fragment\n",
    "wmma::load_matrix_sync(frag, ptr, stride);  \n",
    "\n",
    "// Initialize accumulator\n",
    "wmma::fill_fragment(frag, value);\n",
    "\n",
    "// Matrix multiply-accumulate: D = A √ó B + C\n",
    "wmma::mma_sync(d_frag, a_frag, b_frag, c_frag);\n",
    "\n",
    "// Store fragment to memory\n",
    "wmma::store_matrix_sync(ptr, frag, stride, layout);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716a3fe",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8582f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wmma_tiled.cu\n",
    "// wmma_tiled.cu - Optimized WMMA with shared memory tiling\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <mma.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "// WMMA tile dimensions\n",
    "const int WMMA_M = 16;\n",
    "const int WMMA_N = 16;\n",
    "const int WMMA_K = 16;\n",
    "\n",
    "// Block tile dimensions (multiple WMMA tiles per block)\n",
    "const int BLOCK_ROW_TILES = 4;  // 4 WMMA tiles in M direction\n",
    "const int BLOCK_COL_TILES = 4;  // 4 WMMA tiles in N direction\n",
    "const int BLOCK_K_TILES = 2;    // 2 WMMA tiles in K direction per step\n",
    "\n",
    "const int BLOCK_M = BLOCK_ROW_TILES * WMMA_M;  // 64\n",
    "const int BLOCK_N = BLOCK_COL_TILES * WMMA_N;  // 64\n",
    "const int BLOCK_K = BLOCK_K_TILES * WMMA_K;    // 32\n",
    "\n",
    "__global__ void wmma_matmul_tiled(\n",
    "    half* __restrict__ A,\n",
    "    half* __restrict__ B, \n",
    "    float* __restrict__ C,\n",
    "    int M, int N, int K\n",
    ") {\n",
    "    // Shared memory for A and B tiles\n",
    "    __shared__ half sA[BLOCK_M][BLOCK_K];\n",
    "    __shared__ half sB[BLOCK_K][BLOCK_N];\n",
    "    \n",
    "    // Warp and lane indices\n",
    "    int warpId = threadIdx.x / 32;\n",
    "    int laneId = threadIdx.x % 32;\n",
    "    \n",
    "    // Each block has BLOCK_ROW_TILES * BLOCK_COL_TILES warps\n",
    "    // Map warp to its WMMA tile within the block\n",
    "    int warpRow = warpId / BLOCK_COL_TILES;\n",
    "    int warpCol = warpId % BLOCK_COL_TILES;\n",
    "    \n",
    "    // Block position in output matrix\n",
    "    int blockRow = blockIdx.y * BLOCK_M;\n",
    "    int blockCol = blockIdx.x * BLOCK_N;\n",
    "    \n",
    "    // Declare accumulator fragments (one per warp)\n",
    "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;\n",
    "    wmma::fill_fragment(c_frag, 0.0f);\n",
    "    \n",
    "    // Loop over K dimension\n",
    "    for (int k = 0; k < K; k += BLOCK_K) {\n",
    "        // Collaborative loading of A tile into shared memory\n",
    "        // Each thread loads multiple elements\n",
    "        for (int i = threadIdx.x; i < BLOCK_M * BLOCK_K; i += blockDim.x) {\n",
    "            int row = i / BLOCK_K;\n",
    "            int col = i % BLOCK_K;\n",
    "            int gRow = blockRow + row;\n",
    "            int gCol = k + col;\n",
    "            sA[row][col] = (gRow < M && gCol < K) ? A[gRow * K + gCol] : __float2half(0.0f);\n",
    "        }\n",
    "        \n",
    "        // Collaborative loading of B tile into shared memory\n",
    "        for (int i = threadIdx.x; i < BLOCK_K * BLOCK_N; i += blockDim.x) {\n",
    "            int row = i / BLOCK_N;\n",
    "            int col = i % BLOCK_N;\n",
    "            int gRow = k + row;\n",
    "            int gCol = blockCol + col;\n",
    "            sB[row][col] = (gRow < K && gCol < N) ? B[gRow * N + gCol] : __float2half(0.0f);\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        // Each warp computes its WMMA tiles\n",
    "        #pragma unroll\n",
    "        for (int kTile = 0; kTile < BLOCK_K_TILES; kTile++) {\n",
    "            wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n",
    "            wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b_frag;\n",
    "            \n",
    "            // Load from shared memory\n",
    "            wmma::load_matrix_sync(a_frag, &sA[warpRow * WMMA_M][kTile * WMMA_K], BLOCK_K);\n",
    "            wmma::load_matrix_sync(b_frag, &sB[kTile * WMMA_K][warpCol * WMMA_N], BLOCK_N);\n",
    "            \n",
    "            // Multiply-accumulate\n",
    "            wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Store result\n",
    "    int cRow = blockRow + warpRow * WMMA_M;\n",
    "    int cCol = blockCol + warpCol * WMMA_N;\n",
    "    if (cRow < M && cCol < N) {\n",
    "        wmma::store_matrix_sync(&C[cRow * N + cCol], c_frag, N, wmma::mem_row_major);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Standard CUDA matmul for comparison\n",
    "__global__ void cuda_matmul(float* A, float* B, float* C, int M, int N, int K) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            sum += A[row * K + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void float2half_kernel(float* in, half* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = __float2half(in[idx]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int M = 1024, N = 1024, K = 1024;\n",
    "    \n",
    "    printf(\"=== WMMA Tiled Matrix Multiply ===\\n\");\n",
    "    printf(\"Matrix size: %d x %d x %d\\n\\n\", M, N, K);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_A = new float[M * K];\n",
    "    float *h_B = new float[K * N];\n",
    "    float *h_C_wmma = new float[M * N];\n",
    "    float *h_C_cuda = new float[M * N];\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = (rand() % 10) / 10.0f;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = (rand() % 10) / 10.0f;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_A, *d_B, *d_C_wmma, *d_C_cuda;\n",
    "    half *d_A_half, *d_B_half;\n",
    "    \n",
    "    cudaMalloc(&d_A, M * K * sizeof(float));\n",
    "    cudaMalloc(&d_B, K * N * sizeof(float));\n",
    "    cudaMalloc(&d_A_half, M * K * sizeof(half));\n",
    "    cudaMalloc(&d_B_half, K * N * sizeof(half));\n",
    "    cudaMalloc(&d_C_wmma, M * N * sizeof(float));\n",
    "    cudaMalloc(&d_C_cuda, M * N * sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_A, h_A, M * K * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, K * N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Convert to half\n",
    "    float2half_kernel<<<(M*K+255)/256, 256>>>(d_A, d_A_half, M*K);\n",
    "    float2half_kernel<<<(K*N+255)/256, 256>>>(d_B, d_B_half, K*N);\n",
    "    \n",
    "    // WMMA kernel config\n",
    "    // Each block: BLOCK_ROW_TILES * BLOCK_COL_TILES warps = 16 warps = 512 threads\n",
    "    dim3 wmma_block(BLOCK_ROW_TILES * BLOCK_COL_TILES * 32);  // 512 threads\n",
    "    dim3 wmma_grid((N + BLOCK_N - 1) / BLOCK_N, (M + BLOCK_M - 1) / BLOCK_M);\n",
    "    \n",
    "    // CUDA kernel config\n",
    "    dim3 cuda_block(16, 16);\n",
    "    dim3 cuda_grid((N + 15) / 16, (M + 15) / 16);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    float wmma_time, cuda_time;\n",
    "    \n",
    "    // Benchmark WMMA\n",
    "    wmma_matmul_tiled<<<wmma_grid, wmma_block>>>(d_A_half, d_B_half, d_C_wmma, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        wmma_matmul_tiled<<<wmma_grid, wmma_block>>>(d_A_half, d_B_half, d_C_wmma, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&wmma_time, start, stop);\n",
    "    \n",
    "    // Benchmark CUDA\n",
    "    cuda_matmul<<<cuda_grid, cuda_block>>>(d_A, d_B, d_C_cuda, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        cuda_matmul<<<cuda_grid, cuda_block>>>(d_A, d_B, d_C_cuda, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    cudaEventElapsedTime(&cuda_time, start, stop);\n",
    "    \n",
    "    // Results\n",
    "    printf(\"Performance Results (100 iterations):\\n\");\n",
    "    printf(\"  WMMA Tensor Core: %.2f ms (%.2f TFLOPS)\\n\", \n",
    "           wmma_time, (2.0 * M * N * K * 100) / (wmma_time * 1e9));\n",
    "    printf(\"  CUDA Naive:       %.2f ms (%.2f TFLOPS)\\n\",\n",
    "           cuda_time, (2.0 * M * N * K * 100) / (cuda_time * 1e9));\n",
    "    printf(\"  Speedup:          %.2fx\\n\", cuda_time / wmma_time);\n",
    "    \n",
    "    // Cleanup\n",
    "    delete[] h_A; delete[] h_B; delete[] h_C_wmma; delete[] h_C_cuda;\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_A_half); cudaFree(d_B_half);\n",
    "    cudaFree(d_C_wmma); cudaFree(d_C_cuda);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o wmma_tiled wmma_tiled.cu\n",
    "!./wmma_tiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c9e3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Layout Optimization\n",
    "\n",
    "### Layout Considerations\n",
    "\n",
    "```\n",
    "Tensor Cores support both row-major and column-major:\n",
    "\n",
    "Row-Major (C-style):\n",
    "  A[i][j] stored at A + i*cols + j\n",
    "  Memory: [row0][row1][row2]...\n",
    "\n",
    "Column-Major (Fortran-style):\n",
    "  A[i][j] stored at A + j*rows + i  \n",
    "  Memory: [col0][col1][col2]...\n",
    "\n",
    "For C = A √ó B:\n",
    "  - A: row_major efficient (access rows)\n",
    "  - B: col_major efficient (access cols)\n",
    "  - OR: B transposed in row_major\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a839be",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wmma_layouts.cu\n",
    "// wmma_layouts.cu - Different memory layouts\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <mma.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "const int WMMA_M = 16;\n",
    "const int WMMA_N = 16;\n",
    "const int WMMA_K = 16;\n",
    "\n",
    "// A: row-major, B: row-major\n",
    "__global__ void wmma_row_row(\n",
    "    half* A, half* B, float* C, int M, int N, int K\n",
    ") {\n",
    "    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n",
    "    int warpN = blockIdx.y;\n",
    "    \n",
    "    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a;\n",
    "    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b;\n",
    "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c;\n",
    "    wmma::fill_fragment(c, 0.0f);\n",
    "    \n",
    "    for (int k = 0; k < K; k += WMMA_K) {\n",
    "        wmma::load_matrix_sync(a, A + warpM*WMMA_M*K + k, K);\n",
    "        wmma::load_matrix_sync(b, B + k*N + warpN*WMMA_N, N);\n",
    "        wmma::mma_sync(c, a, b, c);\n",
    "    }\n",
    "    \n",
    "    wmma::store_matrix_sync(C + warpM*WMMA_M*N + warpN*WMMA_N, c, N, wmma::mem_row_major);\n",
    "}\n",
    "\n",
    "// A: row-major, B: col-major (transposed storage)\n",
    "__global__ void wmma_row_col(\n",
    "    half* A, half* B_col, float* C, int M, int N, int K\n",
    ") {\n",
    "    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n",
    "    int warpN = blockIdx.y;\n",
    "    \n",
    "    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a;\n",
    "    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> b;\n",
    "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c;\n",
    "    wmma::fill_fragment(c, 0.0f);\n",
    "    \n",
    "    for (int k = 0; k < K; k += WMMA_K) {\n",
    "        wmma::load_matrix_sync(a, A + warpM*WMMA_M*K + k, K);\n",
    "        // B_col is stored column-major: B_col[k][n] at B_col + n*K + k\n",
    "        wmma::load_matrix_sync(b, B_col + warpN*WMMA_N*K + k, K);\n",
    "        wmma::mma_sync(c, a, b, c);\n",
    "    }\n",
    "    \n",
    "    wmma::store_matrix_sync(C + warpM*WMMA_M*N + warpN*WMMA_N, c, N, wmma::mem_row_major);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== WMMA Memory Layouts ===\\n\");\n",
    "    printf(\"Both row-row and row-col layouts are valid.\\n\");\n",
    "    printf(\"Choose based on your input data format.\\n\");\n",
    "    printf(\"\\nFor best performance:\\n\");\n",
    "    printf(\"  - A in row-major (sequential row access)\\n\");\n",
    "    printf(\"  - B in col-major (sequential col access)\\n\");\n",
    "    printf(\"  - Or: transpose B before kernel\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o wmma_layouts wmma_layouts.cu\n",
    "!./wmma_layouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb94eb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Handling Non-Aligned Sizes\n",
    "\n",
    "### Padding Strategy\n",
    "\n",
    "```\n",
    "When M, N, K are not multiples of 16:\n",
    "\n",
    "Option 1: Pad matrices\n",
    "  M_padded = ((M + 15) / 16) * 16\n",
    "  Allocate larger, zero-pad edges\n",
    "\n",
    "Option 2: Handle edge tiles specially\n",
    "  Full WMMA for interior tiles\n",
    "  Scalar code for edge elements\n",
    "\n",
    "Option 3: Clamp in load/store\n",
    "  Load zeros for out-of-bounds\n",
    "  Skip storing out-of-bounds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefc0ca",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1081e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wmma_padding.cu\n",
    "// wmma_padding.cu - Handle non-aligned matrix sizes\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <mma.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "const int WMMA_M = 16;\n",
    "const int WMMA_N = 16;\n",
    "const int WMMA_K = 16;\n",
    "\n",
    "// Pad value to multiple of 16\n",
    "__host__ __device__ int pad16(int val) {\n",
    "    return ((val + 15) / 16) * 16;\n",
    "}\n",
    "\n",
    "// Create padded copy of matrix\n",
    "__global__ void pad_matrix(\n",
    "    half* src, half* dst,\n",
    "    int srcRows, int srcCols,\n",
    "    int dstRows, int dstCols\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = dstRows * dstCols;\n",
    "    \n",
    "    for (int i = idx; i < total; i += blockDim.x * gridDim.x) {\n",
    "        int row = i / dstCols;\n",
    "        int col = i % dstCols;\n",
    "        \n",
    "        if (row < srcRows && col < srcCols) {\n",
    "            dst[i] = src[row * srcCols + col];\n",
    "        } else {\n",
    "            dst[i] = __float2half(0.0f);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Extract result from padded matrix\n",
    "__global__ void unpad_matrix(\n",
    "    float* src, float* dst,\n",
    "    int srcRows, int srcCols,\n",
    "    int dstRows, int dstCols\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = dstRows * dstCols;\n",
    "    \n",
    "    for (int i = idx; i < total; i += blockDim.x * gridDim.x) {\n",
    "        int row = i / dstCols;\n",
    "        int col = i % dstCols;\n",
    "        dst[i] = src[row * srcCols + col];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Non-aligned sizes\n",
    "    int M = 100, N = 200, K = 150;\n",
    "    \n",
    "    // Padded sizes\n",
    "    int M_pad = pad16(M);  // 112\n",
    "    int N_pad = pad16(N);  // 208\n",
    "    int K_pad = pad16(K);  // 160\n",
    "    \n",
    "    printf(\"Original sizes: %d x %d x %d\\n\", M, N, K);\n",
    "    printf(\"Padded sizes:   %d x %d x %d\\n\", M_pad, N_pad, K_pad);\n",
    "    printf(\"\\nPadding overhead:\\n\");\n",
    "    printf(\"  A: %.1f%% extra elements\\n\", \n",
    "           100.0 * (M_pad*K_pad - M*K) / (M*K));\n",
    "    printf(\"  B: %.1f%% extra elements\\n\",\n",
    "           100.0 * (K_pad*N_pad - K*N) / (K*N));\n",
    "    printf(\"  C: %.1f%% extra elements\\n\",\n",
    "           100.0 * (M_pad*N_pad - M*N) / (M*N));\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eac17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o wmma_padding wmma_padding.cu\n",
    "!./wmma_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81dc753",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Batched WMMA\n",
    "\n",
    "Extend the WMMA kernel to handle batched matrix multiplication:\n",
    "`C[b] = A[b] √ó B[b]` for batch size B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Plan your batched WMMA implementation\n",
    "\n",
    "def plan_batched_wmma():\n",
    "    print(\"Batched WMMA Implementation Plan\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"Option 1: Loop in host\")\n",
    "    print(\"  for b in range(batch):\")\n",
    "    print(\"    wmma_kernel(A[b], B[b], C[b])\")\n",
    "    print(\"  Pros: Simple, reuse existing kernel\")\n",
    "    print(\"  Cons: Kernel launch overhead\")\n",
    "    print()\n",
    "    print(\"Option 2: Batch dimension in grid\")\n",
    "    print(\"  grid = (tiles_n, tiles_m, batch)\")\n",
    "    print(\"  batch_idx = blockIdx.z\")\n",
    "    print(\"  Pros: Single launch\")\n",
    "    print(\"  Cons: Limited by max grid dimensions\")\n",
    "    print()\n",
    "    print(\"Option 3: Strided batching\")\n",
    "    print(\"  Multiple matrices per block\")\n",
    "    print(\"  Pros: High occupancy\")\n",
    "    print(\"  Cons: Complex implementation\")\n",
    "\n",
    "plan_batched_wmma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92654b28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### WMMA API Reference\n",
    "\n",
    "```cpp\n",
    "// Fragment types\n",
    "fragment<matrix_a, M, N, K, half, layout> a_frag;\n",
    "fragment<matrix_b, M, N, K, half, layout> b_frag;\n",
    "fragment<accumulator, M, N, K, float> c_frag;\n",
    "\n",
    "// Operations\n",
    "load_matrix_sync(frag, ptr, stride);\n",
    "fill_fragment(frag, value);\n",
    "mma_sync(d, a, b, c);  // d = a √ó b + c\n",
    "store_matrix_sync(ptr, frag, stride, layout);\n",
    "```\n",
    "\n",
    "### Optimization Tips\n",
    "\n",
    "1. Use shared memory for tile loading\n",
    "2. Ensure coalesced global memory access\n",
    "3. Pad matrices to multiples of 16\n",
    "4. Match layout to your data format\n",
    "5. Unroll K-dimension loop\n",
    "\n",
    "### Tomorrow: Mixed Precision Training\n",
    "We'll apply Tensor Cores to neural network training."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
