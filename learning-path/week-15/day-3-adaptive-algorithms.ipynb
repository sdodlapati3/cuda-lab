{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea062eb",
   "metadata": {},
   "source": [
    "## Adaptive Grid Sizing\n",
    "\n",
    "CDP allows kernels to decide at runtime how much parallelism to spawn based on actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile adaptive_grid.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Worker kernel - processes items in range\n",
    "__global__ void processRange(int* data, int start, int end, int* output) {\n",
    "    int idx = start + blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < end) {\n",
    "        output[idx] = data[idx] * data[idx];  // Square each element\n",
    "    }\n",
    "}\n",
    "\n",
    "// Coordinator kernel - adapts parallelism to workload\n",
    "__global__ void adaptiveProcess(int* data, int* counts, int numRegions, int* output) {\n",
    "    int region = blockIdx.x;\n",
    "    if (region >= numRegions) return;\n",
    "    \n",
    "    int count = counts[region];\n",
    "    int start = 0;\n",
    "    for (int i = 0; i < region; i++) start += counts[i];\n",
    "    int end = start + count;\n",
    "    \n",
    "    // Adapt grid size to actual workload\n",
    "    if (count > 0) {\n",
    "        int threadsPerBlock = 128;\n",
    "        int blocks = (count + threadsPerBlock - 1) / threadsPerBlock;\n",
    "        \n",
    "        printf(\"Region %d: %d items -> launching %d blocks\\n\", region, count, blocks);\n",
    "        processRange<<<blocks, threadsPerBlock>>>(data, start, end, output);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Variable-sized regions (simulating irregular workload)\n",
    "    int h_counts[] = {100, 5, 500, 20, 1000};\n",
    "    int numRegions = 5;\n",
    "    int totalItems = 0;\n",
    "    for (int i = 0; i < numRegions; i++) totalItems += h_counts[i];\n",
    "    \n",
    "    printf(\"Total items: %d across %d regions\\n\", totalItems, numRegions);\n",
    "    \n",
    "    int *d_data, *d_counts, *d_output;\n",
    "    cudaMalloc(&d_data, totalItems * sizeof(int));\n",
    "    cudaMalloc(&d_counts, numRegions * sizeof(int));\n",
    "    cudaMalloc(&d_output, totalItems * sizeof(int));\n",
    "    \n",
    "    // Initialize data\n",
    "    int* h_data = new int[totalItems];\n",
    "    for (int i = 0; i < totalItems; i++) h_data[i] = i;\n",
    "    cudaMemcpy(d_data, h_data, totalItems * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_counts, h_counts, numRegions * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch adaptive coordinator\n",
    "    adaptiveProcess<<<numRegions, 1>>>(d_data, d_counts, numRegions, d_output);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Verify results\n",
    "    int* h_output = new int[totalItems];\n",
    "    cudaMemcpy(h_output, d_output, totalItems * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"\\nSample results: output[0]=%d, output[100]=%d, output[600]=%d\\n\",\n",
    "           h_output[0], h_output[100], h_output[600]);\n",
    "    \n",
    "    delete[] h_data;\n",
    "    delete[] h_output;\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_counts);\n",
    "    cudaFree(d_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt adaptive_grid.cu -o adaptive_grid && ./adaptive_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fd9f9",
   "metadata": {},
   "source": [
    "## Quadtree Construction with CDP\n",
    "\n",
    "Quadtrees adaptively subdivide 2D space based on point density - perfect for CDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cdp_quadtree.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define MAX_POINTS_PER_NODE 4\n",
    "#define MAX_DEPTH 8\n",
    "\n",
    "struct Point {\n",
    "    float x, y;\n",
    "};\n",
    "\n",
    "struct BoundingBox {\n",
    "    float minX, minY, maxX, maxY;\n",
    "    \n",
    "    __device__ bool contains(Point p) {\n",
    "        return p.x >= minX && p.x <= maxX && p.y >= minY && p.y <= maxY;\n",
    "    }\n",
    "    \n",
    "    __device__ BoundingBox quadrant(int q) {\n",
    "        float midX = (minX + maxX) / 2;\n",
    "        float midY = (minY + maxY) / 2;\n",
    "        switch (q) {\n",
    "            case 0: return {minX, midY, midX, maxY};  // NW\n",
    "            case 1: return {midX, midY, maxX, maxY};  // NE\n",
    "            case 2: return {minX, minY, midX, midY};  // SW\n",
    "            case 3: return {midX, minY, maxX, midY};  // SE\n",
    "        }\n",
    "        return *this;\n",
    "    }\n",
    "};\n",
    "\n",
    "// Count points in bounding box\n",
    "__device__ int countPointsInBox(Point* points, int n, BoundingBox box) {\n",
    "    int count = 0;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (box.contains(points[i])) count++;\n",
    "    }\n",
    "    return count;\n",
    "}\n",
    "\n",
    "// Recursive quadtree construction\n",
    "__global__ void buildQuadtree(Point* points, int n, BoundingBox box, int depth, int* nodeCount) {\n",
    "    int count = countPointsInBox(points, n, box);\n",
    "    \n",
    "    if (count == 0 || depth >= MAX_DEPTH) {\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Count this node\n",
    "    int nodeId = atomicAdd(nodeCount, 1);\n",
    "    \n",
    "    if (count <= MAX_POINTS_PER_NODE) {\n",
    "        // Leaf node - no further subdivision\n",
    "        printf(\"Leaf node %d at depth %d: %d points\\n\", nodeId, depth, count);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Internal node - subdivide\n",
    "    printf(\"Internal node %d at depth %d: %d points -> subdividing\\n\", nodeId, depth, count);\n",
    "    \n",
    "    // Launch children for each quadrant\n",
    "    for (int q = 0; q < 4; q++) {\n",
    "        BoundingBox childBox = box.quadrant(q);\n",
    "        buildQuadtree<<<1, 1>>>(points, n, childBox, depth + 1, nodeCount);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 100;\n",
    "    Point h_points[N];\n",
    "    \n",
    "    // Generate clustered points\n",
    "    srand(42);\n",
    "    for (int i = 0; i < N/2; i++) {\n",
    "        // Cluster 1: upper-right\n",
    "        h_points[i] = {0.7f + 0.3f * rand() / RAND_MAX, 0.7f + 0.3f * rand() / RAND_MAX};\n",
    "    }\n",
    "    for (int i = N/2; i < N; i++) {\n",
    "        // Cluster 2: lower-left\n",
    "        h_points[i] = {0.0f + 0.3f * rand() / RAND_MAX, 0.0f + 0.3f * rand() / RAND_MAX};\n",
    "    }\n",
    "    \n",
    "    Point* d_points;\n",
    "    int* d_nodeCount;\n",
    "    cudaMalloc(&d_points, N * sizeof(Point));\n",
    "    cudaMalloc(&d_nodeCount, sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_points, h_points, N * sizeof(Point), cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_nodeCount, 0, sizeof(int));\n",
    "    \n",
    "    BoundingBox rootBox = {0, 0, 1, 1};\n",
    "    buildQuadtree<<<1, 1>>>(d_points, N, rootBox, 0, d_nodeCount);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int h_nodeCount;\n",
    "    cudaMemcpy(&h_nodeCount, d_nodeCount, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"\\nTotal nodes created: %d\\n\", h_nodeCount);\n",
    "    \n",
    "    cudaFree(d_points);\n",
    "    cudaFree(d_nodeCount);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt cdp_quadtree.cu -o cdp_quadtree && ./cdp_quadtree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddead560",
   "metadata": {},
   "source": [
    "## When CDP Shines vs. When to Avoid\n",
    "\n",
    "### Good Use Cases âœ…\n",
    "- Recursive divide-and-conquer\n",
    "- Irregular/adaptive workloads\n",
    "- Tree/graph traversal\n",
    "- Workload discovered at runtime\n",
    "\n",
    "### Avoid When âŒ\n",
    "- Regular, predictable parallelism\n",
    "- Very deep recursion (>16 levels)\n",
    "- Launching many tiny kernels\n",
    "- CPU coordination is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb56c0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercises\n",
    "\n",
    "### ðŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to reinforce your understanding of adaptive algorithms with CDP:\n",
    "\n",
    "**Exercise 1: Adaptive Sparse Matrix Processing**\n",
    "Process only non-zero regions of a sparse matrix using CDP. The coordinator kernel should analyze density and spawn workers only for dense regions.\n",
    "\n",
    "**Exercise 2: Dynamic Load Balancing**\n",
    "Implement a work-stealing pattern where idle regions can help process work from overloaded regions using adaptive kernel launches.\n",
    "\n",
    "**Exercise 3: Hierarchical Octree Traversal**\n",
    "Build a 3D octree and use CDP to adaptively process only occupied nodes, spawning 8 child kernels for non-leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile adaptive_exercises.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Adaptive Sparse Matrix Processing with CDP\n",
    "// ============================================================\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "#define DENSITY_THRESHOLD 0.3f\n",
    "\n",
    "__global__ void processBlock(float* matrix, int startRow, int startCol, \n",
    "                             int blockRows, int blockCols, int matrixWidth,\n",
    "                             float* output) {\n",
    "    int localRow = threadIdx.y;\n",
    "    int localCol = threadIdx.x;\n",
    "    \n",
    "    if (localRow < blockRows && localCol < blockCols) {\n",
    "        int globalRow = startRow + localRow;\n",
    "        int globalCol = startCol + localCol;\n",
    "        int idx = globalRow * matrixWidth + globalCol;\n",
    "        \n",
    "        // Process: square non-zero elements\n",
    "        if (matrix[idx] != 0.0f) {\n",
    "            output[idx] = matrix[idx] * matrix[idx];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void sparseMatrixCoordinator(float* matrix, int rows, int cols, \n",
    "                                        float* output, int* blockCounts) {\n",
    "    int blockRow = blockIdx.y;\n",
    "    int blockCol = blockIdx.x;\n",
    "    \n",
    "    int startRow = blockRow * BLOCK_SIZE;\n",
    "    int startCol = blockCol * BLOCK_SIZE;\n",
    "    int blockRows = min(BLOCK_SIZE, rows - startRow);\n",
    "    int blockCols = min(BLOCK_SIZE, cols - startCol);\n",
    "    \n",
    "    // Count non-zeros in this block\n",
    "    int nonZeros = 0;\n",
    "    for (int r = 0; r < blockRows; r++) {\n",
    "        for (int c = 0; c < blockCols; c++) {\n",
    "            int idx = (startRow + r) * cols + (startCol + c);\n",
    "            if (matrix[idx] != 0.0f) nonZeros++;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    float density = (float)nonZeros / (blockRows * blockCols);\n",
    "    int blockId = blockRow * gridDim.x + blockCol;\n",
    "    blockCounts[blockId] = nonZeros;\n",
    "    \n",
    "    // Only launch child kernel if density exceeds threshold\n",
    "    if (density >= DENSITY_THRESHOLD) {\n",
    "        dim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
    "        processBlock<<<1, threads>>>(matrix, startRow, startCol, \n",
    "                                     blockRows, blockCols, cols, output);\n",
    "        printf(\"Block[%d,%d]: density=%.2f, launching worker\\n\", \n",
    "               blockRow, blockCol, density);\n",
    "    } else {\n",
    "        printf(\"Block[%d,%d]: density=%.2f, skipping (sparse)\\n\", \n",
    "               blockRow, blockCol, density);\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise1_sparse_matrix() {\n",
    "    printf(\"=== Exercise 1: Adaptive Sparse Matrix Processing ===\\n\");\n",
    "    \n",
    "    const int ROWS = 32, COLS = 32;\n",
    "    float* h_matrix = (float*)calloc(ROWS * COLS, sizeof(float));\n",
    "    float* h_output = (float*)calloc(ROWS * COLS, sizeof(float));\n",
    "    \n",
    "    // Create sparse matrix with dense regions\n",
    "    // Dense region in top-left quadrant\n",
    "    for (int r = 0; r < 16; r++) {\n",
    "        for (int c = 0; c < 16; c++) {\n",
    "            if ((r + c) % 2 == 0) h_matrix[r * COLS + c] = (float)(r + c + 1);\n",
    "        }\n",
    "    }\n",
    "    // Sparse bottom-right\n",
    "    h_matrix[20 * COLS + 20] = 5.0f;\n",
    "    \n",
    "    float *d_matrix, *d_output;\n",
    "    int* d_blockCounts;\n",
    "    int numBlocksX = (COLS + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    int numBlocksY = (ROWS + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    cudaMalloc(&d_matrix, ROWS * COLS * sizeof(float));\n",
    "    cudaMalloc(&d_output, ROWS * COLS * sizeof(float));\n",
    "    cudaMalloc(&d_blockCounts, numBlocksX * numBlocksY * sizeof(int));\n",
    "    cudaMemcpy(d_matrix, h_matrix, ROWS * COLS * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_output, 0, ROWS * COLS * sizeof(float));\n",
    "    \n",
    "    dim3 grid(numBlocksX, numBlocksY);\n",
    "    sparseMatrixCoordinator<<<grid, 1>>>(d_matrix, ROWS, COLS, d_output, d_blockCounts);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_output, d_output, ROWS * COLS * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify some outputs\n",
    "    printf(\"Sample outputs: [0,0]=%.0f (expected %.0f), [1,1]=%.0f (expected %.0f)\\n\",\n",
    "           h_output[0], h_matrix[0] * h_matrix[0],\n",
    "           h_output[COLS + 1], h_matrix[COLS + 1] * h_matrix[COLS + 1]);\n",
    "    \n",
    "    free(h_matrix); free(h_output);\n",
    "    cudaFree(d_matrix); cudaFree(d_output); cudaFree(d_blockCounts);\n",
    "    printf(\"\\n\");\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Dynamic Load Balancing with CDP\n",
    "// ============================================================\n",
    "\n",
    "__device__ int workQueue[1024];\n",
    "__device__ int queueHead = 0;\n",
    "__device__ int queueTail = 0;\n",
    "\n",
    "__global__ void workerKernel(int* data, int workId, int workSize, int* results) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < workSize) {\n",
    "        // Simulate work with variable complexity\n",
    "        int sum = 0;\n",
    "        for (int i = 0; i < data[workId] * 10; i++) {\n",
    "            sum += i;\n",
    "        }\n",
    "        atomicAdd(&results[workId], sum);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void loadBalancer(int* workloads, int numWorks, int* results) {\n",
    "    int workerId = blockIdx.x;\n",
    "    int myWork = workerId;\n",
    "    \n",
    "    if (myWork >= numWorks) return;\n",
    "    \n",
    "    int workSize = workloads[myWork];\n",
    "    printf(\"Worker %d: processing work %d (size=%d)\\n\", workerId, myWork, workSize);\n",
    "    \n",
    "    // Adapt parallelism based on workload size\n",
    "    int threads = min(256, workSize);\n",
    "    int blocks = (workSize + threads - 1) / threads;\n",
    "    blocks = max(1, blocks);\n",
    "    \n",
    "    workerKernel<<<blocks, threads>>>(workloads, myWork, workSize, results);\n",
    "}\n",
    "\n",
    "void exercise2_load_balancing() {\n",
    "    printf(\"=== Exercise 2: Dynamic Load Balancing ===\\n\");\n",
    "    \n",
    "    // Variable workloads\n",
    "    int h_workloads[] = {10, 500, 20, 1000, 5, 800, 15, 300};\n",
    "    int numWorks = 8;\n",
    "    \n",
    "    int* d_workloads;\n",
    "    int* d_results;\n",
    "    int h_results[8] = {0};\n",
    "    \n",
    "    cudaMalloc(&d_workloads, numWorks * sizeof(int));\n",
    "    cudaMalloc(&d_results, numWorks * sizeof(int));\n",
    "    cudaMemcpy(d_workloads, h_workloads, numWorks * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_results, 0, numWorks * sizeof(int));\n",
    "    \n",
    "    loadBalancer<<<numWorks, 1>>>(d_workloads, numWorks, d_results);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_results, d_results, numWorks * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Work processing completed.\\n\\n\");\n",
    "    \n",
    "    cudaFree(d_workloads);\n",
    "    cudaFree(d_results);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Hierarchical Octree Traversal with CDP\n",
    "// ============================================================\n",
    "\n",
    "struct OctreeNode {\n",
    "    int occupied;     // 8 bits, one per child\n",
    "    int isLeaf;\n",
    "    int childBase;    // Index of first child (8 consecutive)\n",
    "    float data;\n",
    "};\n",
    "\n",
    "__global__ void processOctreeNode(OctreeNode* octree, int nodeIdx, \n",
    "                                  float* results, int* processedCount) {\n",
    "    OctreeNode node = octree[nodeIdx];\n",
    "    \n",
    "    if (node.isLeaf) {\n",
    "        // Process leaf node\n",
    "        int idx = atomicAdd(processedCount, 1);\n",
    "        results[idx] = node.data;\n",
    "        printf(\"Leaf node %d: data=%.1f\\n\", nodeIdx, node.data);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Count occupied children\n",
    "    int numChildren = __popc(node.occupied);\n",
    "    \n",
    "    if (numChildren > 0) {\n",
    "        printf(\"Node %d: %d occupied children, spawning workers\\n\", nodeIdx, numChildren);\n",
    "        \n",
    "        // Spawn child kernels for occupied nodes\n",
    "        for (int i = 0; i < 8; i++) {\n",
    "            if (node.occupied & (1 << i)) {\n",
    "                int childIdx = node.childBase + i;\n",
    "                processOctreeNode<<<1, 1>>>(octree, childIdx, results, processedCount);\n",
    "            }\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise3_octree() {\n",
    "    printf(\"=== Exercise 3: Hierarchical Octree Traversal ===\\n\");\n",
    "    \n",
    "    // Build simple octree:\n",
    "    // Root (node 0) has 3 occupied children\n",
    "    // Some children are leaves, some have grandchildren\n",
    "    \n",
    "    OctreeNode h_octree[12] = {\n",
    "        {0b00000111, 0, 1, 0.0f},   // Node 0: root, children at 1,2,3\n",
    "        {0, 1, 0, 1.0f},            // Node 1: leaf\n",
    "        {0b00000011, 0, 4, 0.0f},   // Node 2: 2 children at 4,5\n",
    "        {0, 1, 0, 3.0f},            // Node 3: leaf\n",
    "        {0, 1, 0, 4.0f},            // Node 4: leaf\n",
    "        {0, 1, 0, 5.0f},            // Node 5: leaf\n",
    "    };\n",
    "    \n",
    "    OctreeNode* d_octree;\n",
    "    float* d_results;\n",
    "    int* d_count;\n",
    "    float h_results[10] = {0};\n",
    "    int h_count = 0;\n",
    "    \n",
    "    cudaMalloc(&d_octree, 12 * sizeof(OctreeNode));\n",
    "    cudaMalloc(&d_results, 10 * sizeof(float));\n",
    "    cudaMalloc(&d_count, sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_octree, h_octree, 12 * sizeof(OctreeNode), cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_results, 0, 10 * sizeof(float));\n",
    "    cudaMemset(d_count, 0, sizeof(int));\n",
    "    \n",
    "    processOctreeNode<<<1, 1>>>(d_octree, 0, d_results, d_count);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(&h_count, d_count, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_results, d_results, h_count * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Processed %d leaf nodes with values: \", h_count);\n",
    "    for (int i = 0; i < h_count; i++) printf(\"%.0f \", h_results[i]);\n",
    "    printf(\"\\n\\n\");\n",
    "    \n",
    "    cudaFree(d_octree);\n",
    "    cudaFree(d_results);\n",
    "    cudaFree(d_count);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"CDP Adaptive Algorithms Exercises\\n\");\n",
    "    printf(\"==================================\\n\\n\");\n",
    "    \n",
    "    exercise1_sparse_matrix();\n",
    "    exercise2_load_balancing();\n",
    "    exercise3_octree();\n",
    "    \n",
    "    printf(\"All exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91330d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -lcudadevrt -o adaptive_exercises adaptive_exercises.cu && ./adaptive_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856170cb",
   "metadata": {},
   "source": [
    "### ðŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises explore similar adaptive concepts using Python. Note that Numba CUDA does not support dynamic parallelism directly, so these focus on alternative approaches.\n",
    "\n",
    "**Exercise A:** Implement a sparse matrix processor in Numba that uses a pre-computed density map to skip empty regions.\n",
    "\n",
    "**Exercise B:** Create an adaptive workload distributor using Python multiprocessing combined with Numba CUDA kernels for the compute-intensive portions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f10c3",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Adaptive parallelism** - spawn work based on actual data\n",
    "2. **Hierarchical structures** - quadtree/octree natural fit\n",
    "3. **Irregular workloads** - regions with variable sizes\n",
    "4. **Trade-offs** - launch overhead vs. CPU roundtrip\n",
    "\n",
    "## Next: Day 4 - CDP Optimization & Best Practices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
