{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c79c26",
   "metadata": {},
   "source": [
    "## CDP2 Tail Launch Optimization\n",
    "\n",
    "CUDA 12+ introduces CDP2 with tail launch - child kernel reuses parent's resources when parent is about to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cdp_tail_launch.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// With tail launch: child inherits parent slot\n",
    "__global__ void tailRecursive(int* data, int n, int depth) {\n",
    "    if (n <= 1 || depth >= 10) {\n",
    "        if (threadIdx.x == 0) {\n",
    "            printf(\"Base case at depth %d, n=%d\\n\", depth, n);\n",
    "        }\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Process current level\n",
    "    int tid = threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] += depth;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Tail launch - parent exits immediately after\n",
    "    if (tid == 0) {\n",
    "        tailRecursive<<<1, n/2>>>(data, n/2, depth + 1);\n",
    "    }\n",
    "    // No cudaDeviceSynchronize - tail launch pattern!\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 256;\n",
    "    int *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(int));\n",
    "    cudaMemset(d_data, 0, N * sizeof(int));\n",
    "    \n",
    "    tailRecursive<<<1, N>>>(d_data, N, 0);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int h_data[N];\n",
    "    cudaMemcpy(h_data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Results: data[0]=%d, data[10]=%d, data[100]=%d\\n\",\n",
    "           h_data[0], h_data[10], h_data[100]);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ea097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt cdp_tail_launch.cu -o cdp_tail_launch && ./cdp_tail_launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff19537",
   "metadata": {},
   "source": [
    "## Batching Child Launches\n",
    "\n",
    "Launch overhead can dominate - batch work into fewer, larger kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9245e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cdp_batching.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// BAD: Many tiny kernel launches\n",
    "__global__ void badPattern(int* data, int n) {\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        // Each iteration launches a tiny kernel\n",
    "        // Massive overhead!\n",
    "    }\n",
    "}\n",
    "\n",
    "// GOOD: Batch worker kernel\n",
    "__global__ void batchWorker(int* data, int* offsets, int* counts, int numTasks) {\n",
    "    int taskId = blockIdx.x;\n",
    "    if (taskId >= numTasks) return;\n",
    "    \n",
    "    int offset = offsets[taskId];\n",
    "    int count = counts[taskId];\n",
    "    \n",
    "    for (int i = threadIdx.x; i < count; i += blockDim.x) {\n",
    "        data[offset + i] *= 2;\n",
    "    }\n",
    "}\n",
    "\n",
    "// GOOD: Batch coordinator\n",
    "__global__ void batchCoordinator(int* data, int* offsets, int* counts, int numTasks) {\n",
    "    if (threadIdx.x == 0) {\n",
    "        // One launch for all tasks!\n",
    "        int threadsPerBlock = 256;\n",
    "        batchWorker<<<numTasks, threadsPerBlock>>>(data, offsets, counts, numTasks);\n",
    "        cudaDeviceSynchronize();\n",
    "        printf(\"Processed %d tasks in single batched launch\\n\", numTasks);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int NUM_TASKS = 100;\n",
    "    const int TOTAL_DATA = 10000;\n",
    "    \n",
    "    int h_offsets[NUM_TASKS], h_counts[NUM_TASKS];\n",
    "    int offset = 0;\n",
    "    for (int i = 0; i < NUM_TASKS; i++) {\n",
    "        h_offsets[i] = offset;\n",
    "        h_counts[i] = TOTAL_DATA / NUM_TASKS;\n",
    "        offset += h_counts[i];\n",
    "    }\n",
    "    \n",
    "    int *d_data, *d_offsets, *d_counts;\n",
    "    cudaMalloc(&d_data, TOTAL_DATA * sizeof(int));\n",
    "    cudaMalloc(&d_offsets, NUM_TASKS * sizeof(int));\n",
    "    cudaMalloc(&d_counts, NUM_TASKS * sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_offsets, h_offsets, NUM_TASKS * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_counts, h_counts, NUM_TASKS * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Initialize data\n",
    "    int* h_data = new int[TOTAL_DATA];\n",
    "    for (int i = 0; i < TOTAL_DATA; i++) h_data[i] = 1;\n",
    "    cudaMemcpy(d_data, h_data, TOTAL_DATA * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    batchCoordinator<<<1, 1>>>(d_data, d_offsets, d_counts, NUM_TASKS);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_data, d_data, TOTAL_DATA * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Result check: data[0]=%d, data[5000]=%d\\n\", h_data[0], h_data[5000]);\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_offsets);\n",
    "    cudaFree(d_counts);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt cdp_batching.cu -o cdp_batching && ./cdp_batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76402374",
   "metadata": {},
   "source": [
    "## Memory Optimization\n",
    "\n",
    "### Use Unified Memory for CDP\n",
    "- Automatic coherence between parent/child\n",
    "- Simpler programming model\n",
    "- Works well with migration hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09839b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cdp_unified_memory.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void childProcess(int* data, int start, int count) {\n",
    "    int idx = start + blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < start + count) {\n",
    "        data[idx] = data[idx] * 2 + 1;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void parentWithUnified(int* data, int n) {\n",
    "    // With unified memory, parent and child see consistent view\n",
    "    if (threadIdx.x == 0) {\n",
    "        int blocks = (n + 255) / 256;\n",
    "        childProcess<<<blocks, 256>>>(data, 0, n);\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        // Can immediately read child's results!\n",
    "        printf(\"After child: data[0]=%d, data[n-1]=%d\\n\", data[0], data[n-1]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    int* data;\n",
    "    \n",
    "    // Unified memory - works seamlessly with CDP\n",
    "    cudaMallocManaged(&data, N * sizeof(int));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) data[i] = i;\n",
    "    printf(\"Before: data[0]=%d, data[n-1]=%d\\n\", data[0], data[N-1]);\n",
    "    \n",
    "    parentWithUnified<<<1, 1>>>(data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Host can also read (after sync)\n",
    "    printf(\"Host sees: data[0]=%d, data[n-1]=%d\\n\", data[0], data[N-1]);\n",
    "    \n",
    "    cudaFree(data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt cdp_unified_memory.cu -o cdp_unified_memory && ./cdp_unified_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6dc905",
   "metadata": {},
   "source": [
    "## CDP Best Practices Summary\n",
    "\n",
    "| Practice | Why |\n",
    "|----------|-----|\n",
    "| Use base cases | Avoid deep recursion |\n",
    "| Batch launches | Reduce overhead |\n",
    "| Prefer tail launch | Resource reuse |\n",
    "| Use Unified Memory | Simpler coherence |\n",
    "| Profile with Nsight | Identify bottlenecks |\n",
    "| Consider alternatives | Sometimes CPU coord is faster |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386ac73",
   "metadata": {},
   "source": [
    "## Exercise: Optimized CDP Pattern\n",
    "\n",
    "Take a naive CDP implementation and apply these optimizations:\n",
    "1. Add base case threshold\n",
    "2. Batch sibling launches\n",
    "3. Use unified memory\n",
    "4. Compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc1bf0",
   "metadata": {},
   "source": [
    "## Week 15 Summary\n",
    "\n",
    "You've learned:\n",
    "- **CDP fundamentals**: Parent-child kernels, memory visibility\n",
    "- **Recursive algorithms**: Quicksort, tree traversal on GPU\n",
    "- **Adaptive parallelism**: Runtime-determined grid sizes\n",
    "- **Optimization**: Tail launch, batching, unified memory\n",
    "\n",
    "## Complete the Checkpoint Quiz!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
