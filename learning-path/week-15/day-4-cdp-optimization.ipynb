{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c79c26",
   "metadata": {},
   "source": [
    "## CDP2 Tail Launch Optimization\n",
    "\n",
    "CUDA 12+ introduces CDP2 with tail launch - child kernel reuses parent's resources when parent is about to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cdp_tail_launch.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// With tail launch: child inherits parent slot\n",
    "__global__ void tailRecursive(int* data, int n, int depth) {\n",
    "    if (n <= 1 || depth >= 10) {\n",
    "        if (threadIdx.x == 0) {\n",
    "            printf(\"Base case at depth %d, n=%d\\n\", depth, n);\n",
    "        }\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Process current level\n",
    "    int tid = threadIdx.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] += depth;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Tail launch - parent exits immediately after\n",
    "    if (tid == 0) {\n",
    "        tailRecursive<<<1, n/2>>>(data, n/2, depth + 1);\n",
    "    }\n",
    "    // No cudaDeviceSynchronize - tail launch pattern!\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 256;\n",
    "    int *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(int));\n",
    "    cudaMemset(d_data, 0, N * sizeof(int));\n",
    "    \n",
    "    tailRecursive<<<1, N>>>(d_data, N, 0);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int h_data[N];\n",
    "    cudaMemcpy(h_data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Results: data[0]=%d, data[10]=%d, data[100]=%d\\n\",\n",
    "           h_data[0], h_data[10], h_data[100]);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ea097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt cdp_tail_launch.cu -o cdp_tail_launch && ./cdp_tail_launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff19537",
   "metadata": {},
   "source": [
    "## Batching Child Launches\n",
    "\n",
    "Launch overhead can dominate - batch work into fewer, larger kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9245e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cdp_batching.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// BAD: Many tiny kernel launches\n",
    "__global__ void badPattern(int* data, int n) {\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        // Each iteration launches a tiny kernel\n",
    "        // Massive overhead!\n",
    "    }\n",
    "}\n",
    "\n",
    "// GOOD: Batch worker kernel\n",
    "__global__ void batchWorker(int* data, int* offsets, int* counts, int numTasks) {\n",
    "    int taskId = blockIdx.x;\n",
    "    if (taskId >= numTasks) return;\n",
    "    \n",
    "    int offset = offsets[taskId];\n",
    "    int count = counts[taskId];\n",
    "    \n",
    "    for (int i = threadIdx.x; i < count; i += blockDim.x) {\n",
    "        data[offset + i] *= 2;\n",
    "    }\n",
    "}\n",
    "\n",
    "// GOOD: Batch coordinator\n",
    "__global__ void batchCoordinator(int* data, int* offsets, int* counts, int numTasks) {\n",
    "    if (threadIdx.x == 0) {\n",
    "        // One launch for all tasks!\n",
    "        int threadsPerBlock = 256;\n",
    "        batchWorker<<<numTasks, threadsPerBlock>>>(data, offsets, counts, numTasks);\n",
    "        cudaDeviceSynchronize();\n",
    "        printf(\"Processed %d tasks in single batched launch\\n\", numTasks);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int NUM_TASKS = 100;\n",
    "    const int TOTAL_DATA = 10000;\n",
    "    \n",
    "    int h_offsets[NUM_TASKS], h_counts[NUM_TASKS];\n",
    "    int offset = 0;\n",
    "    for (int i = 0; i < NUM_TASKS; i++) {\n",
    "        h_offsets[i] = offset;\n",
    "        h_counts[i] = TOTAL_DATA / NUM_TASKS;\n",
    "        offset += h_counts[i];\n",
    "    }\n",
    "    \n",
    "    int *d_data, *d_offsets, *d_counts;\n",
    "    cudaMalloc(&d_data, TOTAL_DATA * sizeof(int));\n",
    "    cudaMalloc(&d_offsets, NUM_TASKS * sizeof(int));\n",
    "    cudaMalloc(&d_counts, NUM_TASKS * sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_offsets, h_offsets, NUM_TASKS * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_counts, h_counts, NUM_TASKS * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Initialize data\n",
    "    int* h_data = new int[TOTAL_DATA];\n",
    "    for (int i = 0; i < TOTAL_DATA; i++) h_data[i] = 1;\n",
    "    cudaMemcpy(d_data, h_data, TOTAL_DATA * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    batchCoordinator<<<1, 1>>>(d_data, d_offsets, d_counts, NUM_TASKS);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_data, d_data, TOTAL_DATA * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Result check: data[0]=%d, data[5000]=%d\\n\", h_data[0], h_data[5000]);\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_offsets);\n",
    "    cudaFree(d_counts);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt cdp_batching.cu -o cdp_batching && ./cdp_batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76402374",
   "metadata": {},
   "source": [
    "## Memory Optimization\n",
    "\n",
    "### Use Unified Memory for CDP\n",
    "- Automatic coherence between parent/child\n",
    "- Simpler programming model\n",
    "- Works well with migration hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09839b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cdp_unified_memory.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void childProcess(int* data, int start, int count) {\n",
    "    int idx = start + blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < start + count) {\n",
    "        data[idx] = data[idx] * 2 + 1;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void parentWithUnified(int* data, int n) {\n",
    "    // With unified memory, parent and child see consistent view\n",
    "    if (threadIdx.x == 0) {\n",
    "        int blocks = (n + 255) / 256;\n",
    "        childProcess<<<blocks, 256>>>(data, 0, n);\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        // Can immediately read child's results!\n",
    "        printf(\"After child: data[0]=%d, data[n-1]=%d\\n\", data[0], data[n-1]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    int* data;\n",
    "    \n",
    "    // Unified memory - works seamlessly with CDP\n",
    "    cudaMallocManaged(&data, N * sizeof(int));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) data[i] = i;\n",
    "    printf(\"Before: data[0]=%d, data[n-1]=%d\\n\", data[0], data[N-1]);\n",
    "    \n",
    "    parentWithUnified<<<1, 1>>>(data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Host can also read (after sync)\n",
    "    printf(\"Host sees: data[0]=%d, data[n-1]=%d\\n\", data[0], data[N-1]);\n",
    "    \n",
    "    cudaFree(data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt cdp_unified_memory.cu -o cdp_unified_memory && ./cdp_unified_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6dc905",
   "metadata": {},
   "source": [
    "## CDP Best Practices Summary\n",
    "\n",
    "| Practice | Why |\n",
    "|----------|-----|\n",
    "| Use base cases | Avoid deep recursion |\n",
    "| Batch launches | Reduce overhead |\n",
    "| Prefer tail launch | Resource reuse |\n",
    "| Use Unified Memory | Simpler coherence |\n",
    "| Profile with Nsight | Identify bottlenecks |\n",
    "| Consider alternatives | Sometimes CPU coord is faster |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386ac73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercises\n",
    "\n",
    "### ðŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Take a naive CDP implementation and apply these optimizations:\n",
    "1. Add base case threshold\n",
    "2. Batch sibling launches\n",
    "3. Use unified memory\n",
    "4. Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3628e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cdp_optimization_exercises.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <chrono>\n",
    "\n",
    "/*\n",
    " * CDP Optimization Exercises\n",
    " * Compile: nvcc -arch=sm_75 -rdc=true -lcudadevrt -o cdp_optimization_exercises cdp_optimization_exercises.cu\n",
    " * \n",
    " * Exercise 1: Naive vs Optimized CDP\n",
    " * - Compare performance with/without base case threshold\n",
    " * \n",
    " * Exercise 2: Batching Child Launches\n",
    " * - Reduce launch overhead by batching work\n",
    " * \n",
    " * Exercise 3: Unified Memory with CDP\n",
    " * - Simplify memory management with managed memory\n",
    " * \n",
    " * Exercise 4: Tail Launch Pattern\n",
    " * - Use tail launch for resource efficiency\n",
    " */\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Naive vs Optimized Base Case\n",
    "// =============================================================================\n",
    "\n",
    "// Naive: No base case threshold (many tiny kernels)\n",
    "__global__ void naiveRecursive(int* data, int start, int end, int depth) {\n",
    "    if (start >= end) return;\n",
    "    \n",
    "    int mid = (start + end) / 2;\n",
    "    data[mid] = depth;  // Mark with depth\n",
    "    \n",
    "    // Always recurse - no base case!\n",
    "    if (mid > start) {\n",
    "        naiveRecursive<<<1, 1>>>(data, start, mid, depth + 1);\n",
    "    }\n",
    "    if (mid + 1 < end) {\n",
    "        naiveRecursive<<<1, 1>>>(data, mid + 1, end, depth + 1);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "// Optimized: Base case threshold\n",
    "#define BASE_THRESHOLD 16\n",
    "\n",
    "__device__ void iterativeProcess(int* data, int start, int end, int depth) {\n",
    "    for (int i = start; i < end; i++) {\n",
    "        data[i] = depth;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void optimizedRecursive(int* data, int start, int end, int depth) {\n",
    "    if (start >= end) return;\n",
    "    \n",
    "    // Base case: process iteratively\n",
    "    if (end - start <= BASE_THRESHOLD) {\n",
    "        iterativeProcess(data, start, end, depth);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    int mid = (start + end) / 2;\n",
    "    data[mid] = depth;\n",
    "    \n",
    "    // Create streams for concurrent execution\n",
    "    cudaStream_t leftStream, rightStream;\n",
    "    cudaStreamCreateWithFlags(&leftStream, cudaStreamNonBlocking);\n",
    "    cudaStreamCreateWithFlags(&rightStream, cudaStreamNonBlocking);\n",
    "    \n",
    "    if (mid > start) {\n",
    "        optimizedRecursive<<<1, 1, 0, leftStream>>>(data, start, mid, depth + 1);\n",
    "    }\n",
    "    if (mid + 1 < end) {\n",
    "        optimizedRecursive<<<1, 1, 0, rightStream>>>(data, mid + 1, end, depth + 1);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    cudaStreamDestroy(leftStream);\n",
    "    cudaStreamDestroy(rightStream);\n",
    "}\n",
    "\n",
    "void exercise1_base_case() {\n",
    "    printf(\"\\n=== Exercise 1: Naive vs Optimized Base Case ===\\n\");\n",
    "    \n",
    "    const int N = 512;  // Small to avoid too many launches\n",
    "    int *d_data1, *d_data2;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_data1, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_data2, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_data1, 0, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_data2, 0, N * sizeof(int)));\n",
    "    \n",
    "    // Time naive version\n",
    "    cudaEvent_t start1, stop1;\n",
    "    cudaEventCreate(&start1);\n",
    "    cudaEventCreate(&stop1);\n",
    "    \n",
    "    cudaEventRecord(start1);\n",
    "    naiveRecursive<<<1, 1>>>(d_data1, 0, N, 0);\n",
    "    cudaEventRecord(stop1);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    float naiveTime;\n",
    "    cudaEventElapsedTime(&naiveTime, start1, stop1);\n",
    "    \n",
    "    // Time optimized version\n",
    "    cudaEvent_t start2, stop2;\n",
    "    cudaEventCreate(&start2);\n",
    "    cudaEventCreate(&stop2);\n",
    "    \n",
    "    cudaEventRecord(start2);\n",
    "    optimizedRecursive<<<1, 1>>>(d_data2, 0, N, 0);\n",
    "    cudaEventRecord(stop2);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    float optimizedTime;\n",
    "    cudaEventElapsedTime(&optimizedTime, start2, stop2);\n",
    "    \n",
    "    printf(\"Naive time:     %.3f ms\\n\", naiveTime);\n",
    "    printf(\"Optimized time: %.3f ms\\n\", optimizedTime);\n",
    "    printf(\"Speedup: %.2fx\\n\", naiveTime / optimizedTime);\n",
    "    \n",
    "    cudaFree(d_data1);\n",
    "    cudaFree(d_data2);\n",
    "    cudaEventDestroy(start1);\n",
    "    cudaEventDestroy(stop1);\n",
    "    cudaEventDestroy(start2);\n",
    "    cudaEventDestroy(stop2);\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Batching Child Launches\n",
    "// =============================================================================\n",
    "\n",
    "// Worker kernel for batch processing\n",
    "__global__ void batchWorkerKernel(int* data, int* offsets, int* sizes, int numTasks) {\n",
    "    int taskId = blockIdx.x;\n",
    "    if (taskId >= numTasks) return;\n",
    "    \n",
    "    int offset = offsets[taskId];\n",
    "    int size = sizes[taskId];\n",
    "    \n",
    "    // Process this task's data\n",
    "    for (int i = threadIdx.x; i < size; i += blockDim.x) {\n",
    "        data[offset + i] = taskId * 100 + i;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Naive: One launch per task\n",
    "__global__ void naiveLauncher(int* data, int* offsets, int* sizes, int numTasks) {\n",
    "    for (int i = 0; i < numTasks; i++) {\n",
    "        int size = sizes[i];\n",
    "        int blocks = (size + 127) / 128;\n",
    "        // Simulated work - in reality this launches many small kernels\n",
    "        batchWorkerKernel<<<blocks, 128>>>(data, offsets, sizes, 1);\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized: Single batched launch\n",
    "__global__ void batchedLauncher(int* data, int* offsets, int* sizes, int numTasks) {\n",
    "    if (threadIdx.x == 0) {\n",
    "        // Single launch handles all tasks!\n",
    "        batchWorkerKernel<<<numTasks, 128>>>(data, offsets, sizes, numTasks);\n",
    "        cudaDeviceSynchronize();\n",
    "        printf(\"Batched: Processed %d tasks in 1 launch\\n\", numTasks);\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise2_batching() {\n",
    "    printf(\"\\n=== Exercise 2: Batching Child Launches ===\\n\");\n",
    "    \n",
    "    const int NUM_TASKS = 50;\n",
    "    const int TASK_SIZE = 256;\n",
    "    const int TOTAL = NUM_TASKS * TASK_SIZE;\n",
    "    \n",
    "    int h_offsets[NUM_TASKS], h_sizes[NUM_TASKS];\n",
    "    for (int i = 0; i < NUM_TASKS; i++) {\n",
    "        h_offsets[i] = i * TASK_SIZE;\n",
    "        h_sizes[i] = TASK_SIZE;\n",
    "    }\n",
    "    \n",
    "    int *d_data, *d_offsets, *d_sizes;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, TOTAL * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_offsets, NUM_TASKS * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_sizes, NUM_TASKS * sizeof(int)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_offsets, h_offsets, NUM_TASKS * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_sizes, h_sizes, NUM_TASKS * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Run batched version\n",
    "    batchedLauncher<<<1, 1>>>(d_data, d_offsets, d_sizes, NUM_TASKS);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Verify\n",
    "    int* h_data = new int[TOTAL];\n",
    "    CHECK_CUDA(cudaMemcpy(h_data, d_data, TOTAL * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    printf(\"Sample: data[0]=%d, data[256]=%d, data[512]=%d\\n\",\n",
    "           h_data[0], h_data[256], h_data[512]);\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_offsets);\n",
    "    cudaFree(d_sizes);\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Unified Memory with CDP\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void childWithUnifiedMem(int* uniData, int idx, int value) {\n",
    "    // Child can directly access and modify unified memory\n",
    "    uniData[idx] = value * 2;\n",
    "}\n",
    "\n",
    "__global__ void parentWithUnifiedMem(int* uniData, int n) {\n",
    "    if (threadIdx.x == 0) {\n",
    "        printf(\"Parent: Processing %d elements with unified memory\\n\", n);\n",
    "        \n",
    "        // Launch children that modify unified memory\n",
    "        for (int i = 0; i < n; i++) {\n",
    "            childWithUnifiedMem<<<1, 1>>>(uniData, i, i + 1);\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        // Parent can immediately read children's results\n",
    "        printf(\"Parent: Results ready - uniData[0]=%d, uniData[5]=%d\\n\",\n",
    "               uniData[0], uniData[5]);\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise3_unified_memory() {\n",
    "    printf(\"\\n=== Exercise 3: Unified Memory with CDP ===\\n\");\n",
    "    \n",
    "    const int N = 10;\n",
    "    int* uniData;\n",
    "    \n",
    "    // Allocate unified memory - accessible by both host and device\n",
    "    CHECK_CUDA(cudaMallocManaged(&uniData, N * sizeof(int)));\n",
    "    \n",
    "    // Initialize from host\n",
    "    for (int i = 0; i < N; i++) uniData[i] = 0;\n",
    "    \n",
    "    // Launch parent (which launches children)\n",
    "    parentWithUnifiedMem<<<1, 1>>>(uniData, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Read results from host - no explicit copy needed!\n",
    "    printf(\"Host reading unified memory:\\n\");\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        printf(\"  uniData[%d] = %d\\n\", i, uniData[i]);\n",
    "    }\n",
    "    \n",
    "    cudaFree(uniData);\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 4: Tail Launch Pattern\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void tailLaunchKernel(int* data, int n, int depth) {\n",
    "    // Process current level\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (tid < n) {\n",
    "        data[tid] += 1;  // Increment at each level\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Base case\n",
    "    if (n <= 1 || depth >= 8) {\n",
    "        if (tid == 0) {\n",
    "            printf(\"Tail launch: Reached depth %d\\n\", depth);\n",
    "        }\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Tail launch: parent exits immediately after launching child\n",
    "    // Child inherits parent's resources\n",
    "    if (tid == 0) {\n",
    "        int newN = n / 2;\n",
    "        tailLaunchKernel<<<1, newN>>>(data, newN, depth + 1);\n",
    "        // No cudaDeviceSynchronize() - this is the tail launch pattern!\n",
    "        // Parent will exit and child reuses its resources\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise4_tail_launch() {\n",
    "    printf(\"\\n=== Exercise 4: Tail Launch Pattern ===\\n\");\n",
    "    \n",
    "    const int N = 256;\n",
    "    int *d_data;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_data, 0, N * sizeof(int)));\n",
    "    \n",
    "    tailLaunchKernel<<<1, N>>>(d_data, N, 0);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    int* h_data = new int[N];\n",
    "    CHECK_CUDA(cudaMemcpy(h_data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Elements processed at different depths will have different values\n",
    "    printf(\"Values at different positions:\\n\");\n",
    "    printf(\"  data[0] = %d (processed at all levels)\\n\", h_data[0]);\n",
    "    printf(\"  data[64] = %d (processed at fewer levels)\\n\", h_data[64]);\n",
    "    printf(\"  data[128] = %d (processed at even fewer levels)\\n\", h_data[128]);\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Main\n",
    "// =============================================================================\n",
    "\n",
    "int main() {\n",
    "    printf(\"CDP Optimization Exercises\\n\");\n",
    "    printf(\"==========================\\n\");\n",
    "    \n",
    "    // Set CDP limits\n",
    "    cudaDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, 16);\n",
    "    cudaDeviceSetLimit(cudaLimitDevRuntimePendingLaunchCount, 4096);\n",
    "    \n",
    "    exercise1_base_case();\n",
    "    exercise2_batching();\n",
    "    exercise3_unified_memory();\n",
    "    exercise4_tail_launch();\n",
    "    \n",
    "    printf(\"\\nâœ… All CDP optimization exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -lcudadevrt -o cdp_optimization_exercises cdp_optimization_exercises.cu && ./cdp_optimization_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18c0dc",
   "metadata": {},
   "source": [
    "### ðŸ”¶ Python/Numba Exercises (Alternative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc1bf0",
   "metadata": {},
   "source": [
    "## Week 15 Summary\n",
    "\n",
    "You've learned:\n",
    "- **CDP fundamentals**: Parent-child kernels, memory visibility\n",
    "- **Recursive algorithms**: Quicksort, tree traversal on GPU\n",
    "- **Adaptive parallelism**: Runtime-determined grid sizes\n",
    "- **Optimization**: Tail launch, batching, unified memory\n",
    "\n",
    "## Complete the Checkpoint Quiz!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
