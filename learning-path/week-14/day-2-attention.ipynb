{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf50be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Attention Overview\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q @ K.T / sqrt(d_k)) @ V\n",
    "\n",
    "Shapes:\n",
    "  Q: [batch, heads, seq_len, head_dim]  = [B, H, N, D]\n",
    "  K: [batch, heads, seq_len, head_dim]  = [B, H, N, D]\n",
    "  V: [batch, heads, seq_len, head_dim]  = [B, H, N, D]\n",
    "  \n",
    "  Scores = Q @ K.T:    [B, H, N, N]  ‚Üê O(N¬≤) memory!\n",
    "  Attention @ V:       [B, H, N, D]\n",
    "```\n",
    "\n",
    "### Memory Challenge\n",
    "\n",
    "```\n",
    "For GPT-style model:\n",
    "  Sequence length N = 2048\n",
    "  Batch size B = 32\n",
    "  Heads H = 32\n",
    "  \n",
    "  Attention matrix: B √ó H √ó N √ó N √ó 4 bytes\n",
    "                  = 32 √ó 32 √ó 2048 √ó 2048 √ó 4\n",
    "                  = 17 GB just for attention scores!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab74058",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naive_attention.cu\n",
    "// naive_attention.cu - Basic attention implementation\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "#include <float.h>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "// Softmax kernel (in-place, row-wise)\n",
    "__global__ void softmax_kernel(\n",
    "    float* scores, int rows, int cols\n",
    ") {\n",
    "    int row = blockIdx.x;\n",
    "    if (row >= rows) return;\n",
    "    \n",
    "    float* row_data = scores + row * cols;\n",
    "    \n",
    "    // Find max\n",
    "    float max_val = -FLT_MAX;\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        max_val = fmaxf(max_val, row_data[i]);\n",
    "    }\n",
    "    \n",
    "    // Exp and sum\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        row_data[i] = expf(row_data[i] - max_val);\n",
    "        sum += row_data[i];\n",
    "    }\n",
    "    \n",
    "    // Normalize\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        row_data[i] /= sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Naive Attention Implementation ===\\n\\n\");\n",
    "    \n",
    "    // Small example for correctness\n",
    "    const int batch = 1;\n",
    "    const int heads = 8;\n",
    "    const int seq_len = 64;\n",
    "    const int head_dim = 64;\n",
    "    const float scale = 1.0f / sqrtf(head_dim);\n",
    "    \n",
    "    int total_heads = batch * heads;\n",
    "    \n",
    "    printf(\"Configuration:\\n\");\n",
    "    printf(\"  Batch: %d, Heads: %d\\n\", batch, heads);\n",
    "    printf(\"  Sequence length: %d\\n\", seq_len);\n",
    "    printf(\"  Head dimension: %d\\n\\n\", head_dim);\n",
    "    \n",
    "    // Allocate Q, K, V, Scores, Output\n",
    "    float *d_Q, *d_K, *d_V, *d_scores, *d_output;\n",
    "    CHECK_CUDA(cudaMalloc(&d_Q, total_heads * seq_len * head_dim * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_K, total_heads * seq_len * head_dim * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_V, total_heads * seq_len * head_dim * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_scores, total_heads * seq_len * seq_len * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, total_heads * seq_len * head_dim * sizeof(float)));\n",
    "    \n",
    "    // Initialize with random data\n",
    "    float* h_data = new float[total_heads * seq_len * head_dim];\n",
    "    for (int i = 0; i < total_heads * seq_len * head_dim; i++) {\n",
    "        h_data[i] = (rand() % 1000) / 1000.0f - 0.5f;\n",
    "    }\n",
    "    CHECK_CUDA(cudaMemcpy(d_Q, h_data, total_heads * seq_len * head_dim * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_K, h_data, total_heads * seq_len * head_dim * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_V, h_data, total_heads * seq_len * head_dim * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    cublasHandle_t handle;\n",
    "    cublasCreate(&handle);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    for (int h = 0; h < total_heads; h++) {\n",
    "        float* Q_h = d_Q + h * seq_len * head_dim;\n",
    "        float* K_h = d_K + h * seq_len * head_dim;\n",
    "        float* V_h = d_V + h * seq_len * head_dim;\n",
    "        float* scores_h = d_scores + h * seq_len * seq_len;\n",
    "        float* out_h = d_output + h * seq_len * head_dim;\n",
    "        \n",
    "        // Step 1: Scores = Q @ K.T * scale\n",
    "        float alpha = scale, beta = 0.0f;\n",
    "        cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_N,\n",
    "                   seq_len, seq_len, head_dim,\n",
    "                   &alpha,\n",
    "                   K_h, head_dim,\n",
    "                   Q_h, head_dim,\n",
    "                   &beta,\n",
    "                   scores_h, seq_len);\n",
    "        \n",
    "        // Step 2: Softmax\n",
    "        softmax_kernel<<<seq_len, 1>>>(scores_h, seq_len, seq_len);\n",
    "        \n",
    "        // Step 3: Output = Attention @ V\n",
    "        alpha = 1.0f;\n",
    "        cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "                   head_dim, seq_len, seq_len,\n",
    "                   &alpha,\n",
    "                   V_h, head_dim,\n",
    "                   scores_h, seq_len,\n",
    "                   &beta,\n",
    "                   out_h, head_dim);\n",
    "    }\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    printf(\"Attention computed in %.3f ms\\n\", ms);\n",
    "    \n",
    "    // Memory usage\n",
    "    float scores_mem = total_heads * seq_len * seq_len * 4.0f / 1e6;\n",
    "    printf(\"\\nMemory Analysis:\\n\");\n",
    "    printf(\"  Attention scores: %.2f MB\\n\", scores_mem);\n",
    "    printf(\"  For seq_len=2048: %.2f MB\\n\", \n",
    "           total_heads * 2048.0f * 2048.0f * 4.0f / 1e6);\n",
    "    \n",
    "    // Cleanup\n",
    "    delete[] h_data;\n",
    "    cudaFree(d_Q); cudaFree(d_K); cudaFree(d_V);\n",
    "    cudaFree(d_scores); cudaFree(d_output);\n",
    "    cublasDestroy(handle);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -lcublas -o naive_attention naive_attention.cu\n",
    "!./naive_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2524c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Tiled Attention\n",
    "\n",
    "### Reducing Memory with Tiling\n",
    "\n",
    "```\n",
    "Instead of materializing full N√óN attention matrix:\n",
    "\n",
    "For each query block Q_i:\n",
    "    For each key block K_j:\n",
    "        Compute partial scores S_ij = Q_i @ K_j.T\n",
    "        Update running softmax statistics\n",
    "        Accumulate output contribution\n",
    "        \n",
    "Memory: O(block_size¬≤) instead of O(N¬≤)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162b6cb",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tiled_attention.cu\n",
    "// tiled_attention.cu - Memory-efficient tiled attention\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <float.h>\n",
    "\n",
    "#define BLOCK_SIZE 32\n",
    "\n",
    "// Online softmax update\n",
    "__device__ void update_softmax(\n",
    "    float* m_prev,      // Previous max\n",
    "    float* l_prev,      // Previous sum\n",
    "    float* o_prev,      // Previous output (head_dim values)\n",
    "    float* scores,      // Current block scores (BLOCK_SIZE values)\n",
    "    float* v_block,     // Current V block\n",
    "    int head_dim\n",
    ") {\n",
    "    // Find max in current block\n",
    "    float m_new = *m_prev;\n",
    "    for (int i = 0; i < BLOCK_SIZE; i++) {\n",
    "        m_new = fmaxf(m_new, scores[i]);\n",
    "    }\n",
    "    \n",
    "    // Compute exp and sum for current block\n",
    "    float l_new = 0.0f;\n",
    "    float exp_scores[BLOCK_SIZE];\n",
    "    for (int i = 0; i < BLOCK_SIZE; i++) {\n",
    "        exp_scores[i] = expf(scores[i] - m_new);\n",
    "        l_new += exp_scores[i];\n",
    "    }\n",
    "    \n",
    "    // Update running sum with rescaling\n",
    "    float scale_prev = expf(*m_prev - m_new);\n",
    "    float l_combined = scale_prev * (*l_prev) + l_new;\n",
    "    \n",
    "    // Update output with rescaling\n",
    "    for (int d = 0; d < head_dim; d++) {\n",
    "        float o_scaled = scale_prev * (*l_prev) * o_prev[d];\n",
    "        float v_contribution = 0.0f;\n",
    "        for (int i = 0; i < BLOCK_SIZE; i++) {\n",
    "            v_contribution += exp_scores[i] * v_block[i * head_dim + d];\n",
    "        }\n",
    "        o_prev[d] = (o_scaled + v_contribution) / l_combined;\n",
    "    }\n",
    "    \n",
    "    *m_prev = m_new;\n",
    "    *l_prev = l_combined;\n",
    "}\n",
    "\n",
    "// Simplified tiled attention (single query row)\n",
    "__global__ void tiled_attention_kernel(\n",
    "    float* Q, float* K, float* V, float* output,\n",
    "    int seq_len, int head_dim, float scale\n",
    ") {\n",
    "    int query_idx = blockIdx.x;\n",
    "    \n",
    "    if (query_idx >= seq_len) return;\n",
    "    \n",
    "    // Load query row\n",
    "    extern __shared__ float shared[];\n",
    "    float* q_row = shared;  // head_dim\n",
    "    float* k_block = shared + head_dim;  // BLOCK_SIZE * head_dim\n",
    "    float* v_block = shared + head_dim + BLOCK_SIZE * head_dim;\n",
    "    float* scores = shared + head_dim + 2 * BLOCK_SIZE * head_dim;\n",
    "    \n",
    "    // Each thread helps load Q\n",
    "    int tid = threadIdx.x;\n",
    "    if (tid < head_dim) {\n",
    "        q_row[tid] = Q[query_idx * head_dim + tid];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Running softmax statistics\n",
    "    float m = -FLT_MAX;  // Running max\n",
    "    float l = 0.0f;      // Running sum\n",
    "    float o[64];         // Output accumulator (assuming head_dim <= 64)\n",
    "    for (int d = 0; d < head_dim; d++) o[d] = 0.0f;\n",
    "    \n",
    "    // Process K,V in blocks\n",
    "    int num_blocks = (seq_len + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    for (int block = 0; block < num_blocks; block++) {\n",
    "        int block_start = block * BLOCK_SIZE;\n",
    "        int block_len = min(BLOCK_SIZE, seq_len - block_start);\n",
    "        \n",
    "        // Load K block\n",
    "        for (int i = tid; i < block_len * head_dim; i += blockDim.x) {\n",
    "            int row = i / head_dim;\n",
    "            int col = i % head_dim;\n",
    "            k_block[row * head_dim + col] = K[(block_start + row) * head_dim + col];\n",
    "            v_block[row * head_dim + col] = V[(block_start + row) * head_dim + col];\n",
    "        }\n",
    "        __syncthreads();\n",
    "        \n",
    "        // Compute scores for this block\n",
    "        if (tid < block_len) {\n",
    "            float score = 0.0f;\n",
    "            for (int d = 0; d < head_dim; d++) {\n",
    "                score += q_row[d] * k_block[tid * head_dim + d];\n",
    "            }\n",
    "            scores[tid] = score * scale;\n",
    "        }\n",
    "        __syncthreads();\n",
    "        \n",
    "        // Thread 0 updates softmax (simplified)\n",
    "        if (tid == 0) {\n",
    "            // Find new max\n",
    "            float m_new = m;\n",
    "            for (int i = 0; i < block_len; i++) {\n",
    "                m_new = fmaxf(m_new, scores[i]);\n",
    "            }\n",
    "            \n",
    "            // Compute exp scores and sum\n",
    "            float l_new = 0.0f;\n",
    "            for (int i = 0; i < block_len; i++) {\n",
    "                scores[i] = expf(scores[i] - m_new);\n",
    "                l_new += scores[i];\n",
    "            }\n",
    "            \n",
    "            // Rescale previous output\n",
    "            float scale_prev = expf(m - m_new);\n",
    "            for (int d = 0; d < head_dim; d++) {\n",
    "                o[d] *= scale_prev;\n",
    "            }\n",
    "            \n",
    "            // Add contribution from current block\n",
    "            for (int i = 0; i < block_len; i++) {\n",
    "                for (int d = 0; d < head_dim; d++) {\n",
    "                    o[d] += scores[i] * v_block[i * head_dim + d];\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            // Update statistics\n",
    "            l = scale_prev * l + l_new;\n",
    "            m = m_new;\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write output\n",
    "    if (tid == 0) {\n",
    "        for (int d = 0; d < head_dim; d++) {\n",
    "            output[query_idx * head_dim + d] = o[d] / l;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Tiled Attention (Memory Efficient) ===\\n\\n\");\n",
    "    \n",
    "    const int seq_len = 256;\n",
    "    const int head_dim = 64;\n",
    "    const float scale = 1.0f / sqrtf(head_dim);\n",
    "    \n",
    "    printf(\"Sequence length: %d\\n\", seq_len);\n",
    "    printf(\"Block size: %d\\n\", BLOCK_SIZE);\n",
    "    printf(\"Head dimension: %d\\n\\n\", head_dim);\n",
    "    \n",
    "    float *d_Q, *d_K, *d_V, *d_output;\n",
    "    cudaMalloc(&d_Q, seq_len * head_dim * sizeof(float));\n",
    "    cudaMalloc(&d_K, seq_len * head_dim * sizeof(float));\n",
    "    cudaMalloc(&d_V, seq_len * head_dim * sizeof(float));\n",
    "    cudaMalloc(&d_output, seq_len * head_dim * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_data = new float[seq_len * head_dim];\n",
    "    for (int i = 0; i < seq_len * head_dim; i++) {\n",
    "        h_data[i] = (rand() % 1000) / 1000.0f - 0.5f;\n",
    "    }\n",
    "    cudaMemcpy(d_Q, h_data, seq_len * head_dim * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_K, h_data, seq_len * head_dim * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_V, h_data, seq_len * head_dim * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Shared memory size\n",
    "    int shared_size = (head_dim + 2 * BLOCK_SIZE * head_dim + BLOCK_SIZE) * sizeof(float);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        tiled_attention_kernel<<<seq_len, BLOCK_SIZE, shared_size>>>(\n",
    "            d_Q, d_K, d_V, d_output, seq_len, head_dim, scale\n",
    "        );\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    printf(\"Tiled attention: %.3f ms (100 iterations)\\n\", ms);\n",
    "    printf(\"\\nMemory comparison:\\n\");\n",
    "    printf(\"  Naive (full matrix): %.2f KB\\n\", seq_len * seq_len * 4.0f / 1024);\n",
    "    printf(\"  Tiled (per block):   %.2f KB\\n\", BLOCK_SIZE * BLOCK_SIZE * 4.0f / 1024);\n",
    "    printf(\"  Reduction: %.0fx\\n\", (float)(seq_len * seq_len) / (BLOCK_SIZE * BLOCK_SIZE));\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_Q); cudaFree(d_K); cudaFree(d_V); cudaFree(d_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o tiled_attention tiled_attention.cu\n",
    "!./tiled_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b4ded",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Flash Attention Concepts\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "```\n",
    "Flash Attention improvements over naive:\n",
    "\n",
    "1. Online Softmax\n",
    "   - Compute softmax incrementally\n",
    "   - Track running max and sum\n",
    "   - Rescale accumulated outputs\n",
    "\n",
    "2. IO Awareness\n",
    "   - Minimize HBM ‚Üî SRAM transfers\n",
    "   - Keep Q, K, V tiles in shared memory\n",
    "   - Never materialize full attention matrix\n",
    "\n",
    "3. Tiling Strategy\n",
    "   - Tile Q in outer loop\n",
    "   - Tile K, V in inner loop\n",
    "   - Fuse all operations\n",
    "   \n",
    "Result: 2-4x faster, O(N) memory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5ea2d",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Conceptual Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flash Attention algorithm (conceptual Python)\n",
    "def flash_attention_algorithm():\n",
    "    print(\"Flash Attention Algorithm\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"for each query block Q_i:\")\n",
    "    print(\"    initialize: m_i = -inf, l_i = 0, O_i = 0\")\n",
    "    print(\"    \")\n",
    "    print(\"    for each key/value block (K_j, V_j):\")\n",
    "    print(\"        # Compute local attention scores\")\n",
    "        print(\"        S_ij = Q_i @ K_j.T * scale\")\n",
    "    print(\"        \")\n",
    "    print(\"        # Online softmax update\")\n",
    "    print(\"        m_new = max(m_i, rowmax(S_ij))\")\n",
    "    print(\"        P_ij = exp(S_ij - m_new)\")\n",
    "    print(\"        l_new = exp(m_i - m_new) * l_i + rowsum(P_ij)\")\n",
    "    print(\"        \")\n",
    "    print(\"        # Rescale and accumulate output\")\n",
    "    print(\"        O_i = exp(m_i - m_new) * O_i + P_ij @ V_j\")\n",
    "    print(\"        m_i, l_i = m_new, l_new\")\n",
    "    print(\"    \")\n",
    "    print(\"    # Final normalization\")\n",
    "    print(\"    O_i = O_i / l_i\")\n",
    "\n",
    "flash_attention_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7f962",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Causal Masking\n",
    "\n",
    "Modify the attention to support causal (autoregressive) masking:\n",
    "- Only attend to positions j ‚â§ i\n",
    "- Set masked positions to -infinity before softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a396d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Attention Optimization Techniques\n",
    "\n",
    "| Technique | Memory | Speed | Complexity |\n",
    "|-----------|--------|-------|------------|\n",
    "| Naive | O(N¬≤) | Baseline | Simple |\n",
    "| Tiled | O(B¬≤) | 2x | Moderate |\n",
    "| Flash Attention | O(N) | 2-4x | Complex |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Memory is the bottleneck** for long sequences\n",
    "2. **Online softmax** enables streaming computation\n",
    "3. **Tiling** trades compute for memory\n",
    "4. Use **cuBLAS batched GEMM** for multi-head attention"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
