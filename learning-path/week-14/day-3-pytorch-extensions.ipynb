{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c11c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\", \"torch\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not available - examples will be conceptual\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b5671",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Extension Architecture\n",
    "\n",
    "### PyTorch Extension Structure\n",
    "\n",
    "```\n",
    "my_extension/\n",
    "‚îú‚îÄ‚îÄ setup.py           # Build configuration\n",
    "‚îú‚îÄ‚îÄ my_ext.cpp         # C++ bindings\n",
    "‚îú‚îÄ‚îÄ my_ext_cuda.cu     # CUDA kernels\n",
    "‚îî‚îÄ‚îÄ my_ext/\n",
    "    ‚îî‚îÄ‚îÄ __init__.py    # Python interface\n",
    "    \n",
    "Flow:\n",
    "  Python ‚Üí C++ binding ‚Üí CUDA kernel ‚Üí C++ binding ‚Üí Python\n",
    "```\n",
    "\n",
    "### Required Headers\n",
    "\n",
    "```cpp\n",
    "#include <torch/extension.h>  // Main PyTorch C++ API\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e24d3a",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c44823",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_gelu_cuda.cu\n",
    "// fused_gelu_cuda.cu - Custom CUDA kernel for PyTorch\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// GELU constants\n",
    "#define SQRT_2_PI 0.7978845608028654f\n",
    "#define GELU_COEF 0.044715f\n",
    "\n",
    "// Forward kernel: y = GELU(x + bias)\n",
    "__global__ void fused_gelu_bias_forward_kernel(\n",
    "    const float* __restrict__ input,\n",
    "    const float* __restrict__ bias,\n",
    "    float* __restrict__ output,\n",
    "    int batch_size,\n",
    "    int hidden_size\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * hidden_size;\n",
    "    \n",
    "    if (idx < total) {\n",
    "        int bias_idx = idx % hidden_size;\n",
    "        float x = input[idx] + bias[bias_idx];\n",
    "        \n",
    "        // GELU: 0.5 * x * (1 + tanh(sqrt(2/œÄ) * (x + 0.044715 * x¬≥)))\n",
    "        float x3 = x * x * x;\n",
    "        float inner = SQRT_2_PI * (x + GELU_COEF * x3);\n",
    "        output[idx] = 0.5f * x * (1.0f + tanhf(inner));\n",
    "    }\n",
    "}\n",
    "\n",
    "// Backward kernel for GELU\n",
    "__global__ void fused_gelu_bias_backward_kernel(\n",
    "    const float* __restrict__ grad_output,\n",
    "    const float* __restrict__ input,\n",
    "    const float* __restrict__ bias,\n",
    "    float* __restrict__ grad_input,\n",
    "    float* __restrict__ grad_bias,  // Atomically accumulated\n",
    "    int batch_size,\n",
    "    int hidden_size\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * hidden_size;\n",
    "    \n",
    "    if (idx < total) {\n",
    "        int bias_idx = idx % hidden_size;\n",
    "        float x = input[idx] + bias[bias_idx];\n",
    "        float g = grad_output[idx];\n",
    "        \n",
    "        // GELU derivative\n",
    "        float x3 = x * x * x;\n",
    "        float inner = SQRT_2_PI * (x + GELU_COEF * x3);\n",
    "        float tanh_inner = tanhf(inner);\n",
    "        float sech2 = 1.0f - tanh_inner * tanh_inner;\n",
    "        \n",
    "        float gelu = 0.5f * x * (1.0f + tanh_inner);\n",
    "        float dgelu_dx = 0.5f * (1.0f + tanh_inner) + \n",
    "                         0.5f * x * sech2 * SQRT_2_PI * (1.0f + 3.0f * GELU_COEF * x * x);\n",
    "        \n",
    "        grad_input[idx] = g * dgelu_dx;\n",
    "        atomicAdd(&grad_bias[bias_idx], g * dgelu_dx);\n",
    "    }\n",
    "}\n",
    "\n",
    "// C++ wrapper functions\n",
    "torch::Tensor fused_gelu_bias_forward(\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor bias\n",
    ") {\n",
    "    const int batch_size = input.size(0);\n",
    "    const int hidden_size = input.size(1);\n",
    "    \n",
    "    auto output = torch::empty_like(input);\n",
    "    \n",
    "    const int threads = 256;\n",
    "    const int blocks = (batch_size * hidden_size + threads - 1) / threads;\n",
    "    \n",
    "    fused_gelu_bias_forward_kernel<<<blocks, threads>>>(\n",
    "        input.data_ptr<float>(),\n",
    "        bias.data_ptr<float>(),\n",
    "        output.data_ptr<float>(),\n",
    "        batch_size,\n",
    "        hidden_size\n",
    "    );\n",
    "    \n",
    "    return output;\n",
    "}\n",
    "\n",
    "std::vector<torch::Tensor> fused_gelu_bias_backward(\n",
    "    torch::Tensor grad_output,\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor bias\n",
    ") {\n",
    "    const int batch_size = input.size(0);\n",
    "    const int hidden_size = input.size(1);\n",
    "    \n",
    "    auto grad_input = torch::empty_like(input);\n",
    "    auto grad_bias = torch::zeros_like(bias);\n",
    "    \n",
    "    const int threads = 256;\n",
    "    const int blocks = (batch_size * hidden_size + threads - 1) / threads;\n",
    "    \n",
    "    fused_gelu_bias_backward_kernel<<<blocks, threads>>>(\n",
    "        grad_output.data_ptr<float>(),\n",
    "        input.data_ptr<float>(),\n",
    "        bias.data_ptr<float>(),\n",
    "        grad_input.data_ptr<float>(),\n",
    "        grad_bias.data_ptr<float>(),\n",
    "        batch_size,\n",
    "        hidden_size\n",
    "    );\n",
    "    \n",
    "    return {grad_input, grad_bias};\n",
    "}\n",
    "\n",
    "// Python bindings\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"forward\", &fused_gelu_bias_forward, \"Fused GELU+Bias forward\");\n",
    "    m.def(\"backward\", &fused_gelu_bias_backward, \"Fused GELU+Bias backward\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca88d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Build Configuration\n",
    "\n",
    "### setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup\n",
    "from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n",
    "\n",
    "setup(\n",
    "    name='fused_gelu',\n",
    "    ext_modules=[\n",
    "        CUDAExtension(\n",
    "            name='fused_gelu_cuda',\n",
    "            sources=['fused_gelu_cuda.cu'],\n",
    "            extra_compile_args={\n",
    "                'cxx': ['-O3'],\n",
    "                'nvcc': ['-O3', '-arch=sm_75']\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    cmdclass={'build_ext': BuildExtension}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b46e3",
   "metadata": {},
   "source": [
    "### JIT Compilation (Alternative)\n",
    "\n",
    "```python\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "# Compile at runtime\n",
    "fused_gelu = load(\n",
    "    name='fused_gelu',\n",
    "    sources=['fused_gelu_cuda.cu'],\n",
    "    extra_cuda_cflags=['-O3', '-arch=sm_75']\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51c88e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Autograd Integration\n",
    "\n",
    "### Custom autograd.Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_gelu_module.py\n",
    "# fused_gelu_module.py - Python wrapper with autograd\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "# Import the compiled extension\n",
    "try:\n",
    "    import fused_gelu_cuda\n",
    "except ImportError:\n",
    "    fused_gelu_cuda = None\n",
    "    print(\"Warning: fused_gelu_cuda not compiled\")\n",
    "\n",
    "class FusedGELUBias(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bias):\n",
    "        # Save for backward\n",
    "        ctx.save_for_backward(input, bias)\n",
    "        \n",
    "        if fused_gelu_cuda:\n",
    "            output = fused_gelu_cuda.forward(input, bias)\n",
    "        else:\n",
    "            # Fallback to PyTorch\n",
    "            x = input + bias\n",
    "            output = 0.5 * x * (1 + torch.tanh(\n",
    "                0.7978845608 * (x + 0.044715 * x ** 3)\n",
    "            ))\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, bias = ctx.saved_tensors\n",
    "        \n",
    "        if fused_gelu_cuda:\n",
    "            grad_input, grad_bias = fused_gelu_cuda.backward(\n",
    "                grad_output.contiguous(), input, bias\n",
    "            )\n",
    "        else:\n",
    "            # Fallback implementation\n",
    "            x = input + bias\n",
    "            x3 = x ** 3\n",
    "            inner = 0.7978845608 * (x + 0.044715 * x3)\n",
    "            tanh_inner = torch.tanh(inner)\n",
    "            sech2 = 1 - tanh_inner ** 2\n",
    "            dgelu = 0.5 * (1 + tanh_inner) + \\\n",
    "                    0.5 * x * sech2 * 0.7978845608 * (1 + 3 * 0.044715 * x ** 2)\n",
    "            grad_input = grad_output * dgelu\n",
    "            grad_bias = grad_input.sum(dim=0)\n",
    "        \n",
    "        return grad_input, grad_bias\n",
    "\n",
    "class FusedGELUBiasModule(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(hidden_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return FusedGELUBias.apply(x, self.bias)\n",
    "\n",
    "# Test if running directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test\n",
    "    x = torch.randn(32, 768, device='cuda', requires_grad=True)\n",
    "    module = FusedGELUBiasModule(768).cuda()\n",
    "    \n",
    "    y = module(x)\n",
    "    loss = y.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Input: {x.shape}\")\n",
    "    print(f\"Output: {y.shape}\")\n",
    "    print(f\"Gradient: {x.grad.shape}\")\n",
    "    print(\"‚úì Autograd integration working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ceb57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Half Precision Support\n",
    "\n",
    "### Adding FP16 Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_gelu_fp16.cu\n",
    "// fused_gelu_fp16.cu - Half precision variant\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "#define SQRT_2_PI 0.7978845608028654f\n",
    "#define GELU_COEF 0.044715f\n",
    "\n",
    "// Half precision forward kernel\n",
    "__global__ void fused_gelu_bias_forward_fp16(\n",
    "    const half* __restrict__ input,\n",
    "    const half* __restrict__ bias,\n",
    "    half* __restrict__ output,\n",
    "    int batch_size,\n",
    "    int hidden_size\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * hidden_size;\n",
    "    \n",
    "    if (idx < total) {\n",
    "        int bias_idx = idx % hidden_size;\n",
    "        \n",
    "        // Compute in FP32 for accuracy\n",
    "        float x = __half2float(input[idx]) + __half2float(bias[bias_idx]);\n",
    "        \n",
    "        float x3 = x * x * x;\n",
    "        float inner = SQRT_2_PI * (x + GELU_COEF * x3);\n",
    "        float result = 0.5f * x * (1.0f + tanhf(inner));\n",
    "        \n",
    "        output[idx] = __float2half(result);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Vectorized version using half2 for 2x throughput\n",
    "__global__ void fused_gelu_bias_forward_fp16_vec2(\n",
    "    const half2* __restrict__ input,\n",
    "    const half2* __restrict__ bias,\n",
    "    half2* __restrict__ output,\n",
    "    int num_pairs\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (idx < num_pairs) {\n",
    "        half2 x = __hadd2(input[idx], bias[idx % (num_pairs / 2)]);\n",
    "        \n",
    "        // Convert to float2 for computation\n",
    "        float2 xf = __half22float2(x);\n",
    "        \n",
    "        // GELU for first element\n",
    "        float x1_3 = xf.x * xf.x * xf.x;\n",
    "        float inner1 = SQRT_2_PI * (xf.x + GELU_COEF * x1_3);\n",
    "        float result1 = 0.5f * xf.x * (1.0f + tanhf(inner1));\n",
    "        \n",
    "        // GELU for second element\n",
    "        float x2_3 = xf.y * xf.y * xf.y;\n",
    "        float inner2 = SQRT_2_PI * (xf.y + GELU_COEF * x2_3);\n",
    "        float result2 = 0.5f * xf.y * (1.0f + tanhf(inner2));\n",
    "        \n",
    "        output[idx] = __floats2half2_rn(result1, result2);\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor fused_gelu_bias_forward_half(\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor bias\n",
    ") {\n",
    "    TORCH_CHECK(input.dtype() == torch::kHalf, \"Input must be FP16\");\n",
    "    \n",
    "    const int batch_size = input.size(0);\n",
    "    const int hidden_size = input.size(1);\n",
    "    const int total = batch_size * hidden_size;\n",
    "    \n",
    "    auto output = torch::empty_like(input);\n",
    "    \n",
    "    // Use vectorized version if aligned\n",
    "    if (total % 2 == 0) {\n",
    "        const int threads = 256;\n",
    "        const int blocks = (total / 2 + threads - 1) / threads;\n",
    "        \n",
    "        fused_gelu_bias_forward_fp16_vec2<<<blocks, threads>>>(\n",
    "            reinterpret_cast<half2*>(input.data_ptr<at::Half>()),\n",
    "            reinterpret_cast<half2*>(bias.data_ptr<at::Half>()),\n",
    "            reinterpret_cast<half2*>(output.data_ptr<at::Half>()),\n",
    "            total / 2\n",
    "        );\n",
    "    } else {\n",
    "        const int threads = 256;\n",
    "        const int blocks = (total + threads - 1) / threads;\n",
    "        \n",
    "        fused_gelu_bias_forward_fp16<<<blocks, threads>>>(\n",
    "            reinterpret_cast<half*>(input.data_ptr<at::Half>()),\n",
    "            reinterpret_cast<half*>(bias.data_ptr<at::Half>()),\n",
    "            reinterpret_cast<half*>(output.data_ptr<at::Half>()),\n",
    "            batch_size,\n",
    "            hidden_size\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    return output;\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"forward_half\", &fused_gelu_bias_forward_half, \"Fused GELU+Bias FP16\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bea99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70566753",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytorch_ext_exercises.cu\n",
    "// CUDA C++ Exercises - PyTorch Extensions\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Exercise 1: Implement a custom activation function kernel\n",
    "// TODO: Add your implementation here\n",
    "\n",
    "// Exercise 2: Add FP16 support with half2 vectorization\n",
    "// TODO: Add your implementation here\n",
    "\n",
    "// Exercise 3: Create backward pass for autograd support\n",
    "// TODO: Add your implementation here\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== PyTorch Extension Exercises ===\\n\");\n",
    "    printf(\"Implement the exercises above and run!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc48d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o pytorch_ext_exercises pytorch_ext_exercises.cu && ./pytorch_ext_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe7285",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises use Python and Numba for rapid prototyping. Complete the CUDA C++ exercises above first for the primary learning objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b89550",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### PyTorch Extension Workflow\n",
    "\n",
    "1. **Write CUDA kernels** in `.cu` files\n",
    "2. **Create C++ bindings** with `torch/extension.h`\n",
    "3. **Build** with `setup.py` or JIT compilation\n",
    "4. **Wrap in autograd.Function** for training support\n",
    "5. **Create nn.Module** for easy use\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **Do**:\n",
    "- Support both FP32 and FP16\n",
    "- Add proper error checking with `TORCH_CHECK`\n",
    "- Provide Python fallback for debugging\n",
    "- Use vectorized operations (half2, float4)\n",
    "\n",
    "‚ùå **Don't**:\n",
    "- Forget to make tensors contiguous\n",
    "- Ignore CUDA errors\n",
    "- Hardcode block sizes for all GPUs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
