{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711e053a",
   "metadata": {},
   "source": [
    "# Day 3: PyTorch Extensions â€“ Building Custom Power Tools\n",
    "\n",
    "> **Hook:** PyTorch is amazing, but what if your custom operation is 10x slower than it needs to be? Imagine being a chef with only pre-made frozen mealsâ€”you can make good dishes, but for that signature creation, you need to cook from scratch. PyTorch extensions let you write custom CUDA kernels that integrate seamlessly with autograd, giving you both raw performance AND automatic differentiation!\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. **Create** a custom PyTorch extension with CUDA kernels\n",
    "2. **Implement** both forward and backward passes for gradient support\n",
    "3. **Build** extensions using both `setup.py` and JIT compilation\n",
    "4. **Debug** and test custom extensions for correctness and performance\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "- CUDA kernel development experience\n",
    "- Basic PyTorch autograd understanding\n",
    "- C++ fundamentals (pointers, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c11c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\", \"torch\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not available - examples will be conceptual\")\n",
    "\n",
    "print(\"\\nâš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd58637",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ´ Concept Card: The LEGO Power Functions Analogy\n",
    "\n",
    "### PyTorch Extensions = Custom LEGO Motors\n",
    "\n",
    "**Standard PyTorch (Regular LEGO):**\n",
    "```\n",
    "ğŸ§± Pre-built blocks: nn.Linear, nn.Conv2d, nn.ReLU\n",
    "ğŸ”§ Easy to assemble, works great for standard builds\n",
    "ğŸ“¦ Limited by what's in the box\n",
    "```\n",
    "\n",
    "**PyTorch Extensions (Custom Power Functions):**\n",
    "```\n",
    "âš¡ Design your own motorized components\n",
    "ğŸ”Œ Plugs into standard LEGO system seamlessly\n",
    "ğŸï¸ Can be 10-100x faster for specialized tasks\n",
    "ğŸ”™ Autograd integration = motor works in reverse too!\n",
    "```\n",
    "\n",
    "**The Architecture:**\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Python: my_fused_gelu(x)                   â”‚  â† User-friendly API\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  C++: torch/extension.h bindings            â”‚  â† Type checking, dispatch\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  CUDA: fused_gelu_kernel<<<>>>              â”‚  â† Raw GPU performance\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**The GPU Translation:**\n",
    "| LEGO | PyTorch Extension |\n",
    "|------|-------------------|\n",
    "| Standard blocks | Built-in torch.nn |\n",
    "| Custom motors | CUDA kernels |\n",
    "| Power Functions connector | pybind11 bindings |\n",
    "| Works forward & backward | autograd.Function |\n",
    "\n",
    "**ğŸ’¡ Key Insight:** Extensions give you the performance of raw CUDA with the convenience of PyTorch's ecosystemâ€”like getting Formula 1 speeds while still using the same fuel station as everyone else!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b5671",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Extension Architecture\n",
    "\n",
    "### PyTorch Extension Structure\n",
    "\n",
    "```\n",
    "my_extension/\n",
    "â”œâ”€â”€ setup.py           # Build configuration\n",
    "â”œâ”€â”€ my_ext.cpp         # C++ bindings\n",
    "â”œâ”€â”€ my_ext_cuda.cu     # CUDA kernels\n",
    "â””â”€â”€ my_ext/\n",
    "    â””â”€â”€ __init__.py    # Python interface\n",
    "    \n",
    "Flow:\n",
    "  Python â†’ C++ binding â†’ CUDA kernel â†’ C++ binding â†’ Python\n",
    "```\n",
    "\n",
    "### Required Headers\n",
    "\n",
    "```cpp\n",
    "#include <torch/extension.h>  // Main PyTorch C++ API\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e24d3a",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c44823",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_gelu_cuda.cu\n",
    "// fused_gelu_cuda.cu - Custom CUDA kernel for PyTorch\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// GELU constants\n",
    "#define SQRT_2_PI 0.7978845608028654f\n",
    "#define GELU_COEF 0.044715f\n",
    "\n",
    "// Forward kernel: y = GELU(x + bias)\n",
    "__global__ void fused_gelu_bias_forward_kernel(\n",
    "    const float* __restrict__ input,\n",
    "    const float* __restrict__ bias,\n",
    "    float* __restrict__ output,\n",
    "    int batch_size,\n",
    "    int hidden_size\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * hidden_size;\n",
    "    \n",
    "    if (idx < total) {\n",
    "        int bias_idx = idx % hidden_size;\n",
    "        float x = input[idx] + bias[bias_idx];\n",
    "        \n",
    "        // GELU: 0.5 * x * (1 + tanh(sqrt(2/Ï€) * (x + 0.044715 * xÂ³)))\n",
    "        float x3 = x * x * x;\n",
    "        float inner = SQRT_2_PI * (x + GELU_COEF * x3);\n",
    "        output[idx] = 0.5f * x * (1.0f + tanhf(inner));\n",
    "    }\n",
    "}\n",
    "\n",
    "// Backward kernel for GELU\n",
    "__global__ void fused_gelu_bias_backward_kernel(\n",
    "    const float* __restrict__ grad_output,\n",
    "    const float* __restrict__ input,\n",
    "    const float* __restrict__ bias,\n",
    "    float* __restrict__ grad_input,\n",
    "    float* __restrict__ grad_bias,  // Atomically accumulated\n",
    "    int batch_size,\n",
    "    int hidden_size\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * hidden_size;\n",
    "    \n",
    "    if (idx < total) {\n",
    "        int bias_idx = idx % hidden_size;\n",
    "        float x = input[idx] + bias[bias_idx];\n",
    "        float g = grad_output[idx];\n",
    "        \n",
    "        // GELU derivative\n",
    "        float x3 = x * x * x;\n",
    "        float inner = SQRT_2_PI * (x + GELU_COEF * x3);\n",
    "        float tanh_inner = tanhf(inner);\n",
    "        float sech2 = 1.0f - tanh_inner * tanh_inner;\n",
    "        \n",
    "        float gelu = 0.5f * x * (1.0f + tanh_inner);\n",
    "        float dgelu_dx = 0.5f * (1.0f + tanh_inner) + \n",
    "                         0.5f * x * sech2 * SQRT_2_PI * (1.0f + 3.0f * GELU_COEF * x * x);\n",
    "        \n",
    "        grad_input[idx] = g * dgelu_dx;\n",
    "        atomicAdd(&grad_bias[bias_idx], g * dgelu_dx);\n",
    "    }\n",
    "}\n",
    "\n",
    "// C++ wrapper functions\n",
    "torch::Tensor fused_gelu_bias_forward(\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor bias\n",
    ") {\n",
    "    const int batch_size = input.size(0);\n",
    "    const int hidden_size = input.size(1);\n",
    "    \n",
    "    auto output = torch::empty_like(input);\n",
    "    \n",
    "    const int threads = 256;\n",
    "    const int blocks = (batch_size * hidden_size + threads - 1) / threads;\n",
    "    \n",
    "    fused_gelu_bias_forward_kernel<<<blocks, threads>>>(\n",
    "        input.data_ptr<float>(),\n",
    "        bias.data_ptr<float>(),\n",
    "        output.data_ptr<float>(),\n",
    "        batch_size,\n",
    "        hidden_size\n",
    "    );\n",
    "    \n",
    "    return output;\n",
    "}\n",
    "\n",
    "std::vector<torch::Tensor> fused_gelu_bias_backward(\n",
    "    torch::Tensor grad_output,\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor bias\n",
    ") {\n",
    "    const int batch_size = input.size(0);\n",
    "    const int hidden_size = input.size(1);\n",
    "    \n",
    "    auto grad_input = torch::empty_like(input);\n",
    "    auto grad_bias = torch::zeros_like(bias);\n",
    "    \n",
    "    const int threads = 256;\n",
    "    const int blocks = (batch_size * hidden_size + threads - 1) / threads;\n",
    "    \n",
    "    fused_gelu_bias_backward_kernel<<<blocks, threads>>>(\n",
    "        grad_output.data_ptr<float>(),\n",
    "        input.data_ptr<float>(),\n",
    "        bias.data_ptr<float>(),\n",
    "        grad_input.data_ptr<float>(),\n",
    "        grad_bias.data_ptr<float>(),\n",
    "        batch_size,\n",
    "        hidden_size\n",
    "    );\n",
    "    \n",
    "    return {grad_input, grad_bias};\n",
    "}\n",
    "\n",
    "// Python bindings\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"forward\", &fused_gelu_bias_forward, \"Fused GELU+Bias forward\");\n",
    "    m.def(\"backward\", &fused_gelu_bias_backward, \"Fused GELU+Bias backward\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca88d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Build Configuration\n",
    "\n",
    "### setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup\n",
    "from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n",
    "\n",
    "setup(\n",
    "    name='fused_gelu',\n",
    "    ext_modules=[\n",
    "        CUDAExtension(\n",
    "            name='fused_gelu_cuda',\n",
    "            sources=['fused_gelu_cuda.cu'],\n",
    "            extra_compile_args={\n",
    "                'cxx': ['-O3'],\n",
    "                'nvcc': ['-O3', '-arch=sm_75']\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    cmdclass={'build_ext': BuildExtension}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b46e3",
   "metadata": {},
   "source": [
    "### JIT Compilation (Alternative)\n",
    "\n",
    "```python\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "# Compile at runtime\n",
    "fused_gelu = load(\n",
    "    name='fused_gelu',\n",
    "    sources=['fused_gelu_cuda.cu'],\n",
    "    extra_cuda_cflags=['-O3', '-arch=sm_75']\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51c88e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Autograd Integration\n",
    "\n",
    "### Custom autograd.Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_gelu_module.py\n",
    "# fused_gelu_module.py - Python wrapper with autograd\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "# Import the compiled extension\n",
    "try:\n",
    "    import fused_gelu_cuda\n",
    "except ImportError:\n",
    "    fused_gelu_cuda = None\n",
    "    print(\"Warning: fused_gelu_cuda not compiled\")\n",
    "\n",
    "class FusedGELUBias(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bias):\n",
    "        # Save for backward\n",
    "        ctx.save_for_backward(input, bias)\n",
    "        \n",
    "        if fused_gelu_cuda:\n",
    "            output = fused_gelu_cuda.forward(input, bias)\n",
    "        else:\n",
    "            # Fallback to PyTorch\n",
    "            x = input + bias\n",
    "            output = 0.5 * x * (1 + torch.tanh(\n",
    "                0.7978845608 * (x + 0.044715 * x ** 3)\n",
    "            ))\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, bias = ctx.saved_tensors\n",
    "        \n",
    "        if fused_gelu_cuda:\n",
    "            grad_input, grad_bias = fused_gelu_cuda.backward(\n",
    "                grad_output.contiguous(), input, bias\n",
    "            )\n",
    "        else:\n",
    "            # Fallback implementation\n",
    "            x = input + bias\n",
    "            x3 = x ** 3\n",
    "            inner = 0.7978845608 * (x + 0.044715 * x3)\n",
    "            tanh_inner = torch.tanh(inner)\n",
    "            sech2 = 1 - tanh_inner ** 2\n",
    "            dgelu = 0.5 * (1 + tanh_inner) + \\\n",
    "                    0.5 * x * sech2 * 0.7978845608 * (1 + 3 * 0.044715 * x ** 2)\n",
    "            grad_input = grad_output * dgelu\n",
    "            grad_bias = grad_input.sum(dim=0)\n",
    "        \n",
    "        return grad_input, grad_bias\n",
    "\n",
    "class FusedGELUBiasModule(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(hidden_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return FusedGELUBias.apply(x, self.bias)\n",
    "\n",
    "# Test if running directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test\n",
    "    x = torch.randn(32, 768, device='cuda', requires_grad=True)\n",
    "    module = FusedGELUBiasModule(768).cuda()\n",
    "    \n",
    "    y = module(x)\n",
    "    loss = y.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Input: {x.shape}\")\n",
    "    print(f\"Output: {y.shape}\")\n",
    "    print(f\"Gradient: {x.grad.shape}\")\n",
    "    print(\"âœ“ Autograd integration working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ceb57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Half Precision Support\n",
    "\n",
    "### Adding FP16 Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_gelu_fp16.cu\n",
    "// fused_gelu_fp16.cu - Half precision variant\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "#define SQRT_2_PI 0.7978845608028654f\n",
    "#define GELU_COEF 0.044715f\n",
    "\n",
    "// Half precision forward kernel\n",
    "__global__ void fused_gelu_bias_forward_fp16(\n",
    "    const half* __restrict__ input,\n",
    "    const half* __restrict__ bias,\n",
    "    half* __restrict__ output,\n",
    "    int batch_size,\n",
    "    int hidden_size\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = batch_size * hidden_size;\n",
    "    \n",
    "    if (idx < total) {\n",
    "        int bias_idx = idx % hidden_size;\n",
    "        \n",
    "        // Compute in FP32 for accuracy\n",
    "        float x = __half2float(input[idx]) + __half2float(bias[bias_idx]);\n",
    "        \n",
    "        float x3 = x * x * x;\n",
    "        float inner = SQRT_2_PI * (x + GELU_COEF * x3);\n",
    "        float result = 0.5f * x * (1.0f + tanhf(inner));\n",
    "        \n",
    "        output[idx] = __float2half(result);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Vectorized version using half2 for 2x throughput\n",
    "__global__ void fused_gelu_bias_forward_fp16_vec2(\n",
    "    const half2* __restrict__ input,\n",
    "    const half2* __restrict__ bias,\n",
    "    half2* __restrict__ output,\n",
    "    int num_pairs\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (idx < num_pairs) {\n",
    "        half2 x = __hadd2(input[idx], bias[idx % (num_pairs / 2)]);\n",
    "        \n",
    "        // Convert to float2 for computation\n",
    "        float2 xf = __half22float2(x);\n",
    "        \n",
    "        // GELU for first element\n",
    "        float x1_3 = xf.x * xf.x * xf.x;\n",
    "        float inner1 = SQRT_2_PI * (xf.x + GELU_COEF * x1_3);\n",
    "        float result1 = 0.5f * xf.x * (1.0f + tanhf(inner1));\n",
    "        \n",
    "        // GELU for second element\n",
    "        float x2_3 = xf.y * xf.y * xf.y;\n",
    "        float inner2 = SQRT_2_PI * (xf.y + GELU_COEF * x2_3);\n",
    "        float result2 = 0.5f * xf.y * (1.0f + tanhf(inner2));\n",
    "        \n",
    "        output[idx] = __floats2half2_rn(result1, result2);\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor fused_gelu_bias_forward_half(\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor bias\n",
    ") {\n",
    "    TORCH_CHECK(input.dtype() == torch::kHalf, \"Input must be FP16\");\n",
    "    \n",
    "    const int batch_size = input.size(0);\n",
    "    const int hidden_size = input.size(1);\n",
    "    const int total = batch_size * hidden_size;\n",
    "    \n",
    "    auto output = torch::empty_like(input);\n",
    "    \n",
    "    // Use vectorized version if aligned\n",
    "    if (total % 2 == 0) {\n",
    "        const int threads = 256;\n",
    "        const int blocks = (total / 2 + threads - 1) / threads;\n",
    "        \n",
    "        fused_gelu_bias_forward_fp16_vec2<<<blocks, threads>>>(\n",
    "            reinterpret_cast<half2*>(input.data_ptr<at::Half>()),\n",
    "            reinterpret_cast<half2*>(bias.data_ptr<at::Half>()),\n",
    "            reinterpret_cast<half2*>(output.data_ptr<at::Half>()),\n",
    "            total / 2\n",
    "        );\n",
    "    } else {\n",
    "        const int threads = 256;\n",
    "        const int blocks = (total + threads - 1) / threads;\n",
    "        \n",
    "        fused_gelu_bias_forward_fp16<<<blocks, threads>>>(\n",
    "            reinterpret_cast<half*>(input.data_ptr<at::Half>()),\n",
    "            reinterpret_cast<half*>(bias.data_ptr<at::Half>()),\n",
    "            reinterpret_cast<half*>(output.data_ptr<at::Half>()),\n",
    "            batch_size,\n",
    "            hidden_size\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    return output;\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"forward_half\", &fused_gelu_bias_forward_half, \"Fused GELU+Bias FP16\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bea99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70566753",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytorch_ext_exercises.cu\n",
    "// CUDA C++ Exercises - PyTorch Extensions\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Exercise 1: Implement a custom activation function kernel\n",
    "// TODO: Add your implementation here\n",
    "\n",
    "// Exercise 2: Add FP16 support with half2 vectorization\n",
    "// TODO: Add your implementation here\n",
    "\n",
    "// Exercise 3: Create backward pass for autograd support\n",
    "// TODO: Add your implementation here\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== PyTorch Extension Exercises ===\\n\");\n",
    "    printf(\"Implement the exercises above and run!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc48d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o pytorch_ext_exercises pytorch_ext_exercises.cu && ./pytorch_ext_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe7285",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises use Python and Numba for rapid prototyping. Complete the CUDA C++ exercises above first for the primary learning objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b89550",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Summary & Key Takeaways\n",
    "\n",
    "### PyTorch Extension Workflow\n",
    "\n",
    "| Step | Action | Key File |\n",
    "|------|--------|----------|\n",
    "| 1 | Write CUDA kernels | `*.cu` |\n",
    "| 2 | Create C++ bindings | `torch/extension.h` |\n",
    "| 3 | Build (setup.py or JIT) | `setup.py` / inline |\n",
    "| 4 | Wrap in autograd.Function | Forward + backward |\n",
    "| 5 | Create nn.Module | User-friendly API |\n",
    "\n",
    "### ğŸ§  ML Optimization Pattern: The Extension Decision Tree\n",
    "\n",
    "```\n",
    "Do you need a custom CUDA kernel?\n",
    "â”‚\n",
    "â”œâ”€â”€ Is there a torch/cuDNN equivalent?\n",
    "â”‚   â””â”€â”€ YES â†’ Use it! Don't reinvent the wheel\n",
    "â”‚\n",
    "â”œâ”€â”€ Is PyTorch's fused kernel too slow?\n",
    "â”‚   â”œâ”€â”€ NO â†’ Use torch.compile() or torch.jit first\n",
    "â”‚   â””â”€â”€ YES â†’ Custom extension time! âš¡\n",
    "â”‚\n",
    "â””â”€â”€ Need autograd support?\n",
    "    â”œâ”€â”€ YES â†’ Use autograd.Function with explicit backward\n",
    "    â””â”€â”€ NO â†’ Direct C++ binding is fine\n",
    "```\n",
    "\n",
    "### ğŸ”‘ Key Insights\n",
    "\n",
    "1. **The LEGO Rule:** Custom extensions plug into PyTorch's ecosystemâ€”forward AND backward work seamlessly\n",
    "2. **JIT for Development:** Use `torch.utils.cpp_extension.load()` during development, `setup.py` for distribution\n",
    "3. **Test Gradients!:** Use `torch.autograd.gradcheck()` to verify your backward pass is correct\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "âœ… **Do**:\n",
    "- Support both FP32 and FP16/BF16\n",
    "- Add proper error checking with `TORCH_CHECK`\n",
    "- Use `AT_DISPATCH_FLOATING_TYPES` for type dispatch\n",
    "- Make tensors contiguous before kernel launch\n",
    "\n",
    "âŒ **Don't**:\n",
    "- Forget to check `tensor.device().is_cuda()`\n",
    "- Ignore CUDA error codes\n",
    "- Hardcode block sizes (query device properties)\n",
    "\n",
    "### ğŸ“š What's Next?\n",
    "\n",
    "Tomorrow we complete the curriculum with **Benchmarking**â€”the scientific method for proving your optimizations actually work!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
