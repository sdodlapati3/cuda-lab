{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81661b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a6ce9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Why Kernel Fusion?\n",
    "\n",
    "### Memory Bandwidth is the Bottleneck\n",
    "\n",
    "```\n",
    "Unfused (3 kernels):\n",
    "  Input ‚Üí [Load] ‚Üí Kernel1 ‚Üí [Store] ‚Üí Global Memory\n",
    "                                            ‚Üì\n",
    "  Global Memory ‚Üí [Load] ‚Üí Kernel2 ‚Üí [Store] ‚Üí Global Memory\n",
    "                                                     ‚Üì\n",
    "  Global Memory ‚Üí [Load] ‚Üí Kernel3 ‚Üí [Store] ‚Üí Output\n",
    "\n",
    "  Total global memory access: 6 loads/stores!\n",
    "\n",
    "Fused (1 kernel):\n",
    "  Input ‚Üí [Load] ‚Üí Kernel(1+2+3) ‚Üí [Store] ‚Üí Output\n",
    "  \n",
    "  Total global memory access: 2 loads/stores!\n",
    "  \n",
    "  Speedup: Up to 3x for memory-bound operations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10e1e4",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fusion_demo.cu\n",
    "// fusion_demo.cu - Demonstrating fusion benefits\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// ========== UNFUSED VERSION ==========\n",
    "// 3 separate kernels\n",
    "\n",
    "__global__ void add_kernel(float* a, float* b, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = a[idx] + b[idx];\n",
    "}\n",
    "\n",
    "__global__ void mul_kernel(float* in, float scalar, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = in[idx] * scalar;\n",
    "}\n",
    "\n",
    "__global__ void relu_kernel(float* in, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) out[idx] = fmaxf(in[idx], 0.0f);\n",
    "}\n",
    "\n",
    "// ========== FUSED VERSION ==========\n",
    "// Single kernel doing add + mul + relu\n",
    "\n",
    "__global__ void fused_add_mul_relu(float* a, float* b, float scalar, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = a[idx] + b[idx];  // Add\n",
    "        val = val * scalar;            // Multiply\n",
    "        out[idx] = fmaxf(val, 0.0f);  // ReLU\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Kernel Fusion Demonstration ===\\n\\n\");\n",
    "    \n",
    "    const int N = 100000000;  // 100M elements\n",
    "    const float scalar = 2.0f;\n",
    "    \n",
    "    float *d_a, *d_b, *d_temp1, *d_temp2, *d_out;\n",
    "    cudaMalloc(&d_a, N * sizeof(float));\n",
    "    cudaMalloc(&d_b, N * sizeof(float));\n",
    "    cudaMalloc(&d_temp1, N * sizeof(float));\n",
    "    cudaMalloc(&d_temp2, N * sizeof(float));\n",
    "    cudaMalloc(&d_out, N * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    cudaMemset(d_a, 1, N * sizeof(float));\n",
    "    cudaMemset(d_b, 1, N * sizeof(float));\n",
    "    \n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // ===== Unfused benchmark =====\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        add_kernel<<<blocks, threads>>>(d_a, d_b, d_temp1, N);\n",
    "        mul_kernel<<<blocks, threads>>>(d_temp1, scalar, d_temp2, N);\n",
    "        relu_kernel<<<blocks, threads>>>(d_temp2, d_out, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float unfused_ms;\n",
    "    cudaEventElapsedTime(&unfused_ms, start, stop);\n",
    "    \n",
    "    // ===== Fused benchmark =====\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        fused_add_mul_relu<<<blocks, threads>>>(d_a, d_b, scalar, d_out, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float fused_ms;\n",
    "    cudaEventElapsedTime(&fused_ms, start, stop);\n",
    "    \n",
    "    printf(\"Array size: %d elements (%.0f MB)\\n\\n\", N, N * 4.0 / 1e6);\n",
    "    printf(\"Unfused (3 kernels): %.2f ms\\n\", unfused_ms);\n",
    "    printf(\"Fused (1 kernel):    %.2f ms\\n\", fused_ms);\n",
    "    printf(\"Speedup: %.2fx\\n\", unfused_ms / fused_ms);\n",
    "    \n",
    "    // Memory analysis\n",
    "    printf(\"\\nMemory Traffic Analysis:\\n\");\n",
    "    printf(\"  Unfused: 6 * %.0f MB = %.0f MB\\n\", N * 4.0 / 1e6, 6 * N * 4.0 / 1e6);\n",
    "    printf(\"  Fused:   3 * %.0f MB = %.0f MB\\n\", N * 4.0 / 1e6, 3 * N * 4.0 / 1e6);\n",
    "    printf(\"  Bandwidth saved: 50%%\\n\");\n",
    "    \n",
    "    cudaFree(d_a); cudaFree(d_b);\n",
    "    cudaFree(d_temp1); cudaFree(d_temp2); cudaFree(d_out);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668533b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o fusion_demo fusion_demo.cu\n",
    "!./fusion_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36089bb3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Fused Softmax\n",
    "\n",
    "### Softmax Formula\n",
    "\n",
    "```\n",
    "softmax(x_i) = exp(x_i - max(x)) / sum(exp(x - max(x)))\n",
    "\n",
    "Naive implementation (4 passes):\n",
    "  1. Find max\n",
    "  2. Subtract max and exp\n",
    "  3. Sum\n",
    "  4. Divide\n",
    "\n",
    "Fused implementation (1 pass with online algorithm):\n",
    "  - Compute max, sum, output together\n",
    "  - Use warp reductions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ae5b5",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_softmax.cu\n",
    "// fused_softmax.cu - Fused softmax kernel\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <float.h>\n",
    "\n",
    "// Warp-level reduction for max\n",
    "__inline__ __device__ float warpReduceMax(float val) {\n",
    "    for (int offset = 16; offset > 0; offset /= 2)\n",
    "        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n",
    "    return val;\n",
    "}\n",
    "\n",
    "// Warp-level reduction for sum\n",
    "__inline__ __device__ float warpReduceSum(float val) {\n",
    "    for (int offset = 16; offset > 0; offset /= 2)\n",
    "        val += __shfl_down_sync(0xffffffff, val, offset);\n",
    "    return val;\n",
    "}\n",
    "\n",
    "// Fused softmax: one row per warp (for row_size <= 32)\n",
    "__global__ void fused_softmax_warp(\n",
    "    float* input, float* output,\n",
    "    int rows, int cols\n",
    ") {\n",
    "    int row = blockIdx.x * (blockDim.x / 32) + threadIdx.x / 32;\n",
    "    int lane = threadIdx.x % 32;\n",
    "    \n",
    "    if (row >= rows) return;\n",
    "    \n",
    "    float* row_in = input + row * cols;\n",
    "    float* row_out = output + row * cols;\n",
    "    \n",
    "    // Each thread loads one element (assuming cols <= 32)\n",
    "    float val = (lane < cols) ? row_in[lane] : -FLT_MAX;\n",
    "    \n",
    "    // Step 1: Find max\n",
    "    float max_val = warpReduceMax(val);\n",
    "    max_val = __shfl_sync(0xffffffff, max_val, 0);  // Broadcast\n",
    "    \n",
    "    // Step 2: Compute exp(x - max)\n",
    "    float exp_val = (lane < cols) ? expf(val - max_val) : 0.0f;\n",
    "    \n",
    "    // Step 3: Sum of exp values\n",
    "    float sum_exp = warpReduceSum(exp_val);\n",
    "    sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);  // Broadcast\n",
    "    \n",
    "    // Step 4: Divide\n",
    "    if (lane < cols) {\n",
    "        row_out[lane] = exp_val / sum_exp;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Naive softmax for comparison (multiple passes)\n",
    "__global__ void naive_softmax_pass1(float* input, float* max_vals, int rows, int cols) {\n",
    "    int row = blockIdx.x;\n",
    "    if (row >= rows) return;\n",
    "    \n",
    "    float max_val = -FLT_MAX;\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        max_val = fmaxf(max_val, input[row * cols + i]);\n",
    "    }\n",
    "    max_vals[row] = max_val;\n",
    "}\n",
    "\n",
    "__global__ void naive_softmax_pass2(float* input, float* max_vals, float* sum_vals, int rows, int cols) {\n",
    "    int row = blockIdx.x;\n",
    "    if (row >= rows) return;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    float max_val = max_vals[row];\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        sum += expf(input[row * cols + i] - max_val);\n",
    "    }\n",
    "    sum_vals[row] = sum;\n",
    "}\n",
    "\n",
    "__global__ void naive_softmax_pass3(float* input, float* output, float* max_vals, float* sum_vals, int rows, int cols) {\n",
    "    int row = blockIdx.x;\n",
    "    if (row >= rows) return;\n",
    "    \n",
    "    float max_val = max_vals[row];\n",
    "    float sum = sum_vals[row];\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        output[row * cols + i] = expf(input[row * cols + i] - max_val) / sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Fused vs Naive Softmax ===\\n\\n\");\n",
    "    \n",
    "    const int rows = 100000;\n",
    "    const int cols = 32;  // Common for attention heads\n",
    "    \n",
    "    float *d_input, *d_output, *d_max, *d_sum;\n",
    "    cudaMalloc(&d_input, rows * cols * sizeof(float));\n",
    "    cudaMalloc(&d_output, rows * cols * sizeof(float));\n",
    "    cudaMalloc(&d_max, rows * sizeof(float));\n",
    "    cudaMalloc(&d_sum, rows * sizeof(float));\n",
    "    \n",
    "    // Random init\n",
    "    float* h_input = new float[rows * cols];\n",
    "    for (int i = 0; i < rows * cols; i++) h_input[i] = (rand() % 1000) / 100.0f - 5.0f;\n",
    "    cudaMemcpy(d_input, h_input, rows * cols * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Naive benchmark\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        naive_softmax_pass1<<<rows, 1>>>(d_input, d_max, rows, cols);\n",
    "        naive_softmax_pass2<<<rows, 1>>>(d_input, d_max, d_sum, rows, cols);\n",
    "        naive_softmax_pass3<<<rows, 1>>>(d_input, d_output, d_max, d_sum, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float naive_ms;\n",
    "    cudaEventElapsedTime(&naive_ms, start, stop);\n",
    "    \n",
    "    // Fused benchmark\n",
    "    int warps_per_block = 8;\n",
    "    int blocks = (rows + warps_per_block - 1) / warps_per_block;\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        fused_softmax_warp<<<blocks, warps_per_block * 32>>>(d_input, d_output, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float fused_ms;\n",
    "    cudaEventElapsedTime(&fused_ms, start, stop);\n",
    "    \n",
    "    printf(\"Matrix: %d rows x %d cols\\n\\n\", rows, cols);\n",
    "    printf(\"Naive (3 passes):  %.2f ms\\n\", naive_ms);\n",
    "    printf(\"Fused (1 pass):    %.2f ms\\n\", fused_ms);\n",
    "    printf(\"Speedup: %.2fx\\n\", naive_ms / fused_ms);\n",
    "    \n",
    "    delete[] h_input;\n",
    "    cudaFree(d_input); cudaFree(d_output);\n",
    "    cudaFree(d_max); cudaFree(d_sum);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad0b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o fused_softmax fused_softmax.cu\n",
    "!./fused_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259cd6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Fused Layer Normalization\n",
    "\n",
    "### LayerNorm Formula\n",
    "\n",
    "```\n",
    "LayerNorm(x) = gamma * (x - mean) / sqrt(var + eps) + beta\n",
    "\n",
    "Requires:\n",
    "  - Mean computation (reduction)\n",
    "  - Variance computation (reduction)\n",
    "  - Normalization (elementwise)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de20b5",
   "metadata": {},
   "source": [
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea95887",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_layernorm.cu\n",
    "// fused_layernorm.cu - Fused layer normalization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Welford's online algorithm for mean and variance\n",
    "__inline__ __device__ void welford_update(\n",
    "    float val, float& mean, float& m2, float& count\n",
    ") {\n",
    "    count += 1.0f;\n",
    "    float delta = val - mean;\n",
    "    mean += delta / count;\n",
    "    float delta2 = val - mean;\n",
    "    m2 += delta * delta2;\n",
    "}\n",
    "\n",
    "__inline__ __device__ void welford_combine(\n",
    "    float mean_a, float m2_a, float count_a,\n",
    "    float mean_b, float m2_b, float count_b,\n",
    "    float& mean, float& m2, float& count\n",
    ") {\n",
    "    count = count_a + count_b;\n",
    "    float delta = mean_b - mean_a;\n",
    "    mean = mean_a + delta * count_b / count;\n",
    "    m2 = m2_a + m2_b + delta * delta * count_a * count_b / count;\n",
    "}\n",
    "\n",
    "// Fused LayerNorm: one block per row\n",
    "__global__ void fused_layernorm(\n",
    "    float* input, float* output,\n",
    "    float* gamma, float* beta,\n",
    "    int rows, int cols, float eps\n",
    ") {\n",
    "    extern __shared__ float shared[];\n",
    "    float* s_mean = shared;\n",
    "    float* s_m2 = shared + blockDim.x;\n",
    "    float* s_count = shared + 2 * blockDim.x;\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    if (row >= rows) return;\n",
    "    \n",
    "    float* row_in = input + row * cols;\n",
    "    float* row_out = output + row * cols;\n",
    "    \n",
    "    // Each thread processes multiple elements\n",
    "    float local_mean = 0.0f, local_m2 = 0.0f, local_count = 0.0f;\n",
    "    \n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        welford_update(row_in[i], local_mean, local_m2, local_count);\n",
    "    }\n",
    "    \n",
    "    s_mean[tid] = local_mean;\n",
    "    s_m2[tid] = local_m2;\n",
    "    s_count[tid] = local_count;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduction\n",
    "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
    "        if (tid < stride) {\n",
    "            float new_mean, new_m2, new_count;\n",
    "            welford_combine(\n",
    "                s_mean[tid], s_m2[tid], s_count[tid],\n",
    "                s_mean[tid + stride], s_m2[tid + stride], s_count[tid + stride],\n",
    "                new_mean, new_m2, new_count\n",
    "            );\n",
    "            s_mean[tid] = new_mean;\n",
    "            s_m2[tid] = new_m2;\n",
    "            s_count[tid] = new_count;\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    float mean = s_mean[0];\n",
    "    float var = s_m2[0] / s_count[0];\n",
    "    float inv_std = rsqrtf(var + eps);\n",
    "    \n",
    "    // Apply normalization\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        float normalized = (row_in[i] - mean) * inv_std;\n",
    "        row_out[i] = gamma[i] * normalized + beta[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Fused Layer Normalization ===\\n\\n\");\n",
    "    \n",
    "    const int rows = 10000;\n",
    "    const int cols = 768;  // BERT hidden size\n",
    "    const float eps = 1e-5f;\n",
    "    \n",
    "    float *d_input, *d_output, *d_gamma, *d_beta;\n",
    "    cudaMalloc(&d_input, rows * cols * sizeof(float));\n",
    "    cudaMalloc(&d_output, rows * cols * sizeof(float));\n",
    "    cudaMalloc(&d_gamma, cols * sizeof(float));\n",
    "    cudaMalloc(&d_beta, cols * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_gamma = new float[cols];\n",
    "    float* h_beta = new float[cols];\n",
    "    for (int i = 0; i < cols; i++) { h_gamma[i] = 1.0f; h_beta[i] = 0.0f; }\n",
    "    cudaMemcpy(d_gamma, h_gamma, cols * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_beta, h_beta, cols * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threads = 256;\n",
    "    int shared_mem = 3 * threads * sizeof(float);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        fused_layernorm<<<rows, threads, shared_mem>>>(\n",
    "            d_input, d_output, d_gamma, d_beta, rows, cols, eps\n",
    "        );\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    printf(\"Configuration: %d rows x %d cols (BERT-like)\\n\", rows, cols);\n",
    "    printf(\"Time: %.2f ms (100 iterations)\\n\", ms);\n",
    "    printf(\"Throughput: %.2f GB/s\\n\", \n",
    "           2.0 * rows * cols * 4 * 100 / (ms * 1e6));\n",
    "    \n",
    "    delete[] h_gamma; delete[] h_beta;\n",
    "    cudaFree(d_input); cudaFree(d_output);\n",
    "    cudaFree(d_gamma); cudaFree(d_beta);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o fused_layernorm fused_layernorm.cu\n",
    "!./fused_layernorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c668d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fused_kernels_exercises.cu\n",
    "// CUDA C++ Exercises - Kernel Fusion\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Exercise 1: Implement a fused bias + ReLU kernel\n",
    "// TODO: Add your implementation here\n",
    "\n",
    "// Exercise 2: Implement a fused normalize + scale kernel  \n",
    "// TODO: Add your implementation here\n",
    "\n",
    "// Exercise 3: Compare fused vs unfused performance\n",
    "// TODO: Add benchmarking code here\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Kernel Fusion Exercises ===\\n\");\n",
    "    printf(\"Implement the exercises above and run!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126128ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o fused_kernels_exercises fused_kernels_exercises.cu && ./fused_kernels_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380f6b7",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises use Python and Numba for rapid prototyping. Complete the CUDA C++ exercises above first for the primary learning objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b0d2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Kernel Fusion Benefits\n",
    "\n",
    "| Metric | Unfused | Fused |\n",
    "|--------|---------|-------|\n",
    "| Kernel launches | N | 1 |\n",
    "| Global memory access | High | Minimal |\n",
    "| Launch overhead | Significant | Once |\n",
    "\n",
    "### When to Fuse\n",
    "\n",
    "‚úÖ **Good candidates**:\n",
    "- Element-wise chains (add ‚Üí mul ‚Üí activation)\n",
    "- Reduction + normalize (softmax, layernorm)\n",
    "- Bias + activation\n",
    "\n",
    "‚ùå **Poor candidates**:\n",
    "- Large GEMM (already compute-bound)\n",
    "- Operations with dependencies between rows"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
