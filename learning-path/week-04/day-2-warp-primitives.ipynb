{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e84f835",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 2: Warp Primitives & Shuffle Operations\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-04/day-2-warp-primitives.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand warp execution model (32 threads in lockstep)\n",
    "- Use warp shuffle operations (__shfl_down, __shfl_xor)\n",
    "- Implement warp-level reduction without shared memory\n",
    "- Combine warp and block reduction for optimal performance\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7456074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")\n",
    "    print(f\"Warp size: {device.WARP_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42478a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Warps\n",
    "\n",
    "### What is a Warp?\n",
    "\n",
    "```\n",
    "Block of 256 threads:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Warp 0     â”‚ Warp 1     â”‚ Warp 2     â”‚ ... â”‚ Warp 7     â”‚\n",
    "â”‚ tid 0-31   â”‚ tid 32-63  â”‚ tid 64-95  â”‚ ... â”‚ tid 224-255â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â€¢ A warp = 32 threads that execute in LOCKSTEP\n",
    "â€¢ All 32 threads execute the same instruction at the same time\n",
    "â€¢ Threads in a warp can directly exchange data (no shared memory!)\n",
    "```\n",
    "\n",
    "### CUDA C++ Warp Shuffle Operations (Primary)\n",
    "\n",
    "The following CUDA C++ implementation demonstrates warp-level shuffle operations for efficient reduction without shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8011d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile warp_primitives.cu\n",
    "// warp_primitives.cu - Warp shuffle operations\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Warp-level reduction using shuffle\n",
    "__device__ float warpReduceSum(float val) {\n",
    "    // Use __shfl_down_sync to reduce within warp\n",
    "    // All threads in warp must participate (mask = 0xffffffff)\n",
    "    for (int offset = 16; offset > 0; offset >>= 1) {\n",
    "        val += __shfl_down_sync(0xffffffff, val, offset);\n",
    "    }\n",
    "    return val;  // Only lane 0 has the final sum\n",
    "}\n",
    "\n",
    "__device__ float warpReduceMax(float val) {\n",
    "    for (int offset = 16; offset > 0; offset >>= 1) {\n",
    "        float other = __shfl_down_sync(0xffffffff, val, offset);\n",
    "        val = fmaxf(val, other);\n",
    "    }\n",
    "    return val;\n",
    "}\n",
    "\n",
    "// Block-level reduction using warps\n",
    "__global__ void blockReduceSumWarp(const float* input, float* output, int n) {\n",
    "    // Each thread loads and accumulates via grid-stride\n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    for (int i = gid; i < n; i += stride) {\n",
    "        sum += input[i];\n",
    "    }\n",
    "    \n",
    "    // Warp-level reduction\n",
    "    sum = warpReduceSum(sum);\n",
    "    \n",
    "    // First thread in each warp writes to shared memory\n",
    "    __shared__ float warpSums[32];  // Max 32 warps per block\n",
    "    int warpId = tid / 32;\n",
    "    int laneId = tid % 32;\n",
    "    \n",
    "    if (laneId == 0) {\n",
    "        warpSums[warpId] = sum;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // First warp reduces all warp sums\n",
    "    if (warpId == 0) {\n",
    "        sum = (tid < blockDim.x / 32) ? warpSums[tid] : 0.0f;\n",
    "        sum = warpReduceSum(sum);\n",
    "        \n",
    "        if (tid == 0) {\n",
    "            output[blockIdx.x] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Other useful warp operations:\n",
    "// __shfl_sync(mask, val, srcLane)    - Get value from srcLane\n",
    "// __shfl_up_sync(mask, val, delta)   - Get value from (laneId - delta)\n",
    "// __shfl_down_sync(mask, val, delta) - Get value from (laneId + delta)\n",
    "// __shfl_xor_sync(mask, val, mask)   - Get value from (laneId XOR mask)\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate and initialize\n",
    "    float *h_input = (float*)malloc(size);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_input[i] = 1.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, size);\n",
    "    cudaMalloc(&d_output, 256 * sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    blockReduceSumWarp<<<256, 256>>>(d_input, d_output, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Get partial results and sum on CPU\n",
    "    float h_output[256];\n",
    "    cudaMemcpy(h_output, d_output, 256 * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    float total = 0.0f;\n",
    "    for (int i = 0; i < 256; i++) {\n",
    "        total += h_output[i];\n",
    "    }\n",
    "    \n",
    "    printf(\"Warp reduction sum of %d elements: %f (expected %d)\\n\", n, total, n);\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea566787",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o warp_primitives warp_primitives.cu\n",
    "!./warp_primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python equivalent for understanding warp structure\n",
    "@cuda.jit\n",
    "def show_warp_info(output, n):\n",
    "    \"\"\"Display warp information for each thread.\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    \n",
    "    if gid < n:\n",
    "        # Calculate warp ID and lane ID\n",
    "        warp_id = tid // 32  # Which warp within the block\n",
    "        lane_id = tid % 32   # Position within the warp (0-31)\n",
    "        \n",
    "        # Pack info: warp_id * 100 + lane_id\n",
    "        output[gid] = warp_id * 100 + lane_id\n",
    "\n",
    "# Test\n",
    "n = 64\n",
    "d_output = cuda.device_array(n, dtype=np.int32)\n",
    "\n",
    "show_warp_info[1, 64](d_output, n)\n",
    "output = d_output.copy_to_host()\n",
    "\n",
    "print(\"Thread â†’ Warp.Lane\")\n",
    "print(\"=\"*40)\n",
    "for i in range(n):\n",
    "    warp = output[i] // 100\n",
    "    lane = output[i] % 100\n",
    "    if i % 8 == 0:\n",
    "        print()\n",
    "    print(f\"T{i:2d}â†’W{warp}.L{lane:2d}\", end=\"  \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fac98f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Warp Shuffle Operations\n",
    "\n",
    "### The Magic of Shuffle\n",
    "\n",
    "```\n",
    "Shuffle allows threads in a warp to read each other's registers!\n",
    "No shared memory needed. No synchronization needed.\n",
    "\n",
    "shuffle_sync(mask, val, src_lane):\n",
    "  - mask: which threads participate (0xFFFFFFFF = all 32)\n",
    "  - val: the value to share\n",
    "  - src_lane: which lane to read from\n",
    "\n",
    "Example: If lane 5 has val=42, then shuffle_sync(mask, val, 5)\n",
    "         returns 42 for ALL threads in the warp!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ba836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Numba's warp shuffle support\n",
    "# cuda.shfl_sync(mask, value, src_lane)\n",
    "# cuda.shfl_down_sync(mask, value, delta)\n",
    "# cuda.shfl_up_sync(mask, value, delta)\n",
    "# cuda.shfl_xor_sync(mask, value, lane_mask)\n",
    "\n",
    "@cuda.jit\n",
    "def demo_shfl_sync(output, n):\n",
    "    \"\"\"Demo: Read value from lane 0.\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    \n",
    "    if tid < n:\n",
    "        # Each thread has its own value\n",
    "        my_val = tid * 10\n",
    "        \n",
    "        # All threads read from lane 0\n",
    "        # 0xFFFFFFFF = all 32 threads participate\n",
    "        val_from_lane0 = cuda.shfl_sync(0xFFFFFFFF, my_val, 0)\n",
    "        \n",
    "        output[tid] = val_from_lane0\n",
    "\n",
    "d_out = cuda.device_array(32, dtype=np.int32)\n",
    "demo_shfl_sync[1, 32](d_out, 32)\n",
    "result = d_out.copy_to_host()\n",
    "\n",
    "print(\"shfl_sync: All threads read lane 0's value (0)\")\n",
    "print(f\"Result: {result[:8]}... (all should be 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def demo_shfl_down(output, n):\n",
    "    \"\"\"Demo: shfl_down shifts values down by delta lanes.\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    \n",
    "    if tid < n:\n",
    "        my_val = tid  # Lane i has value i\n",
    "        \n",
    "        # Each thread reads from lane (tid + 1)\n",
    "        # Lane 0 gets value from lane 1\n",
    "        # Lane 1 gets value from lane 2\n",
    "        # etc.\n",
    "        val = cuda.shfl_down_sync(0xFFFFFFFF, my_val, 1)\n",
    "        \n",
    "        output[tid] = val\n",
    "\n",
    "d_out = cuda.device_array(32, dtype=np.int32)\n",
    "demo_shfl_down[1, 32](d_out, 32)\n",
    "result = d_out.copy_to_host()\n",
    "\n",
    "print(\"shfl_down_sync(val, 1): Each thread reads from lane+1\")\n",
    "print(f\"Lane 0 gets {result[0]} (from lane 1)\")\n",
    "print(f\"Lane 1 gets {result[1]} (from lane 2)\")\n",
    "print(f\"Lane 30 gets {result[30]} (from lane 31)\")\n",
    "print(f\"Lane 31 gets {result[31]} (undefined, wraps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d863b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Warp-Level Reduction\n",
    "\n",
    "### Using shfl_down for Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def warp_reduce_sum(val):\n",
    "    \"\"\"\n",
    "    Reduce 32 values (one per lane) to a single sum.\n",
    "    Result is in lane 0.\n",
    "    \n",
    "    This uses the \"butterfly\" reduction pattern:\n",
    "    \n",
    "    Initial: [0] [1] [2] [3] [4] [5] [6] [7] ... [31]\n",
    "    \n",
    "    Step 1 (delta=16): lanes 0-15 add lanes 16-31\n",
    "    Step 2 (delta=8):  lanes 0-7 add lanes 8-15\n",
    "    Step 3 (delta=4):  lanes 0-3 add lanes 4-7\n",
    "    Step 4 (delta=2):  lanes 0-1 add lanes 2-3\n",
    "    Step 5 (delta=1):  lane 0 adds lane 1\n",
    "    \n",
    "    Final: lane 0 has the sum of all 32 values!\n",
    "    \"\"\"\n",
    "    # Full warp participates\n",
    "    mask = 0xFFFFFFFF\n",
    "    \n",
    "    # Step through decreasing offsets\n",
    "    val += cuda.shfl_down_sync(mask, val, 16)\n",
    "    val += cuda.shfl_down_sync(mask, val, 8)\n",
    "    val += cuda.shfl_down_sync(mask, val, 4)\n",
    "    val += cuda.shfl_down_sync(mask, val, 2)\n",
    "    val += cuda.shfl_down_sync(mask, val, 1)\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def test_warp_reduce(output):\n",
    "    \"\"\"Test warp reduction: sum values 0-31.\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    lane = tid % 32\n",
    "    \n",
    "    # Each lane has its lane number as value\n",
    "    val = float(lane)\n",
    "    \n",
    "    # Reduce within warp\n",
    "    sum_val = warp_reduce_sum(val)\n",
    "    \n",
    "    # Lane 0 writes result\n",
    "    if lane == 0:\n",
    "        output[0] = sum_val\n",
    "\n",
    "d_out = cuda.device_array(1, dtype=np.float32)\n",
    "test_warp_reduce[1, 32](d_out)\n",
    "result = d_out.copy_to_host()[0]\n",
    "\n",
    "expected = sum(range(32))  # 0+1+2+...+31 = 496\n",
    "print(f\"Warp reduce sum of lanes 0-31: {result:.0f}\")\n",
    "print(f\"Expected (0+1+...+31): {expected}\")\n",
    "print(f\"Match: {'âœ“' if result == expected else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the butterfly pattern\n",
    "def visualize_warp_reduction():\n",
    "    print(\"Warp Reduction (shfl_down pattern)\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Simplified to 8 lanes for visualization\n",
    "    vals = list(range(8))\n",
    "    print(f\"Initial: {vals}\")\n",
    "    print()\n",
    "    \n",
    "    for delta in [4, 2, 1]:\n",
    "        new_vals = vals.copy()\n",
    "        for i in range(8 - delta):\n",
    "            new_vals[i] = vals[i] + vals[i + delta]\n",
    "        print(f\"After delta={delta}:\")\n",
    "        print(f\"  Lane 0 = {vals[0]} + {vals[delta]} = {new_vals[0]}\")\n",
    "        vals = new_vals\n",
    "        print(f\"  Values: {vals}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"Final sum in lane 0: {vals[0]}\")\n",
    "    print(f\"Expected: {sum(range(8))}\")\n",
    "\n",
    "visualize_warp_reduction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6be79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Combining Warp + Block Reduction\n",
    "\n",
    "### Optimal Block Reduction Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def block_reduce_warp_optimized(arr, block_results, n):\n",
    "    \"\"\"\n",
    "    Optimized block reduction using warp primitives.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Each thread accumulates its portion (grid-stride)\n",
    "    2. Warp-level reduction (no shared memory, no sync!)\n",
    "    3. Lane 0 of each warp writes to shared memory\n",
    "    4. Final warp reduces the warp sums\n",
    "    \"\"\"\n",
    "    # Shared memory for warp results (max 32 warps per block)\n",
    "    warp_sums = cuda.shared.array(32, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    lane = tid % 32\n",
    "    warp_id = tid // 32\n",
    "    num_warps = cuda.blockDim.x // 32\n",
    "    \n",
    "    # Phase 1: Grid-stride accumulation\n",
    "    local_sum = 0.0\n",
    "    for i in range(gid, n, stride):\n",
    "        local_sum += arr[i]\n",
    "    \n",
    "    # Phase 2: Warp-level reduction\n",
    "    warp_sum = warp_reduce_sum(local_sum)\n",
    "    \n",
    "    # Phase 3: Lane 0 of each warp stores result\n",
    "    if lane == 0:\n",
    "        warp_sums[warp_id] = warp_sum\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 4: First warp reduces all warp sums\n",
    "    if warp_id == 0:\n",
    "        # Load warp sum (or 0 if beyond num_warps)\n",
    "        val = warp_sums[lane] if lane < num_warps else 0.0\n",
    "        \n",
    "        # Final warp reduction\n",
    "        final_sum = warp_reduce_sum(val)\n",
    "        \n",
    "        if lane == 0:\n",
    "            block_results[bid] = final_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d181d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_reduce_sum_warp(arr):\n",
    "    \"\"\"GPU sum using warp-optimized reduction.\"\"\"\n",
    "    n = len(arr)\n",
    "    blocks = 256\n",
    "    threads = 256\n",
    "    \n",
    "    d_arr = cuda.to_device(arr)\n",
    "    d_block_results = cuda.device_array(blocks, dtype=np.float32)\n",
    "    \n",
    "    # First pass\n",
    "    block_reduce_warp_optimized[blocks, threads](d_arr, d_block_results, n)\n",
    "    \n",
    "    # Second pass\n",
    "    d_final = cuda.device_array(1, dtype=np.float32)\n",
    "    block_reduce_warp_optimized[1, threads](d_block_results, d_final, blocks)\n",
    "    \n",
    "    return d_final.copy_to_host()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test correctness\n",
    "n = 10_000_000\n",
    "arr = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "gpu_result = gpu_reduce_sum_warp(arr)\n",
    "cpu_result = np.sum(arr)\n",
    "\n",
    "print(f\"Warp-optimized reduction test\")\n",
    "print(f\"GPU: {gpu_result:.2f}\")\n",
    "print(f\"CPU: {cpu_result:.2f}\")\n",
    "print(f\"Match: {'âœ“' if np.isclose(gpu_result, cpu_result, rtol=1e-4) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b1a8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Other Warp Operations\n",
    "\n",
    "### Warp Max and Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def warp_reduce_max(val):\n",
    "    \"\"\"Reduce to maximum value across warp.\"\"\"\n",
    "    mask = 0xFFFFFFFF\n",
    "    \n",
    "    other = cuda.shfl_down_sync(mask, val, 16)\n",
    "    val = max(val, other)\n",
    "    other = cuda.shfl_down_sync(mask, val, 8)\n",
    "    val = max(val, other)\n",
    "    other = cuda.shfl_down_sync(mask, val, 4)\n",
    "    val = max(val, other)\n",
    "    other = cuda.shfl_down_sync(mask, val, 2)\n",
    "    val = max(val, other)\n",
    "    other = cuda.shfl_down_sync(mask, val, 1)\n",
    "    val = max(val, other)\n",
    "    \n",
    "    return val\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def warp_reduce_min(val):\n",
    "    \"\"\"Reduce to minimum value across warp.\"\"\"\n",
    "    mask = 0xFFFFFFFF\n",
    "    \n",
    "    other = cuda.shfl_down_sync(mask, val, 16)\n",
    "    val = min(val, other)\n",
    "    other = cuda.shfl_down_sync(mask, val, 8)\n",
    "    val = min(val, other)\n",
    "    other = cuda.shfl_down_sync(mask, val, 4)\n",
    "    val = min(val, other)\n",
    "    other = cuda.shfl_down_sync(mask, val, 2)\n",
    "    val = min(val, other)\n",
    "    other = cuda.shfl_down_sync(mask, val, 1)\n",
    "    val = min(val, other)\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c745ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def test_warp_minmax(arr, out_max, out_min):\n",
    "    \"\"\"Test warp min/max on first 32 elements.\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    lane = tid % 32\n",
    "    \n",
    "    val = arr[lane]\n",
    "    \n",
    "    max_val = warp_reduce_max(val)\n",
    "    min_val = warp_reduce_min(val)\n",
    "    \n",
    "    if lane == 0:\n",
    "        out_max[0] = max_val\n",
    "        out_min[0] = min_val\n",
    "\n",
    "# Test\n",
    "arr = np.random.rand(32).astype(np.float32)\n",
    "d_arr = cuda.to_device(arr)\n",
    "d_max = cuda.device_array(1, dtype=np.float32)\n",
    "d_min = cuda.device_array(1, dtype=np.float32)\n",
    "\n",
    "test_warp_minmax[1, 32](d_arr, d_max, d_min)\n",
    "\n",
    "print(f\"Array: {arr[:8]}... (first 8 of 32)\")\n",
    "print(f\"GPU max: {d_max.copy_to_host()[0]:.4f}, CPU max: {np.max(arr):.4f}\")\n",
    "print(f\"GPU min: {d_min.copy_to_host()[0]:.4f}, CPU min: {np.min(arr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ecbde",
   "metadata": {},
   "source": [
    "### Warp Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ef57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def demo_warp_broadcast(output):\n",
    "    \"\"\"Broadcast a value from one lane to all others.\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    lane = tid % 32\n",
    "    \n",
    "    # Lane 0 has special value\n",
    "    my_val = 999 if lane == 0 else lane\n",
    "    \n",
    "    # Broadcast lane 0's value to all\n",
    "    broadcast_val = cuda.shfl_sync(0xFFFFFFFF, my_val, 0)\n",
    "    \n",
    "    output[tid] = broadcast_val\n",
    "\n",
    "d_out = cuda.device_array(32, dtype=np.int32)\n",
    "demo_warp_broadcast[1, 32](d_out)\n",
    "print(f\"Broadcast from lane 0: {d_out.copy_to_host()}\")\n",
    "print(\"All lanes now have 999!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10661075",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c43258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare shared-memory vs warp-primitive reduction\n",
    "\n",
    "@cuda.jit\n",
    "def block_reduce_shared_only(arr, block_results, n):\n",
    "    \"\"\"Traditional shared memory reduction.\"\"\"\n",
    "    shared = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    local_sum = 0.0\n",
    "    for i in range(gid, n, stride):\n",
    "        local_sum += arr[i]\n",
    "    \n",
    "    shared[tid] = local_sum\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s:\n",
    "            shared[tid] += shared[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    if tid == 0:\n",
    "        block_results[bid] = shared[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_reduction_methods(n, iterations=100):\n",
    "    \"\"\"Compare shared-only vs warp-optimized reduction.\"\"\"\n",
    "    arr = np.random.rand(n).astype(np.float32)\n",
    "    d_arr = cuda.to_device(arr)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # Shared memory only\n",
    "    d_block1 = cuda.device_array(blocks, dtype=np.float32)\n",
    "    d_final1 = cuda.device_array(1, dtype=np.float32)\n",
    "    \n",
    "    block_reduce_shared_only[blocks, threads](d_arr, d_block1, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        block_reduce_shared_only[blocks, threads](d_arr, d_block1, n)\n",
    "        block_reduce_shared_only[1, threads](d_block1, d_final1, blocks)\n",
    "    cuda.synchronize()\n",
    "    shared_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # Warp-optimized\n",
    "    d_block2 = cuda.device_array(blocks, dtype=np.float32)\n",
    "    d_final2 = cuda.device_array(1, dtype=np.float32)\n",
    "    \n",
    "    block_reduce_warp_optimized[blocks, threads](d_arr, d_block2, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        block_reduce_warp_optimized[blocks, threads](d_arr, d_block2, n)\n",
    "        block_reduce_warp_optimized[1, threads](d_block2, d_final2, blocks)\n",
    "    cuda.synchronize()\n",
    "    warp_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    return shared_time, warp_time\n",
    "\n",
    "n = 10_000_000\n",
    "shared_t, warp_t = benchmark_reduction_methods(n)\n",
    "\n",
    "print(f\"Reduction Benchmark (N={n:,})\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Shared memory only: {shared_t:.3f} ms\")\n",
    "print(f\"Warp-optimized:     {warp_t:.3f} ms\")\n",
    "print(f\"Improvement:        {shared_t/warp_t:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853775a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Warp XOR Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use shfl_xor_sync to exchange values between paired lanes\n",
    "# shfl_xor_sync(mask, val, lane_mask) exchanges with lane ^ lane_mask\n",
    "#\n",
    "# Example: lane_mask=1 swaps lanes 0â†”1, 2â†”3, 4â†”5, ...\n",
    "#          lane_mask=2 swaps lanes 0â†”2, 1â†”3, 4â†”6, ...\n",
    "\n",
    "@cuda.jit\n",
    "def demo_xor_shuffle(output):\n",
    "    \"\"\"Swap values with neighboring lanes using XOR shuffle.\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    lane = tid % 32\n",
    "    \n",
    "    my_val = lane  # Each lane has its ID as value\n",
    "    \n",
    "    # TODO: Use cuda.shfl_xor_sync to swap with lane^1\n",
    "    # swapped = cuda.shfl_xor_sync(0xFFFFFFFF, my_val, 1)\n",
    "    \n",
    "    # output[tid] = swapped\n",
    "    pass\n",
    "\n",
    "# After XOR with 1: lane 0 should have value 1, lane 1 should have value 0, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57662e1",
   "metadata": {},
   "source": [
    "### Exercise 2: Warp Prefix Sum (Scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement inclusive prefix sum within a warp\n",
    "# Input:  [1, 2, 3, 4, 5, ...]\n",
    "# Output: [1, 3, 6, 10, 15, ...]\n",
    "#\n",
    "# Use shfl_up_sync to get value from lower lane\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def warp_inclusive_scan(val):\n",
    "    \"\"\"Inclusive prefix sum within warp.\"\"\"\n",
    "    mask = 0xFFFFFFFF\n",
    "    \n",
    "    # Step through increasing offsets\n",
    "    # delta=1: add lane-1\n",
    "    # delta=2: add lane-2  \n",
    "    # etc.\n",
    "    \n",
    "    # TODO: Implement using shfl_up_sync\n",
    "    pass\n",
    "\n",
    "# Test: [1,1,1,...,1] â†’ [1,2,3,...,32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afa36e",
   "metadata": {},
   "source": [
    "### Exercise 3: Warp Vote Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore warp vote functions\n",
    "# cuda.all_sync(mask, predicate) - True if ALL threads have predicate=True\n",
    "# cuda.any_sync(mask, predicate) - True if ANY thread has predicate=True\n",
    "# cuda.ballot_sync(mask, predicate) - Bitmap of which threads have predicate=True\n",
    "\n",
    "@cuda.jit\n",
    "def demo_warp_vote(arr, result, n):\n",
    "    \"\"\"Check if all/any values in warp are positive.\"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    lane = tid % 32\n",
    "    \n",
    "    if lane < n:\n",
    "        val = arr[lane]\n",
    "        is_positive = val > 0\n",
    "        \n",
    "        # TODO: Use cuda.all_sync and cuda.any_sync\n",
    "        # all_positive = cuda.all_sync(0xFFFFFFFF, is_positive)\n",
    "        # any_positive = cuda.any_sync(0xFFFFFFFF, is_positive)\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0362786",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Warp Shuffle Operations\n",
    "\n",
    "| Function | Description | Use Case |\n",
    "|----------|-------------|----------|\n",
    "| `shfl_sync` | Read from specific lane | Broadcast |\n",
    "| `shfl_down_sync` | Read from lane+delta | Reduction |\n",
    "| `shfl_up_sync` | Read from lane-delta | Prefix sum |\n",
    "| `shfl_xor_sync` | Read from lane^mask | Butterfly patterns |\n",
    "\n",
    "### Why Warp Primitives?\n",
    "\n",
    "```\n",
    "Shared Memory Reduction:     Warp Shuffle Reduction:\n",
    "- Write to shared memory     - Direct register exchange\n",
    "- __syncthreads() needed     - No sync needed in warp!\n",
    "- Bank conflict possible     - No bank conflicts\n",
    "- ~8 steps for 256 threads   - 5 steps for 32 lanes\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Warp = 32 threads in lockstep**\n",
    "2. **Shuffle = direct register exchange (fast!)**\n",
    "3. **No synchronization within warp**\n",
    "4. **Use warp reduction for final 32 values**\n",
    "5. **Combine: warp reduce â†’ shared memory â†’ warp reduce**\n",
    "\n",
    "### Next: Day 3 - Atomic Operations\n",
    "Learn about thread-safe updates for when reduction isn't enough!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
