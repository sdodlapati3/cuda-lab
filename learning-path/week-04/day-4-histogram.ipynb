{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2443e5a0",
   "metadata": {},
   "source": [
    "# üöÄ Day 4: Histogram - A Complete Example\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-04/day-4-histogram.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement histogram counting with atomics\n",
    "- Use shared memory privatization for performance\n",
    "- Extend to 2D histograms and image processing\n",
    "- Apply best practices for counting algorithms\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a229f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What is a Histogram?\n",
    "\n",
    "### Counting Values into Bins\n",
    "\n",
    "```\n",
    "Data:  [3, 1, 4, 1, 5, 9, 2, 6, 5, 3]\n",
    "\n",
    "Histogram (bins 0-9):\n",
    "Bin 0: 0 occurrences  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 1: 2 occurrences  ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 2: 1 occurrence   ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 3: 2 occurrences  ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 4: 1 occurrence   ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 5: 2 occurrences  ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 6: 1 occurrence   ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 7: 0 occurrences  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 8: 0 occurrences  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "Bin 9: 1 occurrence   ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)\n",
    "\n",
    "The following CUDA C++ implementation demonstrates histogram computation with shared memory privatization for reduced atomic contention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile histogram.cu\n",
    "// histogram.cu - GPU histogram with privatization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define NUM_BINS 256\n",
    "#define BLOCK_SIZE 256\n",
    "\n",
    "// Naive: Global atomics only (slow due to contention)\n",
    "__global__ void histogramNaive(const unsigned char* data, int* hist, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        atomicAdd(&hist[data[i]], 1);  // High contention!\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized: Shared memory privatization\n",
    "__global__ void histogramPrivatized(const unsigned char* data, int* hist, int n) {\n",
    "    // Private histogram in shared memory\n",
    "    __shared__ int localHist[NUM_BINS];\n",
    "    \n",
    "    // Initialize shared memory\n",
    "    for (int i = threadIdx.x; i < NUM_BINS; i += blockDim.x) {\n",
    "        localHist[i] = 0;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Count into shared memory (low contention within block)\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        atomicAdd(&localHist[data[i]], 1);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge local histograms to global (once per bin per block)\n",
    "    for (int i = threadIdx.x; i < NUM_BINS; i += blockDim.x) {\n",
    "        if (localHist[i] > 0) {\n",
    "            atomicAdd(&hist[i], localHist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000000;  // 10M data points\n",
    "    \n",
    "    unsigned char *h_data = (unsigned char*)malloc(n);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_data[i] = rand() % 256;\n",
    "    }\n",
    "    \n",
    "    unsigned char *d_data;\n",
    "    int *d_hist;\n",
    "    cudaMalloc(&d_data, n);\n",
    "    cudaMalloc(&d_hist, NUM_BINS * sizeof(int));\n",
    "    cudaMemset(d_hist, 0, NUM_BINS * sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_data, h_data, n, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    histogramPrivatized<<<256, 256>>>(d_data, d_hist, n);\n",
    "    \n",
    "    int h_hist[NUM_BINS];\n",
    "    cudaMemcpy(h_hist, d_hist, NUM_BINS * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Sample histogram values:\\n\");\n",
    "    for (int i = 0; i < 5; i++) {\n",
    "        printf(\"  Bin %d: %d\\n\", i, h_hist[i]);\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_data); cudaFree(d_hist);\n",
    "    free(h_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953026b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o histogram histogram.cu\n",
    "!./histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5f0a5",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)\n",
    "\n",
    "CPU baseline for comparison with GPU histogram implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87096de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU baseline for comparison\n",
    "def cpu_histogram(data, num_bins):\n",
    "    \"\"\"Simple CPU histogram.\"\"\"\n",
    "    hist = np.zeros(num_bins, dtype=np.int32)\n",
    "    for val in data:\n",
    "        if 0 <= val < num_bins:\n",
    "            hist[val] += 1\n",
    "    return hist\n",
    "\n",
    "# NumPy optimized\n",
    "def numpy_histogram(data, num_bins):\n",
    "    return np.bincount(data.astype(np.int32), minlength=num_bins)[:num_bins]\n",
    "\n",
    "# Test\n",
    "data = np.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3], dtype=np.int32)\n",
    "hist = cpu_histogram(data, 10)\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"Histogram: {hist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9654eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Naive GPU Histogram\n",
    "\n",
    "### Using Global Memory Atomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25443c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_global_atomic(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Naive histogram: each thread does global atomic add.\n",
    "    Simple but slow due to contention.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(hist, val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add40d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test naive histogram\n",
    "n = 10_000_000\n",
    "num_bins = 256\n",
    "\n",
    "# Random data with values 0-255\n",
    "data = np.random.randint(0, num_bins, n).astype(np.int32)\n",
    "d_data = cuda.to_device(data)\n",
    "d_hist = cuda.device_array(num_bins, dtype=np.int32)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Reset and compute\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "histogram_global_atomic[blocks, threads](d_data, d_hist, n, num_bins)\n",
    "\n",
    "gpu_hist = d_hist.copy_to_host()\n",
    "cpu_hist = numpy_histogram(data, num_bins)\n",
    "\n",
    "print(f\"GPU histogram matches CPU: {'‚úì' if np.array_equal(gpu_hist, cpu_hist) else '‚úó'}\")\n",
    "print(f\"\\nFirst 10 bins: {gpu_hist[:10]}\")\n",
    "print(f\"Total count: {np.sum(gpu_hist):,} (expected: {n:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b60ba9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Optimized Histogram with Shared Memory\n",
    "\n",
    "### Privatization Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87611788",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_shared_atomic(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Optimized histogram using shared memory privatization.\n",
    "    \n",
    "    1. Each block has private histogram in shared memory\n",
    "    2. Threads atomically update shared (fast!)\n",
    "    3. Merge to global at the end (fewer global atomics)\n",
    "    \"\"\"\n",
    "    # Shared memory for block's private histogram\n",
    "    # Assuming max 256 bins\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Phase 1: Initialize shared histogram to zeros\n",
    "    if tid < num_bins:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 2: Count into shared memory (fast atomics!)\n",
    "    for i in range(gid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(shared_hist, val, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 3: Merge to global (one atomic per bin per block)\n",
    "    if tid < num_bins:\n",
    "        if shared_hist[tid] > 0:\n",
    "            cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3387fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimized histogram\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "histogram_shared_atomic[blocks, threads](d_data, d_hist, n, num_bins)\n",
    "\n",
    "gpu_hist_opt = d_hist.copy_to_host()\n",
    "\n",
    "print(f\"Optimized histogram matches CPU: {'‚úì' if np.array_equal(gpu_hist_opt, cpu_hist) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca2249",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_histograms(n, num_bins=256, iterations=50):\n",
    "    \"\"\"Compare histogram implementations.\"\"\"\n",
    "    data = np.random.randint(0, num_bins, n).astype(np.int32)\n",
    "    d_data = cuda.to_device(data)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # CPU (NumPy)\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        _ = numpy_histogram(data, num_bins)\n",
    "    cpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # GPU Global Atomic\n",
    "    d_hist1 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "    histogram_global_atomic[blocks, threads](d_data, d_hist1, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        d_hist1 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "        histogram_global_atomic[blocks, threads](d_data, d_hist1, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    global_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # GPU Shared Atomic\n",
    "    d_hist2 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "    histogram_shared_atomic[blocks, threads](d_data, d_hist2, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        d_hist2 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "        histogram_shared_atomic[blocks, threads](d_data, d_hist2, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    shared_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    return cpu_time, global_time, shared_time\n",
    "\n",
    "# Benchmark\n",
    "sizes = [1_000_000, 5_000_000, 10_000_000, 50_000_000]\n",
    "\n",
    "print(f\"Histogram Benchmark (256 bins)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Size':>12} | {'CPU (ms)':>10} | {'Global (ms)':>12} | {'Shared (ms)':>12} | {'Speedup':>8}\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "for n in sizes:\n",
    "    cpu_t, global_t, shared_t = benchmark_histograms(n)\n",
    "    speedup = cpu_t / shared_t\n",
    "    print(f\"{n:>12,} | {cpu_t:>10.2f} | {global_t:>12.2f} | {shared_t:>12.2f} | {speedup:>7.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b1099",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Real-Value Histograms\n",
    "\n",
    "### Handling Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_float(data, hist, n, num_bins, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Histogram for floating-point data with specified range.\n",
    "    \n",
    "    Maps [min_val, max_val) to bins [0, num_bins)\n",
    "    \"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Initialize shared memory\n",
    "    if tid < num_bins:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Calculate bin width\n",
    "    bin_width = (max_val - min_val) / num_bins\n",
    "    \n",
    "    # Count\n",
    "    for i in range(gid, n, stride):\n",
    "        val = data[i]\n",
    "        \n",
    "        # Calculate bin index\n",
    "        if val >= min_val and val < max_val:\n",
    "            bin_idx = int((val - min_val) / bin_width)\n",
    "            bin_idx = min(bin_idx, num_bins - 1)  # Handle edge case\n",
    "            cuda.atomic.add(shared_hist, bin_idx, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Merge to global\n",
    "    if tid < num_bins:\n",
    "        if shared_hist[tid] > 0:\n",
    "            cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test float histogram\n",
    "n = 1_000_000\n",
    "num_bins = 50\n",
    "\n",
    "# Generate normal distribution\n",
    "data = np.random.randn(n).astype(np.float32)\n",
    "min_val, max_val = -4.0, 4.0\n",
    "\n",
    "d_data = cuda.to_device(data)\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "\n",
    "histogram_float[256, 256](d_data, d_hist, n, num_bins, min_val, max_val)\n",
    "\n",
    "gpu_hist = d_hist.copy_to_host()\n",
    "\n",
    "# Visualize\n",
    "print(f\"Histogram of Normal Distribution (N={n:,})\")\n",
    "print(f\"Range: [{min_val}, {max_val})\")\n",
    "print()\n",
    "\n",
    "max_count = max(gpu_hist)\n",
    "bin_width = (max_val - min_val) / num_bins\n",
    "\n",
    "for i in range(0, num_bins, 5):  # Show every 5th bin\n",
    "    bin_start = min_val + i * bin_width\n",
    "    bar_len = int(gpu_hist[i] / max_count * 30)\n",
    "    print(f\"{bin_start:>6.1f}: {'‚ñà' * bar_len} {gpu_hist[i]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6bbb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: 2D Histogram (Joint Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_2d(x_data, y_data, hist, n, \n",
    "                 x_bins, y_bins, \n",
    "                 x_min, x_max, y_min, y_max):\n",
    "    \"\"\"\n",
    "    2D histogram for joint distribution.\n",
    "    \n",
    "    hist has shape (x_bins, y_bins) flattened to 1D\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    x_width = (x_max - x_min) / x_bins\n",
    "    y_width = (y_max - y_min) / y_bins\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        x = x_data[i]\n",
    "        y = y_data[i]\n",
    "        \n",
    "        if x >= x_min and x < x_max and y >= y_min and y < y_max:\n",
    "            x_bin = int((x - x_min) / x_width)\n",
    "            y_bin = int((y - y_min) / y_width)\n",
    "            \n",
    "            x_bin = min(x_bin, x_bins - 1)\n",
    "            y_bin = min(y_bin, y_bins - 1)\n",
    "            \n",
    "            # Flatten 2D index\n",
    "            flat_idx = x_bin * y_bins + y_bin\n",
    "            cuda.atomic.add(hist, flat_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16efe075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2D histogram\n",
    "n = 1_000_000\n",
    "x_bins, y_bins = 20, 20\n",
    "\n",
    "# Correlated normal distributions\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.8], [0.8, 1]]  # Correlation = 0.8\n",
    "xy = np.random.multivariate_normal(mean, cov, n).astype(np.float32)\n",
    "x_data, y_data = xy[:, 0], xy[:, 1]\n",
    "\n",
    "d_x = cuda.to_device(x_data)\n",
    "d_y = cuda.to_device(y_data)\n",
    "d_hist = cuda.to_device(np.zeros(x_bins * y_bins, dtype=np.int32))\n",
    "\n",
    "histogram_2d[256, 256](d_x, d_y, d_hist, n,\n",
    "                       x_bins, y_bins,\n",
    "                       -4.0, 4.0, -4.0, 4.0)\n",
    "\n",
    "hist_2d = d_hist.copy_to_host().reshape(x_bins, y_bins)\n",
    "\n",
    "# Simple ASCII visualization\n",
    "print(\"2D Histogram (Correlated Normal, œÅ=0.8)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "chars = \" ‚ñë‚ñí‚ñì‚ñà\"\n",
    "max_val = hist_2d.max()\n",
    "\n",
    "for i in range(x_bins-1, -1, -1):  # Reverse for proper orientation\n",
    "    row = \"\"\n",
    "    for j in range(y_bins):\n",
    "        level = int(hist_2d[i, j] / max_val * (len(chars) - 1))\n",
    "        row += chars[level]\n",
    "    print(row)\n",
    "\n",
    "print(f\"\\nPeak count: {max_val:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add50cac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Sparse Histogram (Large Bin Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When num_bins > shared memory size, we need a different approach\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_large_bins(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Histogram for large number of bins (no shared memory).\n",
    "    Uses sorted-segment approach for reduced contention.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Process elements, grouping consecutive same-bin values\n",
    "    for i in range(tid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(hist, val, 1)\n",
    "\n",
    "# For very sparse histograms, consider hash-based approaches\n",
    "print(\"Large Bin Strategies:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. If bins < shared memory: Use privatization\")\n",
    "print(\"2. If bins > shared memory: Direct global atomics\")\n",
    "print(\"3. If very sparse: Hash table or sorting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875c91c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Practical Applications\n",
    "\n",
    "### Image Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def image_histogram_grayscale(image, hist, height, width):\n",
    "    \"\"\"\n",
    "    Compute histogram of grayscale image (0-255).\n",
    "    \"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    n = height * width\n",
    "    \n",
    "    # Initialize\n",
    "    if tid < 256:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Count pixels\n",
    "    for i in range(gid, n, stride):\n",
    "        row = i // width\n",
    "        col = i % width\n",
    "        pixel = image[row, col]\n",
    "        cuda.atomic.add(shared_hist, pixel, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Merge\n",
    "    if tid < 256:\n",
    "        cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a grayscale image\n",
    "height, width = 1080, 1920\n",
    "\n",
    "# Create gradient with some noise\n",
    "image = np.zeros((height, width), dtype=np.uint8)\n",
    "for i in range(height):\n",
    "    image[i, :] = np.clip(i * 256 // height + np.random.randint(-20, 20, width), 0, 255)\n",
    "\n",
    "d_image = cuda.to_device(image)\n",
    "d_hist = cuda.to_device(np.zeros(256, dtype=np.int32))\n",
    "\n",
    "image_histogram_grayscale[256, 256](d_image, d_hist, height, width)\n",
    "\n",
    "hist = d_hist.copy_to_host()\n",
    "\n",
    "print(f\"Image Histogram ({width}x{height} image)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show distribution\n",
    "max_count = max(hist)\n",
    "for i in range(0, 256, 32):\n",
    "    segment_sum = sum(hist[i:i+32])\n",
    "    bar_len = int(segment_sum / (max_count * 10) * 40)\n",
    "    print(f\"{i:3d}-{i+31:3d}: {'‚ñà' * bar_len} {segment_sum:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36054b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0146e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile histogram_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Error checking macro\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Weighted Histogram\n",
    "// ============================================================\n",
    "// Instead of counting +1 per element, add weight[i]\n",
    "// Uses shared memory for local accumulation\n",
    "\n",
    "__global__ void histogramWeighted(const int* data, const float* weights, \n",
    "                                   float* hist, int n, int numBins) {\n",
    "    extern __shared__ float sharedHist[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Initialize shared histogram\n",
    "    for (int i = tid; i < numBins; i += blockDim.x) {\n",
    "        sharedHist[i] = 0.0f;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Accumulate weights in shared memory\n",
    "    for (int i = gid; i < n; i += stride) {\n",
    "        int bin = data[i];\n",
    "        if (bin >= 0 && bin < numBins) {\n",
    "            atomicAdd(&sharedHist[bin], weights[i]);\n",
    "        }\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge to global histogram\n",
    "    for (int i = tid; i < numBins; i += blockDim.x) {\n",
    "        if (sharedHist[i] > 0.0f) {\n",
    "            atomicAdd(&hist[i], sharedHist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Histogram with Overflow Bins\n",
    "// ============================================================\n",
    "// hist[0] = underflow (values < minVal)\n",
    "// hist[1..numBins] = normal bins\n",
    "// hist[numBins+1] = overflow (values >= maxVal)\n",
    "\n",
    "__global__ void histogramWithOverflow(const float* data, int* hist, int n,\n",
    "                                       int numBins, float minVal, float maxVal) {\n",
    "    __shared__ int sharedHist[258];  // Max bins + 2 for under/overflow\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    int totalBins = numBins + 2;\n",
    "    \n",
    "    // Initialize shared histogram\n",
    "    for (int i = tid; i < totalBins; i += blockDim.x) {\n",
    "        sharedHist[i] = 0;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    float binWidth = (maxVal - minVal) / numBins;\n",
    "    \n",
    "    // Bin data with overflow handling\n",
    "    for (int i = gid; i < n; i += stride) {\n",
    "        float val = data[i];\n",
    "        int bin;\n",
    "        \n",
    "        if (val < minVal) {\n",
    "            bin = 0;  // Underflow bin\n",
    "        } else if (val >= maxVal) {\n",
    "            bin = numBins + 1;  // Overflow bin\n",
    "        } else {\n",
    "            bin = 1 + (int)((val - minVal) / binWidth);\n",
    "            bin = min(bin, numBins);  // Handle edge case\n",
    "        }\n",
    "        \n",
    "        atomicAdd(&sharedHist[bin], 1);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge to global\n",
    "    for (int i = tid; i < totalBins; i += blockDim.x) {\n",
    "        if (sharedHist[i] > 0) {\n",
    "            atomicAdd(&hist[i], sharedHist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: RGB Color Histogram\n",
    "// ============================================================\n",
    "// Compute separate histograms for R, G, B channels\n",
    "// Image stored as interleaved RGB (R0,G0,B0,R1,G1,B1,...)\n",
    "\n",
    "__global__ void rgbHistogram(const unsigned char* image, \n",
    "                              int* histR, int* histG, int* histB,\n",
    "                              int numPixels) {\n",
    "    __shared__ int sharedR[256], sharedG[256], sharedB[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Initialize shared histograms\n",
    "    for (int i = tid; i < 256; i += blockDim.x) {\n",
    "        sharedR[i] = 0;\n",
    "        sharedG[i] = 0;\n",
    "        sharedB[i] = 0;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Accumulate in shared memory\n",
    "    for (int i = gid; i < numPixels; i += stride) {\n",
    "        unsigned char r = image[i * 3 + 0];\n",
    "        unsigned char g = image[i * 3 + 1];\n",
    "        unsigned char b = image[i * 3 + 2];\n",
    "        \n",
    "        atomicAdd(&sharedR[r], 1);\n",
    "        atomicAdd(&sharedG[g], 1);\n",
    "        atomicAdd(&sharedB[b], 1);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge to global\n",
    "    for (int i = tid; i < 256; i += blockDim.x) {\n",
    "        if (sharedR[i] > 0) atomicAdd(&histR[i], sharedR[i]);\n",
    "        if (sharedG[i] > 0) atomicAdd(&histG[i], sharedG[i]);\n",
    "        if (sharedB[i] > 0) atomicAdd(&histB[i], sharedB[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Test Functions\n",
    "// ============================================================\n",
    "void testWeightedHistogram() {\n",
    "    printf(\"=== Exercise 1: Weighted Histogram ===\\n\");\n",
    "    \n",
    "    // Simple test: data = [0, 1, 1, 2], weights = [1.0, 2.0, 3.0, 4.0]\n",
    "    const int N = 4;\n",
    "    const int numBins = 8;\n",
    "    int h_data[] = {0, 1, 1, 2};\n",
    "    float h_weights[] = {1.0f, 2.0f, 3.0f, 4.0f};\n",
    "    float h_hist[8] = {0};\n",
    "    \n",
    "    int* d_data;\n",
    "    float* d_weights, *d_hist;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_weights, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_hist, numBins * sizeof(float)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_weights, h_weights, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_hist, 0, numBins * sizeof(float)));\n",
    "    \n",
    "    histogramWeighted<<<1, 32, numBins * sizeof(float)>>>(d_data, d_weights, d_hist, N, numBins);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_hist, d_hist, numBins * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Weighted histogram: \");\n",
    "    for (int i = 0; i < numBins; i++) {\n",
    "        if (h_hist[i] > 0) printf(\"bin[%d]=%.1f \", i, h_hist[i]);\n",
    "    }\n",
    "    printf(\"\\nExpected: bin[0]=1.0 bin[1]=5.0 bin[2]=4.0\\n\");\n",
    "    \n",
    "    bool correct = (h_hist[0] == 1.0f && h_hist[1] == 5.0f && h_hist[2] == 4.0f);\n",
    "    printf(\"Test %s\\n\\n\", correct ? \"PASSED ‚úì\" : \"FAILED ‚úó\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_weights);\n",
    "    cudaFree(d_hist);\n",
    "}\n",
    "\n",
    "void testOverflowHistogram() {\n",
    "    printf(\"=== Exercise 2: Histogram with Overflow Bins ===\\n\");\n",
    "    \n",
    "    const int N = 100;\n",
    "    float h_data[N];\n",
    "    \n",
    "    // Generate data: some under 0, some in [0,10), some >= 10\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_data[i] = (float)(i - 20) / 5.0f;  // Range: -4 to 15.8\n",
    "    }\n",
    "    \n",
    "    int numBins = 10;\n",
    "    int totalBins = numBins + 2;\n",
    "    int h_hist[12] = {0};\n",
    "    \n",
    "    float* d_data;\n",
    "    int* d_hist;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_hist, totalBins * sizeof(int)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_hist, 0, totalBins * sizeof(int)));\n",
    "    \n",
    "    histogramWithOverflow<<<4, 64>>>(d_data, d_hist, N, numBins, 0.0f, 10.0f);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_hist, d_hist, totalBins * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Histogram with overflow (range [0, 10), %d bins):\\n\", numBins);\n",
    "    printf(\"  Underflow (<0): %d\\n\", h_hist[0]);\n",
    "    for (int i = 1; i <= numBins; i++) {\n",
    "        printf(\"  Bin %d [%.1f-%.1f): %d\\n\", i-1, (i-1)*1.0f, i*1.0f, h_hist[i]);\n",
    "    }\n",
    "    printf(\"  Overflow (>=10): %d\\n\", h_hist[numBins + 1]);\n",
    "    \n",
    "    int total = 0;\n",
    "    for (int i = 0; i < totalBins; i++) total += h_hist[i];\n",
    "    printf(\"Total count: %d (expected %d)\\n\", total, N);\n",
    "    printf(\"Test %s\\n\\n\", (total == N) ? \"PASSED ‚úì\" : \"FAILED ‚úó\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_hist);\n",
    "}\n",
    "\n",
    "void testRGBHistogram() {\n",
    "    printf(\"=== Exercise 3: RGB Color Histogram ===\\n\");\n",
    "    \n",
    "    const int numPixels = 10000;\n",
    "    unsigned char* h_image = (unsigned char*)malloc(numPixels * 3);\n",
    "    \n",
    "    // Create test image with known color distribution\n",
    "    srand(42);\n",
    "    for (int i = 0; i < numPixels * 3; i++) {\n",
    "        h_image[i] = rand() % 256;\n",
    "    }\n",
    "    \n",
    "    int h_histR[256] = {0}, h_histG[256] = {0}, h_histB[256] = {0};\n",
    "    \n",
    "    // CPU reference\n",
    "    int cpuR[256] = {0}, cpuG[256] = {0}, cpuB[256] = {0};\n",
    "    for (int i = 0; i < numPixels; i++) {\n",
    "        cpuR[h_image[i*3 + 0]]++;\n",
    "        cpuG[h_image[i*3 + 1]]++;\n",
    "        cpuB[h_image[i*3 + 2]]++;\n",
    "    }\n",
    "    \n",
    "    unsigned char* d_image;\n",
    "    int *d_histR, *d_histG, *d_histB;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_image, numPixels * 3));\n",
    "    CHECK_CUDA(cudaMalloc(&d_histR, 256 * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_histG, 256 * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_histB, 256 * sizeof(int)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_image, h_image, numPixels * 3, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_histR, 0, 256 * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_histG, 0, 256 * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_histB, 0, 256 * sizeof(int)));\n",
    "    \n",
    "    rgbHistogram<<<64, 256>>>(d_image, d_histR, d_histG, d_histB, numPixels);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_histR, d_histR, 256 * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    CHECK_CUDA(cudaMemcpy(h_histG, d_histG, 256 * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    CHECK_CUDA(cudaMemcpy(h_histB, d_histB, 256 * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < 256; i++) {\n",
    "        if (h_histR[i] != cpuR[i] || h_histG[i] != cpuG[i] || h_histB[i] != cpuB[i]) {\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"Sample RGB histogram values:\\n\");\n",
    "    printf(\"  Value 128: R=%d, G=%d, B=%d\\n\", h_histR[128], h_histG[128], h_histB[128]);\n",
    "    printf(\"  Value 0:   R=%d, G=%d, B=%d\\n\", h_histR[0], h_histG[0], h_histB[0]);\n",
    "    printf(\"  Value 255: R=%d, G=%d, B=%d\\n\", h_histR[255], h_histG[255], h_histB[255]);\n",
    "    printf(\"Test %s\\n\\n\", correct ? \"PASSED ‚úì\" : \"FAILED ‚úó\");\n",
    "    \n",
    "    cudaFree(d_image);\n",
    "    cudaFree(d_histR);\n",
    "    cudaFree(d_histG);\n",
    "    cudaFree(d_histB);\n",
    "    free(h_image);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\");\n",
    "    printf(\"‚ïë              CUDA Histogram Exercises                        ‚ïë\\n\");\n",
    "    printf(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\\n\\n\");\n",
    "    \n",
    "    // Print device info\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Compute Capability: %d.%d\\n\\n\", prop.major, prop.minor);\n",
    "    \n",
    "    testWeightedHistogram();\n",
    "    testOverflowHistogram();\n",
    "    testRGBHistogram();\n",
    "    \n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4923d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o histogram_exercises histogram_exercises.cu && ./histogram_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73b3eb",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Weighted Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement weighted histogram\n",
    "# Instead of counting +1 per element, add weight[i]\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_weighted(data, weights, hist, n, num_bins):\n",
    "    \"\"\"Weighted histogram: sum weights instead of counting.\"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # TODO: Initialize shared memory\n",
    "    # TODO: Accumulate weights in shared memory\n",
    "    # TODO: Merge to global\n",
    "    pass\n",
    "\n",
    "# Test: data = [0, 1, 1, 2], weights = [1.0, 2.0, 3.0, 4.0]\n",
    "# Result: hist[0]=1.0, hist[1]=5.0, hist[2]=4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d112d2",
   "metadata": {},
   "source": [
    "### Exercise 2: Histogram with Overflow Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add underflow and overflow bins\n",
    "# hist[0] = count of values < min_val\n",
    "# hist[1..num_bins] = normal bins\n",
    "# hist[num_bins+1] = count of values >= max_val\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_with_overflow(data, hist, n, num_bins, min_val, max_val):\n",
    "    \"\"\"Histogram with underflow/overflow bins.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Total output size = num_bins + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8116f11",
   "metadata": {},
   "source": [
    "### Exercise 3: RGB Color Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde50fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute separate histograms for R, G, B channels\n",
    "# image is shape (height, width, 3)\n",
    "\n",
    "@cuda.jit\n",
    "def rgb_histogram(image, hist_r, hist_g, hist_b, height, width):\n",
    "    \"\"\"Compute histograms for each RGB channel.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Each histogram should have 256 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba84e03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Histogram Implementation Strategies\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    HISTOGRAM STRATEGIES                     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  Small bins (< 256):                                        ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Shared memory privatization (fastest)                   ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  Medium bins (< 4096):                                      ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Multiple shared histograms per block                    ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  Large bins:                                                ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Direct global atomics (fallback)                        ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  Very sparse:                                               ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Sort + unique count                                     ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Always use shared memory** for bins that fit\n",
    "2. **Initialize shared memory in parallel**\n",
    "3. **Skip merge for zero counts** (minor optimization)\n",
    "4. **Consider data distribution** - uniform is worst case\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Histograms are atomic-heavy** - need optimization\n",
    "2. **Privatization is essential** for performance\n",
    "3. **Shared memory atomics** are ~10x faster than global\n",
    "4. **2D histograms** work the same way with flattened indices\n",
    "\n",
    "---\n",
    "\n",
    "## Week 4 Complete! üéâ\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "| Day | Topic | Key Skills |\n",
    "|-----|-------|------------|\n",
    "| 1 | Parallel Reduction | Tree reduction, multi-pass |\n",
    "| 2 | Warp Primitives | Shuffle, no-sync reduction |\n",
    "| 3 | Atomic Operations | Thread-safe updates, CAS |\n",
    "| 4 | Histogram | Privatization, shared atomics |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "üìã **Day 5:** Complete the checkpoint quiz\n",
    "\n",
    "üìã **Week 5 Preview:** Matrix Operations\n",
    "- Matrix-vector multiplication\n",
    "- Matrix-matrix multiplication (tiled)\n",
    "- Memory access optimization\n",
    "- Cache blocking"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
