{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2443e5a0",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 4: Histogram - A Complete Example\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-04/day-4-histogram.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement histogram counting with atomics\n",
    "- Use shared memory privatization for performance\n",
    "- Extend to 2D histograms and image processing\n",
    "- Apply best practices for counting algorithms\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a229f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What is a Histogram?\n",
    "\n",
    "### Counting Values into Bins\n",
    "\n",
    "```\n",
    "Data:  [3, 1, 4, 1, 5, 9, 2, 6, 5, 3]\n",
    "\n",
    "Histogram (bins 0-9):\n",
    "Bin 0: 0 occurrences  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 1: 2 occurrences  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 2: 1 occurrence   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 3: 2 occurrences  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 4: 1 occurrence   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 5: 2 occurrences  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 6: 1 occurrence   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 7: 0 occurrences  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 8: 0 occurrences  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 9: 1 occurrence   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "```\n",
    "\n",
    "### CUDA C++ Histogram Implementations (Primary)\n",
    "\n",
    "The following CUDA C++ implementation demonstrates histogram computation with shared memory privatization for reduced atomic contention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile histogram.cu\n",
    "// histogram.cu - GPU histogram with privatization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define NUM_BINS 256\n",
    "#define BLOCK_SIZE 256\n",
    "\n",
    "// Naive: Global atomics only (slow due to contention)\n",
    "__global__ void histogramNaive(const unsigned char* data, int* hist, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        atomicAdd(&hist[data[i]], 1);  // High contention!\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized: Shared memory privatization\n",
    "__global__ void histogramPrivatized(const unsigned char* data, int* hist, int n) {\n",
    "    // Private histogram in shared memory\n",
    "    __shared__ int localHist[NUM_BINS];\n",
    "    \n",
    "    // Initialize shared memory\n",
    "    for (int i = threadIdx.x; i < NUM_BINS; i += blockDim.x) {\n",
    "        localHist[i] = 0;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Count into shared memory (low contention within block)\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        atomicAdd(&localHist[data[i]], 1);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge local histograms to global (once per bin per block)\n",
    "    for (int i = threadIdx.x; i < NUM_BINS; i += blockDim.x) {\n",
    "        if (localHist[i] > 0) {\n",
    "            atomicAdd(&hist[i], localHist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000000;  // 10M data points\n",
    "    \n",
    "    unsigned char *h_data = (unsigned char*)malloc(n);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_data[i] = rand() % 256;\n",
    "    }\n",
    "    \n",
    "    unsigned char *d_data;\n",
    "    int *d_hist;\n",
    "    cudaMalloc(&d_data, n);\n",
    "    cudaMalloc(&d_hist, NUM_BINS * sizeof(int));\n",
    "    cudaMemset(d_hist, 0, NUM_BINS * sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_data, h_data, n, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    histogramPrivatized<<<256, 256>>>(d_data, d_hist, n);\n",
    "    \n",
    "    int h_hist[NUM_BINS];\n",
    "    cudaMemcpy(h_hist, d_hist, NUM_BINS * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Sample histogram values:\\n\");\n",
    "    for (int i = 0; i < 5; i++) {\n",
    "        printf(\"  Bin %d: %d\\n\", i, h_hist[i]);\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_data); cudaFree(d_hist);\n",
    "    free(h_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953026b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o histogram histogram.cu\n",
    "!./histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87096de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU baseline for comparison\n",
    "def cpu_histogram(data, num_bins):\n",
    "    \"\"\"Simple CPU histogram.\"\"\"\n",
    "    hist = np.zeros(num_bins, dtype=np.int32)\n",
    "    for val in data:\n",
    "        if 0 <= val < num_bins:\n",
    "            hist[val] += 1\n",
    "    return hist\n",
    "\n",
    "# NumPy optimized\n",
    "def numpy_histogram(data, num_bins):\n",
    "    return np.bincount(data.astype(np.int32), minlength=num_bins)[:num_bins]\n",
    "\n",
    "# Test\n",
    "data = np.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3], dtype=np.int32)\n",
    "hist = cpu_histogram(data, 10)\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"Histogram: {hist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9654eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Naive GPU Histogram\n",
    "\n",
    "### Using Global Memory Atomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25443c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_global_atomic(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Naive histogram: each thread does global atomic add.\n",
    "    Simple but slow due to contention.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(hist, val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add40d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test naive histogram\n",
    "n = 10_000_000\n",
    "num_bins = 256\n",
    "\n",
    "# Random data with values 0-255\n",
    "data = np.random.randint(0, num_bins, n).astype(np.int32)\n",
    "d_data = cuda.to_device(data)\n",
    "d_hist = cuda.device_array(num_bins, dtype=np.int32)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Reset and compute\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "histogram_global_atomic[blocks, threads](d_data, d_hist, n, num_bins)\n",
    "\n",
    "gpu_hist = d_hist.copy_to_host()\n",
    "cpu_hist = numpy_histogram(data, num_bins)\n",
    "\n",
    "print(f\"GPU histogram matches CPU: {'âœ“' if np.array_equal(gpu_hist, cpu_hist) else 'âœ—'}\")\n",
    "print(f\"\\nFirst 10 bins: {gpu_hist[:10]}\")\n",
    "print(f\"Total count: {np.sum(gpu_hist):,} (expected: {n:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b60ba9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Optimized Histogram with Shared Memory\n",
    "\n",
    "### Privatization Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87611788",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_shared_atomic(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Optimized histogram using shared memory privatization.\n",
    "    \n",
    "    1. Each block has private histogram in shared memory\n",
    "    2. Threads atomically update shared (fast!)\n",
    "    3. Merge to global at the end (fewer global atomics)\n",
    "    \"\"\"\n",
    "    # Shared memory for block's private histogram\n",
    "    # Assuming max 256 bins\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Phase 1: Initialize shared histogram to zeros\n",
    "    if tid < num_bins:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 2: Count into shared memory (fast atomics!)\n",
    "    for i in range(gid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(shared_hist, val, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 3: Merge to global (one atomic per bin per block)\n",
    "    if tid < num_bins:\n",
    "        if shared_hist[tid] > 0:\n",
    "            cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3387fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimized histogram\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "histogram_shared_atomic[blocks, threads](d_data, d_hist, n, num_bins)\n",
    "\n",
    "gpu_hist_opt = d_hist.copy_to_host()\n",
    "\n",
    "print(f\"Optimized histogram matches CPU: {'âœ“' if np.array_equal(gpu_hist_opt, cpu_hist) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca2249",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_histograms(n, num_bins=256, iterations=50):\n",
    "    \"\"\"Compare histogram implementations.\"\"\"\n",
    "    data = np.random.randint(0, num_bins, n).astype(np.int32)\n",
    "    d_data = cuda.to_device(data)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # CPU (NumPy)\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        _ = numpy_histogram(data, num_bins)\n",
    "    cpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # GPU Global Atomic\n",
    "    d_hist1 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "    histogram_global_atomic[blocks, threads](d_data, d_hist1, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        d_hist1 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "        histogram_global_atomic[blocks, threads](d_data, d_hist1, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    global_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # GPU Shared Atomic\n",
    "    d_hist2 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "    histogram_shared_atomic[blocks, threads](d_data, d_hist2, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        d_hist2 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "        histogram_shared_atomic[blocks, threads](d_data, d_hist2, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    shared_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    return cpu_time, global_time, shared_time\n",
    "\n",
    "# Benchmark\n",
    "sizes = [1_000_000, 5_000_000, 10_000_000, 50_000_000]\n",
    "\n",
    "print(f\"Histogram Benchmark (256 bins)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Size':>12} | {'CPU (ms)':>10} | {'Global (ms)':>12} | {'Shared (ms)':>12} | {'Speedup':>8}\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "for n in sizes:\n",
    "    cpu_t, global_t, shared_t = benchmark_histograms(n)\n",
    "    speedup = cpu_t / shared_t\n",
    "    print(f\"{n:>12,} | {cpu_t:>10.2f} | {global_t:>12.2f} | {shared_t:>12.2f} | {speedup:>7.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b1099",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Real-Value Histograms\n",
    "\n",
    "### Handling Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_float(data, hist, n, num_bins, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Histogram for floating-point data with specified range.\n",
    "    \n",
    "    Maps [min_val, max_val) to bins [0, num_bins)\n",
    "    \"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Initialize shared memory\n",
    "    if tid < num_bins:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Calculate bin width\n",
    "    bin_width = (max_val - min_val) / num_bins\n",
    "    \n",
    "    # Count\n",
    "    for i in range(gid, n, stride):\n",
    "        val = data[i]\n",
    "        \n",
    "        # Calculate bin index\n",
    "        if val >= min_val and val < max_val:\n",
    "            bin_idx = int((val - min_val) / bin_width)\n",
    "            bin_idx = min(bin_idx, num_bins - 1)  # Handle edge case\n",
    "            cuda.atomic.add(shared_hist, bin_idx, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Merge to global\n",
    "    if tid < num_bins:\n",
    "        if shared_hist[tid] > 0:\n",
    "            cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test float histogram\n",
    "n = 1_000_000\n",
    "num_bins = 50\n",
    "\n",
    "# Generate normal distribution\n",
    "data = np.random.randn(n).astype(np.float32)\n",
    "min_val, max_val = -4.0, 4.0\n",
    "\n",
    "d_data = cuda.to_device(data)\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "\n",
    "histogram_float[256, 256](d_data, d_hist, n, num_bins, min_val, max_val)\n",
    "\n",
    "gpu_hist = d_hist.copy_to_host()\n",
    "\n",
    "# Visualize\n",
    "print(f\"Histogram of Normal Distribution (N={n:,})\")\n",
    "print(f\"Range: [{min_val}, {max_val})\")\n",
    "print()\n",
    "\n",
    "max_count = max(gpu_hist)\n",
    "bin_width = (max_val - min_val) / num_bins\n",
    "\n",
    "for i in range(0, num_bins, 5):  # Show every 5th bin\n",
    "    bin_start = min_val + i * bin_width\n",
    "    bar_len = int(gpu_hist[i] / max_count * 30)\n",
    "    print(f\"{bin_start:>6.1f}: {'â–ˆ' * bar_len} {gpu_hist[i]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6bbb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: 2D Histogram (Joint Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_2d(x_data, y_data, hist, n, \n",
    "                 x_bins, y_bins, \n",
    "                 x_min, x_max, y_min, y_max):\n",
    "    \"\"\"\n",
    "    2D histogram for joint distribution.\n",
    "    \n",
    "    hist has shape (x_bins, y_bins) flattened to 1D\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    x_width = (x_max - x_min) / x_bins\n",
    "    y_width = (y_max - y_min) / y_bins\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        x = x_data[i]\n",
    "        y = y_data[i]\n",
    "        \n",
    "        if x >= x_min and x < x_max and y >= y_min and y < y_max:\n",
    "            x_bin = int((x - x_min) / x_width)\n",
    "            y_bin = int((y - y_min) / y_width)\n",
    "            \n",
    "            x_bin = min(x_bin, x_bins - 1)\n",
    "            y_bin = min(y_bin, y_bins - 1)\n",
    "            \n",
    "            # Flatten 2D index\n",
    "            flat_idx = x_bin * y_bins + y_bin\n",
    "            cuda.atomic.add(hist, flat_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16efe075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2D histogram\n",
    "n = 1_000_000\n",
    "x_bins, y_bins = 20, 20\n",
    "\n",
    "# Correlated normal distributions\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.8], [0.8, 1]]  # Correlation = 0.8\n",
    "xy = np.random.multivariate_normal(mean, cov, n).astype(np.float32)\n",
    "x_data, y_data = xy[:, 0], xy[:, 1]\n",
    "\n",
    "d_x = cuda.to_device(x_data)\n",
    "d_y = cuda.to_device(y_data)\n",
    "d_hist = cuda.to_device(np.zeros(x_bins * y_bins, dtype=np.int32))\n",
    "\n",
    "histogram_2d[256, 256](d_x, d_y, d_hist, n,\n",
    "                       x_bins, y_bins,\n",
    "                       -4.0, 4.0, -4.0, 4.0)\n",
    "\n",
    "hist_2d = d_hist.copy_to_host().reshape(x_bins, y_bins)\n",
    "\n",
    "# Simple ASCII visualization\n",
    "print(\"2D Histogram (Correlated Normal, Ï=0.8)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "chars = \" â–‘â–’â–“â–ˆ\"\n",
    "max_val = hist_2d.max()\n",
    "\n",
    "for i in range(x_bins-1, -1, -1):  # Reverse for proper orientation\n",
    "    row = \"\"\n",
    "    for j in range(y_bins):\n",
    "        level = int(hist_2d[i, j] / max_val * (len(chars) - 1))\n",
    "        row += chars[level]\n",
    "    print(row)\n",
    "\n",
    "print(f\"\\nPeak count: {max_val:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add50cac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Sparse Histogram (Large Bin Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When num_bins > shared memory size, we need a different approach\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_large_bins(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Histogram for large number of bins (no shared memory).\n",
    "    Uses sorted-segment approach for reduced contention.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Process elements, grouping consecutive same-bin values\n",
    "    for i in range(tid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(hist, val, 1)\n",
    "\n",
    "# For very sparse histograms, consider hash-based approaches\n",
    "print(\"Large Bin Strategies:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. If bins < shared memory: Use privatization\")\n",
    "print(\"2. If bins > shared memory: Direct global atomics\")\n",
    "print(\"3. If very sparse: Hash table or sorting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875c91c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Practical Applications\n",
    "\n",
    "### Image Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def image_histogram_grayscale(image, hist, height, width):\n",
    "    \"\"\"\n",
    "    Compute histogram of grayscale image (0-255).\n",
    "    \"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    n = height * width\n",
    "    \n",
    "    # Initialize\n",
    "    if tid < 256:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Count pixels\n",
    "    for i in range(gid, n, stride):\n",
    "        row = i // width\n",
    "        col = i % width\n",
    "        pixel = image[row, col]\n",
    "        cuda.atomic.add(shared_hist, pixel, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Merge\n",
    "    if tid < 256:\n",
    "        cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a grayscale image\n",
    "height, width = 1080, 1920\n",
    "\n",
    "# Create gradient with some noise\n",
    "image = np.zeros((height, width), dtype=np.uint8)\n",
    "for i in range(height):\n",
    "    image[i, :] = np.clip(i * 256 // height + np.random.randint(-20, 20, width), 0, 255)\n",
    "\n",
    "d_image = cuda.to_device(image)\n",
    "d_hist = cuda.to_device(np.zeros(256, dtype=np.int32))\n",
    "\n",
    "image_histogram_grayscale[256, 256](d_image, d_hist, height, width)\n",
    "\n",
    "hist = d_hist.copy_to_host()\n",
    "\n",
    "print(f\"Image Histogram ({width}x{height} image)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show distribution\n",
    "max_count = max(hist)\n",
    "for i in range(0, 256, 32):\n",
    "    segment_sum = sum(hist[i:i+32])\n",
    "    bar_len = int(segment_sum / (max_count * 10) * 40)\n",
    "    print(f\"{i:3d}-{i+31:3d}: {'â–ˆ' * bar_len} {segment_sum:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36054b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Weighted Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement weighted histogram\n",
    "# Instead of counting +1 per element, add weight[i]\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_weighted(data, weights, hist, n, num_bins):\n",
    "    \"\"\"Weighted histogram: sum weights instead of counting.\"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # TODO: Initialize shared memory\n",
    "    # TODO: Accumulate weights in shared memory\n",
    "    # TODO: Merge to global\n",
    "    pass\n",
    "\n",
    "# Test: data = [0, 1, 1, 2], weights = [1.0, 2.0, 3.0, 4.0]\n",
    "# Result: hist[0]=1.0, hist[1]=5.0, hist[2]=4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d112d2",
   "metadata": {},
   "source": [
    "### Exercise 2: Histogram with Overflow Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add underflow and overflow bins\n",
    "# hist[0] = count of values < min_val\n",
    "# hist[1..num_bins] = normal bins\n",
    "# hist[num_bins+1] = count of values >= max_val\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_with_overflow(data, hist, n, num_bins, min_val, max_val):\n",
    "    \"\"\"Histogram with underflow/overflow bins.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Total output size = num_bins + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8116f11",
   "metadata": {},
   "source": [
    "### Exercise 3: RGB Color Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde50fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute separate histograms for R, G, B channels\n",
    "# image is shape (height, width, 3)\n",
    "\n",
    "@cuda.jit\n",
    "def rgb_histogram(image, hist_r, hist_g, hist_b, height, width):\n",
    "    \"\"\"Compute histograms for each RGB channel.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Each histogram should have 256 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba84e03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Histogram Implementation Strategies\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    HISTOGRAM STRATEGIES                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Small bins (< 256):                                        â”‚\n",
    "â”‚  â””â”€ Shared memory privatization (fastest)                   â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Medium bins (< 4096):                                      â”‚\n",
    "â”‚  â””â”€ Multiple shared histograms per block                    â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Large bins:                                                â”‚\n",
    "â”‚  â””â”€ Direct global atomics (fallback)                        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Very sparse:                                               â”‚\n",
    "â”‚  â””â”€ Sort + unique count                                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Always use shared memory** for bins that fit\n",
    "2. **Initialize shared memory in parallel**\n",
    "3. **Skip merge for zero counts** (minor optimization)\n",
    "4. **Consider data distribution** - uniform is worst case\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Histograms are atomic-heavy** - need optimization\n",
    "2. **Privatization is essential** for performance\n",
    "3. **Shared memory atomics** are ~10x faster than global\n",
    "4. **2D histograms** work the same way with flattened indices\n",
    "\n",
    "---\n",
    "\n",
    "## Week 4 Complete! ğŸ‰\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "| Day | Topic | Key Skills |\n",
    "|-----|-------|------------|\n",
    "| 1 | Parallel Reduction | Tree reduction, multi-pass |\n",
    "| 2 | Warp Primitives | Shuffle, no-sync reduction |\n",
    "| 3 | Atomic Operations | Thread-safe updates, CAS |\n",
    "| 4 | Histogram | Privatization, shared atomics |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "ğŸ“‹ **Day 5:** Complete the checkpoint quiz\n",
    "\n",
    "ğŸ“‹ **Week 5 Preview:** Matrix Operations\n",
    "- Matrix-vector multiplication\n",
    "- Matrix-matrix multiplication (tiled)\n",
    "- Memory access optimization\n",
    "- Cache blocking"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
