{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2443e5a0",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 4: Histogram - A Complete Example\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-04/day-4-histogram.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ£ The Hook: Counting a Million Votes\n",
    "\n",
    "**It's election night.** 10 million votes need counting across 50 candidates. One approach: everyone shouts their vote at a single counterâ€”chaos! The better way: divide volunteers into precincts, count locally, then merge totals. That's exactly how GPU histograms work!\n",
    "\n",
    "> **ğŸ—³ï¸ Vote Counting Analogy:** Each polling station (thread block) maintains its own tally sheet (shared memory histogram). Volunteers (threads) mark votes locally with minimal crowding. At the end, precinct totals merge into the final count. Parallel counting with coordinated mergingâ€”the histogram pattern!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "| # | Objective | Skill Level |\n",
    "|---|-----------|-------------|\n",
    "| 1 | **Implement** histogram counting with atomics for thread-safe bin updates | Implementation |\n",
    "| 2 | **Use** shared memory privatization for 10-100x performance improvement | Optimization |\n",
    "| 3 | **Extend** to 2D histograms and image processing applications | Application |\n",
    "| 4 | **Apply** best practices for high-performance counting algorithms | Mastery |\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeaada2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ Concept Card: The Vote Counting Strategy\n",
    "\n",
    "> ### ğŸ—³ï¸ The Histogram Mental Model\n",
    ">\n",
    "> Think of **histogram computation** as organizing a large-scale vote count:\n",
    ">\n",
    "> | Vote Counting | GPU Histogram |\n",
    "> |---------------|---------------|\n",
    "> | Each precinct has a tally sheet | Each block has shared memory bins |\n",
    "> | Volunteers count locally | Threads use shared memory atomics |\n",
    "> | Low crowding per sheet | Low contention (256 threads/block) |\n",
    "> | Merge precinct totals | Merge block histograms to global |\n",
    ">\n",
    "> **Naive approach (bad):**\n",
    "> ```\n",
    "> 10,000 volunteers â†’ 1 tally sheet\n",
    "> Everyone pushing and shoving to mark votes!\n",
    "> ```\n",
    ">\n",
    "> **Privatization approach (good):**\n",
    "> ```\n",
    "> 10,000 volunteers Ã· 40 precincts = 250 per precinct\n",
    "> Each precinct: 250 people â†’ 1 local tally (manageable!)\n",
    "> At end: Merge 40 tallies â†’ final count (fast!)\n",
    "> ```\n",
    ">\n",
    "> **The Key Insight:** Contention is the enemy. By privatizing to blocks first, we reduce contention from 10,000:1 to 250:1, then merge with just 40 global atomics per bin!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a229f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What is a Histogram?\n",
    "\n",
    "### Counting Values into Bins\n",
    "\n",
    "```\n",
    "Data:  [3, 1, 4, 1, 5, 9, 2, 6, 5, 3]\n",
    "\n",
    "Histogram (bins 0-9):\n",
    "Bin 0: 0 occurrences  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 1: 2 occurrences  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 2: 1 occurrence   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 3: 2 occurrences  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 4: 1 occurrence   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 5: 2 occurrences  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 6: 1 occurrence   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 7: 0 occurrences  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 8: 0 occurrences  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Bin 9: 1 occurrence   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "The following CUDA C++ implementation demonstrates histogram computation with shared memory privatization for reduced atomic contention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile histogram.cu\n",
    "// histogram.cu - GPU histogram with privatization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define NUM_BINS 256\n",
    "#define BLOCK_SIZE 256\n",
    "\n",
    "// Naive: Global atomics only (slow due to contention)\n",
    "__global__ void histogramNaive(const unsigned char* data, int* hist, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        atomicAdd(&hist[data[i]], 1);  // High contention!\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized: Shared memory privatization\n",
    "__global__ void histogramPrivatized(const unsigned char* data, int* hist, int n) {\n",
    "    // Private histogram in shared memory\n",
    "    __shared__ int localHist[NUM_BINS];\n",
    "    \n",
    "    // Initialize shared memory\n",
    "    for (int i = threadIdx.x; i < NUM_BINS; i += blockDim.x) {\n",
    "        localHist[i] = 0;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Count into shared memory (low contention within block)\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = tid; i < n; i += stride) {\n",
    "        atomicAdd(&localHist[data[i]], 1);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge local histograms to global (once per bin per block)\n",
    "    for (int i = threadIdx.x; i < NUM_BINS; i += blockDim.x) {\n",
    "        if (localHist[i] > 0) {\n",
    "            atomicAdd(&hist[i], localHist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000000;  // 10M data points\n",
    "    \n",
    "    unsigned char *h_data = (unsigned char*)malloc(n);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_data[i] = rand() % 256;\n",
    "    }\n",
    "    \n",
    "    unsigned char *d_data;\n",
    "    int *d_hist;\n",
    "    cudaMalloc(&d_data, n);\n",
    "    cudaMalloc(&d_hist, NUM_BINS * sizeof(int));\n",
    "    cudaMemset(d_hist, 0, NUM_BINS * sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_data, h_data, n, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    histogramPrivatized<<<256, 256>>>(d_data, d_hist, n);\n",
    "    \n",
    "    int h_hist[NUM_BINS];\n",
    "    cudaMemcpy(h_hist, d_hist, NUM_BINS * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Sample histogram values:\\n\");\n",
    "    for (int i = 0; i < 5; i++) {\n",
    "        printf(\"  Bin %d: %d\\n\", i, h_hist[i]);\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_data); cudaFree(d_hist);\n",
    "    free(h_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953026b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o histogram histogram.cu\n",
    "!./histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5f0a5",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)\n",
    "\n",
    "CPU baseline for comparison with GPU histogram implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87096de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU baseline for comparison\n",
    "def cpu_histogram(data, num_bins):\n",
    "    \"\"\"Simple CPU histogram.\"\"\"\n",
    "    hist = np.zeros(num_bins, dtype=np.int32)\n",
    "    for val in data:\n",
    "        if 0 <= val < num_bins:\n",
    "            hist[val] += 1\n",
    "    return hist\n",
    "\n",
    "# NumPy optimized\n",
    "def numpy_histogram(data, num_bins):\n",
    "    return np.bincount(data.astype(np.int32), minlength=num_bins)[:num_bins]\n",
    "\n",
    "# Test\n",
    "data = np.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3], dtype=np.int32)\n",
    "hist = cpu_histogram(data, 10)\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"Histogram: {hist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9654eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Naive GPU Histogram\n",
    "\n",
    "### Using Global Memory Atomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25443c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_global_atomic(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Naive histogram: each thread does global atomic add.\n",
    "    Simple but slow due to contention.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(hist, val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add40d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test naive histogram\n",
    "n = 10_000_000\n",
    "num_bins = 256\n",
    "\n",
    "# Random data with values 0-255\n",
    "data = np.random.randint(0, num_bins, n).astype(np.int32)\n",
    "d_data = cuda.to_device(data)\n",
    "d_hist = cuda.device_array(num_bins, dtype=np.int32)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Reset and compute\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "histogram_global_atomic[blocks, threads](d_data, d_hist, n, num_bins)\n",
    "\n",
    "gpu_hist = d_hist.copy_to_host()\n",
    "cpu_hist = numpy_histogram(data, num_bins)\n",
    "\n",
    "print(f\"GPU histogram matches CPU: {'âœ“' if np.array_equal(gpu_hist, cpu_hist) else 'âœ—'}\")\n",
    "print(f\"\\nFirst 10 bins: {gpu_hist[:10]}\")\n",
    "print(f\"Total count: {np.sum(gpu_hist):,} (expected: {n:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b60ba9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Optimized Histogram with Shared Memory\n",
    "\n",
    "### Privatization Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87611788",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_shared_atomic(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Optimized histogram using shared memory privatization.\n",
    "    \n",
    "    1. Each block has private histogram in shared memory\n",
    "    2. Threads atomically update shared (fast!)\n",
    "    3. Merge to global at the end (fewer global atomics)\n",
    "    \"\"\"\n",
    "    # Shared memory for block's private histogram\n",
    "    # Assuming max 256 bins\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Phase 1: Initialize shared histogram to zeros\n",
    "    if tid < num_bins:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 2: Count into shared memory (fast atomics!)\n",
    "    for i in range(gid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(shared_hist, val, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 3: Merge to global (one atomic per bin per block)\n",
    "    if tid < num_bins:\n",
    "        if shared_hist[tid] > 0:\n",
    "            cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3387fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimized histogram\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "histogram_shared_atomic[blocks, threads](d_data, d_hist, n, num_bins)\n",
    "\n",
    "gpu_hist_opt = d_hist.copy_to_host()\n",
    "\n",
    "print(f\"Optimized histogram matches CPU: {'âœ“' if np.array_equal(gpu_hist_opt, cpu_hist) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca2249",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_histograms(n, num_bins=256, iterations=50):\n",
    "    \"\"\"Compare histogram implementations.\"\"\"\n",
    "    data = np.random.randint(0, num_bins, n).astype(np.int32)\n",
    "    d_data = cuda.to_device(data)\n",
    "    \n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    # CPU (NumPy)\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        _ = numpy_histogram(data, num_bins)\n",
    "    cpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # GPU Global Atomic\n",
    "    d_hist1 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "    histogram_global_atomic[blocks, threads](d_data, d_hist1, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        d_hist1 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "        histogram_global_atomic[blocks, threads](d_data, d_hist1, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    global_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # GPU Shared Atomic\n",
    "    d_hist2 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "    histogram_shared_atomic[blocks, threads](d_data, d_hist2, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        d_hist2 = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "        histogram_shared_atomic[blocks, threads](d_data, d_hist2, n, num_bins)\n",
    "    cuda.synchronize()\n",
    "    shared_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    return cpu_time, global_time, shared_time\n",
    "\n",
    "# Benchmark\n",
    "sizes = [1_000_000, 5_000_000, 10_000_000, 50_000_000]\n",
    "\n",
    "print(f\"Histogram Benchmark (256 bins)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Size':>12} | {'CPU (ms)':>10} | {'Global (ms)':>12} | {'Shared (ms)':>12} | {'Speedup':>8}\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "for n in sizes:\n",
    "    cpu_t, global_t, shared_t = benchmark_histograms(n)\n",
    "    speedup = cpu_t / shared_t\n",
    "    print(f\"{n:>12,} | {cpu_t:>10.2f} | {global_t:>12.2f} | {shared_t:>12.2f} | {speedup:>7.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b1099",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Real-Value Histograms\n",
    "\n",
    "### Handling Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_float(data, hist, n, num_bins, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Histogram for floating-point data with specified range.\n",
    "    \n",
    "    Maps [min_val, max_val) to bins [0, num_bins)\n",
    "    \"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Initialize shared memory\n",
    "    if tid < num_bins:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Calculate bin width\n",
    "    bin_width = (max_val - min_val) / num_bins\n",
    "    \n",
    "    # Count\n",
    "    for i in range(gid, n, stride):\n",
    "        val = data[i]\n",
    "        \n",
    "        # Calculate bin index\n",
    "        if val >= min_val and val < max_val:\n",
    "            bin_idx = int((val - min_val) / bin_width)\n",
    "            bin_idx = min(bin_idx, num_bins - 1)  # Handle edge case\n",
    "            cuda.atomic.add(shared_hist, bin_idx, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Merge to global\n",
    "    if tid < num_bins:\n",
    "        if shared_hist[tid] > 0:\n",
    "            cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test float histogram\n",
    "n = 1_000_000\n",
    "num_bins = 50\n",
    "\n",
    "# Generate normal distribution\n",
    "data = np.random.randn(n).astype(np.float32)\n",
    "min_val, max_val = -4.0, 4.0\n",
    "\n",
    "d_data = cuda.to_device(data)\n",
    "d_hist = cuda.to_device(np.zeros(num_bins, dtype=np.int32))\n",
    "\n",
    "histogram_float[256, 256](d_data, d_hist, n, num_bins, min_val, max_val)\n",
    "\n",
    "gpu_hist = d_hist.copy_to_host()\n",
    "\n",
    "# Visualize\n",
    "print(f\"Histogram of Normal Distribution (N={n:,})\")\n",
    "print(f\"Range: [{min_val}, {max_val})\")\n",
    "print()\n",
    "\n",
    "max_count = max(gpu_hist)\n",
    "bin_width = (max_val - min_val) / num_bins\n",
    "\n",
    "for i in range(0, num_bins, 5):  # Show every 5th bin\n",
    "    bin_start = min_val + i * bin_width\n",
    "    bar_len = int(gpu_hist[i] / max_count * 30)\n",
    "    print(f\"{bin_start:>6.1f}: {'â–ˆ' * bar_len} {gpu_hist[i]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6bbb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: 2D Histogram (Joint Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def histogram_2d(x_data, y_data, hist, n, \n",
    "                 x_bins, y_bins, \n",
    "                 x_min, x_max, y_min, y_max):\n",
    "    \"\"\"\n",
    "    2D histogram for joint distribution.\n",
    "    \n",
    "    hist has shape (x_bins, y_bins) flattened to 1D\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    x_width = (x_max - x_min) / x_bins\n",
    "    y_width = (y_max - y_min) / y_bins\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        x = x_data[i]\n",
    "        y = y_data[i]\n",
    "        \n",
    "        if x >= x_min and x < x_max and y >= y_min and y < y_max:\n",
    "            x_bin = int((x - x_min) / x_width)\n",
    "            y_bin = int((y - y_min) / y_width)\n",
    "            \n",
    "            x_bin = min(x_bin, x_bins - 1)\n",
    "            y_bin = min(y_bin, y_bins - 1)\n",
    "            \n",
    "            # Flatten 2D index\n",
    "            flat_idx = x_bin * y_bins + y_bin\n",
    "            cuda.atomic.add(hist, flat_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16efe075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2D histogram\n",
    "n = 1_000_000\n",
    "x_bins, y_bins = 20, 20\n",
    "\n",
    "# Correlated normal distributions\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.8], [0.8, 1]]  # Correlation = 0.8\n",
    "xy = np.random.multivariate_normal(mean, cov, n).astype(np.float32)\n",
    "x_data, y_data = xy[:, 0], xy[:, 1]\n",
    "\n",
    "d_x = cuda.to_device(x_data)\n",
    "d_y = cuda.to_device(y_data)\n",
    "d_hist = cuda.to_device(np.zeros(x_bins * y_bins, dtype=np.int32))\n",
    "\n",
    "histogram_2d[256, 256](d_x, d_y, d_hist, n,\n",
    "                       x_bins, y_bins,\n",
    "                       -4.0, 4.0, -4.0, 4.0)\n",
    "\n",
    "hist_2d = d_hist.copy_to_host().reshape(x_bins, y_bins)\n",
    "\n",
    "# Simple ASCII visualization\n",
    "print(\"2D Histogram (Correlated Normal, Ï=0.8)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "chars = \" â–‘â–’â–“â–ˆ\"\n",
    "max_val = hist_2d.max()\n",
    "\n",
    "for i in range(x_bins-1, -1, -1):  # Reverse for proper orientation\n",
    "    row = \"\"\n",
    "    for j in range(y_bins):\n",
    "        level = int(hist_2d[i, j] / max_val * (len(chars) - 1))\n",
    "        row += chars[level]\n",
    "    print(row)\n",
    "\n",
    "print(f\"\\nPeak count: {max_val:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add50cac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Sparse Histogram (Large Bin Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When num_bins > shared memory size, we need a different approach\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_large_bins(data, hist, n, num_bins):\n",
    "    \"\"\"\n",
    "    Histogram for large number of bins (no shared memory).\n",
    "    Uses sorted-segment approach for reduced contention.\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Process elements, grouping consecutive same-bin values\n",
    "    for i in range(tid, n, stride):\n",
    "        val = data[i]\n",
    "        if 0 <= val < num_bins:\n",
    "            cuda.atomic.add(hist, val, 1)\n",
    "\n",
    "# For very sparse histograms, consider hash-based approaches\n",
    "print(\"Large Bin Strategies:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. If bins < shared memory: Use privatization\")\n",
    "print(\"2. If bins > shared memory: Direct global atomics\")\n",
    "print(\"3. If very sparse: Hash table or sorting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875c91c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Practical Applications\n",
    "\n",
    "### Image Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def image_histogram_grayscale(image, hist, height, width):\n",
    "    \"\"\"\n",
    "    Compute histogram of grayscale image (0-255).\n",
    "    \"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    n = height * width\n",
    "    \n",
    "    # Initialize\n",
    "    if tid < 256:\n",
    "        shared_hist[tid] = 0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Count pixels\n",
    "    for i in range(gid, n, stride):\n",
    "        row = i // width\n",
    "        col = i % width\n",
    "        pixel = image[row, col]\n",
    "        cuda.atomic.add(shared_hist, pixel, 1)\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Merge\n",
    "    if tid < 256:\n",
    "        cuda.atomic.add(hist, tid, shared_hist[tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a grayscale image\n",
    "height, width = 1080, 1920\n",
    "\n",
    "# Create gradient with some noise\n",
    "image = np.zeros((height, width), dtype=np.uint8)\n",
    "for i in range(height):\n",
    "    image[i, :] = np.clip(i * 256 // height + np.random.randint(-20, 20, width), 0, 255)\n",
    "\n",
    "d_image = cuda.to_device(image)\n",
    "d_hist = cuda.to_device(np.zeros(256, dtype=np.int32))\n",
    "\n",
    "image_histogram_grayscale[256, 256](d_image, d_hist, height, width)\n",
    "\n",
    "hist = d_hist.copy_to_host()\n",
    "\n",
    "print(f\"Image Histogram ({width}x{height} image)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show distribution\n",
    "max_count = max(hist)\n",
    "for i in range(0, 256, 32):\n",
    "    segment_sum = sum(hist[i:i+32])\n",
    "    bar_len = int(segment_sum / (max_count * 10) * 40)\n",
    "    print(f\"{i:3d}-{i+31:3d}: {'â–ˆ' * bar_len} {segment_sum:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36054b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0146e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile histogram_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Error checking macro\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Weighted Histogram\n",
    "// ============================================================\n",
    "// Instead of counting +1 per element, add weight[i]\n",
    "// Uses shared memory for local accumulation\n",
    "\n",
    "__global__ void histogramWeighted(const int* data, const float* weights, \n",
    "                                   float* hist, int n, int numBins) {\n",
    "    extern __shared__ float sharedHist[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Initialize shared histogram\n",
    "    for (int i = tid; i < numBins; i += blockDim.x) {\n",
    "        sharedHist[i] = 0.0f;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Accumulate weights in shared memory\n",
    "    for (int i = gid; i < n; i += stride) {\n",
    "        int bin = data[i];\n",
    "        if (bin >= 0 && bin < numBins) {\n",
    "            atomicAdd(&sharedHist[bin], weights[i]);\n",
    "        }\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge to global histogram\n",
    "    for (int i = tid; i < numBins; i += blockDim.x) {\n",
    "        if (sharedHist[i] > 0.0f) {\n",
    "            atomicAdd(&hist[i], sharedHist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Histogram with Overflow Bins\n",
    "// ============================================================\n",
    "// hist[0] = underflow (values < minVal)\n",
    "// hist[1..numBins] = normal bins\n",
    "// hist[numBins+1] = overflow (values >= maxVal)\n",
    "\n",
    "__global__ void histogramWithOverflow(const float* data, int* hist, int n,\n",
    "                                       int numBins, float minVal, float maxVal) {\n",
    "    __shared__ int sharedHist[258];  // Max bins + 2 for under/overflow\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    int totalBins = numBins + 2;\n",
    "    \n",
    "    // Initialize shared histogram\n",
    "    for (int i = tid; i < totalBins; i += blockDim.x) {\n",
    "        sharedHist[i] = 0;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    float binWidth = (maxVal - minVal) / numBins;\n",
    "    \n",
    "    // Bin data with overflow handling\n",
    "    for (int i = gid; i < n; i += stride) {\n",
    "        float val = data[i];\n",
    "        int bin;\n",
    "        \n",
    "        if (val < minVal) {\n",
    "            bin = 0;  // Underflow bin\n",
    "        } else if (val >= maxVal) {\n",
    "            bin = numBins + 1;  // Overflow bin\n",
    "        } else {\n",
    "            bin = 1 + (int)((val - minVal) / binWidth);\n",
    "            bin = min(bin, numBins);  // Handle edge case\n",
    "        }\n",
    "        \n",
    "        atomicAdd(&sharedHist[bin], 1);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge to global\n",
    "    for (int i = tid; i < totalBins; i += blockDim.x) {\n",
    "        if (sharedHist[i] > 0) {\n",
    "            atomicAdd(&hist[i], sharedHist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: RGB Color Histogram\n",
    "// ============================================================\n",
    "// Compute separate histograms for R, G, B channels\n",
    "// Image stored as interleaved RGB (R0,G0,B0,R1,G1,B1,...)\n",
    "\n",
    "__global__ void rgbHistogram(const unsigned char* image, \n",
    "                              int* histR, int* histG, int* histB,\n",
    "                              int numPixels) {\n",
    "    __shared__ int sharedR[256], sharedG[256], sharedB[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Initialize shared histograms\n",
    "    for (int i = tid; i < 256; i += blockDim.x) {\n",
    "        sharedR[i] = 0;\n",
    "        sharedG[i] = 0;\n",
    "        sharedB[i] = 0;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Accumulate in shared memory\n",
    "    for (int i = gid; i < numPixels; i += stride) {\n",
    "        unsigned char r = image[i * 3 + 0];\n",
    "        unsigned char g = image[i * 3 + 1];\n",
    "        unsigned char b = image[i * 3 + 2];\n",
    "        \n",
    "        atomicAdd(&sharedR[r], 1);\n",
    "        atomicAdd(&sharedG[g], 1);\n",
    "        atomicAdd(&sharedB[b], 1);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Merge to global\n",
    "    for (int i = tid; i < 256; i += blockDim.x) {\n",
    "        if (sharedR[i] > 0) atomicAdd(&histR[i], sharedR[i]);\n",
    "        if (sharedG[i] > 0) atomicAdd(&histG[i], sharedG[i]);\n",
    "        if (sharedB[i] > 0) atomicAdd(&histB[i], sharedB[i]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Test Functions\n",
    "// ============================================================\n",
    "void testWeightedHistogram() {\n",
    "    printf(\"=== Exercise 1: Weighted Histogram ===\\n\");\n",
    "    \n",
    "    // Simple test: data = [0, 1, 1, 2], weights = [1.0, 2.0, 3.0, 4.0]\n",
    "    const int N = 4;\n",
    "    const int numBins = 8;\n",
    "    int h_data[] = {0, 1, 1, 2};\n",
    "    float h_weights[] = {1.0f, 2.0f, 3.0f, 4.0f};\n",
    "    float h_hist[8] = {0};\n",
    "    \n",
    "    int* d_data;\n",
    "    float* d_weights, *d_hist;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_weights, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_hist, numBins * sizeof(float)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_weights, h_weights, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_hist, 0, numBins * sizeof(float)));\n",
    "    \n",
    "    histogramWeighted<<<1, 32, numBins * sizeof(float)>>>(d_data, d_weights, d_hist, N, numBins);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_hist, d_hist, numBins * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Weighted histogram: \");\n",
    "    for (int i = 0; i < numBins; i++) {\n",
    "        if (h_hist[i] > 0) printf(\"bin[%d]=%.1f \", i, h_hist[i]);\n",
    "    }\n",
    "    printf(\"\\nExpected: bin[0]=1.0 bin[1]=5.0 bin[2]=4.0\\n\");\n",
    "    \n",
    "    bool correct = (h_hist[0] == 1.0f && h_hist[1] == 5.0f && h_hist[2] == 4.0f);\n",
    "    printf(\"Test %s\\n\\n\", correct ? \"PASSED âœ“\" : \"FAILED âœ—\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_weights);\n",
    "    cudaFree(d_hist);\n",
    "}\n",
    "\n",
    "void testOverflowHistogram() {\n",
    "    printf(\"=== Exercise 2: Histogram with Overflow Bins ===\\n\");\n",
    "    \n",
    "    const int N = 100;\n",
    "    float h_data[N];\n",
    "    \n",
    "    // Generate data: some under 0, some in [0,10), some >= 10\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_data[i] = (float)(i - 20) / 5.0f;  // Range: -4 to 15.8\n",
    "    }\n",
    "    \n",
    "    int numBins = 10;\n",
    "    int totalBins = numBins + 2;\n",
    "    int h_hist[12] = {0};\n",
    "    \n",
    "    float* d_data;\n",
    "    int* d_hist;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_hist, totalBins * sizeof(int)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_hist, 0, totalBins * sizeof(int)));\n",
    "    \n",
    "    histogramWithOverflow<<<4, 64>>>(d_data, d_hist, N, numBins, 0.0f, 10.0f);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_hist, d_hist, totalBins * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Histogram with overflow (range [0, 10), %d bins):\\n\", numBins);\n",
    "    printf(\"  Underflow (<0): %d\\n\", h_hist[0]);\n",
    "    for (int i = 1; i <= numBins; i++) {\n",
    "        printf(\"  Bin %d [%.1f-%.1f): %d\\n\", i-1, (i-1)*1.0f, i*1.0f, h_hist[i]);\n",
    "    }\n",
    "    printf(\"  Overflow (>=10): %d\\n\", h_hist[numBins + 1]);\n",
    "    \n",
    "    int total = 0;\n",
    "    for (int i = 0; i < totalBins; i++) total += h_hist[i];\n",
    "    printf(\"Total count: %d (expected %d)\\n\", total, N);\n",
    "    printf(\"Test %s\\n\\n\", (total == N) ? \"PASSED âœ“\" : \"FAILED âœ—\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_hist);\n",
    "}\n",
    "\n",
    "void testRGBHistogram() {\n",
    "    printf(\"=== Exercise 3: RGB Color Histogram ===\\n\");\n",
    "    \n",
    "    const int numPixels = 10000;\n",
    "    unsigned char* h_image = (unsigned char*)malloc(numPixels * 3);\n",
    "    \n",
    "    // Create test image with known color distribution\n",
    "    srand(42);\n",
    "    for (int i = 0; i < numPixels * 3; i++) {\n",
    "        h_image[i] = rand() % 256;\n",
    "    }\n",
    "    \n",
    "    int h_histR[256] = {0}, h_histG[256] = {0}, h_histB[256] = {0};\n",
    "    \n",
    "    // CPU reference\n",
    "    int cpuR[256] = {0}, cpuG[256] = {0}, cpuB[256] = {0};\n",
    "    for (int i = 0; i < numPixels; i++) {\n",
    "        cpuR[h_image[i*3 + 0]]++;\n",
    "        cpuG[h_image[i*3 + 1]]++;\n",
    "        cpuB[h_image[i*3 + 2]]++;\n",
    "    }\n",
    "    \n",
    "    unsigned char* d_image;\n",
    "    int *d_histR, *d_histG, *d_histB;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_image, numPixels * 3));\n",
    "    CHECK_CUDA(cudaMalloc(&d_histR, 256 * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_histG, 256 * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_histB, 256 * sizeof(int)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_image, h_image, numPixels * 3, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_histR, 0, 256 * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_histG, 0, 256 * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_histB, 0, 256 * sizeof(int)));\n",
    "    \n",
    "    rgbHistogram<<<64, 256>>>(d_image, d_histR, d_histG, d_histB, numPixels);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_histR, d_histR, 256 * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    CHECK_CUDA(cudaMemcpy(h_histG, d_histG, 256 * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    CHECK_CUDA(cudaMemcpy(h_histB, d_histB, 256 * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < 256; i++) {\n",
    "        if (h_histR[i] != cpuR[i] || h_histG[i] != cpuG[i] || h_histB[i] != cpuB[i]) {\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"Sample RGB histogram values:\\n\");\n",
    "    printf(\"  Value 128: R=%d, G=%d, B=%d\\n\", h_histR[128], h_histG[128], h_histB[128]);\n",
    "    printf(\"  Value 0:   R=%d, G=%d, B=%d\\n\", h_histR[0], h_histG[0], h_histB[0]);\n",
    "    printf(\"  Value 255: R=%d, G=%d, B=%d\\n\", h_histR[255], h_histG[255], h_histB[255]);\n",
    "    printf(\"Test %s\\n\\n\", correct ? \"PASSED âœ“\" : \"FAILED âœ—\");\n",
    "    \n",
    "    cudaFree(d_image);\n",
    "    cudaFree(d_histR);\n",
    "    cudaFree(d_histG);\n",
    "    cudaFree(d_histB);\n",
    "    free(h_image);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘              CUDA Histogram Exercises                        â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    // Print device info\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Compute Capability: %d.%d\\n\\n\", prop.major, prop.minor);\n",
    "    \n",
    "    testWeightedHistogram();\n",
    "    testOverflowHistogram();\n",
    "    testRGBHistogram();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4923d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o histogram_exercises histogram_exercises.cu && ./histogram_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73b3eb",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Weighted Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement weighted histogram\n",
    "# Instead of counting +1 per element, add weight[i]\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_weighted(data, weights, hist, n, num_bins):\n",
    "    \"\"\"Weighted histogram: sum weights instead of counting.\"\"\"\n",
    "    shared_hist = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # TODO: Initialize shared memory\n",
    "    # TODO: Accumulate weights in shared memory\n",
    "    # TODO: Merge to global\n",
    "    pass\n",
    "\n",
    "# Test: data = [0, 1, 1, 2], weights = [1.0, 2.0, 3.0, 4.0]\n",
    "# Result: hist[0]=1.0, hist[1]=5.0, hist[2]=4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d112d2",
   "metadata": {},
   "source": [
    "### Exercise 2: Histogram with Overflow Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add underflow and overflow bins\n",
    "# hist[0] = count of values < min_val\n",
    "# hist[1..num_bins] = normal bins\n",
    "# hist[num_bins+1] = count of values >= max_val\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_with_overflow(data, hist, n, num_bins, min_val, max_val):\n",
    "    \"\"\"Histogram with underflow/overflow bins.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Total output size = num_bins + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8116f11",
   "metadata": {},
   "source": [
    "### Exercise 3: RGB Color Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde50fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute separate histograms for R, G, B channels\n",
    "# image is shape (height, width, 3)\n",
    "\n",
    "@cuda.jit\n",
    "def rgb_histogram(image, hist_r, hist_g, hist_b, height, width):\n",
    "    \"\"\"Compute histograms for each RGB channel.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Each histogram should have 256 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba84e03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Summary & Key Takeaways\n",
    "\n",
    "### ğŸ¯ Quick Reference Card: Histogram Patterns\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    HISTOGRAM RECIPE                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. ALLOCATE SHARED HISTOGRAM                                  â”‚\n",
    "â”‚     __shared__ int localHist[NUM_BINS];                        â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  2. INITIALIZE TO ZERO (parallel)                              â”‚\n",
    "â”‚     for (i = tid; i < NUM_BINS; i += blockDim.x)              â”‚\n",
    "â”‚         localHist[i] = 0;                                      â”‚\n",
    "â”‚     __syncthreads();                                           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  3. COUNT INTO SHARED MEMORY                                   â”‚\n",
    "â”‚     atomicAdd(&localHist[data[gid]], 1);                       â”‚\n",
    "â”‚     __syncthreads();                                           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  4. MERGE TO GLOBAL                                            â”‚\n",
    "â”‚     for (i = tid; i < NUM_BINS; i += blockDim.x)              â”‚\n",
    "â”‚         atomicAdd(&globalHist[i], localHist[i]);               â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“‹ Strategy Selection Guide\n",
    "\n",
    "| Bin Count | Best Strategy | Why |\n",
    "|-----------|---------------|-----|\n",
    "| < 256 | Shared memory privatization | Fits in shared memory |\n",
    "| 256-4096 | Multiple histograms per block | More shared memory |\n",
    "| > 4096 | Direct global atomics | Too large for shared |\n",
    "| Sparse data | Sort + unique count | Avoids atomic overhead |\n",
    "\n",
    "### ğŸ—³ï¸ The Privatization Speedup\n",
    "\n",
    "```\n",
    "NAIVE (global atomics only):\n",
    "  1M threads â†’ 256 bins = ~4000 threads/bin contention ğŸŒ\n",
    "\n",
    "PRIVATIZED (shared then global):\n",
    "  1M threads Ã· 1000 blocks = 1000 threads/block\n",
    "  1000 threads â†’ 256 bins = ~4 threads/bin contention âš¡\n",
    "  \n",
    "  Then: 1000 blocks Ã— 256 bins = 256K global atomics\n",
    "  But each bin: only 1000 updates (vs 1M naive)\n",
    "  \n",
    "  Speedup: 10-100x depending on data distribution!\n",
    "```\n",
    "\n",
    "### ğŸ§  Key Takeaways\n",
    "\n",
    "1. **ğŸ—³ï¸ Histograms are atomic-heavy** â€” optimization is essential for performance\n",
    "2. **ğŸ  Privatization is the key pattern** â€” count locally, merge globally\n",
    "3. **âš¡ Shared memory atomics are ~10x faster** than global memory atomics\n",
    "4. **ğŸ“Š Data distribution matters** â€” clustered data = more contention\n",
    "5. **ğŸ”§ Same pattern works for 2D** â€” just flatten indices: `bin = y * width + x`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Week 4 Complete!\n",
    "\n",
    "### ğŸ“Š What You Learned This Week\n",
    "\n",
    "| Day | Topic | Key Pattern | Analogy |\n",
    "|-----|-------|-------------|---------|\n",
    "| 1 | Parallel Reduction | Tree reduction, multi-pass | ğŸ† Tournament bracket |\n",
    "| 2 | Warp Primitives | Shuffle, lockstep execution | ğŸŠ Synchronized swimmers |\n",
    "| 3 | Atomic Operations | Thread-safe updates, CAS | ğŸ¦ Bank teller window |\n",
    "| 4 | Histogram | Privatization, shared atomics | ğŸ—³ï¸ Vote counting |\n",
    "\n",
    "### ğŸ”— How It All Connects\n",
    "\n",
    "```\n",
    "Week 4 Flow:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Reduction teaches: Combining many values â†’ one result         â”‚\n",
    "â”‚       â†“                                                         â”‚\n",
    "â”‚  Warp Primitives: Making reduction faster with shuffles        â”‚\n",
    "â”‚       â†“                                                         â”‚\n",
    "â”‚  Atomics: When threads update DIFFERENT locations              â”‚\n",
    "â”‚       â†“                                                         â”‚\n",
    "â”‚  Histogram: Combining ALL techniques for real-world counting   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® What's Next?\n",
    "\n",
    "**Day 5:** Complete the checkpoint quiz to test your understanding!\n",
    "\n",
    "**Week 5 Preview: Matrix Operations**\n",
    "\n",
    "We'll apply everything you've learned to the workhorses of scientific computing and deep learning:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    WEEK 5: MATRIX OPERATIONS                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â€¢ Matrix-vector multiplication                                â”‚\n",
    "â”‚  â€¢ Tiled matrix-matrix multiplication (GEMM)                   â”‚\n",
    "â”‚  â€¢ Memory access optimization patterns                         â”‚\n",
    "â”‚  â€¢ Cache blocking strategies                                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Connection: Matrices combine EVERYTHING:                      â”‚\n",
    "â”‚    - Coalesced memory access (Week 2)                         â”‚\n",
    "â”‚    - Shared memory tiling (Week 2)                            â”‚\n",
    "â”‚    - Reduction patterns (Week 4)                              â”‚\n",
    "â”‚    - Warp efficiency (Week 4)                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
