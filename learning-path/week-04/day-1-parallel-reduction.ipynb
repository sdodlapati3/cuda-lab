{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481d3170",
   "metadata": {},
   "source": [
    "# üöÄ Day 1: Parallel Reduction\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-04/day-1-parallel-reduction.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the parallel reduction problem\n",
    "- Implement tree reduction with shared memory\n",
    "- Handle multi-pass reduction for large arrays\n",
    "- Apply reduction to sum, max, min\n",
    "\n",
    "> **Primary Focus:** CUDA C++ code examples first, Python/Numba backup for interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")\n",
    "    print(f\"Warp size: {device.WARP_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ace365",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Reduction Problem\n",
    "\n",
    "### What is Reduction?\n",
    "\n",
    "Reduction combines all elements of an array into a single value using an associative operator.\n",
    "\n",
    "```\n",
    "Examples:\n",
    "‚Ä¢ Sum:    [1, 2, 3, 4] ‚Üí 10\n",
    "‚Ä¢ Max:    [1, 5, 3, 2] ‚Üí 5\n",
    "‚Ä¢ Min:    [4, 1, 7, 2] ‚Üí 1\n",
    "‚Ä¢ Product: [2, 3, 4]   ‚Üí 24\n",
    "```\n",
    "\n",
    "### Why is Parallel Reduction Hard?\n",
    "\n",
    "```\n",
    "Sequential (CPU):        Parallel (GPU):\n",
    "sum = 0                  ??? How to combine?\n",
    "for x in arr:            Each thread has a value\n",
    "    sum += x             Need to merge them!\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)\n",
    "\n",
    "The following CUDA C++ implementation demonstrates a two-pass parallel reduction algorithm using shared memory for efficient block-level reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f53034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reduction.cu\n",
    "// reduction.cu - Parallel sum reduction with shared memory\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 256\n",
    "\n",
    "// Block-level reduction: Each block produces one partial sum\n",
    "__global__ void blockReduceSum(const float* input, float* blockSums, int n) {\n",
    "    __shared__ float sdata[BLOCK_SIZE];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Phase 1: Grid-stride load and accumulate\n",
    "    float sum = 0.0f;\n",
    "    for (int i = gid; i < n; i += stride) {\n",
    "        sum += input[i];\n",
    "    }\n",
    "    sdata[tid] = sum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Phase 2: Tree reduction within block\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] += sdata[tid + s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Thread 0 writes block result\n",
    "    if (tid == 0) {\n",
    "        blockSums[blockIdx.x] = sdata[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Final reduction of block sums\n",
    "__global__ void finalReduceSum(const float* blockSums, float* result, int n) {\n",
    "    __shared__ float sdata[BLOCK_SIZE];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Load block sums\n",
    "    sdata[tid] = (tid < n) ? blockSums[tid] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Tree reduction\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] += sdata[tid + s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        result[0] = sdata[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000000;  // 10M elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate and initialize host array\n",
    "    float *h_input = (float*)malloc(size);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_input[i] = 1.0f;  // Sum should be n\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_blockSums, *d_result;\n",
    "    cudaMalloc(&d_input, size);\n",
    "    \n",
    "    int numBlocks = 256;\n",
    "    cudaMalloc(&d_blockSums, numBlocks * sizeof(float));\n",
    "    cudaMalloc(&d_result, sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // First pass: reduce to block sums\n",
    "    blockReduceSum<<<numBlocks, BLOCK_SIZE>>>(d_input, d_blockSums, n);\n",
    "    \n",
    "    // Second pass: reduce block sums to final result\n",
    "    finalReduceSum<<<1, BLOCK_SIZE>>>(d_blockSums, d_result, numBlocks);\n",
    "    \n",
    "    // Get result\n",
    "    float result;\n",
    "    cudaMemcpy(&result, d_result, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Sum of %d elements: %f (expected %d)\\n\", n, result, n);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_input); cudaFree(d_blockSums); cudaFree(d_result);\n",
    "    free(h_input);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o reduction reduction.cu\n",
    "!./reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU baseline\n",
    "def cpu_sum(arr):\n",
    "    return np.sum(arr)\n",
    "\n",
    "# Test\n",
    "n = 10_000_000\n",
    "arr = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "start = time.perf_counter()\n",
    "cpu_result = cpu_sum(arr)\n",
    "cpu_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(f\"CPU sum of {n:,} elements: {cpu_result:.2f}\")\n",
    "print(f\"CPU time: {cpu_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b846a",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)\n",
    "\n",
    "CPU baseline for comparison with GPU reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f57fa5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Naive Approach (Don't Do This!)\n",
    "\n",
    "### Using Atomic Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def naive_atomic_sum(arr, result, n):\n",
    "    \"\"\"Naive: every thread does atomicAdd to global memory.\n",
    "    \n",
    "    WARNING: This is SLOW due to atomic contention!\n",
    "    \"\"\"\n",
    "    tid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    for i in range(tid, n, stride):\n",
    "        cuda.atomic.add(result, 0, arr[i])  # All threads fight for same location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test naive approach\n",
    "n = 1_000_000  # Smaller N because it's so slow!\n",
    "arr = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "d_arr = cuda.to_device(arr)\n",
    "d_result = cuda.to_device(np.zeros(1, dtype=np.float32))\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "\n",
    "# Warmup\n",
    "naive_atomic_sum[blocks, threads](d_arr, d_result, n)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Reset and time\n",
    "d_result = cuda.to_device(np.zeros(1, dtype=np.float32))\n",
    "start = time.perf_counter()\n",
    "naive_atomic_sum[blocks, threads](d_arr, d_result, n)\n",
    "cuda.synchronize()\n",
    "naive_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "result = d_result.copy_to_host()[0]\n",
    "expected = np.sum(arr)\n",
    "\n",
    "print(f\"Naive atomic sum: {result:.2f} (expected: {expected:.2f})\")\n",
    "print(f\"Time: {naive_time:.2f} ms\")\n",
    "print(f\"\\n‚ö†Ô∏è  This is SLOW due to atomic contention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931f554",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Tree Reduction Pattern\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "```\n",
    "Instead of N atomic operations, use log‚ÇÇ(N) parallel steps:\n",
    "\n",
    "Input:  [1] [2] [3] [4] [5] [6] [7] [8]\n",
    "         ‚Üò   ‚Üô   ‚Üò   ‚Üô   ‚Üò   ‚Üô   ‚Üò   ‚Üô\n",
    "Step 1:  [3]     [7]     [11]    [15]\n",
    "          ‚Üò       ‚Üô       ‚Üò       ‚Üô\n",
    "Step 2:    [10]             [26]\n",
    "             ‚Üò               ‚Üô\n",
    "Step 3:           [36] ‚Üê Final sum!\n",
    "\n",
    "N = 8 elements ‚Üí log‚ÇÇ(8) = 3 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def block_reduce_sum(arr, block_results, n):\n",
    "    \"\"\"\n",
    "    Tree reduction within each block using shared memory.\n",
    "    Each block produces one partial sum.\n",
    "    \"\"\"\n",
    "    # Shared memory for this block\n",
    "    shared = cuda.shared.array(256, dtype=np.float32)  # Assume 256 threads\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Phase 1: Each thread sums its portion (grid-stride)\n",
    "    local_sum = 0.0\n",
    "    for i in range(gid, n, stride):\n",
    "        local_sum += arr[i]\n",
    "    \n",
    "    # Store in shared memory\n",
    "    shared[tid] = local_sum\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 2: Tree reduction within block\n",
    "    # Stride starts at half the block size\n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s:\n",
    "            shared[tid] += shared[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    # Thread 0 writes block result\n",
    "    if tid == 0:\n",
    "        block_results[bid] = shared[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tree reduction\n",
    "def visualize_tree_reduction(values):\n",
    "    \"\"\"Show step-by-step tree reduction.\"\"\"\n",
    "    arr = list(values)\n",
    "    step = 0\n",
    "    \n",
    "    print(f\"Input:  {arr}\")\n",
    "    print()\n",
    "    \n",
    "    while len(arr) > 1:\n",
    "        new_arr = []\n",
    "        for i in range(0, len(arr), 2):\n",
    "            if i + 1 < len(arr):\n",
    "                new_arr.append(arr[i] + arr[i+1])\n",
    "            else:\n",
    "                new_arr.append(arr[i])\n",
    "        step += 1\n",
    "        print(f\"Step {step}: {new_arr}\")\n",
    "        arr = new_arr\n",
    "    \n",
    "    print(f\"\\nFinal sum: {arr[0]}\")\n",
    "    return arr[0]\n",
    "\n",
    "visualize_tree_reduction([1, 2, 3, 4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f22b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tree reduction\n",
    "n = 10_000_000\n",
    "arr = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "d_arr = cuda.to_device(arr)\n",
    "\n",
    "blocks, threads = 256, 256\n",
    "d_block_results = cuda.device_array(blocks, dtype=np.float32)\n",
    "\n",
    "# First kernel: reduce to block sums\n",
    "block_reduce_sum[blocks, threads](d_arr, d_block_results, n)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Second pass: sum block results on CPU (or another kernel)\n",
    "block_results = d_block_results.copy_to_host()\n",
    "gpu_sum = np.sum(block_results)\n",
    "\n",
    "expected = np.sum(arr)\n",
    "print(f\"Tree reduction result: {gpu_sum:.2f}\")\n",
    "print(f\"Expected:              {expected:.2f}\")\n",
    "print(f\"Match: {'‚úì' if np.isclose(gpu_sum, expected, rtol=1e-4) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc56cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Complete Two-Pass Reduction\n",
    "\n",
    "### Full GPU Sum Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b82e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_reduce_sum(arr):\n",
    "    \"\"\"\n",
    "    Complete GPU reduction with two kernel passes.\n",
    "    \n",
    "    Pass 1: Each block reduces its portion ‚Üí block_results\n",
    "    Pass 2: Reduce block_results ‚Üí final sum\n",
    "    \"\"\"\n",
    "    n = len(arr)\n",
    "    \n",
    "    blocks = 256\n",
    "    threads = 256\n",
    "    \n",
    "    d_arr = cuda.to_device(arr)\n",
    "    d_block_results = cuda.device_array(blocks, dtype=np.float32)\n",
    "    \n",
    "    # Pass 1: Reduce to block sums\n",
    "    block_reduce_sum[blocks, threads](d_arr, d_block_results, n)\n",
    "    \n",
    "    # Pass 2: Reduce block results (use 1 block)\n",
    "    d_final = cuda.device_array(1, dtype=np.float32)\n",
    "    block_reduce_sum[1, threads](d_block_results, d_final, blocks)\n",
    "    \n",
    "    return d_final.copy_to_host()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce50f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "n = 10_000_000\n",
    "arr = np.random.rand(n).astype(np.float32)\n",
    "iterations = 100\n",
    "\n",
    "# CPU\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    cpu_result = np.sum(arr)\n",
    "cpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "\n",
    "# GPU - warmup\n",
    "gpu_result = gpu_reduce_sum(arr)\n",
    "\n",
    "# GPU - timed\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    gpu_result = gpu_reduce_sum(arr)\n",
    "cuda.synchronize()\n",
    "gpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "\n",
    "print(f\"Sum of {n:,} float32 elements\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"CPU (numpy):  {cpu_time:.3f} ms ‚Üí {cpu_result:.2f}\")\n",
    "print(f\"GPU (2-pass): {gpu_time:.3f} ms ‚Üí {gpu_result:.2f}\")\n",
    "print(f\"Speedup:      {cpu_time/gpu_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607e9a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Reduction Variants (Max, Min)\n",
    "\n",
    "### Generalizing the Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def block_reduce_max(arr, block_results, n):\n",
    "    \"\"\"Tree reduction for maximum value.\"\"\"\n",
    "    shared = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Phase 1: Each thread finds max in its portion\n",
    "    local_max = -np.inf  # Initialize to negative infinity\n",
    "    for i in range(gid, n, stride):\n",
    "        if arr[i] > local_max:\n",
    "            local_max = arr[i]\n",
    "    \n",
    "    shared[tid] = local_max\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 2: Tree reduction\n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s:\n",
    "            if shared[tid + s] > shared[tid]:\n",
    "                shared[tid] = shared[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    if tid == 0:\n",
    "        block_results[bid] = shared[0]\n",
    "\n",
    "@cuda.jit\n",
    "def block_reduce_min(arr, block_results, n):\n",
    "    \"\"\"Tree reduction for minimum value.\"\"\"\n",
    "    shared = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # Phase 1: Each thread finds min in its portion\n",
    "    local_min = np.inf  # Initialize to positive infinity\n",
    "    for i in range(gid, n, stride):\n",
    "        if arr[i] < local_min:\n",
    "            local_min = arr[i]\n",
    "    \n",
    "    shared[tid] = local_min\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 2: Tree reduction\n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s:\n",
    "            if shared[tid + s] < shared[tid]:\n",
    "                shared[tid] = shared[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    if tid == 0:\n",
    "        block_results[bid] = shared[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_reduce_max(arr):\n",
    "    \"\"\"GPU max reduction.\"\"\"\n",
    "    n = len(arr)\n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    d_arr = cuda.to_device(arr)\n",
    "    d_block_results = cuda.device_array(blocks, dtype=np.float32)\n",
    "    \n",
    "    block_reduce_max[blocks, threads](d_arr, d_block_results, n)\n",
    "    \n",
    "    d_final = cuda.device_array(1, dtype=np.float32)\n",
    "    block_reduce_max[1, threads](d_block_results, d_final, blocks)\n",
    "    \n",
    "    return d_final.copy_to_host()[0]\n",
    "\n",
    "def gpu_reduce_min(arr):\n",
    "    \"\"\"GPU min reduction.\"\"\"\n",
    "    n = len(arr)\n",
    "    blocks, threads = 256, 256\n",
    "    \n",
    "    d_arr = cuda.to_device(arr)\n",
    "    d_block_results = cuda.device_array(blocks, dtype=np.float32)\n",
    "    \n",
    "    block_reduce_min[blocks, threads](d_arr, d_block_results, n)\n",
    "    \n",
    "    d_final = cuda.device_array(1, dtype=np.float32)\n",
    "    block_reduce_min[1, threads](d_block_results, d_final, blocks)\n",
    "    \n",
    "    return d_final.copy_to_host()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test max and min\n",
    "n = 10_000_000\n",
    "arr = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "gpu_max = gpu_reduce_max(arr)\n",
    "gpu_min = gpu_reduce_min(arr)\n",
    "\n",
    "cpu_max = np.max(arr)\n",
    "cpu_min = np.min(arr)\n",
    "\n",
    "print(f\"Max: GPU={gpu_max:.6f}, CPU={cpu_max:.6f} {'‚úì' if np.isclose(gpu_max, cpu_max) else '‚úó'}\")\n",
    "print(f\"Min: GPU={gpu_min:.6f}, CPU={cpu_min:.6f} {'‚úì' if np.isclose(gpu_min, cpu_min) else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555166d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Optimized Reduction\n",
    "\n",
    "### Sequential Addressing (Avoids Bank Conflicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474eaafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def block_reduce_sum_optimized(arr, block_results, n):\n",
    "    \"\"\"\n",
    "    Optimized reduction with sequential addressing.\n",
    "    Avoids shared memory bank conflicts.\n",
    "    \"\"\"\n",
    "    shared = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    block_size = cuda.blockDim.x\n",
    "    \n",
    "    # Phase 1: Grid-stride accumulation\n",
    "    local_sum = 0.0\n",
    "    for i in range(gid, n, stride):\n",
    "        local_sum += arr[i]\n",
    "    \n",
    "    shared[tid] = local_sum\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 2: Sequential addressing reduction\n",
    "    # Active threads are at the beginning (no divergence until end)\n",
    "    s = block_size // 2\n",
    "    while s > 0:\n",
    "        if tid < s:\n",
    "            shared[tid] += shared[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    if tid == 0:\n",
    "        block_results[bid] = shared[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare interleaved vs sequential addressing\n",
    "print(\"Reduction Addressing Patterns\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"INTERLEAVED (can cause bank conflicts):\")\n",
    "print(\"Step 1: Thread 0 adds [0]+[1], Thread 2 adds [2]+[3], ...\")\n",
    "print(\"Step 2: Thread 0 adds [0]+[2], Thread 4 adds [4]+[6], ...\")\n",
    "print(\"  ‚Üí Gaps between active threads grow!\")\n",
    "print()\n",
    "print(\"SEQUENTIAL (preferred):\")\n",
    "print(\"Step 1: Thread 0 adds [0]+[128], Thread 1 adds [1]+[129], ...\")\n",
    "print(\"Step 2: Thread 0 adds [0]+[64], Thread 1 adds [1]+[65], ...\")\n",
    "print(\"  ‚Üí Active threads always contiguous!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94386a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Comprehensive Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_reductions(sizes, iterations=50):\n",
    "    \"\"\"Benchmark reduction across different sizes.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n in sizes:\n",
    "        arr = np.random.rand(n).astype(np.float32)\n",
    "        \n",
    "        # CPU\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            _ = np.sum(arr)\n",
    "        cpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "        \n",
    "        # GPU warmup\n",
    "        _ = gpu_reduce_sum(arr)\n",
    "        \n",
    "        # GPU timed\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            _ = gpu_reduce_sum(arr)\n",
    "        cuda.synchronize()\n",
    "        gpu_time = (time.perf_counter() - start) / iterations * 1000\n",
    "        \n",
    "        speedup = cpu_time / gpu_time\n",
    "        results.append((n, cpu_time, gpu_time, speedup))\n",
    "    \n",
    "    return results\n",
    "\n",
    "sizes = [100_000, 500_000, 1_000_000, 5_000_000, 10_000_000, 50_000_000]\n",
    "results = benchmark_reductions(sizes)\n",
    "\n",
    "print(f\"\\n{'Size':>12} | {'CPU (ms)':>10} | {'GPU (ms)':>10} | {'Speedup':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for n, cpu_t, gpu_t, speedup in results:\n",
    "    print(f\"{n:>12,} | {cpu_t:>10.3f} | {gpu_t:>10.3f} | {speedup:>7.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcf4c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Product Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36971aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement product reduction (multiply all elements)\n",
    "# Hint: Use local_product = 1.0, then multiply\n",
    "\n",
    "@cuda.jit\n",
    "def block_reduce_product(arr, block_results, n):\n",
    "    \"\"\"Tree reduction for product of elements.\"\"\"\n",
    "    shared = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    \n",
    "    # TODO: Implement product reduction\n",
    "    pass\n",
    "\n",
    "# Test with small numbers to avoid overflow\n",
    "# arr = [1.01, 1.02, 1.03, ...] ‚Üí product should be reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3503c2",
   "metadata": {},
   "source": [
    "### Exercise 2: Mean Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate mean using sum reduction\n",
    "# mean = sum / n\n",
    "\n",
    "def gpu_mean(arr):\n",
    "    \"\"\"Calculate mean using GPU reduction.\"\"\"\n",
    "    # Hint: Reuse gpu_reduce_sum and divide by len(arr)\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# arr = np.random.rand(1_000_000).astype(np.float32)\n",
    "# print(f\"GPU mean: {gpu_mean(arr)}, NumPy mean: {np.mean(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70115c51",
   "metadata": {},
   "source": [
    "### Exercise 3: ArgMax (Index of Maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find index of maximum element\n",
    "# This is trickier - need to track both value AND index\n",
    "\n",
    "@cuda.jit\n",
    "def block_reduce_argmax(arr, block_vals, block_idxs, n):\n",
    "    \"\"\"Find index of maximum element.\"\"\"\n",
    "    # Hint: Store both value and index in shared memory\n",
    "    # Compare values, but propagate indices\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49d49d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Reduction Pattern Overview\n",
    "\n",
    "```\n",
    "1. GRID-STRIDE ACCUMULATION\n",
    "   Each thread reduces its portion of the array\n",
    "   \n",
    "2. SHARED MEMORY STORAGE\n",
    "   Threads store local results in shared memory\n",
    "   \n",
    "3. TREE REDUCTION\n",
    "   log‚ÇÇ(blockDim) steps to reduce to single value\n",
    "   \n",
    "4. BLOCK RESULT OUTPUT\n",
    "   Thread 0 writes block's result\n",
    "   \n",
    "5. SECOND PASS\n",
    "   Reduce block results to final answer\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Naive atomic reduction is SLOW** - O(N) atomic operations\n",
    "2. **Tree reduction is fast** - O(log N) parallel steps\n",
    "3. **Use shared memory** for intra-block communication\n",
    "4. **Two-pass approach** handles any array size\n",
    "5. **Sequential addressing** avoids bank conflicts\n",
    "\n",
    "### Complexity\n",
    "\n",
    "| Approach | Time Complexity | Atomic Ops |\n",
    "|----------|-----------------|------------|\n",
    "| Naive | O(N) serial | N |\n",
    "| Tree | O(log N) parallel | ~blocks |\n",
    "\n",
    "### Next: Day 2 - Warp Primitives\n",
    "Learn about warp-level shuffle operations for even faster reductions!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
