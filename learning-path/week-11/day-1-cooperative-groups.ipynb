{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ff20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"⚠️  Cooperative Groups are a CUDA C++ feature!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067d815",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What Are Cooperative Groups?\n",
    "\n",
    "### Traditional CUDA Synchronization\n",
    "\n",
    "```\n",
    "Traditional CUDA:\n",
    "━━━━━━━━━━━━━━━━━\n",
    "__syncthreads()  → Only syncs within block\n",
    "Warp shuffle     → Only within warp\n",
    "\n",
    "Problems:\n",
    "• No grid-wide sync\n",
    "• No sub-warp sync\n",
    "• No dynamic grouping\n",
    "\n",
    "Cooperative Groups Solution:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "• Flexible group abstraction\n",
    "• Groups can be partitioned\n",
    "• Sync at any granularity\n",
    "• Works with divergent code\n",
    "```\n",
    "\n",
    "### Group Hierarchy\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────┐\n",
    "│               Grid Group               │\n",
    "│  ┌──────────────────────────────────┐  │\n",
    "│  │         Thread Block             │  │\n",
    "│  │  ┌────────────────────────────┐  │  │\n",
    "│  │  │     Tile (32 threads)      │  │  │\n",
    "│  │  │  ┌──────────────────────┐  │  │  │\n",
    "│  │  │  │ Coalesced (active)   │  │  │  │\n",
    "│  │  │  └──────────────────────┘  │  │  │\n",
    "│  │  └────────────────────────────┘  │  │\n",
    "│  └──────────────────────────────────┘  │\n",
    "└────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ea2d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Thread Block Groups\n",
    "\n",
    "### CUDA C++ Thread Block Group (Primary)\n",
    "\n",
    "```cpp\n",
    "// thread_block_group.cu - Basic cooperative groups\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void blockGroupDemo(float* data, int n) {\n",
    "    // ============================================\n",
    "    // Get Thread Block Group\n",
    "    // ============================================\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    int tid = block.thread_rank();  // Same as threadIdx.x\n",
    "    int size = block.size();        // Same as blockDim.x\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        printf(\"Block %d: size=%d\\n\", blockIdx.x, size);\n",
    "    }\n",
    "    \n",
    "    // Perform work\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= 2.0f;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Synchronize Using Group\n",
    "    // ============================================\n",
    "    block.sync();  // Same as __syncthreads()\n",
    "    \n",
    "    // More work after sync\n",
    "    if (idx < n) {\n",
    "        data[idx] += 1.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    blockGroupDemo<<<4, 256>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Compile: `nvcc -o block_group thread_block_group.cu`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37eb9ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Tile Groups\n",
    "\n",
    "### CUDA C++ Tile Partitioning (Primary)\n",
    "\n",
    "```cpp\n",
    "// tile_groups.cu - Warp-level programming with tiles\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void tileGroupDemo(float* data, int n) {\n",
    "    // Get thread block\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    // ============================================\n",
    "    // Create Tile of 32 Threads (Warp)\n",
    "    // ============================================\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    // Tile properties\n",
    "    int lane = warp.thread_rank();    // 0-31 within warp\n",
    "    int warp_id = warp.meta_group_rank();  // Which warp\n",
    "    \n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float val = (idx < n) ? data[idx] : 0.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // Warp-Level Reduction Using Tile\n",
    "    // ============================================\n",
    "    // Shuffle within tile\n",
    "    for (int offset = warp.size() / 2; offset > 0; offset /= 2) {\n",
    "        val += warp.shfl_down(val, offset);\n",
    "    }\n",
    "    \n",
    "    // Lane 0 has the warp sum\n",
    "    if (lane == 0) {\n",
    "        printf(\"Block %d, Warp %d: sum = %.1f\\n\", \n",
    "               blockIdx.x, warp_id, val);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Tile Synchronization\n",
    "    // ============================================\n",
    "    warp.sync();  // Sync only within this tile\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Smaller Tiles (Sub-Warp)\n",
    "// ============================================\n",
    "__global__ void smallTileDemo() {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    // Create tiles of 4 threads\n",
    "    cg::thread_block_tile<4> tile4 = cg::tiled_partition<4>(block);\n",
    "    \n",
    "    // Create tiles of 8 threads\n",
    "    cg::thread_block_tile<8> tile8 = cg::tiled_partition<8>(block);\n",
    "    \n",
    "    // Create tiles of 16 threads\n",
    "    cg::thread_block_tile<16> tile16 = cg::tiled_partition<16>(block);\n",
    "    \n",
    "    // Each tile can sync independently\n",
    "    tile4.sync();\n",
    "    tile8.sync();\n",
    "    tile16.sync();\n",
    "    \n",
    "    // Shuffle within small tiles\n",
    "    float val = (float)threadIdx.x;\n",
    "    float sum4 = val;\n",
    "    for (int i = tile4.size()/2; i > 0; i /= 2) {\n",
    "        sum4 += tile4.shfl_down(sum4, i);\n",
    "    }\n",
    "    \n",
    "    if (tile4.thread_rank() == 0) {\n",
    "        printf(\"Thread %d: tile4 sum = %.0f\\n\", threadIdx.x, sum4);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 256;\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    float h_data[N];\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    printf(\"=== Warp Tiles ===\\n\");\n",
    "    tileGroupDemo<<<1, 128>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"\\n=== Small Tiles ===\\n\");\n",
    "    smallTileDemo<<<1, 32>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325fde11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Coalesced Groups\n",
    "\n",
    "### Handling Divergent Code\n",
    "\n",
    "```cpp\n",
    "// coalesced_groups.cu - Groups for divergent code\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void coalescedDemo(int* flags, float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // ============================================\n",
    "    // Divergent Branch\n",
    "    // ============================================\n",
    "    if (tid < n && flags[tid]) {  // Only some threads active!\n",
    "        \n",
    "        // Get group of active threads\n",
    "        cg::coalesced_group active = cg::coalesced_threads();\n",
    "        \n",
    "        int rank = active.thread_rank();  // Rank among active threads\n",
    "        int size = active.size();         // How many active\n",
    "        \n",
    "        // Collective operation among active threads only\n",
    "        float val = data[tid];\n",
    "        \n",
    "        // Reduction among active threads\n",
    "        for (int offset = active.size() / 2; offset > 0; offset /= 2) {\n",
    "            val += active.shfl_down(val, offset);\n",
    "        }\n",
    "        \n",
    "        if (rank == 0) {\n",
    "            printf(\"Block %d: %d active threads, sum = %.1f\\n\",\n",
    "                   blockIdx.x, size, val);\n",
    "        }\n",
    "        \n",
    "        // Sync among active threads\n",
    "        active.sync();\n",
    "        \n",
    "        // Continue processing...\n",
    "        data[tid] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 32;\n",
    "    int* d_flags;\n",
    "    float* d_data;\n",
    "    \n",
    "    cudaMalloc(&d_flags, N * sizeof(int));\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Set alternating flags (half threads active)\n",
    "    int h_flags[N];\n",
    "    float h_data[N];\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_flags[i] = (i % 2 == 0) ? 1 : 0;  // Even threads active\n",
    "        h_data[i] = 1.0f;\n",
    "    }\n",
    "    \n",
    "    cudaMemcpy(d_flags, h_flags, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    coalescedDemo<<<1, 32>>>(d_flags, d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_flags);\n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d85b04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Practical Example - Block Reduction\n",
    "\n",
    "### CUDA C++ Optimized Reduction (Primary)\n",
    "\n",
    "```cpp\n",
    "// cg_reduction.cu - Reduction using cooperative groups\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "// Warp reduction using tile shuffle\n",
    "__device__ float warpReduce(cg::thread_block_tile<32>& warp, float val) {\n",
    "    for (int offset = warp.size() / 2; offset > 0; offset /= 2) {\n",
    "        val += warp.shfl_down(val, offset);\n",
    "    }\n",
    "    return val;\n",
    "}\n",
    "\n",
    "// Block reduction using cooperative groups\n",
    "__device__ float blockReduce(cg::thread_block& block, float val) {\n",
    "    __shared__ float warpSums[32];  // Max 32 warps per block\n",
    "    \n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    int lane = warp.thread_rank();\n",
    "    int warp_id = warp.meta_group_rank();\n",
    "    \n",
    "    // Reduce within warp\n",
    "    val = warpReduce(warp, val);\n",
    "    \n",
    "    // Lane 0 writes warp sum\n",
    "    if (lane == 0) {\n",
    "        warpSums[warp_id] = val;\n",
    "    }\n",
    "    \n",
    "    block.sync();  // Wait for all warps\n",
    "    \n",
    "    // First warp reduces warp sums\n",
    "    int numWarps = (block.size() + 31) / 32;\n",
    "    val = (block.thread_rank() < numWarps) ? warpSums[block.thread_rank()] : 0.0f;\n",
    "    \n",
    "    if (warp_id == 0) {\n",
    "        val = warpReduce(warp, val);\n",
    "    }\n",
    "    \n",
    "    return val;\n",
    "}\n",
    "\n",
    "__global__ void reduceKernel(float* input, float* output, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float val = (tid < n) ? input[tid] : 0.0f;\n",
    "    \n",
    "    float blockSum = blockReduce(block, val);\n",
    "    \n",
    "    if (block.thread_rank() == 0) {\n",
    "        output[blockIdx.x] = blockSum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const int BLOCK_SIZE = 256;\n",
    "    const int NUM_BLOCKS = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, N * sizeof(float));\n",
    "    cudaMalloc(&d_output, NUM_BLOCKS * sizeof(float));\n",
    "    \n",
    "    // Initialize with 1s\n",
    "    float* h_input = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1.0f;\n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Reduce\n",
    "    reduceKernel<<<NUM_BLOCKS, BLOCK_SIZE>>>(d_input, d_output, N);\n",
    "    \n",
    "    // Final reduction on host\n",
    "    float* h_output = new float[NUM_BLOCKS];\n",
    "    cudaMemcpy(h_output, d_output, NUM_BLOCKS * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    float total = 0;\n",
    "    for (int i = 0; i < NUM_BLOCKS; i++) total += h_output[i];\n",
    "    \n",
    "    printf(\"Sum of %d elements: %.0f (expected %d)\\n\", N, total, N);\n",
    "    \n",
    "    delete[] h_input;\n",
    "    delete[] h_output;\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364886f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Tile Broadcast\n",
    "Use `tile.shfl()` to broadcast a value from lane 0 to all threads in a tile.\n",
    "\n",
    "### Exercise 2: Ballot and Count\n",
    "Use `cg::ballot()` to count how many threads satisfy a condition.\n",
    "\n",
    "### Exercise 3: Coalesced Histogram\n",
    "Build a histogram using coalesced groups for divergent binning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a81a1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│           COOPERATIVE GROUPS BASICS                     │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Include:                                               │\n",
    "│  #include <cooperative_groups.h>                        │\n",
    "│  namespace cg = cooperative_groups;                     │\n",
    "│                                                         │\n",
    "│  Group Types:                                           │\n",
    "│  • cg::thread_block - all threads in block              │\n",
    "│  • cg::thread_block_tile<N> - tile of N threads         │\n",
    "│  • cg::coalesced_group - active threads only            │\n",
    "│                                                         │\n",
    "│  Operations:                                            │\n",
    "│  • group.sync() - synchronize group                     │\n",
    "│  • group.thread_rank() - thread index in group          │\n",
    "│  • group.size() - number of threads                     │\n",
    "│  • tile.shfl_down() - shuffle within tile               │\n",
    "│                                                         │\n",
    "│  Benefits:                                              │\n",
    "│  • Cleaner code                                         │\n",
    "│  • Sub-warp operations                                  │\n",
    "│  • Divergent code support                               │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 2 - Grid-Wide Synchronization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
