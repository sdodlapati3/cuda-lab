{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ff20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"âš ï¸  Cooperative Groups are a CUDA C++ feature!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067d815",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What Are Cooperative Groups?\n",
    "\n",
    "### Traditional CUDA Synchronization\n",
    "\n",
    "```\n",
    "Traditional CUDA:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "__syncthreads()  â†’ Only syncs within block\n",
    "Warp shuffle     â†’ Only within warp\n",
    "\n",
    "Problems:\n",
    "â€¢ No grid-wide sync\n",
    "â€¢ No sub-warp sync\n",
    "â€¢ No dynamic grouping\n",
    "\n",
    "Cooperative Groups Solution:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "â€¢ Flexible group abstraction\n",
    "â€¢ Groups can be partitioned\n",
    "â€¢ Sync at any granularity\n",
    "â€¢ Works with divergent code\n",
    "```\n",
    "\n",
    "### Group Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚               Grid Group               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚         Thread Block             â”‚  â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚     Tile (32 threads)      â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚  â”‚ Coalesced (active)   â”‚  â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ea2d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Thread Block Groups\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "Compile: `nvcc -o block_group thread_block_group.cu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile thread_block_group.cu\n",
    "// thread_block_group.cu - Basic cooperative groups\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void blockGroupDemo(float* data, int n) {\n",
    "    // ============================================\n",
    "    // Get Thread Block Group\n",
    "    // ============================================\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    int tid = block.thread_rank();  // Same as threadIdx.x\n",
    "    int size = block.size();        // Same as blockDim.x\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        printf(\"Block %d: size=%d\\n\", blockIdx.x, size);\n",
    "    }\n",
    "    \n",
    "    // Perform work\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= 2.0f;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Synchronize Using Group\n",
    "    // ============================================\n",
    "    block.sync();  // Same as __syncthreads()\n",
    "    \n",
    "    // More work after sync\n",
    "    if (idx < n) {\n",
    "        data[idx] += 1.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    blockGroupDemo<<<4, 256>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee39aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o block_group thread_block_group.cu\n",
    "!./block_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37eb9ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Tile Groups\n",
    "\n",
    "### ğŸ”· CUDA C++ Tile Partitioning (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15819320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tile_groups.cu\n",
    "// tile_groups.cu - Warp-level programming with tiles\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void tileGroupDemo(float* data, int n) {\n",
    "    // Get thread block\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    // ============================================\n",
    "    // Create Tile of 32 Threads (Warp)\n",
    "    // ============================================\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    // Tile properties\n",
    "    int lane = warp.thread_rank();    // 0-31 within warp\n",
    "    int warp_id = warp.meta_group_rank();  // Which warp\n",
    "    \n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float val = (idx < n) ? data[idx] : 0.0f;\n",
    "    \n",
    "    // ============================================\n",
    "    // Warp-Level Reduction Using Tile\n",
    "    // ============================================\n",
    "    // Shuffle within tile\n",
    "    for (int offset = warp.size() / 2; offset > 0; offset /= 2) {\n",
    "        val += warp.shfl_down(val, offset);\n",
    "    }\n",
    "    \n",
    "    // Lane 0 has the warp sum\n",
    "    if (lane == 0) {\n",
    "        printf(\"Block %d, Warp %d: sum = %.1f\\n\", \n",
    "               blockIdx.x, warp_id, val);\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Tile Synchronization\n",
    "    // ============================================\n",
    "    warp.sync();  // Sync only within this tile\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Smaller Tiles (Sub-Warp)\n",
    "// ============================================\n",
    "__global__ void smallTileDemo() {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    // Create tiles of 4 threads\n",
    "    cg::thread_block_tile<4> tile4 = cg::tiled_partition<4>(block);\n",
    "    \n",
    "    // Create tiles of 8 threads\n",
    "    cg::thread_block_tile<8> tile8 = cg::tiled_partition<8>(block);\n",
    "    \n",
    "    // Create tiles of 16 threads\n",
    "    cg::thread_block_tile<16> tile16 = cg::tiled_partition<16>(block);\n",
    "    \n",
    "    // Each tile can sync independently\n",
    "    tile4.sync();\n",
    "    tile8.sync();\n",
    "    tile16.sync();\n",
    "    \n",
    "    // Shuffle within small tiles\n",
    "    float val = (float)threadIdx.x;\n",
    "    float sum4 = val;\n",
    "    for (int i = tile4.size()/2; i > 0; i /= 2) {\n",
    "        sum4 += tile4.shfl_down(sum4, i);\n",
    "    }\n",
    "    \n",
    "    if (tile4.thread_rank() == 0) {\n",
    "        printf(\"Thread %d: tile4 sum = %.0f\\n\", threadIdx.x, sum4);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 256;\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    float h_data[N];\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 1.0f;\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    printf(\"=== Warp Tiles ===\\n\");\n",
    "    tileGroupDemo<<<1, 128>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"\\n=== Small Tiles ===\\n\");\n",
    "    smallTileDemo<<<1, 32>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o tile_groups tile_groups.cu\n",
    "!./tile_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325fde11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Coalesced Groups\n",
    "\n",
    "### ğŸ”· CUDA C++ Handling Divergent Code (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coalesced_groups.cu\n",
    "// coalesced_groups.cu - Groups for divergent code\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void coalescedDemo(int* flags, float* data, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // ============================================\n",
    "    // Divergent Branch\n",
    "    // ============================================\n",
    "    if (tid < n && flags[tid]) {  // Only some threads active!\n",
    "        \n",
    "        // Get group of active threads\n",
    "        cg::coalesced_group active = cg::coalesced_threads();\n",
    "        \n",
    "        int rank = active.thread_rank();  // Rank among active threads\n",
    "        int size = active.size();         // How many active\n",
    "        \n",
    "        // Collective operation among active threads only\n",
    "        float val = data[tid];\n",
    "        \n",
    "        // Reduction among active threads\n",
    "        for (int offset = active.size() / 2; offset > 0; offset /= 2) {\n",
    "            val += active.shfl_down(val, offset);\n",
    "        }\n",
    "        \n",
    "        if (rank == 0) {\n",
    "            printf(\"Block %d: %d active threads, sum = %.1f\\n\",\n",
    "                   blockIdx.x, size, val);\n",
    "        }\n",
    "        \n",
    "        // Sync among active threads\n",
    "        active.sync();\n",
    "        \n",
    "        // Continue processing...\n",
    "        data[tid] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 32;\n",
    "    int* d_flags;\n",
    "    float* d_data;\n",
    "    \n",
    "    cudaMalloc(&d_flags, N * sizeof(int));\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Set alternating flags (half threads active)\n",
    "    int h_flags[N];\n",
    "    float h_data[N];\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_flags[i] = (i % 2 == 0) ? 1 : 0;  // Even threads active\n",
    "        h_data[i] = 1.0f;\n",
    "    }\n",
    "    \n",
    "    cudaMemcpy(d_flags, h_flags, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    coalescedDemo<<<1, 32>>>(d_flags, d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_flags);\n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o coalesced_groups coalesced_groups.cu\n",
    "!./coalesced_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d85b04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Practical Example - Block Reduction\n",
    "\n",
    "### ğŸ”· CUDA C++ Optimized Reduction (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_reduction.cu\n",
    "// cg_reduction.cu - Reduction using cooperative groups\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "// Warp reduction using tile shuffle\n",
    "__device__ float warpReduce(cg::thread_block_tile<32>& warp, float val) {\n",
    "    for (int offset = warp.size() / 2; offset > 0; offset /= 2) {\n",
    "        val += warp.shfl_down(val, offset);\n",
    "    }\n",
    "    return val;\n",
    "}\n",
    "\n",
    "// Block reduction using cooperative groups\n",
    "__device__ float blockReduce(cg::thread_block& block, float val) {\n",
    "    __shared__ float warpSums[32];  // Max 32 warps per block\n",
    "    \n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    int lane = warp.thread_rank();\n",
    "    int warp_id = warp.meta_group_rank();\n",
    "    \n",
    "    // Reduce within warp\n",
    "    val = warpReduce(warp, val);\n",
    "    \n",
    "    // Lane 0 writes warp sum\n",
    "    if (lane == 0) {\n",
    "        warpSums[warp_id] = val;\n",
    "    }\n",
    "    \n",
    "    block.sync();  // Wait for all warps\n",
    "    \n",
    "    // First warp reduces warp sums\n",
    "    int numWarps = (block.size() + 31) / 32;\n",
    "    val = (block.thread_rank() < numWarps) ? warpSums[block.thread_rank()] : 0.0f;\n",
    "    \n",
    "    if (warp_id == 0) {\n",
    "        val = warpReduce(warp, val);\n",
    "    }\n",
    "    \n",
    "    return val;\n",
    "}\n",
    "\n",
    "__global__ void reduceKernel(float* input, float* output, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float val = (tid < n) ? input[tid] : 0.0f;\n",
    "    \n",
    "    float blockSum = blockReduce(block, val);\n",
    "    \n",
    "    if (block.thread_rank() == 0) {\n",
    "        output[blockIdx.x] = blockSum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const int BLOCK_SIZE = 256;\n",
    "    const int NUM_BLOCKS = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, N * sizeof(float));\n",
    "    cudaMalloc(&d_output, NUM_BLOCKS * sizeof(float));\n",
    "    \n",
    "    // Initialize with 1s\n",
    "    float* h_input = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1.0f;\n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Reduce\n",
    "    reduceKernel<<<NUM_BLOCKS, BLOCK_SIZE>>>(d_input, d_output, N);\n",
    "    \n",
    "    // Final reduction on host\n",
    "    float* h_output = new float[NUM_BLOCKS];\n",
    "    cudaMemcpy(h_output, d_output, NUM_BLOCKS * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    float total = 0;\n",
    "    for (int i = 0; i < NUM_BLOCKS; i++) total += h_output[i];\n",
    "    \n",
    "    printf(\"Sum of %d elements: %.0f (expected %d)\\n\", N, total, N);\n",
    "    \n",
    "    delete[] h_input;\n",
    "    delete[] h_output;\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2528825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cg_reduction cg_reduction.cu\n",
    "!./cg_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364886f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cooperative_groups_exercises.cu\n",
    "/*\n",
    " * Cooperative Groups Exercises\n",
    " * Exercise 1: Tile Broadcast - Use tile.shfl() to broadcast from lane 0\n",
    " * Exercise 2: Ballot and Count - Use cg::ballot() to count threads\n",
    " * Exercise 3: Coalesced Histogram - Histogram using coalesced groups\n",
    " */\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================\n",
    "// Exercise 1: Tile Broadcast\n",
    "// Broadcast a value from lane 0 to all threads in a tile\n",
    "// =============================================================\n",
    "template<int TILE_SIZE>\n",
    "__global__ void tileBroadcastKernel(int* input, int* output, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<TILE_SIZE> tile = cg::tiled_partition<TILE_SIZE>(block);\n",
    "    \n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx >= n) return;\n",
    "    \n",
    "    // Each thread loads its value\n",
    "    int myValue = input[idx];\n",
    "    \n",
    "    // Get the value from lane 0 of this tile\n",
    "    int lane0Value = tile.shfl(myValue, 0);\n",
    "    \n",
    "    // Store: lane 0's value + this thread's rank\n",
    "    output[idx] = lane0Value * 1000 + tile.thread_rank();\n",
    "}\n",
    "\n",
    "void exercise1_tile_broadcast() {\n",
    "    printf(\"\\n=== Exercise 1: Tile Broadcast ===\\n\");\n",
    "    \n",
    "    const int N = 64;\n",
    "    const int TILE_SIZE = 8;\n",
    "    \n",
    "    int *h_input = new int[N];\n",
    "    int *h_output = new int[N];\n",
    "    int *d_input, *d_output;\n",
    "    \n",
    "    // Initialize: each element is its index\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_input[i] = i;\n",
    "    }\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_input, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_input, h_input, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    tileBroadcastKernel<TILE_SIZE><<<1, N>>>(d_input, d_output, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_output, d_output, N * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Tile size: %d\\n\", TILE_SIZE);\n",
    "    printf(\"Format: output = (lane0_value * 1000) + thread_rank\\n\\n\");\n",
    "    \n",
    "    // Show first 3 tiles\n",
    "    for (int tile = 0; tile < 3; tile++) {\n",
    "        printf(\"Tile %d (lane 0 value = %d):\\n  \", tile, tile * TILE_SIZE);\n",
    "        for (int lane = 0; lane < TILE_SIZE; lane++) {\n",
    "            int idx = tile * TILE_SIZE + lane;\n",
    "            printf(\"%d \", h_output[idx]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    // Verify: lane 0 broadcasts to all\n",
    "    bool correct = true;\n",
    "    for (int tile = 0; tile < N / TILE_SIZE; tile++) {\n",
    "        int lane0Val = tile * TILE_SIZE;\n",
    "        for (int lane = 0; lane < TILE_SIZE; lane++) {\n",
    "            int idx = tile * TILE_SIZE + lane;\n",
    "            int expected = lane0Val * 1000 + lane;\n",
    "            if (h_output[idx] != expected) correct = false;\n",
    "        }\n",
    "    }\n",
    "    printf(\"\\nVerification: %s\\n\", correct ? \"PASSED âœ“\" : \"FAILED âœ—\");\n",
    "    \n",
    "    delete[] h_input;\n",
    "    delete[] h_output;\n",
    "    CHECK_CUDA(cudaFree(d_input));\n",
    "    CHECK_CUDA(cudaFree(d_output));\n",
    "}\n",
    "\n",
    "// =============================================================\n",
    "// Exercise 2: Ballot and Count\n",
    "// Use cg::ballot() to count threads satisfying a condition\n",
    "// =============================================================\n",
    "__global__ void ballotCountKernel(int* data, int* counts, int n, int threshold) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Each thread checks if its value exceeds threshold\n",
    "    bool predicate = (idx < n) && (data[idx] > threshold);\n",
    "    \n",
    "    // Get ballot mask - each bit indicates if corresponding lane satisfied predicate\n",
    "    unsigned int ballot = warp.ballot(predicate);\n",
    "    \n",
    "    // Count bits (popcount) - all threads in warp get same result\n",
    "    int count = __popc(ballot);\n",
    "    \n",
    "    // Only lane 0 of each warp stores the result\n",
    "    if (warp.thread_rank() == 0) {\n",
    "        int warpIdx = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n",
    "        counts[warpIdx] = count;\n",
    "        \n",
    "        // Debug: also print the ballot mask\n",
    "        if (warpIdx < 4) {\n",
    "            printf(\"Warp %d: ballot=0x%08x, count=%d\\n\", warpIdx, ballot, count);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise2_ballot_count() {\n",
    "    printf(\"\\n=== Exercise 2: Ballot and Count ===\\n\");\n",
    "    \n",
    "    const int N = 256;\n",
    "    const int THRESHOLD = 150;\n",
    "    const int NUM_WARPS = N / 32;\n",
    "    \n",
    "    int *h_data = new int[N];\n",
    "    int *h_counts = new int[NUM_WARPS];\n",
    "    int *d_data, *d_counts;\n",
    "    \n",
    "    // Initialize with values 0 to N-1\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_data[i] = i;\n",
    "    }\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_counts, NUM_WARPS * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    printf(\"Counting values > %d (out of %d elements)\\n\\n\", THRESHOLD, N);\n",
    "    \n",
    "    ballotCountKernel<<<1, N>>>(d_data, d_counts, N, THRESHOLD);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_counts, d_counts, NUM_WARPS * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Sum all warp counts\n",
    "    int totalCount = 0;\n",
    "    printf(\"\\nPer-warp counts:\\n\");\n",
    "    for (int w = 0; w < NUM_WARPS; w++) {\n",
    "        printf(\"  Warp %d: %d threads with value > %d\\n\", w, h_counts[w], THRESHOLD);\n",
    "        totalCount += h_counts[w];\n",
    "    }\n",
    "    \n",
    "    // Verify\n",
    "    int expectedCount = N - THRESHOLD - 1;  // Values 151, 152, ..., 255\n",
    "    printf(\"\\nTotal count: %d (expected: %d)\\n\", totalCount, expectedCount);\n",
    "    printf(\"Verification: %s\\n\", totalCount == expectedCount ? \"PASSED âœ“\" : \"FAILED âœ—\");\n",
    "    \n",
    "    delete[] h_data;\n",
    "    delete[] h_counts;\n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "    CHECK_CUDA(cudaFree(d_counts));\n",
    "}\n",
    "\n",
    "// =============================================================\n",
    "// Exercise 3: Coalesced Histogram\n",
    "// Build histogram using coalesced groups for divergent binning\n",
    "// =============================================================\n",
    "#define NUM_BINS 16\n",
    "\n",
    "__global__ void coalescedHistogramKernel(unsigned char* data, int* histogram, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    // Shared memory for block-local histogram\n",
    "    __shared__ int sharedHist[NUM_BINS];\n",
    "    \n",
    "    // Initialize shared histogram\n",
    "    if (threadIdx.x < NUM_BINS) {\n",
    "        sharedHist[threadIdx.x] = 0;\n",
    "    }\n",
    "    block.sync();\n",
    "    \n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Process multiple elements per thread (grid-stride loop)\n",
    "    for (int i = idx; i < n; i += blockDim.x * gridDim.x) {\n",
    "        unsigned char value = data[i];\n",
    "        int bin = value / (256 / NUM_BINS);  // Map to bin 0-15\n",
    "        \n",
    "        // Create coalesced group of threads going to same bin\n",
    "        // Note: coalesced_threads() returns active threads after divergence\n",
    "        cg::coalesced_group active = cg::coalesced_threads();\n",
    "        \n",
    "        // Elect one thread from the coalesced group to update\n",
    "        // This reduces atomic contention\n",
    "        int laneMask = __match_any_sync(0xFFFFFFFF, bin);\n",
    "        int leader = __ffs(laneMask) - 1;  // Find first set bit\n",
    "        int laneId = threadIdx.x % 32;\n",
    "        \n",
    "        if (laneId == leader) {\n",
    "            int count = __popc(laneMask);  // Count threads with same bin\n",
    "            atomicAdd(&sharedHist[bin], count);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    block.sync();\n",
    "    \n",
    "    // Merge shared histogram to global\n",
    "    if (threadIdx.x < NUM_BINS) {\n",
    "        atomicAdd(&histogram[threadIdx.x], sharedHist[threadIdx.x]);\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise3_coalesced_histogram() {\n",
    "    printf(\"\\n=== Exercise 3: Coalesced Histogram ===\\n\");\n",
    "    \n",
    "    const int N = 1024 * 1024;\n",
    "    \n",
    "    unsigned char *h_data = new unsigned char[N];\n",
    "    int *h_histogram = new int[NUM_BINS];\n",
    "    int *h_reference = new int[NUM_BINS];\n",
    "    unsigned char *d_data;\n",
    "    int *d_histogram;\n",
    "    \n",
    "    // Initialize with somewhat uniform distribution\n",
    "    srand(42);\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_data[i] = rand() % 256;\n",
    "    }\n",
    "    \n",
    "    // Compute reference histogram on CPU\n",
    "    memset(h_reference, 0, NUM_BINS * sizeof(int));\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        int bin = h_data[i] / (256 / NUM_BINS);\n",
    "        h_reference[bin]++;\n",
    "    }\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(unsigned char)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_histogram, NUM_BINS * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, N, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_histogram, 0, NUM_BINS * sizeof(int)));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    numBlocks = min(numBlocks, 256);  // Limit blocks for better SM utilization\n",
    "    \n",
    "    // Time GPU histogram\n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    coalescedHistogramKernel<<<numBlocks, blockSize>>>(d_data, d_histogram, N);\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    \n",
    "    float gpuTime;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&gpuTime, start, stop));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_histogram, d_histogram, NUM_BINS * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Display results\n",
    "    printf(\"Histogram of %d elements into %d bins:\\n\\n\", N, NUM_BINS);\n",
    "    printf(\"%-8s %-10s %-10s %-10s\\n\", \"Bin\", \"GPU\", \"Reference\", \"Status\");\n",
    "    printf(\"%-8s %-10s %-10s %-10s\\n\", \"---\", \"---\", \"---------\", \"------\");\n",
    "    \n",
    "    bool allCorrect = true;\n",
    "    for (int i = 0; i < NUM_BINS; i++) {\n",
    "        const char* status = (h_histogram[i] == h_reference[i]) ? \"âœ“\" : \"âœ—\";\n",
    "        if (h_histogram[i] != h_reference[i]) allCorrect = false;\n",
    "        printf(\"%-8d %-10d %-10d %-10s\\n\", i, h_histogram[i], h_reference[i], status);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\nGPU time: %.3f ms\\n\", gpuTime);\n",
    "    printf(\"Throughput: %.2f GB/s\\n\", (N / 1e9) / (gpuTime / 1000.0));\n",
    "    printf(\"Verification: %s\\n\", allCorrect ? \"PASSED âœ“\" : \"FAILED âœ—\");\n",
    "    \n",
    "    delete[] h_data;\n",
    "    delete[] h_histogram;\n",
    "    delete[] h_reference;\n",
    "    CHECK_CUDA(cudaEventDestroy(start));\n",
    "    CHECK_CUDA(cudaEventDestroy(stop));\n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "    CHECK_CUDA(cudaFree(d_histogram));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Cooperative Groups Exercises\\n\");\n",
    "    printf(\"============================\\n\");\n",
    "    \n",
    "    int device;\n",
    "    cudaDeviceProp prop;\n",
    "    CHECK_CUDA(cudaGetDevice(&device));\n",
    "    CHECK_CUDA(cudaGetDeviceProperties(&prop, device));\n",
    "    printf(\"Device: %s (Compute %d.%d)\\n\", prop.name, prop.major, prop.minor);\n",
    "    \n",
    "    exercise1_tile_broadcast();\n",
    "    exercise2_ballot_count();\n",
    "    exercise3_coalesced_histogram();\n",
    "    \n",
    "    printf(\"\\nâœ“ All exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cooperative_groups_exercises cooperative_groups_exercises.cu && ./cooperative_groups_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d09f5",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Tile Broadcast\n",
    "Use `tile.shfl()` to broadcast a value from lane 0 to all threads in a tile.\n",
    "\n",
    "### Exercise 2: Ballot and Count\n",
    "Use `cg::ballot()` to count how many threads satisfy a condition.\n",
    "\n",
    "### Exercise 3: Coalesced Histogram\n",
    "Build a histogram using coalesced groups for divergent binning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a81a1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           COOPERATIVE GROUPS BASICS                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Include:                                               â”‚\n",
    "â”‚  #include <cooperative_groups.h>                        â”‚\n",
    "â”‚  namespace cg = cooperative_groups;                     â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Group Types:                                           â”‚\n",
    "â”‚  â€¢ cg::thread_block - all threads in block              â”‚\n",
    "â”‚  â€¢ cg::thread_block_tile<N> - tile of N threads         â”‚\n",
    "â”‚  â€¢ cg::coalesced_group - active threads only            â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Operations:                                            â”‚\n",
    "â”‚  â€¢ group.sync() - synchronize group                     â”‚\n",
    "â”‚  â€¢ group.thread_rank() - thread index in group          â”‚\n",
    "â”‚  â€¢ group.size() - number of threads                     â”‚\n",
    "â”‚  â€¢ tile.shfl_down() - shuffle within tile               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Benefits:                                              â”‚\n",
    "â”‚  â€¢ Cleaner code                                         â”‚\n",
    "â”‚  â€¢ Sub-warp operations                                  â”‚\n",
    "â”‚  â€¢ Divergent code support                               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Next: Day 2 - Grid-Wide Synchronization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
