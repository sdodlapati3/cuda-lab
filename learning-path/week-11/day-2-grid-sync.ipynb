{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666deb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"⚠️  Grid-wide sync is a CUDA C++ feature requiring cooperative launch!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814c0d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Why Grid-Wide Sync?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "```\n",
    "Traditional CUDA - Multiple Kernel Launches:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "kernel1<<<blocks, threads>>>(data);  // Pass 1\n",
    "cudaDeviceSynchronize();              // Host overhead!\n",
    "kernel2<<<blocks, threads>>>(data);  // Pass 2\n",
    "cudaDeviceSynchronize();              // Host overhead!\n",
    "kernel3<<<blocks, threads>>>(data);  // Pass 3\n",
    "\n",
    "Problems:\n",
    "• Each launch has ~5-15μs overhead\n",
    "• Data goes through L2 cache multiple times\n",
    "• Can't keep data in L1/registers between passes\n",
    "\n",
    "With Grid Sync - Single Kernel:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "__global__ void fusedKernel(data) {\n",
    "    // Pass 1\n",
    "    process1(data);\n",
    "    grid.sync();      // GPU-only sync, ~1μs!\n",
    "    \n",
    "    // Pass 2\n",
    "    process2(data);\n",
    "    grid.sync();\n",
    "    \n",
    "    // Pass 3\n",
    "    process3(data);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b0c16b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Basic Grid Synchronization\n",
    "\n",
    "### CUDA C++ Grid Group (Primary)\n",
    "\n",
    "```cpp\n",
    "// grid_sync.cu - Basic grid-wide synchronization\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void gridSyncDemo(float* data, int n) {\n",
    "    // ============================================\n",
    "    // Get Grid Group\n",
    "    // ============================================\n",
    "    cg::grid_group grid = cg::this_grid();\n",
    "    \n",
    "    int tid = grid.thread_rank();  // Global thread index\n",
    "    int gridSize = grid.size();    // Total threads in grid\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        printf(\"Grid has %d threads\\n\", gridSize);\n",
    "    }\n",
    "    \n",
    "    // Pass 1: Multiply\n",
    "    if (tid < n) {\n",
    "        data[tid] *= 2.0f;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Grid-Wide Synchronization\n",
    "    // ============================================\n",
    "    grid.sync();  // ALL blocks wait here!\n",
    "    \n",
    "    // Pass 2: Add (safe - all threads done with pass 1)\n",
    "    if (tid < n) {\n",
    "        data[tid] += 1.0f;\n",
    "    }\n",
    "    \n",
    "    grid.sync();  // Sync again\n",
    "    \n",
    "    // Pass 3: Sqrt\n",
    "    if (tid < n) {\n",
    "        data[tid] = sqrtf(data[tid]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    const int BLOCK_SIZE = 256;\n",
    "    \n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_data = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_data[i] = 4.0f;\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // ============================================\n",
    "    // Query Maximum Blocks for Cooperative Launch\n",
    "    // ============================================\n",
    "    int device = 0;\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, device);\n",
    "    \n",
    "    int numBlocksPerSM;\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "        &numBlocksPerSM, gridSyncDemo, BLOCK_SIZE, 0);\n",
    "    \n",
    "    int maxBlocks = numBlocksPerSM * prop.multiProcessorCount;\n",
    "    int numBlocks = min(maxBlocks, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    printf(\"Using %d blocks (max %d)\\n\", numBlocks, maxBlocks);\n",
    "    \n",
    "    // ============================================\n",
    "    // Cooperative Kernel Launch\n",
    "    // ============================================\n",
    "    void* args[] = { &d_data, (void*)&N };\n",
    "    \n",
    "    cudaLaunchCooperativeKernel(\n",
    "        (void*)gridSyncDemo,\n",
    "        dim3(numBlocks),\n",
    "        dim3(BLOCK_SIZE),\n",
    "        args,\n",
    "        0,       // Shared memory\n",
    "        0        // Stream (default)\n",
    "    );\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Verify\n",
    "    cudaMemcpy(h_data, d_data, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Result[0] = %.4f (expected %.4f)\\n\", h_data[0], sqrtf(4.0f * 2.0f + 1.0f));\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Compile: `nvcc -arch=sm_70 -o grid_sync grid_sync.cu`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5827a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Occupancy Requirements\n",
    "\n",
    "### Why Limit Grid Size?\n",
    "\n",
    "```\n",
    "Grid Sync Requirement:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "ALL blocks must be resident simultaneously!\n",
    "\n",
    "If you launch too many blocks:\n",
    "• Some blocks can't start\n",
    "• Running blocks wait at sync()\n",
    "• Waiting blocks never start\n",
    "• DEADLOCK!\n",
    "\n",
    "Solution: Query max occupancy\n",
    "\n",
    "cudaOccupancyMaxActiveBlocksPerMultiprocessor(&blocksPerSM, kernel, blockSize, sharedMem);\n",
    "maxBlocks = blocksPerSM × numSMs;\n",
    "```\n",
    "\n",
    "### Occupancy Calculator\n",
    "\n",
    "```cpp\n",
    "// occupancy_check.cu - Calculate safe grid size\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void cooperativeKernel(float* data, int n) {\n",
    "    cg::grid_group grid = cg::this_grid();\n",
    "    // ... work ...\n",
    "    grid.sync();\n",
    "}\n",
    "\n",
    "int getMaxCooperativeBlocks(int blockSize, size_t sharedMem = 0) {\n",
    "    int device;\n",
    "    cudaGetDevice(&device);\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, device);\n",
    "    \n",
    "    // Check if cooperative launch is supported\n",
    "    if (!prop.cooperativeLaunch) {\n",
    "        printf(\"Cooperative launch not supported!\\n\");\n",
    "        return 0;\n",
    "    }\n",
    "    \n",
    "    int blocksPerSM;\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "        &blocksPerSM, \n",
    "        cooperativeKernel, \n",
    "        blockSize, \n",
    "        sharedMem);\n",
    "    \n",
    "    return blocksPerSM * prop.multiProcessorCount;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Max cooperative blocks (256 threads): %d\\n\", getMaxCooperativeBlocks(256));\n",
    "    printf(\"Max cooperative blocks (512 threads): %d\\n\", getMaxCooperativeBlocks(512));\n",
    "    printf(\"Max cooperative blocks (1024 threads): %d\\n\", getMaxCooperativeBlocks(1024));\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7c89b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Practical Example - Multi-Pass Reduction\n",
    "\n",
    "### CUDA C++ Grid Reduction (Primary)\n",
    "\n",
    "```cpp\n",
    "// grid_reduction.cu - Single-kernel full reduction\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__device__ float warpReduce(cg::thread_block_tile<32>& warp, float val) {\n",
    "    for (int i = warp.size()/2; i > 0; i /= 2) {\n",
    "        val += warp.shfl_down(val, i);\n",
    "    }\n",
    "    return val;\n",
    "}\n",
    "\n",
    "__global__ void gridReduce(float* input, float* output, int n) {\n",
    "    cg::grid_group grid = cg::this_grid();\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    extern __shared__ float sdata[];\n",
    "    \n",
    "    int tid = grid.thread_rank();\n",
    "    int gridSize = grid.size();\n",
    "    \n",
    "    // ============================================\n",
    "    // Phase 1: Grid-stride loop to load & reduce\n",
    "    // ============================================\n",
    "    float sum = 0.0f;\n",
    "    for (int i = tid; i < n; i += gridSize) {\n",
    "        sum += input[i];\n",
    "    }\n",
    "    \n",
    "    // Warp reduction\n",
    "    sum = warpReduce(warp, sum);\n",
    "    \n",
    "    // Store warp results to shared memory\n",
    "    int lane = warp.thread_rank();\n",
    "    int warp_id = block.thread_rank() / 32;\n",
    "    if (lane == 0) {\n",
    "        sdata[warp_id] = sum;\n",
    "    }\n",
    "    block.sync();\n",
    "    \n",
    "    // First warp reduces shared memory\n",
    "    int numWarps = (block.size() + 31) / 32;\n",
    "    sum = (block.thread_rank() < numWarps) ? sdata[block.thread_rank()] : 0.0f;\n",
    "    if (warp_id == 0) {\n",
    "        sum = warpReduce(warp, sum);\n",
    "    }\n",
    "    \n",
    "    // Block leader writes to global array\n",
    "    if (block.thread_rank() == 0) {\n",
    "        output[blockIdx.x] = sum;\n",
    "    }\n",
    "    \n",
    "    // ============================================\n",
    "    // Grid Sync - Wait for all blocks\n",
    "    // ============================================\n",
    "    grid.sync();\n",
    "    \n",
    "    // ============================================\n",
    "    // Phase 2: Final reduction (first block only)\n",
    "    // ============================================\n",
    "    if (blockIdx.x == 0) {\n",
    "        int numBlocks = gridDim.x;\n",
    "        sum = 0.0f;\n",
    "        \n",
    "        // Each thread sums subset of block results\n",
    "        for (int i = block.thread_rank(); i < numBlocks; i += block.size()) {\n",
    "            sum += output[i];\n",
    "        }\n",
    "        \n",
    "        // Reduce within block\n",
    "        sum = warpReduce(warp, sum);\n",
    "        if (lane == 0) sdata[warp_id] = sum;\n",
    "        block.sync();\n",
    "        \n",
    "        sum = (block.thread_rank() < numWarps) ? sdata[block.thread_rank()] : 0.0f;\n",
    "        if (warp_id == 0) sum = warpReduce(warp, sum);\n",
    "        \n",
    "        // Final result\n",
    "        if (block.thread_rank() == 0) {\n",
    "            output[0] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 24;  // 16M elements\n",
    "    const int BLOCK_SIZE = 256;\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, N * sizeof(float));\n",
    "    cudaMalloc(&d_output, 1024 * sizeof(float));  // Workspace\n",
    "    \n",
    "    // Initialize with 1s\n",
    "    float* h_input = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1.0f;\n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Get max blocks\n",
    "    int device;\n",
    "    cudaGetDevice(&device);\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, device);\n",
    "    \n",
    "    int blocksPerSM;\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "        &blocksPerSM, gridReduce, BLOCK_SIZE, 32 * sizeof(float));\n",
    "    int numBlocks = blocksPerSM * prop.multiProcessorCount;\n",
    "    \n",
    "    printf(\"Launching %d blocks\\n\", numBlocks);\n",
    "    \n",
    "    // Launch\n",
    "    void* args[] = { &d_input, &d_output, (void*)&N };\n",
    "    cudaLaunchCooperativeKernel(\n",
    "        (void*)gridReduce,\n",
    "        dim3(numBlocks),\n",
    "        dim3(BLOCK_SIZE),\n",
    "        args,\n",
    "        32 * sizeof(float),\n",
    "        0);\n",
    "    \n",
    "    float result;\n",
    "    cudaMemcpy(&result, d_output, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Sum = %.0f (expected %d)\\n\", result, N);\n",
    "    \n",
    "    delete[] h_input;\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d5da89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Grid Prefix Sum\n",
    "Implement Blelloch scan using grid sync for multiple phases.\n",
    "\n",
    "### Exercise 2: Iterative Solver\n",
    "Implement Jacobi iteration with grid sync between iterations.\n",
    "\n",
    "### Exercise 3: Histogram with Grid Sync\n",
    "Build a global histogram in a single kernel using grid sync."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5276da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│           GRID-WIDE SYNCHRONIZATION                     │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Get Grid Group:                                        │\n",
    "│  cg::grid_group grid = cg::this_grid();                 │\n",
    "│                                                         │\n",
    "│  Synchronize:                                           │\n",
    "│  grid.sync();  // All blocks wait here                  │\n",
    "│                                                         │\n",
    "│  Launch Requirements:                                   │\n",
    "│  • Use cudaLaunchCooperativeKernel()                    │\n",
    "│  • Limit blocks to max occupancy                        │\n",
    "│  • Check prop.cooperativeLaunch                         │\n",
    "│                                                         │\n",
    "│  Benefits:                                              │\n",
    "│  • Eliminate kernel launch overhead                     │\n",
    "│  • Keep data in cache between phases                    │\n",
    "│  • Single kernel for multi-pass algorithms              │\n",
    "│                                                         │\n",
    "│  Limitations:                                           │\n",
    "│  • Grid size limited by occupancy                       │\n",
    "│  • Requires compute capability 3.5+                     │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 3 - Dynamic Parallelism Basics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
