{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa72fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"âš ï¸  Nested kernels require CUDA C++ with -rdc=true!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f6e9e",
   "metadata": {},
   "source": [
    "# Day 4: Nested Kernel Patterns - Russian Nesting Dolls\n",
    "\n",
    "## ğŸ¯ The Hook: Matryoshka Dolls\n",
    "\n",
    "**Have you ever opened a Russian nesting doll (matryoshka)?** You open the outer doll, and inside is another doll. Open that one, and there's another. And another. Each doll contains a complete, smaller version of itself.\n",
    "\n",
    "That's exactly how **nested kernels** work in CUDA:\n",
    "- The outer kernel launches child kernels\n",
    "- Those children can launch THEIR children\n",
    "- Each level is a complete, functional kernel\n",
    "- Up to 24 levels deep (that's a LOT of dolls!)\n",
    "\n",
    "**Why nest kernels?**\n",
    "- **Recursive algorithms**: Quicksort naturally divides and conquers\n",
    "- **Adaptive detail**: Refine only where needed (like zooming into fractals)\n",
    "- **Tree structures**: Each node spawns work for its children\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "\n",
    "1. **Implement** recursive parallel algorithms using nested kernels\n",
    "2. **Design** effective base cases and recursion limits\n",
    "3. **Optimize** nested launches with thresholds and batching\n",
    "4. **Apply** patterns for quicksort, tree traversal, and adaptive refinement\n",
    "5. **Avoid** common pitfalls (stack overflow, excessive overhead)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸª† Concept Card: Russian Nesting Dolls (Matryoshka)\n",
    "\n",
    "```\n",
    "NESTED KERNELS = MATRYOSHKA DOLLS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "Open the outer doll â†’ Find another doll inside\n",
    "Open that doll     â†’ Another one!\n",
    "Keep opening       â†’ Until you reach the tiny solid doll\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Outer Kernel (Biggest Doll)                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚  Child Kernel (Medium Doll)            â”‚  â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚  Grandchild Kernel (Small Doll)  â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚  â”‚  Base Case (Solid Doll)   â”‚  â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚  â”‚  No more nesting!         â”‚  â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Quicksort Example:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  sortKernel([8,3,7,1,5,2,9,4])                 â”‚\n",
    "â”‚  â”‚ pivot = 5                                   â”‚\n",
    "â”‚  â”‚                                             â”‚\n",
    "â”‚  â”œâ”€â†’ sortKernel([3,1,2,4])    // Left doll    â”‚\n",
    "â”‚  â”‚   â”‚ pivot = 2                               â”‚\n",
    "â”‚  â”‚   â”œâ”€â†’ sortKernel([1])      // Base case!   â”‚\n",
    "â”‚  â”‚   â””â”€â†’ sortKernel([3,4])                    â”‚\n",
    "â”‚  â”‚       â””â”€â†’ ...                               â”‚\n",
    "â”‚  â”‚                                             â”‚\n",
    "â”‚  â””â”€â†’ sortKernel([8,7,9])      // Right doll   â”‚\n",
    "â”‚      â””â”€â†’ ...                                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Key Design Patterns:\n",
    "â€¢ MIN_SIZE = 32:   \"Doll too small? Just sort directly!\"\n",
    "â€¢ MAX_DEPTH = 16:  \"Don't open more than 16 dolls\"\n",
    "â€¢ Non-blocking:    \"Siblings can open in parallel\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Recursive Patterns\n",
    "\n",
    "### ğŸ”· CUDA C++ Parallel Quicksort (Primary)\n",
    "\n",
    "This example demonstrates recursive quicksort using dynamic parallelism:\n",
    "- Child kernels are launched from within device code\n",
    "- Uses non-blocking streams for parallel left/right partitioning\n",
    "- Falls back to insertion sort for small arrays (MIN_SIZE threshold)\n",
    "- Limits recursion depth (MAX_DEPTH) to prevent stack overflow\n",
    "\n",
    "**Compile with:** `nvcc -arch=sm_75 -rdc=true -o quicksort quicksort_dp.cu -lcudadevrt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile quicksort_dp.cu\n",
    "// quicksort_dp.cu - Recursive quicksort with dynamic parallelism\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define MAX_DEPTH 16\n",
    "#define MIN_SIZE 32  // Switch to sequential sort\n",
    "\n",
    "// ============================================\n",
    "// Sequential Sort for Small Arrays\n",
    "// ============================================\n",
    "__device__ void insertionSort(int* data, int left, int right) {\n",
    "    for (int i = left + 1; i <= right; i++) {\n",
    "        int key = data[i];\n",
    "        int j = i - 1;\n",
    "        while (j >= left && data[j] > key) {\n",
    "            data[j + 1] = data[j];\n",
    "            j--;\n",
    "        }\n",
    "        data[j + 1] = key;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Partition\n",
    "// ============================================\n",
    "__device__ int partition(int* data, int left, int right) {\n",
    "    int pivot = data[right];\n",
    "    int i = left - 1;\n",
    "    \n",
    "    for (int j = left; j < right; j++) {\n",
    "        if (data[j] <= pivot) {\n",
    "            i++;\n",
    "            int temp = data[i];\n",
    "            data[i] = data[j];\n",
    "            data[j] = temp;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    int temp = data[i + 1];\n",
    "    data[i + 1] = data[right];\n",
    "    data[right] = temp;\n",
    "    \n",
    "    return i + 1;\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Recursive Quicksort Kernel\n",
    "// ============================================\n",
    "__global__ void quicksortKernel(int* data, int left, int right, int depth) {\n",
    "    // Base case: small array or max depth\n",
    "    if (right - left < MIN_SIZE || depth >= MAX_DEPTH) {\n",
    "        insertionSort(data, left, right);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Partition\n",
    "    int pivotIdx = partition(data, left, right);\n",
    "    \n",
    "    // Launch child kernels for left and right partitions\n",
    "    cudaStream_t s1, s2;\n",
    "    cudaStreamCreateWithFlags(&s1, cudaStreamNonBlocking);\n",
    "    cudaStreamCreateWithFlags(&s2, cudaStreamNonBlocking);\n",
    "    \n",
    "    // Left partition\n",
    "    if (left < pivotIdx - 1) {\n",
    "        quicksortKernel<<<1, 1, 0, s1>>>(data, left, pivotIdx - 1, depth + 1);\n",
    "    }\n",
    "    \n",
    "    // Right partition\n",
    "    if (pivotIdx + 1 < right) {\n",
    "        quicksortKernel<<<1, 1, 0, s2>>>(data, pivotIdx + 1, right, depth + 1);\n",
    "    }\n",
    "    \n",
    "    // Wait for children\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaStreamDestroy(s1);\n",
    "    cudaStreamDestroy(s2);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Host Entry Point\n",
    "// ============================================\n",
    "__global__ void quicksortEntry(int* data, int n) {\n",
    "    quicksortKernel<<<1, 1>>>(data, 0, n - 1, 0);\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    int* h_data = new int[N];\n",
    "    int* d_data;\n",
    "    \n",
    "    // Random data\n",
    "    srand(42);\n",
    "    for (int i = 0; i < N; i++) h_data[i] = rand() % 10000;\n",
    "    \n",
    "    printf(\"First 10 elements before: \");\n",
    "    for (int i = 0; i < 10; i++) printf(\"%d \", h_data[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaMalloc(&d_data, N * sizeof(int));\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Sort\n",
    "    quicksortEntry<<<1, 1>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Check for errors\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    // Verify\n",
    "    cudaMemcpy(h_data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"First 10 elements after:  \");\n",
    "    for (int i = 0; i < 10; i++) printf(\"%d \", h_data[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    bool sorted = true;\n",
    "    for (int i = 1; i < N; i++) {\n",
    "        if (h_data[i] < h_data[i-1]) {\n",
    "            sorted = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Array sorted: %s\\n\", sorted ? \"YES\" : \"NO\");\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07514c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -o quicksort_dp quicksort_dp.cu -lcudadevrt && ./quicksort_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec25f86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Tree/Graph Traversal\n",
    "\n",
    "### ğŸ”· CUDA C++ Parallel Tree Processing (Primary)\n",
    "\n",
    "This example demonstrates parallel tree traversal using dynamic parallelism:\n",
    "- Processes a binary tree structure stored in device memory\n",
    "- Recursively launches child kernels for left and right subtrees\n",
    "- Uses atomicAdd to accumulate values across all nodes\n",
    "- Depth guard prevents infinite recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea909a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tree_traversal.cu\n",
    "// tree_traversal.cu - Parallel tree processing\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "struct TreeNode {\n",
    "    float value;\n",
    "    int leftChild;   // Index, -1 if none\n",
    "    int rightChild;  // Index, -1 if none\n",
    "};\n",
    "\n",
    "// ============================================\n",
    "// Process Node and Recurse to Children\n",
    "// ============================================\n",
    "__global__ void processTree(TreeNode* tree, float* results, int nodeIdx, int depth) {\n",
    "    if (nodeIdx < 0 || depth > 20) return;  // Guard\n",
    "    \n",
    "    TreeNode* node = &tree[nodeIdx];\n",
    "    \n",
    "    // Process this node (accumulate value)\n",
    "    atomicAdd(&results[0], node->value);\n",
    "    \n",
    "    // Launch children in parallel\n",
    "    if (node->leftChild >= 0 && node->rightChild >= 0) {\n",
    "        // Both children exist - launch in parallel\n",
    "        cudaStream_t s1, s2;\n",
    "        cudaStreamCreateWithFlags(&s1, cudaStreamNonBlocking);\n",
    "        cudaStreamCreateWithFlags(&s2, cudaStreamNonBlocking);\n",
    "        \n",
    "        processTree<<<1, 1, 0, s1>>>(tree, results, node->leftChild, depth + 1);\n",
    "        processTree<<<1, 1, 0, s2>>>(tree, results, node->rightChild, depth + 1);\n",
    "        \n",
    "        cudaDeviceSynchronize();\n",
    "        cudaStreamDestroy(s1);\n",
    "        cudaStreamDestroy(s2);\n",
    "    } else {\n",
    "        // One or no children\n",
    "        if (node->leftChild >= 0) {\n",
    "            processTree<<<1, 1>>>(tree, results, node->leftChild, depth + 1);\n",
    "            cudaDeviceSynchronize();\n",
    "        }\n",
    "        if (node->rightChild >= 0) {\n",
    "            processTree<<<1, 1>>>(tree, results, node->rightChild, depth + 1);\n",
    "            cudaDeviceSynchronize();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Build simple tree:\n",
    "    //       0(10)\n",
    "    //      /    \\\n",
    "    //    1(20)  2(30)\n",
    "    //    /  \\\n",
    "    //  3(5) 4(15)\n",
    "    \n",
    "    TreeNode h_tree[5];\n",
    "    h_tree[0] = {10.0f, 1, 2};    // Root\n",
    "    h_tree[1] = {20.0f, 3, 4};    // Left child\n",
    "    h_tree[2] = {30.0f, -1, -1};  // Right child (leaf)\n",
    "    h_tree[3] = {5.0f, -1, -1};   // Leaf\n",
    "    h_tree[4] = {15.0f, -1, -1};  // Leaf\n",
    "    \n",
    "    printf(\"Tree structure:\\n\");\n",
    "    printf(\"       0(10)\\n\");\n",
    "    printf(\"      /    \\\\\\n\");\n",
    "    printf(\"    1(20)  2(30)\\n\");\n",
    "    printf(\"    /  \\\\\\n\");\n",
    "    printf(\"  3(5) 4(15)\\n\\n\");\n",
    "    \n",
    "    TreeNode* d_tree;\n",
    "    float* d_result;\n",
    "    \n",
    "    cudaMalloc(&d_tree, 5 * sizeof(TreeNode));\n",
    "    cudaMalloc(&d_result, sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_tree, h_tree, 5 * sizeof(TreeNode), cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_result, 0, sizeof(float));\n",
    "    \n",
    "    // Process tree starting at root\n",
    "    processTree<<<1, 1>>>(d_tree, d_result, 0, 0);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Check for errors\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    float sum;\n",
    "    cudaMemcpy(&sum, d_result, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Tree sum: %.0f (expected 80 = 10+20+30+5+15)\\n\", sum);\n",
    "    \n",
    "    cudaFree(d_tree);\n",
    "    cudaFree(d_result);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5506f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -o tree_traversal tree_traversal.cu -lcudadevrt && ./tree_traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52d94f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Adaptive Mesh Refinement\n",
    "\n",
    "### ğŸ”· CUDA C++ Conditional Work Spawning (Primary)\n",
    "\n",
    "This example demonstrates adaptive mesh refinement using dynamic parallelism:\n",
    "- Cells are processed and refined based on error estimates\n",
    "- Refinement creates 4 child cells (quad subdivision pattern)\n",
    "- Uses atomicAdd to track dynamic cell allocation\n",
    "- Recurses only when error exceeds threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dccad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile adaptive_mesh.cu\n",
    "// adaptive_mesh.cu - Adaptive refinement pattern\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "\n",
    "struct Cell {\n",
    "    float value;\n",
    "    float error;  // Error estimate\n",
    "    int refined;  // Has been refined\n",
    "};\n",
    "\n",
    "#define ERROR_THRESHOLD 0.1f\n",
    "#define MAX_DEPTH 3\n",
    "\n",
    "// ============================================\n",
    "// Process Cell - Refine if Error Too High\n",
    "// ============================================\n",
    "__global__ void processCell(Cell* cells, int cellIdx, int* childStart, int depth) {\n",
    "    if (depth >= MAX_DEPTH) return;\n",
    "    \n",
    "    Cell* cell = &cells[cellIdx];\n",
    "    \n",
    "    // Compute work (simplified)\n",
    "    cell->value = sinf((float)cellIdx * 0.5f);\n",
    "    cell->error = fabsf(cell->value - 0.5f) * 0.2f;  // Fake error\n",
    "    \n",
    "    // Check if refinement needed\n",
    "    if (cell->error > ERROR_THRESHOLD && depth < MAX_DEPTH) {\n",
    "        cell->refined = 1;\n",
    "        \n",
    "        // Allocate child cells atomically\n",
    "        int childBase = atomicAdd(childStart, 4);  // 4 children (2D quad)\n",
    "        \n",
    "        // Launch refinement for each child\n",
    "        for (int i = 0; i < 4; i++) {\n",
    "            processCell<<<1, 1>>>(cells, childBase + i, childStart, depth + 1);\n",
    "        }\n",
    "        \n",
    "        cudaDeviceSynchronize();\n",
    "    } else {\n",
    "        cell->refined = 0;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Entry Kernel - Start with Coarse Grid\n",
    "// ============================================\n",
    "__global__ void adaptiveMesh(Cell* cells, int initialCells, int* childStart) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (tid < initialCells) {\n",
    "        processCell<<<1, 1>>>(cells, tid, childStart, 0);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int INITIAL_CELLS = 16;\n",
    "    const int MAX_CELLS = 10000;  // Preallocate\n",
    "    \n",
    "    Cell* d_cells;\n",
    "    int* d_childStart;\n",
    "    \n",
    "    cudaMalloc(&d_cells, MAX_CELLS * sizeof(Cell));\n",
    "    cudaMalloc(&d_childStart, sizeof(int));\n",
    "    \n",
    "    int initialChild = INITIAL_CELLS;  // Children start after initial\n",
    "    cudaMemcpy(d_childStart, &initialChild, sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    printf(\"Starting adaptive mesh refinement...\\n\");\n",
    "    printf(\"Initial cells: %d\\n\", INITIAL_CELLS);\n",
    "    printf(\"Error threshold: %.2f\\n\", ERROR_THRESHOLD);\n",
    "    printf(\"Max depth: %d\\n\\n\", MAX_DEPTH);\n",
    "    \n",
    "    // Start adaptive refinement\n",
    "    adaptiveMesh<<<1, INITIAL_CELLS>>>(d_cells, INITIAL_CELLS, d_childStart);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Check for errors\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    int totalCells;\n",
    "    cudaMemcpy(&totalCells, d_childStart, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Total cells after refinement: %d\\n\", totalCells);\n",
    "    printf(\"Cells created by refinement: %d\\n\", totalCells - INITIAL_CELLS);\n",
    "    \n",
    "    cudaFree(d_cells);\n",
    "    cudaFree(d_childStart);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd75827",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -o adaptive_mesh adaptive_mesh.cu -lcudadevrt && ./adaptive_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7900d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Considerations\n",
    "\n",
    "### Overhead and Optimization\n",
    "\n",
    "```\n",
    "Dynamic Parallelism Overhead:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "Each child launch has overhead:\n",
    "â€¢ ~5-20 Î¼s per launch (varies)\n",
    "â€¢ Memory for child kernel state\n",
    "â€¢ Device runtime overhead\n",
    "\n",
    "Optimization Strategies:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. Batch Work\n",
    "   âŒ Launch child for each element\n",
    "   âœ… Launch child for chunk of elements\n",
    "\n",
    "2. Limit Depth\n",
    "   âŒ Recurse until single element\n",
    "   âœ… Switch to sequential at threshold\n",
    "\n",
    "3. Use Streams\n",
    "   âŒ Serial child launches\n",
    "   âœ… Parallel with non-blocking streams\n",
    "\n",
    "4. Consider Alternatives\n",
    "   â€¢ Cooperative groups for some cases\n",
    "   â€¢ Flattened iteration\n",
    "   â€¢ Work queues\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ When NOT to Use Dynamic Parallelism (Primary)\n",
    "\n",
    "```cpp\n",
    "// BAD: Simple vector add - no need for DP!\n",
    "__global__ void vectorAddDP(float* a, float* b, float* c, int n) {\n",
    "    int tid = threadIdx.x;\n",
    "    // DON'T DO THIS!\n",
    "    vectorAddChild<<<1, 1>>>(a, b, c, tid);\n",
    "}\n",
    "\n",
    "// GOOD: Just use regular parallelism\n",
    "__global__ void vectorAdd(float* a, float* b, float* c, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) c[tid] = a[tid] + b[tid];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c49956",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4388c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nested_kernels_exercises.cu\n",
    "// nested_kernels_exercises.cu - Nested Kernel Pattern Exercises\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "               cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 1: Parallel Merge Sort with DP\n",
    "// ============================================\n",
    "#define MIN_SIZE 32  // Switch to insertion sort\n",
    "\n",
    "__device__ void insertionSort(int* arr, int left, int right) {\n",
    "    for (int i = left + 1; i <= right; i++) {\n",
    "        int key = arr[i];\n",
    "        int j = i - 1;\n",
    "        while (j >= left && arr[j] > key) {\n",
    "            arr[j + 1] = arr[j];\n",
    "            j--;\n",
    "        }\n",
    "        arr[j + 1] = key;\n",
    "    }\n",
    "}\n",
    "\n",
    "__device__ void merge(int* arr, int left, int mid, int right, int* temp) {\n",
    "    int i = left, j = mid + 1, k = left;\n",
    "    \n",
    "    while (i <= mid && j <= right) {\n",
    "        if (arr[i] <= arr[j]) {\n",
    "            temp[k++] = arr[i++];\n",
    "        } else {\n",
    "            temp[k++] = arr[j++];\n",
    "        }\n",
    "    }\n",
    "    while (i <= mid) temp[k++] = arr[i++];\n",
    "    while (j <= right) temp[k++] = arr[j++];\n",
    "    \n",
    "    for (int l = left; l <= right; l++) {\n",
    "        arr[l] = temp[l];\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void mergeSortKernel(int* arr, int* temp, int left, int right, int depth);\n",
    "\n",
    "__global__ void mergeSortKernel(int* arr, int* temp, int left, int right, int depth) {\n",
    "    if (right - left < MIN_SIZE) {\n",
    "        insertionSort(arr, left, right);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    int mid = left + (right - left) / 2;\n",
    "    \n",
    "    if (depth > 0) {\n",
    "        cudaStream_t s1, s2;\n",
    "        cudaStreamCreateWithFlags(&s1, cudaStreamNonBlocking);\n",
    "        cudaStreamCreateWithFlags(&s2, cudaStreamNonBlocking);\n",
    "        \n",
    "        // Launch child kernels for left and right halves\n",
    "        mergeSortKernel<<<1, 1, 0, s1>>>(arr, temp, left, mid, depth - 1);\n",
    "        mergeSortKernel<<<1, 1, 0, s2>>>(arr, temp, mid + 1, right, depth - 1);\n",
    "        \n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        cudaStreamDestroy(s1);\n",
    "        cudaStreamDestroy(s2);\n",
    "    } else {\n",
    "        // Sequential recursion at max depth\n",
    "        insertionSort(arr, left, mid);\n",
    "        insertionSort(arr, mid + 1, right);\n",
    "    }\n",
    "    \n",
    "    merge(arr, left, mid, right, temp);\n",
    "}\n",
    "\n",
    "void testMergeSort() {\n",
    "    printf(\"\\n=== Exercise 1: Parallel Merge Sort ===\\n\");\n",
    "    \n",
    "    const int N = 1024;\n",
    "    int* h_arr = (int*)malloc(N * sizeof(int));\n",
    "    \n",
    "    // Initialize with random values\n",
    "    srand(42);\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_arr[i] = rand() % 10000;\n",
    "    }\n",
    "    \n",
    "    int *d_arr, *d_temp;\n",
    "    CHECK_CUDA(cudaMalloc(&d_arr, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_temp, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_arr, h_arr, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    printf(\"Sorting %d elements with nested merge sort...\\n\", N);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    mergeSortKernel<<<1, 1>>>(d_arr, d_temp, 0, N - 1, 4);  // depth = 4\n",
    "    cudaEventRecord(stop);\n",
    "    \n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_arr, d_arr, N * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Verify sorted\n",
    "    bool sorted = true;\n",
    "    for (int i = 1; i < N; i++) {\n",
    "        if (h_arr[i] < h_arr[i-1]) {\n",
    "            sorted = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"Result: %s\\n\", sorted ? \"SORTED âœ“\" : \"NOT SORTED âœ—\");\n",
    "    printf(\"Time: %.3f ms\\n\", ms);\n",
    "    printf(\"First 10: \");\n",
    "    for (int i = 0; i < 10; i++) printf(\"%d \", h_arr[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaFree(d_arr);\n",
    "    cudaFree(d_temp);\n",
    "    free(h_arr);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 2: Adaptive Quadtree Construction\n",
    "// ============================================\n",
    "struct Point { float x, y; };\n",
    "struct BBox { float xmin, ymin, xmax, ymax; };\n",
    "\n",
    "#define MAX_POINTS_PER_NODE 4\n",
    "#define MAX_DEPTH 6\n",
    "\n",
    "__device__ bool inBox(Point p, BBox box) {\n",
    "    return p.x >= box.xmin && p.x < box.xmax &&\n",
    "           p.y >= box.ymin && p.y < box.ymax;\n",
    "}\n",
    "\n",
    "__global__ void quadtreeKernel(Point* points, int* indices, int n,\n",
    "                                BBox box, int depth, int* nodeCount) {\n",
    "    if (n <= MAX_POINTS_PER_NODE || depth >= MAX_DEPTH) {\n",
    "        atomicAdd(nodeCount, 1);  // Count leaf node\n",
    "        if (threadIdx.x == 0) {\n",
    "            printf(\"Leaf at depth %d: %d points in [%.1f,%.1f]-[%.1f,%.1f]\\n\",\n",
    "                   depth, n, box.xmin, box.ymin, box.xmax, box.ymax);\n",
    "        }\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    float midX = (box.xmin + box.xmax) / 2;\n",
    "    float midY = (box.ymin + box.ymax) / 2;\n",
    "    \n",
    "    // Count points in each quadrant\n",
    "    int counts[4] = {0, 0, 0, 0};\n",
    "    BBox quadrants[4] = {\n",
    "        {box.xmin, box.ymin, midX, midY},       // SW\n",
    "        {midX, box.ymin, box.xmax, midY},       // SE\n",
    "        {box.xmin, midY, midX, box.ymax},       // NW\n",
    "        {midX, midY, box.xmax, box.ymax}        // NE\n",
    "    };\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        Point p = points[indices[i]];\n",
    "        for (int q = 0; q < 4; q++) {\n",
    "            if (inBox(p, quadrants[q])) {\n",
    "                counts[q]++;\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Launch child kernels for non-empty quadrants\n",
    "    for (int q = 0; q < 4; q++) {\n",
    "        if (counts[q] > 0) {\n",
    "            // Allocate child indices (simplified - in production use proper allocation)\n",
    "            quadtreeKernel<<<1, 1>>>(points, indices, counts[q],\n",
    "                                      quadrants[q], depth + 1, nodeCount);\n",
    "        }\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "void testQuadtree() {\n",
    "    printf(\"\\n=== Exercise 2: Adaptive Quadtree ===\\n\");\n",
    "    \n",
    "    const int N = 64;\n",
    "    Point* h_points = (Point*)malloc(N * sizeof(Point));\n",
    "    int* h_indices = (int*)malloc(N * sizeof(int));\n",
    "    \n",
    "    // Create clustered point distribution\n",
    "    srand(42);\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        // Cluster points in different regions\n",
    "        int cluster = i % 4;\n",
    "        float cx = (cluster % 2) * 0.5f + 0.25f;\n",
    "        float cy = (cluster / 2) * 0.5f + 0.25f;\n",
    "        h_points[i].x = cx + (rand() % 100) / 1000.0f;\n",
    "        h_points[i].y = cy + (rand() % 100) / 1000.0f;\n",
    "        h_indices[i] = i;\n",
    "    }\n",
    "    \n",
    "    Point *d_points;\n",
    "    int *d_indices, *d_nodeCount;\n",
    "    CHECK_CUDA(cudaMalloc(&d_points, N * sizeof(Point)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_indices, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_nodeCount, sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_points, h_points, N * sizeof(Point), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_indices, h_indices, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_nodeCount, 0, sizeof(int)));\n",
    "    \n",
    "    BBox rootBox = {0.0f, 0.0f, 1.0f, 1.0f};\n",
    "    \n",
    "    printf(\"Building quadtree for %d points...\\n\\n\", N);\n",
    "    quadtreeKernel<<<1, 1>>>(d_points, d_indices, N, rootBox, 0, d_nodeCount);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    int nodeCount;\n",
    "    CHECK_CUDA(cudaMemcpy(&nodeCount, d_nodeCount, sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    printf(\"\\nTotal leaf nodes created: %d\\n\", nodeCount);\n",
    "    \n",
    "    cudaFree(d_points);\n",
    "    cudaFree(d_indices);\n",
    "    cudaFree(d_nodeCount);\n",
    "    free(h_points);\n",
    "    free(h_indices);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 3: Adaptive Mandelbrot Refinement\n",
    "// ============================================\n",
    "__device__ int mandelbrot(float cx, float cy, int maxIter) {\n",
    "    float zx = 0, zy = 0;\n",
    "    int iter = 0;\n",
    "    while (zx*zx + zy*zy < 4.0f && iter < maxIter) {\n",
    "        float tmp = zx*zx - zy*zy + cx;\n",
    "        zy = 2*zx*zy + cy;\n",
    "        zx = tmp;\n",
    "        iter++;\n",
    "    }\n",
    "    return iter;\n",
    "}\n",
    "\n",
    "#define REFINE_THRESHOLD 0.3f  // Variance threshold for refinement\n",
    "#define MIN_CELL_SIZE 4\n",
    "\n",
    "__global__ void mandelbrotRefineKernel(int* output, int width, int height,\n",
    "                                        int x, int y, int cellSize,\n",
    "                                        float xmin, float xmax,\n",
    "                                        float ymin, float ymax,\n",
    "                                        int maxIter, int depth) {\n",
    "    if (cellSize <= MIN_CELL_SIZE || depth > 4) {\n",
    "        // Compute all pixels in this cell\n",
    "        for (int py = y; py < y + cellSize && py < height; py++) {\n",
    "            for (int px = x; px < x + cellSize && px < width; px++) {\n",
    "                float cx = xmin + (xmax - xmin) * px / width;\n",
    "                float cy = ymin + (ymax - ymin) * py / height;\n",
    "                output[py * width + px] = mandelbrot(cx, cy, maxIter);\n",
    "            }\n",
    "        }\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Sample corners and center\n",
    "    float corners[5][2] = {\n",
    "        {(float)x, (float)y},\n",
    "        {(float)(x + cellSize - 1), (float)y},\n",
    "        {(float)x, (float)(y + cellSize - 1)},\n",
    "        {(float)(x + cellSize - 1), (float)(y + cellSize - 1)},\n",
    "        {(float)(x + cellSize/2), (float)(y + cellSize/2)}\n",
    "    };\n",
    "    \n",
    "    int values[5];\n",
    "    float sum = 0, sumSq = 0;\n",
    "    for (int i = 0; i < 5; i++) {\n",
    "        float cx = xmin + (xmax - xmin) * corners[i][0] / width;\n",
    "        float cy = ymin + (ymax - ymin) * corners[i][1] / height;\n",
    "        values[i] = mandelbrot(cx, cy, maxIter);\n",
    "        float v = (float)values[i] / maxIter;\n",
    "        sum += v;\n",
    "        sumSq += v * v;\n",
    "    }\n",
    "    \n",
    "    float mean = sum / 5;\n",
    "    float variance = sumSq / 5 - mean * mean;\n",
    "    \n",
    "    if (variance > REFINE_THRESHOLD * REFINE_THRESHOLD) {\n",
    "        // High variance - subdivide\n",
    "        int half = cellSize / 2;\n",
    "        cudaStream_t s1, s2, s3, s4;\n",
    "        cudaStreamCreateWithFlags(&s1, cudaStreamNonBlocking);\n",
    "        cudaStreamCreateWithFlags(&s2, cudaStreamNonBlocking);\n",
    "        cudaStreamCreateWithFlags(&s3, cudaStreamNonBlocking);\n",
    "        cudaStreamCreateWithFlags(&s4, cudaStreamNonBlocking);\n",
    "        \n",
    "        mandelbrotRefineKernel<<<1, 1, 0, s1>>>(output, width, height,\n",
    "            x, y, half, xmin, xmax, ymin, ymax, maxIter, depth + 1);\n",
    "        mandelbrotRefineKernel<<<1, 1, 0, s2>>>(output, width, height,\n",
    "            x + half, y, half, xmin, xmax, ymin, ymax, maxIter, depth + 1);\n",
    "        mandelbrotRefineKernel<<<1, 1, 0, s3>>>(output, width, height,\n",
    "            x, y + half, half, xmin, xmax, ymin, ymax, maxIter, depth + 1);\n",
    "        mandelbrotRefineKernel<<<1, 1, 0, s4>>>(output, width, height,\n",
    "            x + half, y + half, half, xmin, xmax, ymin, ymax, maxIter, depth + 1);\n",
    "        \n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        cudaStreamDestroy(s1);\n",
    "        cudaStreamDestroy(s2);\n",
    "        cudaStreamDestroy(s3);\n",
    "        cudaStreamDestroy(s4);\n",
    "    } else {\n",
    "        // Low variance - fill with average value\n",
    "        int avg = (int)(mean * maxIter);\n",
    "        for (int py = y; py < y + cellSize && py < height; py++) {\n",
    "            for (int px = x; px < x + cellSize && px < width; px++) {\n",
    "                output[py * width + px] = avg;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void testMandelbrot() {\n",
    "    printf(\"\\n=== Exercise 3: Adaptive Mandelbrot ===\\n\");\n",
    "    \n",
    "    const int W = 128, H = 128;\n",
    "    const int maxIter = 256;\n",
    "    \n",
    "    int* d_output;\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, W * H * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_output, 0, W * H * sizeof(int)));\n",
    "    \n",
    "    printf(\"Rendering %dx%d Mandelbrot with adaptive refinement...\\n\", W, H);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    \n",
    "    // Launch for 4 initial quadrants\n",
    "    int half = W / 2;\n",
    "    cudaStream_t streams[4];\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "        cudaStreamCreate(&streams[i]);\n",
    "    }\n",
    "    \n",
    "    mandelbrotRefineKernel<<<1, 1, 0, streams[0]>>>(d_output, W, H, 0, 0, half,\n",
    "        -2.0f, 1.0f, -1.5f, 1.5f, maxIter, 0);\n",
    "    mandelbrotRefineKernel<<<1, 1, 0, streams[1]>>>(d_output, W, H, half, 0, half,\n",
    "        -2.0f, 1.0f, -1.5f, 1.5f, maxIter, 0);\n",
    "    mandelbrotRefineKernel<<<1, 1, 0, streams[2]>>>(d_output, W, H, 0, half, half,\n",
    "        -2.0f, 1.0f, -1.5f, 1.5f, maxIter, 0);\n",
    "    mandelbrotRefineKernel<<<1, 1, 0, streams[3]>>>(d_output, W, H, half, half, half,\n",
    "        -2.0f, 1.0f, -1.5f, 1.5f, maxIter, 0);\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    int* h_output = (int*)malloc(W * H * sizeof(int));\n",
    "    CHECK_CUDA(cudaMemcpy(h_output, d_output, W * H * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Count non-zero pixels (computed regions)\n",
    "    int computed = 0;\n",
    "    for (int i = 0; i < W * H; i++) {\n",
    "        if (h_output[i] > 0) computed++;\n",
    "    }\n",
    "    \n",
    "    printf(\"Render time: %.3f ms\\n\", ms);\n",
    "    printf(\"Computed pixels: %d/%d (%.1f%%)\\n\", \n",
    "           computed, W * H, 100.0f * computed / (W * H));\n",
    "    \n",
    "    // Show small ASCII visualization\n",
    "    printf(\"\\nAscii preview (16x8):\\n\");\n",
    "    const char* chars = \" .:-=+*#%@\";\n",
    "    for (int y = 0; y < H; y += H/8) {\n",
    "        for (int x = 0; x < W; x += W/16) {\n",
    "            int val = h_output[y * W + x];\n",
    "            int idx = (val * 9) / maxIter;\n",
    "            printf(\"%c\", chars[idx < 10 ? idx : 9]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < 4; i++) cudaStreamDestroy(streams[i]);\n",
    "    cudaFree(d_output);\n",
    "    free(h_output);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Main\n",
    "// ============================================\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘         NESTED KERNEL PATTERNS - EXERCISES                    â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    int device;\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDevice(&device);\n",
    "    cudaGetDeviceProperties(&prop, device);\n",
    "    printf(\"\\nDevice: %s (Compute %d.%d)\\n\", prop.name, prop.major, prop.minor);\n",
    "    \n",
    "    testMergeSort();\n",
    "    testQuadtree();\n",
    "    testMandelbrot();\n",
    "    \n",
    "    printf(\"\\nâœ“ All exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -lcudadevrt -o nested_kernels_exercises nested_kernels_exercises.cu && ./nested_kernels_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5e4a3",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Merge Sort\n",
    "Implement parallel merge sort using dynamic parallelism.\n",
    "\n",
    "### Exercise 2: Quadtree Construction\n",
    "Build a quadtree for 2D points using adaptive subdivision.\n",
    "\n",
    "### Exercise 3: Fractal Rendering\n",
    "Use DP to adaptively refine Mandelbrot set regions with high detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee127c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚          NESTED KERNELS - MATRYOSHKA PATTERNS           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  ğŸª† Matryoshka Analogy:                                 â”‚\n",
    "â”‚  â€¢ Each kernel is a complete doll containing others     â”‚\n",
    "â”‚  â€¢ Keep opening until you hit the solid base case       â”‚\n",
    "â”‚  â€¢ Each level is independent and self-contained         â”‚\n",
    "â”‚  â€¢ Don't nest too deep (24 levels max, 16 recommended)  â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Design Patterns:                                       â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  1. Base Case (Solid Doll):                             â”‚\n",
    "â”‚     if (size < MIN_SIZE) {                              â”‚\n",
    "â”‚         insertionSort();  // No more nesting!           â”‚\n",
    "â”‚         return;                                         â”‚\n",
    "â”‚     }                                                   â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  2. Depth Limit:                                        â”‚\n",
    "â”‚     if (depth >= MAX_DEPTH) {                           â”‚\n",
    "â”‚         serialProcess();  // Stop recursing             â”‚\n",
    "â”‚         return;                                         â”‚\n",
    "â”‚     }                                                   â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  3. Parallel Siblings:                                  â”‚\n",
    "â”‚     cudaStream_t left, right;                           â”‚\n",
    "â”‚     kernel<<<g,b,0,left>>>();   // Parallel!            â”‚\n",
    "â”‚     kernel<<<g,b,0,right>>>();  // Parallel!            â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Good Use Cases:                                        â”‚\n",
    "â”‚  âœ“ Recursive algorithms (sort, tree traversal)          â”‚\n",
    "â”‚  âœ“ Adaptive refinement (fractals, mesh)                 â”‚\n",
    "â”‚  âœ“ Data-dependent parallelism (sparse)                  â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Avoid When:                                            â”‚\n",
    "â”‚  âœ— Regular parallelism works fine                       â”‚\n",
    "â”‚  âœ— Work is uniform/predictable                          â”‚\n",
    "â”‚  âœ— Launch overhead dominates compute                    â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Memory: Only global memory shared between dolls!       â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Week 11 Complete!\n",
    "\n",
    "**Congratulations!** You've mastered the advanced cooperative programming patterns:\n",
    "\n",
    "| Day | Topic | Analogy | Key Skill |\n",
    "|-----|-------|---------|-----------|\n",
    "| 1 | Cooperative Groups | ğŸ€ Basketball team plays | Thread coordination at any level |\n",
    "| 2 | Grid Sync | ğŸŸï¸ Stadium wave | All-blocks synchronization |\n",
    "| 3 | Dynamic Parallelism | ğŸ‘” Manager delegation | Kernels launching kernels |\n",
    "| 4 | Nested Kernels | ğŸª† Matryoshka dolls | Recursive parallel patterns |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— What's Next?\n",
    "\n",
    "**Week 12: Multi-GPU & Advanced Topics** - You've mastered single-GPU cooperative patterns. Now it's time to scale to MULTIPLE GPUs!\n",
    "\n",
    "You'll learn:\n",
    "- Multi-GPU programming with peer-to-peer access\n",
    "- CUDA streams for maximum concurrency\n",
    "- NVTX profiling for complex applications\n",
    "- Advanced optimization techniques\n",
    "\n",
    "*From matryoshka dolls to a whole doll factory running in parallel!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
