{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa72fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"‚ö†Ô∏è  Nested kernels require CUDA C++ with -rdc=true!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f6e9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Recursive Patterns\n",
    "\n",
    "### üî∑ CUDA C++ Parallel Quicksort (Primary)\n",
    "\n",
    "This example demonstrates recursive quicksort using dynamic parallelism:\n",
    "- Child kernels are launched from within device code\n",
    "- Uses non-blocking streams for parallel left/right partitioning\n",
    "- Falls back to insertion sort for small arrays (MIN_SIZE threshold)\n",
    "- Limits recursion depth (MAX_DEPTH) to prevent stack overflow\n",
    "\n",
    "**Compile with:** `nvcc -arch=sm_75 -rdc=true -o quicksort quicksort_dp.cu -lcudadevrt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile quicksort_dp.cu\n",
    "// quicksort_dp.cu - Recursive quicksort with dynamic parallelism\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define MAX_DEPTH 16\n",
    "#define MIN_SIZE 32  // Switch to sequential sort\n",
    "\n",
    "// ============================================\n",
    "// Sequential Sort for Small Arrays\n",
    "// ============================================\n",
    "__device__ void insertionSort(int* data, int left, int right) {\n",
    "    for (int i = left + 1; i <= right; i++) {\n",
    "        int key = data[i];\n",
    "        int j = i - 1;\n",
    "        while (j >= left && data[j] > key) {\n",
    "            data[j + 1] = data[j];\n",
    "            j--;\n",
    "        }\n",
    "        data[j + 1] = key;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Partition\n",
    "// ============================================\n",
    "__device__ int partition(int* data, int left, int right) {\n",
    "    int pivot = data[right];\n",
    "    int i = left - 1;\n",
    "    \n",
    "    for (int j = left; j < right; j++) {\n",
    "        if (data[j] <= pivot) {\n",
    "            i++;\n",
    "            int temp = data[i];\n",
    "            data[i] = data[j];\n",
    "            data[j] = temp;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    int temp = data[i + 1];\n",
    "    data[i + 1] = data[right];\n",
    "    data[right] = temp;\n",
    "    \n",
    "    return i + 1;\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Recursive Quicksort Kernel\n",
    "// ============================================\n",
    "__global__ void quicksortKernel(int* data, int left, int right, int depth) {\n",
    "    // Base case: small array or max depth\n",
    "    if (right - left < MIN_SIZE || depth >= MAX_DEPTH) {\n",
    "        insertionSort(data, left, right);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Partition\n",
    "    int pivotIdx = partition(data, left, right);\n",
    "    \n",
    "    // Launch child kernels for left and right partitions\n",
    "    cudaStream_t s1, s2;\n",
    "    cudaStreamCreateWithFlags(&s1, cudaStreamNonBlocking);\n",
    "    cudaStreamCreateWithFlags(&s2, cudaStreamNonBlocking);\n",
    "    \n",
    "    // Left partition\n",
    "    if (left < pivotIdx - 1) {\n",
    "        quicksortKernel<<<1, 1, 0, s1>>>(data, left, pivotIdx - 1, depth + 1);\n",
    "    }\n",
    "    \n",
    "    // Right partition\n",
    "    if (pivotIdx + 1 < right) {\n",
    "        quicksortKernel<<<1, 1, 0, s2>>>(data, pivotIdx + 1, right, depth + 1);\n",
    "    }\n",
    "    \n",
    "    // Wait for children\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaStreamDestroy(s1);\n",
    "    cudaStreamDestroy(s2);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Host Entry Point\n",
    "// ============================================\n",
    "__global__ void quicksortEntry(int* data, int n) {\n",
    "    quicksortKernel<<<1, 1>>>(data, 0, n - 1, 0);\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    int* h_data = new int[N];\n",
    "    int* d_data;\n",
    "    \n",
    "    // Random data\n",
    "    srand(42);\n",
    "    for (int i = 0; i < N; i++) h_data[i] = rand() % 10000;\n",
    "    \n",
    "    printf(\"First 10 elements before: \");\n",
    "    for (int i = 0; i < 10; i++) printf(\"%d \", h_data[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaMalloc(&d_data, N * sizeof(int));\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Sort\n",
    "    quicksortEntry<<<1, 1>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Check for errors\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    // Verify\n",
    "    cudaMemcpy(h_data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"First 10 elements after:  \");\n",
    "    for (int i = 0; i < 10; i++) printf(\"%d \", h_data[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    bool sorted = true;\n",
    "    for (int i = 1; i < N; i++) {\n",
    "        if (h_data[i] < h_data[i-1]) {\n",
    "            sorted = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Array sorted: %s\\n\", sorted ? \"YES\" : \"NO\");\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07514c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -o quicksort_dp quicksort_dp.cu -lcudadevrt && ./quicksort_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec25f86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Tree/Graph Traversal\n",
    "\n",
    "### üî∑ CUDA C++ Parallel Tree Processing (Primary)\n",
    "\n",
    "This example demonstrates parallel tree traversal using dynamic parallelism:\n",
    "- Processes a binary tree structure stored in device memory\n",
    "- Recursively launches child kernels for left and right subtrees\n",
    "- Uses atomicAdd to accumulate values across all nodes\n",
    "- Depth guard prevents infinite recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea909a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tree_traversal.cu\n",
    "// tree_traversal.cu - Parallel tree processing\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "struct TreeNode {\n",
    "    float value;\n",
    "    int leftChild;   // Index, -1 if none\n",
    "    int rightChild;  // Index, -1 if none\n",
    "};\n",
    "\n",
    "// ============================================\n",
    "// Process Node and Recurse to Children\n",
    "// ============================================\n",
    "__global__ void processTree(TreeNode* tree, float* results, int nodeIdx, int depth) {\n",
    "    if (nodeIdx < 0 || depth > 20) return;  // Guard\n",
    "    \n",
    "    TreeNode* node = &tree[nodeIdx];\n",
    "    \n",
    "    // Process this node (accumulate value)\n",
    "    atomicAdd(&results[0], node->value);\n",
    "    \n",
    "    // Launch children in parallel\n",
    "    if (node->leftChild >= 0 && node->rightChild >= 0) {\n",
    "        // Both children exist - launch in parallel\n",
    "        cudaStream_t s1, s2;\n",
    "        cudaStreamCreateWithFlags(&s1, cudaStreamNonBlocking);\n",
    "        cudaStreamCreateWithFlags(&s2, cudaStreamNonBlocking);\n",
    "        \n",
    "        processTree<<<1, 1, 0, s1>>>(tree, results, node->leftChild, depth + 1);\n",
    "        processTree<<<1, 1, 0, s2>>>(tree, results, node->rightChild, depth + 1);\n",
    "        \n",
    "        cudaDeviceSynchronize();\n",
    "        cudaStreamDestroy(s1);\n",
    "        cudaStreamDestroy(s2);\n",
    "    } else {\n",
    "        // One or no children\n",
    "        if (node->leftChild >= 0) {\n",
    "            processTree<<<1, 1>>>(tree, results, node->leftChild, depth + 1);\n",
    "            cudaDeviceSynchronize();\n",
    "        }\n",
    "        if (node->rightChild >= 0) {\n",
    "            processTree<<<1, 1>>>(tree, results, node->rightChild, depth + 1);\n",
    "            cudaDeviceSynchronize();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Build simple tree:\n",
    "    //       0(10)\n",
    "    //      /    \\\n",
    "    //    1(20)  2(30)\n",
    "    //    /  \\\n",
    "    //  3(5) 4(15)\n",
    "    \n",
    "    TreeNode h_tree[5];\n",
    "    h_tree[0] = {10.0f, 1, 2};    // Root\n",
    "    h_tree[1] = {20.0f, 3, 4};    // Left child\n",
    "    h_tree[2] = {30.0f, -1, -1};  // Right child (leaf)\n",
    "    h_tree[3] = {5.0f, -1, -1};   // Leaf\n",
    "    h_tree[4] = {15.0f, -1, -1};  // Leaf\n",
    "    \n",
    "    printf(\"Tree structure:\\n\");\n",
    "    printf(\"       0(10)\\n\");\n",
    "    printf(\"      /    \\\\\\n\");\n",
    "    printf(\"    1(20)  2(30)\\n\");\n",
    "    printf(\"    /  \\\\\\n\");\n",
    "    printf(\"  3(5) 4(15)\\n\\n\");\n",
    "    \n",
    "    TreeNode* d_tree;\n",
    "    float* d_result;\n",
    "    \n",
    "    cudaMalloc(&d_tree, 5 * sizeof(TreeNode));\n",
    "    cudaMalloc(&d_result, sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_tree, h_tree, 5 * sizeof(TreeNode), cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_result, 0, sizeof(float));\n",
    "    \n",
    "    // Process tree starting at root\n",
    "    processTree<<<1, 1>>>(d_tree, d_result, 0, 0);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Check for errors\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    float sum;\n",
    "    cudaMemcpy(&sum, d_result, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Tree sum: %.0f (expected 80 = 10+20+30+5+15)\\n\", sum);\n",
    "    \n",
    "    cudaFree(d_tree);\n",
    "    cudaFree(d_result);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5506f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -o tree_traversal tree_traversal.cu -lcudadevrt && ./tree_traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52d94f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Adaptive Mesh Refinement\n",
    "\n",
    "### üî∑ CUDA C++ Conditional Work Spawning (Primary)\n",
    "\n",
    "This example demonstrates adaptive mesh refinement using dynamic parallelism:\n",
    "- Cells are processed and refined based on error estimates\n",
    "- Refinement creates 4 child cells (quad subdivision pattern)\n",
    "- Uses atomicAdd to track dynamic cell allocation\n",
    "- Recurses only when error exceeds threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dccad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile adaptive_mesh.cu\n",
    "// adaptive_mesh.cu - Adaptive refinement pattern\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "\n",
    "struct Cell {\n",
    "    float value;\n",
    "    float error;  // Error estimate\n",
    "    int refined;  // Has been refined\n",
    "};\n",
    "\n",
    "#define ERROR_THRESHOLD 0.1f\n",
    "#define MAX_DEPTH 3\n",
    "\n",
    "// ============================================\n",
    "// Process Cell - Refine if Error Too High\n",
    "// ============================================\n",
    "__global__ void processCell(Cell* cells, int cellIdx, int* childStart, int depth) {\n",
    "    if (depth >= MAX_DEPTH) return;\n",
    "    \n",
    "    Cell* cell = &cells[cellIdx];\n",
    "    \n",
    "    // Compute work (simplified)\n",
    "    cell->value = sinf((float)cellIdx * 0.5f);\n",
    "    cell->error = fabsf(cell->value - 0.5f) * 0.2f;  // Fake error\n",
    "    \n",
    "    // Check if refinement needed\n",
    "    if (cell->error > ERROR_THRESHOLD && depth < MAX_DEPTH) {\n",
    "        cell->refined = 1;\n",
    "        \n",
    "        // Allocate child cells atomically\n",
    "        int childBase = atomicAdd(childStart, 4);  // 4 children (2D quad)\n",
    "        \n",
    "        // Launch refinement for each child\n",
    "        for (int i = 0; i < 4; i++) {\n",
    "            processCell<<<1, 1>>>(cells, childBase + i, childStart, depth + 1);\n",
    "        }\n",
    "        \n",
    "        cudaDeviceSynchronize();\n",
    "    } else {\n",
    "        cell->refined = 0;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Entry Kernel - Start with Coarse Grid\n",
    "// ============================================\n",
    "__global__ void adaptiveMesh(Cell* cells, int initialCells, int* childStart) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (tid < initialCells) {\n",
    "        processCell<<<1, 1>>>(cells, tid, childStart, 0);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int INITIAL_CELLS = 16;\n",
    "    const int MAX_CELLS = 10000;  // Preallocate\n",
    "    \n",
    "    Cell* d_cells;\n",
    "    int* d_childStart;\n",
    "    \n",
    "    cudaMalloc(&d_cells, MAX_CELLS * sizeof(Cell));\n",
    "    cudaMalloc(&d_childStart, sizeof(int));\n",
    "    \n",
    "    int initialChild = INITIAL_CELLS;  // Children start after initial\n",
    "    cudaMemcpy(d_childStart, &initialChild, sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    printf(\"Starting adaptive mesh refinement...\\n\");\n",
    "    printf(\"Initial cells: %d\\n\", INITIAL_CELLS);\n",
    "    printf(\"Error threshold: %.2f\\n\", ERROR_THRESHOLD);\n",
    "    printf(\"Max depth: %d\\n\\n\", MAX_DEPTH);\n",
    "    \n",
    "    // Start adaptive refinement\n",
    "    adaptiveMesh<<<1, INITIAL_CELLS>>>(d_cells, INITIAL_CELLS, d_childStart);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Check for errors\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    int totalCells;\n",
    "    cudaMemcpy(&totalCells, d_childStart, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Total cells after refinement: %d\\n\", totalCells);\n",
    "    printf(\"Cells created by refinement: %d\\n\", totalCells - INITIAL_CELLS);\n",
    "    \n",
    "    cudaFree(d_cells);\n",
    "    cudaFree(d_childStart);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd75827",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -o adaptive_mesh adaptive_mesh.cu -lcudadevrt && ./adaptive_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7900d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Considerations\n",
    "\n",
    "### Overhead and Optimization\n",
    "\n",
    "```\n",
    "Dynamic Parallelism Overhead:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Each child launch has overhead:\n",
    "‚Ä¢ ~5-20 Œºs per launch (varies)\n",
    "‚Ä¢ Memory for child kernel state\n",
    "‚Ä¢ Device runtime overhead\n",
    "\n",
    "Optimization Strategies:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. Batch Work\n",
    "   ‚ùå Launch child for each element\n",
    "   ‚úÖ Launch child for chunk of elements\n",
    "\n",
    "2. Limit Depth\n",
    "   ‚ùå Recurse until single element\n",
    "   ‚úÖ Switch to sequential at threshold\n",
    "\n",
    "3. Use Streams\n",
    "   ‚ùå Serial child launches\n",
    "   ‚úÖ Parallel with non-blocking streams\n",
    "\n",
    "4. Consider Alternatives\n",
    "   ‚Ä¢ Cooperative groups for some cases\n",
    "   ‚Ä¢ Flattened iteration\n",
    "   ‚Ä¢ Work queues\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ When NOT to Use Dynamic Parallelism (Primary)\n",
    "\n",
    "```cpp\n",
    "// BAD: Simple vector add - no need for DP!\n",
    "__global__ void vectorAddDP(float* a, float* b, float* c, int n) {\n",
    "    int tid = threadIdx.x;\n",
    "    // DON'T DO THIS!\n",
    "    vectorAddChild<<<1, 1>>>(a, b, c, tid);\n",
    "}\n",
    "\n",
    "// GOOD: Just use regular parallelism\n",
    "__global__ void vectorAdd(float* a, float* b, float* c, int n) {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid < n) c[tid] = a[tid] + b[tid];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c49956",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Merge Sort\n",
    "Implement parallel merge sort using dynamic parallelism.\n",
    "\n",
    "### Exercise 2: Quadtree Construction\n",
    "Build a quadtree for 2D points using adaptive subdivision.\n",
    "\n",
    "### Exercise 3: Fractal Rendering\n",
    "Use DP to adaptively refine Mandelbrot set regions with high detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee127c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           NESTED KERNEL PATTERNS                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Good Use Cases:                                        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Recursive algorithms (sort, tree traversal)          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Data-dependent parallelism                           ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Adaptive refinement                                  ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Optimization:                                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Use MIN_SIZE threshold for base case                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Limit recursion depth (MAX_DEPTH)                    ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Use non-blocking streams for siblings                ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Batch work in child kernels                          ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Avoid When:                                            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Regular parallelism works                            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Work is uniform/predictable                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Overhead dominates compute                           ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Memory: Only global memory shared between kernels      ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Week 11 Complete! Next: Week 12 - Multi-GPU & Advanced Topics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
