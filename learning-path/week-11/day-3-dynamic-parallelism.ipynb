{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a608f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"âš ï¸  Dynamic Parallelism is a CUDA C++ feature (CC 3.5+)!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a2206",
   "metadata": {},
   "source": [
    "# Day 3: Dynamic Parallelism - Manager Delegation\n",
    "\n",
    "## ğŸ¯ The Hook: The Autonomous Manager\n",
    "\n",
    "**Imagine a company where every decision requires CEO approval.** Employee needs resources? Ask the CEO. Team needs more people? Ask the CEO. Even the simplest sub-task? CEO.\n",
    "\n",
    "That's traditional CUDA: the host (CEO) must launch EVERY kernel. Even if a running kernel knows exactly what work needs to be done next, it has to finish, report back, and wait for the host to launch the next kernel.\n",
    "\n",
    "**Dynamic Parallelism** is like hiring autonomous managers:\n",
    "- The CEO (host) launches a top-level manager (parent kernel)\n",
    "- That manager can hire their OWN sub-teams (child kernels)\n",
    "- Sub-teams can hire THEIR sub-teams (nested children)\n",
    "- All without going back to the CEO!\n",
    "\n",
    "**When is this powerful?**\n",
    "- Recursive algorithms (quicksort, tree traversal)\n",
    "- Work that discovers MORE work (adaptive refinement)\n",
    "- Data-dependent parallelism (sparse matrices)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "\n",
    "1. **Understand** how kernels can launch other kernels from device code\n",
    "2. **Implement** basic parent-child kernel patterns\n",
    "3. **Manage** memory visibility between parent and child kernels\n",
    "4. **Use** `cudaDeviceSynchronize()` for device-side synchronization\n",
    "5. **Recognize** overhead trade-offs of dynamic parallelism\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘” Concept Card: Manager Delegating to Sub-Teams\n",
    "\n",
    "```\n",
    "DYNAMIC PARALLELISM = AUTONOMOUS MANAGEMENT\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "Traditional CUDA:                Dynamic Parallelism:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      CEO         â”‚            â”‚        CEO             â”‚\n",
    "â”‚ (Host Program)   â”‚            â”‚    (Host Program)      â”‚\n",
    "â”‚        â”‚         â”‚            â”‚         â”‚              â”‚\n",
    "â”‚        â–¼         â”‚            â”‚         â–¼              â”‚\n",
    "â”‚   [Task A]       â”‚            â”‚   [Manager Kernel]     â”‚\n",
    "â”‚        â”‚         â”‚            â”‚     /    â”‚    \\        â”‚\n",
    "â”‚  â”€â”€â”€â”€ CEO â”€â”€â”€â”€   â”‚            â”‚    â–¼     â–¼     â–¼       â”‚\n",
    "â”‚        â–¼         â”‚            â”‚  [Sub] [Sub] [Sub]     â”‚\n",
    "â”‚   [Task B]       â”‚            â”‚         â”‚              â”‚\n",
    "â”‚        â”‚         â”‚            â”‚         â–¼              â”‚\n",
    "â”‚  â”€â”€â”€â”€ CEO â”€â”€â”€â”€   â”‚            â”‚   [Sub-sub tasks]      â”‚\n",
    "â”‚        â–¼         â”‚            â”‚                        â”‚\n",
    "â”‚   [Task C]       â”‚            â”‚  No CEO involvement!   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Example: Quicksort\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Parent: \"Partition the array\"             â”‚\n",
    "â”‚         pivot = 42                         â”‚\n",
    "â”‚         left = [smaller], right = [larger] â”‚\n",
    "â”‚                                            â”‚\n",
    "â”‚         // Delegate to sub-managers!       â”‚\n",
    "â”‚         quicksort<<<1,256>>>(left);        â”‚\n",
    "â”‚         quicksort<<<1,256>>>(right);       â”‚\n",
    "â”‚         cudaDeviceSynchronize();           â”‚\n",
    "â”‚                                            â”‚\n",
    "â”‚  Children can spawn THEIR children too!    â”‚\n",
    "â”‚  (Up to 24 levels of nesting)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Memory Rules:\n",
    "â€¢ Global memory: Visible after sync\n",
    "â€¢ Local/Shared: NOT visible to children\n",
    "â€¢ Use __threadfence() for visibility\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is Dynamic Parallelism?\n",
    "\n",
    "```\n",
    "Traditional CUDA:                  Dynamic Parallelism:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Host controls everything           Kernels launch kernels!\n",
    "\n",
    "[Host] â”€â”€> [Kernel A]              [Host] â”€â”€> [Parent Kernel]\n",
    "[Host] â”€â”€> [Kernel B]                            â”‚\n",
    "[Host] â”€â”€> [Kernel C]                            â”œâ”€â”€> [Child 1]\n",
    "                                                 â”œâ”€â”€> [Child 2]\n",
    "                                                 â””â”€â”€> [Child 3]\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- Recursive algorithms (quicksort, tree traversal)\n",
    "- Adaptive mesh refinement\n",
    "- Work that generates more work\n",
    "- Data-dependent parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0ce48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Child Kernel Launch\n",
    "\n",
    "The simplest example: a parent kernel launches child kernels from the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_basic.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// ============================================\n",
    "// Child Kernel - Launched FROM the GPU\n",
    "// ============================================\n",
    "__global__ void childKernel(int parentId) {\n",
    "    int tid = threadIdx.x;\n",
    "    printf(\"    Child of parent %d: thread %d\\n\", parentId, tid);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Parent Kernel - Launches Children\n",
    "// ============================================\n",
    "__global__ void parentKernel() {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    printf(\"Parent thread %d launching child kernel...\\n\", tid);\n",
    "    \n",
    "    // Launch child kernel from GPU!\n",
    "    // Same <<<blocks, threads>>> syntax\n",
    "    childKernel<<<1, 4>>>(tid);\n",
    "    \n",
    "    // Device-side synchronization\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Parent thread %d: child completed\\n\", tid);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Dynamic Parallelism Basic Demo ===\\n\\n\");\n",
    "    printf(\"Host launching parent kernel with 2 threads...\\n\\n\");\n",
    "    \n",
    "    // Host launches parent\n",
    "    parentKernel<<<1, 2>>>();\n",
    "    \n",
    "    // Host-side sync waits for ALL work (parent + children)\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"\\nAll done!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59768da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: -rdc=true enables relocatable device code (required for dynamic parallelism)\n",
    "# Note: -lcudadevrt links the device runtime library\n",
    "!nvcc -rdc=true -lcudadevrt -o dynamic_basic dynamic_basic.cu && ./dynamic_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b52e97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Memory Visibility Between Parent and Child\n",
    "\n",
    "Understanding what memory is visible where is critical:\n",
    "\n",
    "| Memory Type | Parent â†’ Child | Child â†’ Parent |\n",
    "|-------------|----------------|----------------|\n",
    "| **Global** | âœ… After launch | âœ… After sync |\n",
    "| **Local** | âŒ | âŒ |\n",
    "| **Shared** | âŒ | âŒ |\n",
    "| **Constant** | âœ… (set before host launch) | âœ… |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b58f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_memory.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void childKernel(float* data, int idx) {\n",
    "    // Child CAN read parent's global memory writes\n",
    "    float parentValue = data[idx];\n",
    "    printf(\"    Child[%d]: read %.1f from parent\\n\", idx, parentValue);\n",
    "    \n",
    "    // Child writes to global memory\n",
    "    data[idx] = parentValue * 2.0f;\n",
    "    printf(\"    Child[%d]: wrote %.1f back\\n\", idx, data[idx]);\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(float* data) {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Parent writes to global memory\n",
    "    data[tid] = (float)(tid + 1) * 10.0f;\n",
    "    printf(\"Parent %d: wrote %.1f to global memory\\n\", tid, data[tid]);\n",
    "    \n",
    "    // __threadfence() ensures memory writes are visible\n",
    "    // to child kernels BEFORE we launch them\n",
    "    __threadfence();\n",
    "    \n",
    "    // Launch child\n",
    "    childKernel<<<1, 1>>>(data, tid);\n",
    "    \n",
    "    // Must sync to see child's writes!\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Now parent can see child's writes\n",
    "    printf(\"Parent %d: after sync, data = %.1f\\n\", tid, data[tid]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Dynamic Parallelism Memory Visibility ===\\n\\n\");\n",
    "    \n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, 4 * sizeof(float));\n",
    "    cudaMemset(d_data, 0, 4 * sizeof(float));\n",
    "    \n",
    "    parentKernel<<<1, 4>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Verify from host\n",
    "    float h_data[4];\n",
    "    cudaMemcpy(h_data, d_data, 4 * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"\\nHost verification:\\n\");\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "        float expected = (float)(i + 1) * 10.0f * 2.0f;  // parent wrote * child doubled\n",
    "        printf(\"  data[%d] = %.1f (expected: %.1f)\\n\", i, h_data[i], expected);\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o dynamic_memory dynamic_memory.cu && ./dynamic_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6c882",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Device-Side Streams\n",
    "\n",
    "Kernels on the GPU can create and use streams, just like on the host. This enables concurrent child kernel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315dcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_streams.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void childA(int* counter, int id) {\n",
    "    atomicAdd(counter, 1);\n",
    "    printf(\"    ChildA[%d] executed, counter now %d\\n\", id, *counter);\n",
    "}\n",
    "\n",
    "__global__ void childB(int* counter, int id) {\n",
    "    atomicAdd(counter, 10);\n",
    "    printf(\"    ChildB[%d] executed, counter now %d\\n\", id, *counter);\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(int* counter) {\n",
    "    printf(\"=== Default Stream (Sequential) ===\\n\");\n",
    "    \n",
    "    // Default stream: children execute sequentially\n",
    "    childA<<<1, 1>>>(counter, 0);  // First\n",
    "    childB<<<1, 1>>>(counter, 0);  // Second (waits for childA)\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"After default stream: counter = %d\\n\\n\", *counter);\n",
    "    \n",
    "    printf(\"=== Named Streams (Concurrent) ===\\n\");\n",
    "    \n",
    "    // Create device-side streams\n",
    "    cudaStream_t stream1, stream2;\n",
    "    cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking);\n",
    "    cudaStreamCreateWithFlags(&stream2, cudaStreamNonBlocking);\n",
    "    \n",
    "    // Launch children on different streams - can run concurrently!\n",
    "    childA<<<1, 1, 0, stream1>>>(counter, 1);\n",
    "    childB<<<1, 1, 0, stream2>>>(counter, 1);\n",
    "    \n",
    "    // Sync each stream\n",
    "    cudaStreamSynchronize(stream1);\n",
    "    cudaStreamSynchronize(stream2);\n",
    "    \n",
    "    printf(\"After named streams: counter = %d\\n\", *counter);\n",
    "    \n",
    "    // Cleanup device-side streams\n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(stream2);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Device-Side Streams Demo ===\\n\\n\");\n",
    "    \n",
    "    int* d_counter;\n",
    "    cudaMalloc(&d_counter, sizeof(int));\n",
    "    cudaMemset(d_counter, 0, sizeof(int));\n",
    "    \n",
    "    parentKernel<<<1, 1>>>(d_counter);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int result;\n",
    "    cudaMemcpy(&result, d_counter, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"\\nFinal counter value: %d (expected: 22 = 1+10+1+10)\\n\", result);\n",
    "    \n",
    "    cudaFree(d_counter);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad968c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o dynamic_streams dynamic_streams.cu && ./dynamic_streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc2fb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Synchronization Patterns\n",
    "\n",
    "There are several ways to synchronize parent with child kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sync_patterns.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void workKernel(float* data, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        data[i] = data[i] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Pattern 1: Explicit sync before reading results\n",
    "__global__ void pattern1_explicit_sync(float* data, int n) {\n",
    "    printf(\"Pattern 1: Explicit sync\\n\");\n",
    "    \n",
    "    workKernel<<<(n + 255) / 256, 256>>>(data, n);\n",
    "    \n",
    "    // REQUIRED before reading child's results\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"  First result: %.1f\\n\", data[0]);\n",
    "}\n",
    "\n",
    "// Pattern 2: Fire and forget (no parent sync)\n",
    "__global__ void pattern2_fire_forget(float* data, int n) {\n",
    "    printf(\"Pattern 2: Fire and forget\\n\");\n",
    "    \n",
    "    workKernel<<<(n + 255) / 256, 256>>>(data, n);\n",
    "    \n",
    "    // NO sync - parent returns immediately\n",
    "    // Child continues running\n",
    "    // Result visible to HOST after host's cudaDeviceSynchronize()\n",
    "    \n",
    "    printf(\"  Parent returning (child may still be running)\\n\");\n",
    "}\n",
    "\n",
    "// Pattern 3: Stream-based sync (more granular control)\n",
    "__global__ void pattern3_stream_sync(float* data, int n) {\n",
    "    printf(\"Pattern 3: Stream-based sync\\n\");\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);\n",
    "    \n",
    "    workKernel<<<(n + 255) / 256, 256, 0, stream>>>(data, n);\n",
    "    \n",
    "    // Sync only this stream\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    printf(\"  First result: %.1f\\n\", data[0]);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Synchronization Patterns ===\\n\\n\");\n",
    "    \n",
    "    const int N = 1024;\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Initialize to 1.0\n",
    "    float h_init[N];\n",
    "    for (int i = 0; i < N; i++) h_init[i] = 1.0f;\n",
    "    \n",
    "    // Test Pattern 1\n",
    "    cudaMemcpy(d_data, h_init, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    pattern1_explicit_sync<<<1, 1>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    // Test Pattern 2\n",
    "    cudaMemcpy(d_data, h_init, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    pattern2_fire_forget<<<1, 1>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();  // Host waits for all\n",
    "    float result;\n",
    "    cudaMemcpy(&result, d_data, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"  (Host sees result: %.1f)\\n\\n\", result);\n",
    "    \n",
    "    // Test Pattern 3\n",
    "    cudaMemcpy(d_data, h_init, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    pattern3_stream_sync<<<1, 1>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ec0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o sync_patterns sync_patterns.cu && ./sync_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8db124",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Error Handling in Dynamic Parallelism\n",
    "\n",
    "Device-side error checking works similarly to host-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce816bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_errors.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void simpleChild(float* data) {\n",
    "    data[threadIdx.x] = 1.0f;\n",
    "}\n",
    "\n",
    "__global__ void errorCheckingParent(float* data) {\n",
    "    printf(\"=== Device-Side Error Checking ===\\n\\n\");\n",
    "    \n",
    "    // Attempt 1: Valid launch\n",
    "    printf(\"Attempt 1: Valid launch (1 block, 32 threads)\\n\");\n",
    "    simpleChild<<<1, 32>>>(data);\n",
    "    \n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"  Launch error: %s\\n\", cudaGetErrorString(err));\n",
    "    } else {\n",
    "        printf(\"  Launch successful!\\n\");\n",
    "    }\n",
    "    \n",
    "    err = cudaDeviceSynchronize();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"  Execution error: %s\\n\", cudaGetErrorString(err));\n",
    "    } else {\n",
    "        printf(\"  Execution successful!\\n\");\n",
    "    }\n",
    "    \n",
    "    // Attempt 2: Invalid launch (too many threads)\n",
    "    printf(\"\\nAttempt 2: Invalid launch (1 block, 2048 threads)\\n\");\n",
    "    simpleChild<<<1, 2048>>>(data);  // Exceeds max threads per block!\n",
    "    \n",
    "    err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"  Launch error: %s\\n\", cudaGetErrorString(err));\n",
    "    } else {\n",
    "        printf(\"  Launch seemed OK (checking execution...)\\n\");\n",
    "        err = cudaDeviceSynchronize();\n",
    "        if (err != cudaSuccess) {\n",
    "            printf(\"  Execution error: %s\\n\", cudaGetErrorString(err));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Attempt 3: Another valid launch to show recovery\n",
    "    printf(\"\\nAttempt 3: Recovery - valid launch again\\n\");\n",
    "    simpleChild<<<1, 32>>>(data);\n",
    "    err = cudaDeviceSynchronize();\n",
    "    printf(\"  Result: %s\\n\", err == cudaSuccess ? \"Success!\" : cudaGetErrorString(err));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, 2048 * sizeof(float));\n",
    "    \n",
    "    errorCheckingParent<<<1, 1>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o dynamic_errors dynamic_errors.cu && ./dynamic_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7a26f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Practical Example: Recursive Sum\n",
    "\n",
    "A classic example: recursively sum an array by dividing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile recursive_sum.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define MAX_DEPTH 4\n",
    "\n",
    "// Recursive kernel that sums array segments\n",
    "__global__ void recursiveSum(float* data, float* result, int start, int end, int depth) {\n",
    "    int n = end - start;\n",
    "    \n",
    "    // Base case: small enough to sum directly\n",
    "    if (n <= 32 || depth >= MAX_DEPTH) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = start; i < end; i++) {\n",
    "            sum += data[i];\n",
    "        }\n",
    "        atomicAdd(result, sum);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Recursive case: split and launch children\n",
    "    int mid = start + n / 2;\n",
    "    \n",
    "    // Launch two children for each half\n",
    "    recursiveSum<<<1, 1>>>(data, result, start, mid, depth + 1);\n",
    "    recursiveSum<<<1, 1>>>(data, result, mid, end, depth + 1);\n",
    "    \n",
    "    // Wait for children\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Recursive Sum with Dynamic Parallelism ===\\n\\n\");\n",
    "    \n",
    "    const int N = 1024;\n",
    "    float* d_data;\n",
    "    float* d_result;\n",
    "    \n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMalloc(&d_result, sizeof(float));\n",
    "    \n",
    "    // Initialize: 1, 2, 3, ..., N\n",
    "    float* h_data = (float*)malloc(N * sizeof(float));\n",
    "    float expected = 0.0f;\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_data[i] = (float)(i + 1);\n",
    "        expected += h_data[i];\n",
    "    }\n",
    "    \n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_result, 0, sizeof(float));\n",
    "    \n",
    "    printf(\"Summing 1 + 2 + 3 + ... + %d\\n\", N);\n",
    "    printf(\"Expected: %.0f\\n\\n\", expected);\n",
    "    \n",
    "    // Launch recursive sum\n",
    "    recursiveSum<<<1, 1>>>(d_data, d_result, 0, N, 0);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Get result\n",
    "    float result;\n",
    "    cudaMemcpy(&result, d_result, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Result: %.0f\\n\", result);\n",
    "    printf(\"Match: %s\\n\", (result == expected) ? \"YES\" : \"NO\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "    free(h_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o recursive_sum recursive_sum.cu && ./recursive_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe640c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practical Example: Adaptive Work Generation\n",
    "\n",
    "Only launch child kernels where work is needed (data-dependent parallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile adaptive_work.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Child kernel: process a segment that needs work\n",
    "__global__ void processSegment(float* data, int start, int count, int* processed) {\n",
    "    int i = start + blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < start + count) {\n",
    "        // Simulate processing: square the value\n",
    "        data[i] = data[i] * data[i];\n",
    "        atomicAdd(processed, 1);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Parent: analyze data and launch children only where needed\n",
    "__global__ void adaptiveParent(float* data, int n, float threshold, int* processed) {\n",
    "    int tid = threadIdx.x;\n",
    "    int segments = 8;  // Divide into 8 segments\n",
    "    int segmentSize = n / segments;\n",
    "    \n",
    "    if (tid < segments) {\n",
    "        int start = tid * segmentSize;\n",
    "        int end = (tid == segments - 1) ? n : start + segmentSize;\n",
    "        int count = end - start;\n",
    "        \n",
    "        // Check if this segment needs processing\n",
    "        // (contains values above threshold)\n",
    "        bool needsWork = false;\n",
    "        for (int i = start; i < end && !needsWork; i++) {\n",
    "            if (data[i] > threshold) needsWork = true;\n",
    "        }\n",
    "        \n",
    "        if (needsWork) {\n",
    "            printf(\"Segment %d [%d-%d]: launching child kernel\\n\", tid, start, end - 1);\n",
    "            \n",
    "            int threads = 256;\n",
    "            int blocks = (count + threads - 1) / threads;\n",
    "            processSegment<<<blocks, threads>>>(data, start, count, processed);\n",
    "        } else {\n",
    "            printf(\"Segment %d [%d-%d]: skipping (no work needed)\\n\", tid, start, end - 1);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Sync all children\n",
    "    __syncthreads();\n",
    "    if (tid == 0) {\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Adaptive Work Generation ===\\n\\n\");\n",
    "    \n",
    "    const int N = 8192;\n",
    "    const float THRESHOLD = 0.5f;\n",
    "    \n",
    "    float* d_data;\n",
    "    int* d_processed;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMalloc(&d_processed, sizeof(int));\n",
    "    cudaMemset(d_processed, 0, sizeof(int));\n",
    "    \n",
    "    // Initialize: some segments have values > threshold, some don't\n",
    "    float* h_data = (float*)malloc(N * sizeof(float));\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        // Segments 0,2,4,6 have low values; 1,3,5,7 have high values\n",
    "        int segment = i / (N / 8);\n",
    "        h_data[i] = (segment % 2 == 0) ? 0.3f : 0.7f;\n",
    "    }\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    printf(\"Threshold: %.1f\\n\", THRESHOLD);\n",
    "    printf(\"Only segments with values > threshold will be processed.\\n\\n\");\n",
    "    \n",
    "    adaptiveParent<<<1, 8>>>(d_data, N, THRESHOLD, d_processed);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int processed;\n",
    "    cudaMemcpy(&processed, d_processed, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"\\nElements processed: %d (out of %d)\\n\", processed, N);\n",
    "    printf(\"Expected: %d (half of elements)\\n\", N / 2);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_processed);\n",
    "    free(h_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o adaptive_work adaptive_work.cu && ./adaptive_work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bb244",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Dynamic Parallelism Key Points\n",
    "\n",
    "### Compilation\n",
    "```bash\n",
    "nvcc -rdc=true -lcudadevrt -o program program.cu\n",
    "```\n",
    "\n",
    "### Launch Syntax (Same as Host)\n",
    "```cpp\n",
    "childKernel<<<blocks, threads, sharedMem, stream>>>(args);\n",
    "```\n",
    "\n",
    "### Synchronization\n",
    "```cpp\n",
    "cudaDeviceSynchronize();      // Wait for all children\n",
    "cudaStreamSynchronize(s);     // Wait for specific stream\n",
    "```\n",
    "\n",
    "### Memory Visibility\n",
    "- Global memory: visible after sync\n",
    "- Local/Shared: NOT visible between parent/child\n",
    "- Use `__threadfence()` before launching children\n",
    "\n",
    "### Limits\n",
    "- Maximum nesting depth: 24 levels\n",
    "- Compute Capability: 3.5+\n",
    "- Child kernel launches have overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfe758",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_parallelism_exercises.cu\n",
    "// dynamic_parallelism_exercises.cu - Dynamic Parallelism Exercises\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "               cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 1: Recursive Quicksort with DP\n",
    "// ============================================\n",
    "#define INSERTION_THRESHOLD 32\n",
    "\n",
    "__device__ void insertionSort(int* arr, int left, int right) {\n",
    "    for (int i = left + 1; i <= right; i++) {\n",
    "        int key = arr[i];\n",
    "        int j = i - 1;\n",
    "        while (j >= left && arr[j] > key) {\n",
    "            arr[j + 1] = arr[j];\n",
    "            j--;\n",
    "        }\n",
    "        arr[j + 1] = key;\n",
    "    }\n",
    "}\n",
    "\n",
    "__device__ int partition(int* arr, int left, int right) {\n",
    "    // Median-of-three pivot selection\n",
    "    int mid = left + (right - left) / 2;\n",
    "    if (arr[mid] < arr[left]) {\n",
    "        int tmp = arr[mid]; arr[mid] = arr[left]; arr[left] = tmp;\n",
    "    }\n",
    "    if (arr[right] < arr[left]) {\n",
    "        int tmp = arr[right]; arr[right] = arr[left]; arr[left] = tmp;\n",
    "    }\n",
    "    if (arr[mid] < arr[right]) {\n",
    "        int tmp = arr[mid]; arr[mid] = arr[right]; arr[right] = tmp;\n",
    "    }\n",
    "    int pivot = arr[right];\n",
    "    \n",
    "    int i = left - 1;\n",
    "    for (int j = left; j < right; j++) {\n",
    "        if (arr[j] <= pivot) {\n",
    "            i++;\n",
    "            int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;\n",
    "        }\n",
    "    }\n",
    "    int tmp = arr[i + 1]; arr[i + 1] = arr[right]; arr[right] = tmp;\n",
    "    return i + 1;\n",
    "}\n",
    "\n",
    "__global__ void quicksortKernel(int* arr, int left, int right, int depth);\n",
    "\n",
    "__global__ void quicksortKernel(int* arr, int left, int right, int depth) {\n",
    "    if (right - left < INSERTION_THRESHOLD) {\n",
    "        insertionSort(arr, left, right);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    int pivotIdx = partition(arr, left, right);\n",
    "    \n",
    "    if (depth > 0) {\n",
    "        cudaStream_t s1, s2;\n",
    "        cudaStreamCreateWithFlags(&s1, cudaStreamNonBlocking);\n",
    "        cudaStreamCreateWithFlags(&s2, cudaStreamNonBlocking);\n",
    "        \n",
    "        if (pivotIdx - 1 > left) {\n",
    "            quicksortKernel<<<1, 1, 0, s1>>>(arr, left, pivotIdx - 1, depth - 1);\n",
    "        }\n",
    "        if (pivotIdx + 1 < right) {\n",
    "            quicksortKernel<<<1, 1, 0, s2>>>(arr, pivotIdx + 1, right, depth - 1);\n",
    "        }\n",
    "        \n",
    "        cudaDeviceSynchronize();\n",
    "        cudaStreamDestroy(s1);\n",
    "        cudaStreamDestroy(s2);\n",
    "    } else {\n",
    "        // Bottom out to sequential\n",
    "        if (pivotIdx - 1 > left) insertionSort(arr, left, pivotIdx - 1);\n",
    "        if (pivotIdx + 1 < right) insertionSort(arr, pivotIdx + 1, right);\n",
    "    }\n",
    "}\n",
    "\n",
    "void testQuicksort() {\n",
    "    printf(\"\\n=== Exercise 1: Recursive Quicksort ===\\n\");\n",
    "    \n",
    "    const int N = 4096;\n",
    "    int* h_arr = (int*)malloc(N * sizeof(int));\n",
    "    \n",
    "    srand(42);\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_arr[i] = rand() % 10000;\n",
    "    }\n",
    "    \n",
    "    int* d_arr;\n",
    "    CHECK_CUDA(cudaMalloc(&d_arr, N * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_arr, h_arr, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    printf(\"Sorting %d elements with recursive quicksort...\\n\", N);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    quicksortKernel<<<1, 1>>>(d_arr, 0, N - 1, 8);  // depth=8 for recursion\n",
    "    cudaEventRecord(stop);\n",
    "    \n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_arr, d_arr, N * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Verify\n",
    "    bool sorted = true;\n",
    "    for (int i = 1; i < N; i++) {\n",
    "        if (h_arr[i] < h_arr[i-1]) {\n",
    "            sorted = false;\n",
    "            printf(\"Error at index %d: %d > %d\\n\", i, h_arr[i-1], h_arr[i]);\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"Time: %.3f ms\\n\", ms);\n",
    "    printf(\"Result: %s\\n\", sorted ? \"SORTED âœ“\" : \"NOT SORTED âœ—\");\n",
    "    printf(\"First 10: \");\n",
    "    for (int i = 0; i < 10; i++) printf(\"%d \", h_arr[i]);\n",
    "    printf(\"\\nLast 10: \");\n",
    "    for (int i = N-10; i < N; i++) printf(\"%d \", h_arr[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaFree(d_arr);\n",
    "    free(h_arr);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 2: Tree Traversal with DP\n",
    "// ============================================\n",
    "struct TreeNode {\n",
    "    int value;\n",
    "    int leftChild;   // Index of left child (-1 if none)\n",
    "    int rightChild;  // Index of right child (-1 if none)\n",
    "};\n",
    "\n",
    "__device__ int d_visitOrder[64];\n",
    "__device__ int d_visitCount;\n",
    "\n",
    "__global__ void traverseTreeKernel(TreeNode* tree, int nodeIdx, int* results, int depth) {\n",
    "    if (nodeIdx < 0 || depth > 10) return;\n",
    "    \n",
    "    TreeNode node = tree[nodeIdx];\n",
    "    \n",
    "    // Process this node (in-order: left, self, right)\n",
    "    \n",
    "    // Launch child for left subtree\n",
    "    if (node.leftChild >= 0) {\n",
    "        traverseTreeKernel<<<1, 1>>>(tree, node.leftChild, results, depth + 1);\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "    \n",
    "    // Visit this node\n",
    "    int order = atomicAdd(&d_visitCount, 1);\n",
    "    d_visitOrder[order] = node.value;\n",
    "    \n",
    "    // Launch child for right subtree\n",
    "    if (node.rightChild >= 0) {\n",
    "        traverseTreeKernel<<<1, 1>>>(tree, node.rightChild, results, depth + 1);\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "}\n",
    "\n",
    "void testTreeTraversal() {\n",
    "    printf(\"\\n=== Exercise 2: Tree Traversal ===\\n\");\n",
    "    \n",
    "    // Build a small binary tree:\n",
    "    //        4\n",
    "    //       / \\\n",
    "    //      2   6\n",
    "    //     / \\ / \\\n",
    "    //    1  3 5  7\n",
    "    \n",
    "    const int numNodes = 7;\n",
    "    TreeNode h_tree[numNodes] = {\n",
    "        {4, 1, 2},   // Node 0: value=4, left=1, right=2\n",
    "        {2, 3, 4},   // Node 1: value=2, left=3, right=4\n",
    "        {6, 5, 6},   // Node 2: value=6, left=5, right=6\n",
    "        {1, -1, -1}, // Node 3: value=1, leaf\n",
    "        {3, -1, -1}, // Node 4: value=3, leaf\n",
    "        {5, -1, -1}, // Node 5: value=5, leaf\n",
    "        {7, -1, -1}  // Node 6: value=7, leaf\n",
    "    };\n",
    "    \n",
    "    TreeNode* d_tree;\n",
    "    int* d_results;\n",
    "    CHECK_CUDA(cudaMalloc(&d_tree, numNodes * sizeof(TreeNode)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_results, numNodes * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_tree, h_tree, numNodes * sizeof(TreeNode), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Reset visit counter\n",
    "    int zero = 0;\n",
    "    CHECK_CUDA(cudaMemcpyToSymbol(d_visitCount, &zero, sizeof(int)));\n",
    "    \n",
    "    printf(\"Tree structure:\\n\");\n",
    "    printf(\"        4\\n       / \\\\\\n      2   6\\n     / \\\\ / \\\\\\n    1  3 5  7\\n\\n\");\n",
    "    printf(\"In-order traversal with Dynamic Parallelism...\\n\");\n",
    "    \n",
    "    traverseTreeKernel<<<1, 1>>>(d_tree, 0, d_results, 0);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Get results\n",
    "    int h_visitOrder[64];\n",
    "    int h_count;\n",
    "    CHECK_CUDA(cudaMemcpyFromSymbol(h_visitOrder, d_visitOrder, numNodes * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpyFromSymbol(&h_count, d_visitCount, sizeof(int)));\n",
    "    \n",
    "    printf(\"Visit order: \");\n",
    "    for (int i = 0; i < h_count; i++) {\n",
    "        printf(\"%d \", h_visitOrder[i]);\n",
    "    }\n",
    "    printf(\"\\nExpected:    1 2 3 4 5 6 7\\n\");\n",
    "    \n",
    "    // Verify in-order\n",
    "    bool correct = (h_count == 7);\n",
    "    int expected[] = {1, 2, 3, 4, 5, 6, 7};\n",
    "    for (int i = 0; i < 7 && correct; i++) {\n",
    "        if (h_visitOrder[i] != expected[i]) correct = false;\n",
    "    }\n",
    "    printf(\"Result: %s\\n\", correct ? \"CORRECT âœ“\" : \"INCORRECT âœ—\");\n",
    "    \n",
    "    cudaFree(d_tree);\n",
    "    cudaFree(d_results);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Exercise 3: Adaptive Grid Refinement\n",
    "// ============================================\n",
    "#define REFINE_THRESHOLD 0.5f\n",
    "#define MAX_REFINE_DEPTH 3\n",
    "#define GRID_SIZE 8\n",
    "\n",
    "__device__ float computeCellValue(int x, int y, int level) {\n",
    "    // Simulate varying complexity - higher values near center\n",
    "    float cx = (float)x / (GRID_SIZE << level) - 0.5f;\n",
    "    float cy = (float)y / (GRID_SIZE << level) - 0.5f;\n",
    "    return expf(-(cx*cx + cy*cy) * 10.0f);\n",
    "}\n",
    "\n",
    "__global__ void refineGridKernel(float* results, int x, int y, \n",
    "                                  int cellSize, int depth, int* refinedCount) {\n",
    "    // Compute value for this cell\n",
    "    float value = computeCellValue(x + cellSize/2, y + cellSize/2, depth);\n",
    "    \n",
    "    if (value > REFINE_THRESHOLD && depth < MAX_REFINE_DEPTH && cellSize > 1) {\n",
    "        // Refine: subdivide into 4 children\n",
    "        int half = cellSize / 2;\n",
    "        \n",
    "        cudaStream_t streams[4];\n",
    "        for (int i = 0; i < 4; i++) {\n",
    "            cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);\n",
    "        }\n",
    "        \n",
    "        refineGridKernel<<<1, 1, 0, streams[0]>>>(results, x, y, half, depth + 1, refinedCount);\n",
    "        refineGridKernel<<<1, 1, 0, streams[1]>>>(results, x + half, y, half, depth + 1, refinedCount);\n",
    "        refineGridKernel<<<1, 1, 0, streams[2]>>>(results, x, y + half, half, depth + 1, refinedCount);\n",
    "        refineGridKernel<<<1, 1, 0, streams[3]>>>(results, x + half, y + half, half, depth + 1, refinedCount);\n",
    "        \n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        for (int i = 0; i < 4; i++) {\n",
    "            cudaStreamDestroy(streams[i]);\n",
    "        }\n",
    "        \n",
    "        atomicAdd(refinedCount, 1);\n",
    "    } else {\n",
    "        // Store result for this cell (leaf node)\n",
    "        int idx = (y / cellSize) * (GRID_SIZE >> depth) + (x / cellSize);\n",
    "        if (idx < GRID_SIZE * GRID_SIZE) {\n",
    "            results[idx] = value;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void testAdaptiveGrid() {\n",
    "    printf(\"\\n=== Exercise 3: Adaptive Grid Refinement ===\\n\");\n",
    "    \n",
    "    float* d_results;\n",
    "    int* d_refinedCount;\n",
    "    CHECK_CUDA(cudaMalloc(&d_results, GRID_SIZE * GRID_SIZE * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_refinedCount, sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_results, 0, GRID_SIZE * GRID_SIZE * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMemset(d_refinedCount, 0, sizeof(int)));\n",
    "    \n",
    "    printf(\"Initial grid: %dx%d\\n\", GRID_SIZE, GRID_SIZE);\n",
    "    printf(\"Threshold for refinement: %.1f\\n\", REFINE_THRESHOLD);\n",
    "    printf(\"Max refinement depth: %d\\n\\n\", MAX_REFINE_DEPTH);\n",
    "    \n",
    "    // Start with full grid cell\n",
    "    refineGridKernel<<<1, 1>>>(d_results, 0, 0, GRID_SIZE, 0, d_refinedCount);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    int h_refinedCount;\n",
    "    CHECK_CUDA(cudaMemcpy(&h_refinedCount, d_refinedCount, sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    float h_results[GRID_SIZE * GRID_SIZE];\n",
    "    CHECK_CUDA(cudaMemcpy(h_results, d_results, GRID_SIZE * GRID_SIZE * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"Cells refined: %d\\n\", h_refinedCount);\n",
    "    printf(\"Adaptive refinement concentrated computation near high-value regions.\\n\");\n",
    "    \n",
    "    // Visualize\n",
    "    printf(\"\\nGrid values (showing refinement pattern):\\n\");\n",
    "    for (int y = 0; y < GRID_SIZE; y++) {\n",
    "        for (int x = 0; x < GRID_SIZE; x++) {\n",
    "            float v = h_results[y * GRID_SIZE + x];\n",
    "            char c = (v > 0.8f) ? '#' : (v > 0.5f) ? '+' : (v > 0.2f) ? '.' : ' ';\n",
    "            printf(\"%c \", c);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    printf(\"\\nâœ“ Adaptive refinement complete\\n\");\n",
    "    \n",
    "    cudaFree(d_results);\n",
    "    cudaFree(d_refinedCount);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Main\n",
    "// ============================================\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘         DYNAMIC PARALLELISM - EXERCISES                       â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    int device;\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDevice(&device);\n",
    "    cudaGetDeviceProperties(&prop, device);\n",
    "    printf(\"\\nDevice: %s (Compute %d.%d)\\n\", prop.name, prop.major, prop.minor);\n",
    "    \n",
    "    if (prop.major < 3 || (prop.major == 3 && prop.minor < 5)) {\n",
    "        printf(\"WARNING: Dynamic Parallelism requires compute capability 3.5+\\n\");\n",
    "    }\n",
    "    \n",
    "    testQuicksort();\n",
    "    testTreeTraversal();\n",
    "    testAdaptiveGrid();\n",
    "    \n",
    "    printf(\"\\nâœ“ All exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcae6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -lcudadevrt -o dynamic_parallelism_exercises dynamic_parallelism_exercises.cu && ./dynamic_parallelism_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01bdd8",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Recursive Quicksort\n",
    "Implement quicksort using dynamic parallelism - each partition launches children.\n",
    "\n",
    "### Exercise 2: Tree Traversal\n",
    "Process a tree structure where each node can launch children for its subtrees.\n",
    "\n",
    "### Exercise 3: Adaptive Grid Refinement\n",
    "Implement a 2D grid where cells above a threshold launch finer grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup generated files\n",
    "!rm -f dynamic_basic dynamic_memory dynamic_streams sync_patterns dynamic_errors recursive_sum adaptive_work *.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e9f6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What is Dynamic Parallelism?\n",
    "\n",
    "### Concept\n",
    "\n",
    "```\n",
    "Traditional CUDA:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Host (CPU) launches kernels on Device (GPU)\n",
    "\n",
    "[Host] â”€â”€launchâ”€â”€> [Kernel A]\n",
    "[Host] â”€â”€launchâ”€â”€> [Kernel B]\n",
    "[Host] â”€â”€launchâ”€â”€> [Kernel C]\n",
    "\n",
    "Dynamic Parallelism:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "GPU kernels can launch other kernels!\n",
    "\n",
    "[Host] â”€â”€launchâ”€â”€> [Kernel A]\n",
    "                      â”‚\n",
    "                      â”œâ”€â”€launchâ”€â”€> [Child 1]\n",
    "                      â”œâ”€â”€launchâ”€â”€> [Child 2]\n",
    "                      â””â”€â”€launchâ”€â”€> [Child 3]\n",
    "\n",
    "Use Cases:\n",
    "â€¢ Recursive algorithms (quicksort, tree traversal)\n",
    "â€¢ Adaptive mesh refinement\n",
    "â€¢ Work that generates more work\n",
    "â€¢ Data-dependent parallelism\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41428807",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Basic Child Kernel Launch\n",
    "\n",
    "### ğŸ”· CUDA C++ Dynamic Parallelism (Primary)\n",
    "\n",
    "**Compile with relocatable device code:**\n",
    "```bash\n",
    "nvcc -arch=sm_75 -rdc=true -lcudadevrt -o dynamic_basic dynamic_basic.cu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_basic.cu\n",
    "// dynamic_basic.cu - Basic child kernel launch\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// ============================================\n",
    "// Child Kernel - Launched from GPU\n",
    "// ============================================\n",
    "__global__ void childKernel(int parentId) {\n",
    "    int tid = threadIdx.x;\n",
    "    printf(\"  Child of parent %d: thread %d\\n\", parentId, tid);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Parent Kernel - Launches Children\n",
    "// ============================================\n",
    "__global__ void parentKernel() {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    printf(\"Parent thread %d launching child...\\n\", tid);\n",
    "    \n",
    "    // Launch child kernel from GPU!\n",
    "    childKernel<<<1, 4>>>(tid);\n",
    "    \n",
    "    // Wait for child to complete\n",
    "    cudaDeviceSynchronize();  // Device-side sync!\n",
    "    \n",
    "    printf(\"Parent thread %d: child completed\\n\", tid);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Host launching parent kernel...\\n\");\n",
    "    \n",
    "    parentKernel<<<1, 2>>>();\n",
    "    cudaDeviceSynchronize();  // Host-side sync\n",
    "    \n",
    "    printf(\"All done!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36d7b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Visibility\n",
    "\n",
    "### Memory Rules for Dynamic Parallelism\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           MEMORY VISIBILITY RULES                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Global Memory:                                         â”‚\n",
    "â”‚  âœ… Parent writes visible to child after launch         â”‚\n",
    "â”‚  âœ… Child writes visible to parent after sync           â”‚\n",
    "â”‚  âš ï¸  Must sync properly!                                â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Local Memory:                                          â”‚\n",
    "â”‚  âŒ Parent's local memory NOT visible to child          â”‚\n",
    "â”‚  âŒ Child's local memory NOT visible to parent          â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Shared Memory:                                         â”‚\n",
    "â”‚  âŒ Parent's shared NOT visible to child                â”‚\n",
    "â”‚  âŒ Each kernel has its own shared memory               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Constant Memory:                                       â”‚\n",
    "â”‚  âœ… Visible to all (set before host launch)             â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Memory Example (Primary)\n",
    "\n",
    "```cpp\n",
    "// dynamic_memory.cu - Memory visibility\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void childKernel(float* data, int idx) {\n",
    "    // Child can read parent's global memory writes\n",
    "    float parentValue = data[idx];\n",
    "    printf(\"Child: read %.1f from parent\\n\", parentValue);\n",
    "    \n",
    "    // Child writes to global memory\n",
    "    data[idx] = parentValue * 2.0f;\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(float* data) {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Parent writes to global memory\n",
    "    data[tid] = (float)(tid + 1) * 10.0f;\n",
    "    printf(\"Parent %d: wrote %.1f\\n\", tid, data[tid]);\n",
    "    \n",
    "    // __threadfence() ensures visibility before child launch\n",
    "    __threadfence();\n",
    "    \n",
    "    // Launch child\n",
    "    childKernel<<<1, 1>>>(data, tid);\n",
    "    \n",
    "    // Must sync to see child's writes!\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Now parent can see child's writes\n",
    "    printf(\"Parent %d: child wrote %.1f\\n\", tid, data[tid]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, 4 * sizeof(float));\n",
    "    \n",
    "    parentKernel<<<1, 4>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d104787",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Device-Side Streams\n",
    "\n",
    "### ğŸ”· CUDA C++ Streams in Dynamic Parallelism (Primary)\n",
    "\n",
    "```cpp\n",
    "// dynamic_streams.cu - Device-side streams\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void childA(int* data) {\n",
    "    atomicAdd(data, 1);\n",
    "}\n",
    "\n",
    "__global__ void childB(int* data) {\n",
    "    atomicAdd(data, 10);\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(int* data) {\n",
    "    // ============================================\n",
    "    // Default Stream (NULL)\n",
    "    // ============================================\n",
    "    // Children on default stream serialize with each other\n",
    "    // but are async with parent\n",
    "    \n",
    "    childA<<<1, 1>>>(data);  // On implicit NULL stream\n",
    "    childB<<<1, 1>>>(data);  // Waits for childA\n",
    "    \n",
    "    cudaDeviceSynchronize();  // Wait for both\n",
    "    printf(\"After default stream children: %d\\n\", *data);\n",
    "    \n",
    "    // ============================================\n",
    "    // Named Streams (Device-Side)\n",
    "    // ============================================\n",
    "    cudaStream_t stream1, stream2;\n",
    "    cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking);\n",
    "    cudaStreamCreateWithFlags(&stream2, cudaStreamNonBlocking);\n",
    "    \n",
    "    // Concurrent children on different streams\n",
    "    childA<<<1, 1, 0, stream1>>>(data);\n",
    "    childB<<<1, 1, 0, stream2>>>(data);\n",
    "    \n",
    "    // Sync specific streams\n",
    "    cudaStreamSynchronize(stream1);\n",
    "    cudaStreamSynchronize(stream2);\n",
    "    \n",
    "    printf(\"After named stream children: %d\\n\", *data);\n",
    "    \n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(stream2);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int* d_data;\n",
    "    cudaMalloc(&d_data, sizeof(int));\n",
    "    cudaMemset(d_data, 0, sizeof(int));\n",
    "    \n",
    "    parentKernel<<<1, 1>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12bfa2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Synchronization Patterns\n",
    "\n",
    "### ğŸ”· CUDA C++ When to Sync (Primary)\n",
    "\n",
    "```cpp\n",
    "// sync_patterns.cu - Synchronization patterns\n",
    "\n",
    "// Pattern 1: Sync before reading child results\n",
    "__global__ void pattern1(float* data) {\n",
    "    childKernel<<<1, 32>>>(data);\n",
    "    \n",
    "    cudaDeviceSynchronize();  // REQUIRED!\n",
    "    \n",
    "    float result = data[0];  // Safe to read\n",
    "}\n",
    "\n",
    "// Pattern 2: Fire and forget (parent returns)\n",
    "__global__ void pattern2(float* data) {\n",
    "    childKernel<<<1, 32>>>(data);\n",
    "    \n",
    "    // NO sync - parent returns immediately\n",
    "    // Child continues running\n",
    "    // Result visible to HOST after host sync\n",
    "}\n",
    "\n",
    "// Pattern 3: Implicit sync at parent completion\n",
    "// When parent kernel ends, all children are synced\n",
    "// before control returns to host\n",
    "\n",
    "__global__ void parentFireForget(float* data) {\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        childKernel<<<1, 32>>>(data + i * 32);\n",
    "        // No explicit sync\n",
    "    }\n",
    "    // When parent ends, runtime syncs all children\n",
    "}\n",
    "\n",
    "// Host code:\n",
    "// parentFireForget<<<1, 1>>>(data);\n",
    "// cudaDeviceSynchronize();  // Waits for parent AND all children\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Error Checking (Primary)\n",
    "\n",
    "```cpp\n",
    "__global__ void errorCheckingPattern(float* data) {\n",
    "    // Launch child\n",
    "    childKernel<<<1000000, 1024>>>(data);  // Might fail!\n",
    "    \n",
    "    // Check for launch errors (device-side)\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"Launch error: %s\\n\", cudaGetErrorString(err));\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Sync and check for execution errors\n",
    "    err = cudaDeviceSynchronize();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"Execution error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d039cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Simple Recursion\n",
    "Write a kernel that recursively launches itself (with depth limit).\n",
    "\n",
    "### Exercise 2: Fan-Out Pattern\n",
    "One parent thread launches multiple children that process different data.\n",
    "\n",
    "### Exercise 3: Memory Communication\n",
    "Parent writes config to global memory, child reads it and writes results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a22b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        DYNAMIC PARALLELISM - MANAGER DELEGATION         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  ğŸ‘” Management Analogy:                                 â”‚\n",
    "â”‚  â€¢ Parent kernel = Manager with hiring authority        â”‚\n",
    "â”‚  â€¢ Child kernels = Sub-teams spawned on demand          â”‚\n",
    "â”‚  â€¢ No CEO (host) approval needed for sub-tasks          â”‚\n",
    "â”‚  â€¢ Managers sync with their teams before reporting up   â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Launch from GPU:                                       â”‚\n",
    "â”‚  childKernel<<<blocks, threads>>>(args);                â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Synchronization:                                       â”‚\n",
    "â”‚  cudaDeviceSynchronize();  // Device-side               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Compile Flag:                                          â”‚\n",
    "â”‚  nvcc -rdc=true ...                                     â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Memory Rules (Manager's Desk):                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\n",
    "â”‚  â”‚ Global Memory:  Shared filing cabinet  â”‚             â”‚\n",
    "â”‚  â”‚                 (visible after sync)   â”‚             â”‚\n",
    "â”‚  â”‚ Local/Shared:   Manager's private desk â”‚             â”‚\n",
    "â”‚  â”‚                 (NOT visible to subs)  â”‚             â”‚\n",
    "â”‚  â”‚ __threadfence: \"I've filed the docs!\"  â”‚             â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Limits:                                                â”‚\n",
    "â”‚  â€¢ Nesting: 24 levels (org chart depth)                 â”‚\n",
    "â”‚  â€¢ Compute Capability: 3.5+                             â”‚\n",
    "â”‚  â€¢ Overhead: ~5Î¼s per launch (use wisely!)              â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Best For:                                              â”‚\n",
    "â”‚  âœ“ Recursive algorithms (quicksort, trees)              â”‚\n",
    "â”‚  âœ“ Adaptive refinement (mesh, detail)                   â”‚\n",
    "â”‚  âœ“ Data-dependent work (sparse patterns)                â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— What's Next?\n",
    "\n",
    "**Day 4: Nested Kernel Patterns** - Now that you understand delegation, let's master the **Russian nesting doll** patternsâ€”kernels within kernels within kernels!\n",
    "\n",
    "You'll learn:\n",
    "- Recursive quicksort with dynamic parallelism\n",
    "- Tree traversal patterns\n",
    "- Adaptive refinement techniques\n",
    "- Performance optimization for nested launches\n",
    "\n",
    "*From single-level delegation to the full matryoshka experience!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
