{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a608f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"âš ï¸  Dynamic Parallelism is a CUDA C++ feature (CC 3.5+)!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a2206",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. What is Dynamic Parallelism?\n",
    "\n",
    "```\n",
    "Traditional CUDA:                  Dynamic Parallelism:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Host controls everything           Kernels launch kernels!\n",
    "\n",
    "[Host] â”€â”€> [Kernel A]              [Host] â”€â”€> [Parent Kernel]\n",
    "[Host] â”€â”€> [Kernel B]                            â”‚\n",
    "[Host] â”€â”€> [Kernel C]                            â”œâ”€â”€> [Child 1]\n",
    "                                                 â”œâ”€â”€> [Child 2]\n",
    "                                                 â””â”€â”€> [Child 3]\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- Recursive algorithms (quicksort, tree traversal)\n",
    "- Adaptive mesh refinement\n",
    "- Work that generates more work\n",
    "- Data-dependent parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0ce48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Child Kernel Launch\n",
    "\n",
    "The simplest example: a parent kernel launches child kernels from the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_basic.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// ============================================\n",
    "// Child Kernel - Launched FROM the GPU\n",
    "// ============================================\n",
    "__global__ void childKernel(int parentId) {\n",
    "    int tid = threadIdx.x;\n",
    "    printf(\"    Child of parent %d: thread %d\\n\", parentId, tid);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Parent Kernel - Launches Children\n",
    "// ============================================\n",
    "__global__ void parentKernel() {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    printf(\"Parent thread %d launching child kernel...\\n\", tid);\n",
    "    \n",
    "    // Launch child kernel from GPU!\n",
    "    // Same <<<blocks, threads>>> syntax\n",
    "    childKernel<<<1, 4>>>(tid);\n",
    "    \n",
    "    // Device-side synchronization\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Parent thread %d: child completed\\n\", tid);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Dynamic Parallelism Basic Demo ===\\n\\n\");\n",
    "    printf(\"Host launching parent kernel with 2 threads...\\n\\n\");\n",
    "    \n",
    "    // Host launches parent\n",
    "    parentKernel<<<1, 2>>>();\n",
    "    \n",
    "    // Host-side sync waits for ALL work (parent + children)\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"\\nAll done!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59768da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: -rdc=true enables relocatable device code (required for dynamic parallelism)\n",
    "# Note: -lcudadevrt links the device runtime library\n",
    "!nvcc -rdc=true -lcudadevrt -o dynamic_basic dynamic_basic.cu && ./dynamic_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b52e97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Memory Visibility Between Parent and Child\n",
    "\n",
    "Understanding what memory is visible where is critical:\n",
    "\n",
    "| Memory Type | Parent â†’ Child | Child â†’ Parent |\n",
    "|-------------|----------------|----------------|\n",
    "| **Global** | âœ… After launch | âœ… After sync |\n",
    "| **Local** | âŒ | âŒ |\n",
    "| **Shared** | âŒ | âŒ |\n",
    "| **Constant** | âœ… (set before host launch) | âœ… |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b58f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_memory.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void childKernel(float* data, int idx) {\n",
    "    // Child CAN read parent's global memory writes\n",
    "    float parentValue = data[idx];\n",
    "    printf(\"    Child[%d]: read %.1f from parent\\n\", idx, parentValue);\n",
    "    \n",
    "    // Child writes to global memory\n",
    "    data[idx] = parentValue * 2.0f;\n",
    "    printf(\"    Child[%d]: wrote %.1f back\\n\", idx, data[idx]);\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(float* data) {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Parent writes to global memory\n",
    "    data[tid] = (float)(tid + 1) * 10.0f;\n",
    "    printf(\"Parent %d: wrote %.1f to global memory\\n\", tid, data[tid]);\n",
    "    \n",
    "    // __threadfence() ensures memory writes are visible\n",
    "    // to child kernels BEFORE we launch them\n",
    "    __threadfence();\n",
    "    \n",
    "    // Launch child\n",
    "    childKernel<<<1, 1>>>(data, tid);\n",
    "    \n",
    "    // Must sync to see child's writes!\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Now parent can see child's writes\n",
    "    printf(\"Parent %d: after sync, data = %.1f\\n\", tid, data[tid]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Dynamic Parallelism Memory Visibility ===\\n\\n\");\n",
    "    \n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, 4 * sizeof(float));\n",
    "    cudaMemset(d_data, 0, 4 * sizeof(float));\n",
    "    \n",
    "    parentKernel<<<1, 4>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Verify from host\n",
    "    float h_data[4];\n",
    "    cudaMemcpy(h_data, d_data, 4 * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"\\nHost verification:\\n\");\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "        float expected = (float)(i + 1) * 10.0f * 2.0f;  // parent wrote * child doubled\n",
    "        printf(\"  data[%d] = %.1f (expected: %.1f)\\n\", i, h_data[i], expected);\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o dynamic_memory dynamic_memory.cu && ./dynamic_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6c882",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Device-Side Streams\n",
    "\n",
    "Kernels on the GPU can create and use streams, just like on the host. This enables concurrent child kernel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315dcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_streams.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void childA(int* counter, int id) {\n",
    "    atomicAdd(counter, 1);\n",
    "    printf(\"    ChildA[%d] executed, counter now %d\\n\", id, *counter);\n",
    "}\n",
    "\n",
    "__global__ void childB(int* counter, int id) {\n",
    "    atomicAdd(counter, 10);\n",
    "    printf(\"    ChildB[%d] executed, counter now %d\\n\", id, *counter);\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(int* counter) {\n",
    "    printf(\"=== Default Stream (Sequential) ===\\n\");\n",
    "    \n",
    "    // Default stream: children execute sequentially\n",
    "    childA<<<1, 1>>>(counter, 0);  // First\n",
    "    childB<<<1, 1>>>(counter, 0);  // Second (waits for childA)\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"After default stream: counter = %d\\n\\n\", *counter);\n",
    "    \n",
    "    printf(\"=== Named Streams (Concurrent) ===\\n\");\n",
    "    \n",
    "    // Create device-side streams\n",
    "    cudaStream_t stream1, stream2;\n",
    "    cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking);\n",
    "    cudaStreamCreateWithFlags(&stream2, cudaStreamNonBlocking);\n",
    "    \n",
    "    // Launch children on different streams - can run concurrently!\n",
    "    childA<<<1, 1, 0, stream1>>>(counter, 1);\n",
    "    childB<<<1, 1, 0, stream2>>>(counter, 1);\n",
    "    \n",
    "    // Sync each stream\n",
    "    cudaStreamSynchronize(stream1);\n",
    "    cudaStreamSynchronize(stream2);\n",
    "    \n",
    "    printf(\"After named streams: counter = %d\\n\", *counter);\n",
    "    \n",
    "    // Cleanup device-side streams\n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(stream2);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Device-Side Streams Demo ===\\n\\n\");\n",
    "    \n",
    "    int* d_counter;\n",
    "    cudaMalloc(&d_counter, sizeof(int));\n",
    "    cudaMemset(d_counter, 0, sizeof(int));\n",
    "    \n",
    "    parentKernel<<<1, 1>>>(d_counter);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int result;\n",
    "    cudaMemcpy(&result, d_counter, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"\\nFinal counter value: %d (expected: 22 = 1+10+1+10)\\n\", result);\n",
    "    \n",
    "    cudaFree(d_counter);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad968c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o dynamic_streams dynamic_streams.cu && ./dynamic_streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc2fb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Synchronization Patterns\n",
    "\n",
    "There are several ways to synchronize parent with child kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sync_patterns.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void workKernel(float* data, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        data[i] = data[i] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Pattern 1: Explicit sync before reading results\n",
    "__global__ void pattern1_explicit_sync(float* data, int n) {\n",
    "    printf(\"Pattern 1: Explicit sync\\n\");\n",
    "    \n",
    "    workKernel<<<(n + 255) / 256, 256>>>(data, n);\n",
    "    \n",
    "    // REQUIRED before reading child's results\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"  First result: %.1f\\n\", data[0]);\n",
    "}\n",
    "\n",
    "// Pattern 2: Fire and forget (no parent sync)\n",
    "__global__ void pattern2_fire_forget(float* data, int n) {\n",
    "    printf(\"Pattern 2: Fire and forget\\n\");\n",
    "    \n",
    "    workKernel<<<(n + 255) / 256, 256>>>(data, n);\n",
    "    \n",
    "    // NO sync - parent returns immediately\n",
    "    // Child continues running\n",
    "    // Result visible to HOST after host's cudaDeviceSynchronize()\n",
    "    \n",
    "    printf(\"  Parent returning (child may still be running)\\n\");\n",
    "}\n",
    "\n",
    "// Pattern 3: Stream-based sync (more granular control)\n",
    "__global__ void pattern3_stream_sync(float* data, int n) {\n",
    "    printf(\"Pattern 3: Stream-based sync\\n\");\n",
    "    \n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);\n",
    "    \n",
    "    workKernel<<<(n + 255) / 256, 256, 0, stream>>>(data, n);\n",
    "    \n",
    "    // Sync only this stream\n",
    "    cudaStreamSynchronize(stream);\n",
    "    \n",
    "    printf(\"  First result: %.1f\\n\", data[0]);\n",
    "    \n",
    "    cudaStreamDestroy(stream);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Synchronization Patterns ===\\n\\n\");\n",
    "    \n",
    "    const int N = 1024;\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    // Initialize to 1.0\n",
    "    float h_init[N];\n",
    "    for (int i = 0; i < N; i++) h_init[i] = 1.0f;\n",
    "    \n",
    "    // Test Pattern 1\n",
    "    cudaMemcpy(d_data, h_init, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    pattern1_explicit_sync<<<1, 1>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    // Test Pattern 2\n",
    "    cudaMemcpy(d_data, h_init, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    pattern2_fire_forget<<<1, 1>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();  // Host waits for all\n",
    "    float result;\n",
    "    cudaMemcpy(&result, d_data, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"  (Host sees result: %.1f)\\n\\n\", result);\n",
    "    \n",
    "    // Test Pattern 3\n",
    "    cudaMemcpy(d_data, h_init, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    pattern3_stream_sync<<<1, 1>>>(d_data, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ec0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o sync_patterns sync_patterns.cu && ./sync_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8db124",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Error Handling in Dynamic Parallelism\n",
    "\n",
    "Device-side error checking works similarly to host-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce816bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_errors.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void simpleChild(float* data) {\n",
    "    data[threadIdx.x] = 1.0f;\n",
    "}\n",
    "\n",
    "__global__ void errorCheckingParent(float* data) {\n",
    "    printf(\"=== Device-Side Error Checking ===\\n\\n\");\n",
    "    \n",
    "    // Attempt 1: Valid launch\n",
    "    printf(\"Attempt 1: Valid launch (1 block, 32 threads)\\n\");\n",
    "    simpleChild<<<1, 32>>>(data);\n",
    "    \n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"  Launch error: %s\\n\", cudaGetErrorString(err));\n",
    "    } else {\n",
    "        printf(\"  Launch successful!\\n\");\n",
    "    }\n",
    "    \n",
    "    err = cudaDeviceSynchronize();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"  Execution error: %s\\n\", cudaGetErrorString(err));\n",
    "    } else {\n",
    "        printf(\"  Execution successful!\\n\");\n",
    "    }\n",
    "    \n",
    "    // Attempt 2: Invalid launch (too many threads)\n",
    "    printf(\"\\nAttempt 2: Invalid launch (1 block, 2048 threads)\\n\");\n",
    "    simpleChild<<<1, 2048>>>(data);  // Exceeds max threads per block!\n",
    "    \n",
    "    err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"  Launch error: %s\\n\", cudaGetErrorString(err));\n",
    "    } else {\n",
    "        printf(\"  Launch seemed OK (checking execution...)\\n\");\n",
    "        err = cudaDeviceSynchronize();\n",
    "        if (err != cudaSuccess) {\n",
    "            printf(\"  Execution error: %s\\n\", cudaGetErrorString(err));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Attempt 3: Another valid launch to show recovery\n",
    "    printf(\"\\nAttempt 3: Recovery - valid launch again\\n\");\n",
    "    simpleChild<<<1, 32>>>(data);\n",
    "    err = cudaDeviceSynchronize();\n",
    "    printf(\"  Result: %s\\n\", err == cudaSuccess ? \"Success!\" : cudaGetErrorString(err));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, 2048 * sizeof(float));\n",
    "    \n",
    "    errorCheckingParent<<<1, 1>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o dynamic_errors dynamic_errors.cu && ./dynamic_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7a26f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Practical Example: Recursive Sum\n",
    "\n",
    "A classic example: recursively sum an array by dividing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile recursive_sum.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define MAX_DEPTH 4\n",
    "\n",
    "// Recursive kernel that sums array segments\n",
    "__global__ void recursiveSum(float* data, float* result, int start, int end, int depth) {\n",
    "    int n = end - start;\n",
    "    \n",
    "    // Base case: small enough to sum directly\n",
    "    if (n <= 32 || depth >= MAX_DEPTH) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = start; i < end; i++) {\n",
    "            sum += data[i];\n",
    "        }\n",
    "        atomicAdd(result, sum);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Recursive case: split and launch children\n",
    "    int mid = start + n / 2;\n",
    "    \n",
    "    // Launch two children for each half\n",
    "    recursiveSum<<<1, 1>>>(data, result, start, mid, depth + 1);\n",
    "    recursiveSum<<<1, 1>>>(data, result, mid, end, depth + 1);\n",
    "    \n",
    "    // Wait for children\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Recursive Sum with Dynamic Parallelism ===\\n\\n\");\n",
    "    \n",
    "    const int N = 1024;\n",
    "    float* d_data;\n",
    "    float* d_result;\n",
    "    \n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMalloc(&d_result, sizeof(float));\n",
    "    \n",
    "    // Initialize: 1, 2, 3, ..., N\n",
    "    float* h_data = (float*)malloc(N * sizeof(float));\n",
    "    float expected = 0.0f;\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_data[i] = (float)(i + 1);\n",
    "        expected += h_data[i];\n",
    "    }\n",
    "    \n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_result, 0, sizeof(float));\n",
    "    \n",
    "    printf(\"Summing 1 + 2 + 3 + ... + %d\\n\", N);\n",
    "    printf(\"Expected: %.0f\\n\\n\", expected);\n",
    "    \n",
    "    // Launch recursive sum\n",
    "    recursiveSum<<<1, 1>>>(d_data, d_result, 0, N, 0);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Get result\n",
    "    float result;\n",
    "    cudaMemcpy(&result, d_result, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Result: %.0f\\n\", result);\n",
    "    printf(\"Match: %s\\n\", (result == expected) ? \"YES\" : \"NO\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "    free(h_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o recursive_sum recursive_sum.cu && ./recursive_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe640c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practical Example: Adaptive Work Generation\n",
    "\n",
    "Only launch child kernels where work is needed (data-dependent parallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile adaptive_work.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Child kernel: process a segment that needs work\n",
    "__global__ void processSegment(float* data, int start, int count, int* processed) {\n",
    "    int i = start + blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < start + count) {\n",
    "        // Simulate processing: square the value\n",
    "        data[i] = data[i] * data[i];\n",
    "        atomicAdd(processed, 1);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Parent: analyze data and launch children only where needed\n",
    "__global__ void adaptiveParent(float* data, int n, float threshold, int* processed) {\n",
    "    int tid = threadIdx.x;\n",
    "    int segments = 8;  // Divide into 8 segments\n",
    "    int segmentSize = n / segments;\n",
    "    \n",
    "    if (tid < segments) {\n",
    "        int start = tid * segmentSize;\n",
    "        int end = (tid == segments - 1) ? n : start + segmentSize;\n",
    "        int count = end - start;\n",
    "        \n",
    "        // Check if this segment needs processing\n",
    "        // (contains values above threshold)\n",
    "        bool needsWork = false;\n",
    "        for (int i = start; i < end && !needsWork; i++) {\n",
    "            if (data[i] > threshold) needsWork = true;\n",
    "        }\n",
    "        \n",
    "        if (needsWork) {\n",
    "            printf(\"Segment %d [%d-%d]: launching child kernel\\n\", tid, start, end - 1);\n",
    "            \n",
    "            int threads = 256;\n",
    "            int blocks = (count + threads - 1) / threads;\n",
    "            processSegment<<<blocks, threads>>>(data, start, count, processed);\n",
    "        } else {\n",
    "            printf(\"Segment %d [%d-%d]: skipping (no work needed)\\n\", tid, start, end - 1);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Sync all children\n",
    "    __syncthreads();\n",
    "    if (tid == 0) {\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Adaptive Work Generation ===\\n\\n\");\n",
    "    \n",
    "    const int N = 8192;\n",
    "    const float THRESHOLD = 0.5f;\n",
    "    \n",
    "    float* d_data;\n",
    "    int* d_processed;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    cudaMalloc(&d_processed, sizeof(int));\n",
    "    cudaMemset(d_processed, 0, sizeof(int));\n",
    "    \n",
    "    // Initialize: some segments have values > threshold, some don't\n",
    "    float* h_data = (float*)malloc(N * sizeof(float));\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        // Segments 0,2,4,6 have low values; 1,3,5,7 have high values\n",
    "        int segment = i / (N / 8);\n",
    "        h_data[i] = (segment % 2 == 0) ? 0.3f : 0.7f;\n",
    "    }\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    printf(\"Threshold: %.1f\\n\", THRESHOLD);\n",
    "    printf(\"Only segments with values > threshold will be processed.\\n\\n\");\n",
    "    \n",
    "    adaptiveParent<<<1, 8>>>(d_data, N, THRESHOLD, d_processed);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int processed;\n",
    "    cudaMemcpy(&processed, d_processed, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"\\nElements processed: %d (out of %d)\\n\", processed, N);\n",
    "    printf(\"Expected: %d (half of elements)\\n\", N / 2);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_processed);\n",
    "    free(h_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -lcudadevrt -o adaptive_work adaptive_work.cu && ./adaptive_work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bb244",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Dynamic Parallelism Key Points\n",
    "\n",
    "### Compilation\n",
    "```bash\n",
    "nvcc -rdc=true -lcudadevrt -o program program.cu\n",
    "```\n",
    "\n",
    "### Launch Syntax (Same as Host)\n",
    "```cpp\n",
    "childKernel<<<blocks, threads, sharedMem, stream>>>(args);\n",
    "```\n",
    "\n",
    "### Synchronization\n",
    "```cpp\n",
    "cudaDeviceSynchronize();      // Wait for all children\n",
    "cudaStreamSynchronize(s);     // Wait for specific stream\n",
    "```\n",
    "\n",
    "### Memory Visibility\n",
    "- Global memory: visible after sync\n",
    "- Local/Shared: NOT visible between parent/child\n",
    "- Use `__threadfence()` before launching children\n",
    "\n",
    "### Limits\n",
    "- Maximum nesting depth: 24 levels\n",
    "- Compute Capability: 3.5+\n",
    "- Child kernel launches have overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfe758",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Recursive Quicksort\n",
    "Implement quicksort using dynamic parallelism - each partition launches children.\n",
    "\n",
    "### Exercise 2: Tree Traversal\n",
    "Process a tree structure where each node can launch children for its subtrees.\n",
    "\n",
    "### Exercise 3: Adaptive Grid Refinement\n",
    "Implement a 2D grid where cells above a threshold launch finer grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup generated files\n",
    "!rm -f dynamic_basic dynamic_memory dynamic_streams sync_patterns dynamic_errors recursive_sum adaptive_work *.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e9f6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What is Dynamic Parallelism?\n",
    "\n",
    "### Concept\n",
    "\n",
    "```\n",
    "Traditional CUDA:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Host (CPU) launches kernels on Device (GPU)\n",
    "\n",
    "[Host] â”€â”€launchâ”€â”€> [Kernel A]\n",
    "[Host] â”€â”€launchâ”€â”€> [Kernel B]\n",
    "[Host] â”€â”€launchâ”€â”€> [Kernel C]\n",
    "\n",
    "Dynamic Parallelism:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "GPU kernels can launch other kernels!\n",
    "\n",
    "[Host] â”€â”€launchâ”€â”€> [Kernel A]\n",
    "                      â”‚\n",
    "                      â”œâ”€â”€launchâ”€â”€> [Child 1]\n",
    "                      â”œâ”€â”€launchâ”€â”€> [Child 2]\n",
    "                      â””â”€â”€launchâ”€â”€> [Child 3]\n",
    "\n",
    "Use Cases:\n",
    "â€¢ Recursive algorithms (quicksort, tree traversal)\n",
    "â€¢ Adaptive mesh refinement\n",
    "â€¢ Work that generates more work\n",
    "â€¢ Data-dependent parallelism\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41428807",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Basic Child Kernel Launch\n",
    "\n",
    "### ğŸ”· CUDA C++ Dynamic Parallelism (Primary)\n",
    "\n",
    "**Compile with relocatable device code:**\n",
    "```bash\n",
    "nvcc -arch=sm_75 -rdc=true -lcudadevrt -o dynamic_basic dynamic_basic.cu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dynamic_basic.cu\n",
    "// dynamic_basic.cu - Basic child kernel launch\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// ============================================\n",
    "// Child Kernel - Launched from GPU\n",
    "// ============================================\n",
    "__global__ void childKernel(int parentId) {\n",
    "    int tid = threadIdx.x;\n",
    "    printf(\"  Child of parent %d: thread %d\\n\", parentId, tid);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Parent Kernel - Launches Children\n",
    "// ============================================\n",
    "__global__ void parentKernel() {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    printf(\"Parent thread %d launching child...\\n\", tid);\n",
    "    \n",
    "    // Launch child kernel from GPU!\n",
    "    childKernel<<<1, 4>>>(tid);\n",
    "    \n",
    "    // Wait for child to complete\n",
    "    cudaDeviceSynchronize();  // Device-side sync!\n",
    "    \n",
    "    printf(\"Parent thread %d: child completed\\n\", tid);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Host launching parent kernel...\\n\");\n",
    "    \n",
    "    parentKernel<<<1, 2>>>();\n",
    "    cudaDeviceSynchronize();  // Host-side sync\n",
    "    \n",
    "    printf(\"All done!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36d7b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Visibility\n",
    "\n",
    "### Memory Rules for Dynamic Parallelism\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           MEMORY VISIBILITY RULES                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Global Memory:                                         â”‚\n",
    "â”‚  âœ… Parent writes visible to child after launch         â”‚\n",
    "â”‚  âœ… Child writes visible to parent after sync           â”‚\n",
    "â”‚  âš ï¸  Must sync properly!                                â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Local Memory:                                          â”‚\n",
    "â”‚  âŒ Parent's local memory NOT visible to child          â”‚\n",
    "â”‚  âŒ Child's local memory NOT visible to parent          â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Shared Memory:                                         â”‚\n",
    "â”‚  âŒ Parent's shared NOT visible to child                â”‚\n",
    "â”‚  âŒ Each kernel has its own shared memory               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Constant Memory:                                       â”‚\n",
    "â”‚  âœ… Visible to all (set before host launch)             â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Memory Example (Primary)\n",
    "\n",
    "```cpp\n",
    "// dynamic_memory.cu - Memory visibility\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void childKernel(float* data, int idx) {\n",
    "    // Child can read parent's global memory writes\n",
    "    float parentValue = data[idx];\n",
    "    printf(\"Child: read %.1f from parent\\n\", parentValue);\n",
    "    \n",
    "    // Child writes to global memory\n",
    "    data[idx] = parentValue * 2.0f;\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(float* data) {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Parent writes to global memory\n",
    "    data[tid] = (float)(tid + 1) * 10.0f;\n",
    "    printf(\"Parent %d: wrote %.1f\\n\", tid, data[tid]);\n",
    "    \n",
    "    // __threadfence() ensures visibility before child launch\n",
    "    __threadfence();\n",
    "    \n",
    "    // Launch child\n",
    "    childKernel<<<1, 1>>>(data, tid);\n",
    "    \n",
    "    // Must sync to see child's writes!\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Now parent can see child's writes\n",
    "    printf(\"Parent %d: child wrote %.1f\\n\", tid, data[tid]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, 4 * sizeof(float));\n",
    "    \n",
    "    parentKernel<<<1, 4>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d104787",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Device-Side Streams\n",
    "\n",
    "### ğŸ”· CUDA C++ Streams in Dynamic Parallelism (Primary)\n",
    "\n",
    "```cpp\n",
    "// dynamic_streams.cu - Device-side streams\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void childA(int* data) {\n",
    "    atomicAdd(data, 1);\n",
    "}\n",
    "\n",
    "__global__ void childB(int* data) {\n",
    "    atomicAdd(data, 10);\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(int* data) {\n",
    "    // ============================================\n",
    "    // Default Stream (NULL)\n",
    "    // ============================================\n",
    "    // Children on default stream serialize with each other\n",
    "    // but are async with parent\n",
    "    \n",
    "    childA<<<1, 1>>>(data);  // On implicit NULL stream\n",
    "    childB<<<1, 1>>>(data);  // Waits for childA\n",
    "    \n",
    "    cudaDeviceSynchronize();  // Wait for both\n",
    "    printf(\"After default stream children: %d\\n\", *data);\n",
    "    \n",
    "    // ============================================\n",
    "    // Named Streams (Device-Side)\n",
    "    // ============================================\n",
    "    cudaStream_t stream1, stream2;\n",
    "    cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking);\n",
    "    cudaStreamCreateWithFlags(&stream2, cudaStreamNonBlocking);\n",
    "    \n",
    "    // Concurrent children on different streams\n",
    "    childA<<<1, 1, 0, stream1>>>(data);\n",
    "    childB<<<1, 1, 0, stream2>>>(data);\n",
    "    \n",
    "    // Sync specific streams\n",
    "    cudaStreamSynchronize(stream1);\n",
    "    cudaStreamSynchronize(stream2);\n",
    "    \n",
    "    printf(\"After named stream children: %d\\n\", *data);\n",
    "    \n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(stream2);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int* d_data;\n",
    "    cudaMalloc(&d_data, sizeof(int));\n",
    "    cudaMemset(d_data, 0, sizeof(int));\n",
    "    \n",
    "    parentKernel<<<1, 1>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12bfa2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Synchronization Patterns\n",
    "\n",
    "### ğŸ”· CUDA C++ When to Sync (Primary)\n",
    "\n",
    "```cpp\n",
    "// sync_patterns.cu - Synchronization patterns\n",
    "\n",
    "// Pattern 1: Sync before reading child results\n",
    "__global__ void pattern1(float* data) {\n",
    "    childKernel<<<1, 32>>>(data);\n",
    "    \n",
    "    cudaDeviceSynchronize();  // REQUIRED!\n",
    "    \n",
    "    float result = data[0];  // Safe to read\n",
    "}\n",
    "\n",
    "// Pattern 2: Fire and forget (parent returns)\n",
    "__global__ void pattern2(float* data) {\n",
    "    childKernel<<<1, 32>>>(data);\n",
    "    \n",
    "    // NO sync - parent returns immediately\n",
    "    // Child continues running\n",
    "    // Result visible to HOST after host sync\n",
    "}\n",
    "\n",
    "// Pattern 3: Implicit sync at parent completion\n",
    "// When parent kernel ends, all children are synced\n",
    "// before control returns to host\n",
    "\n",
    "__global__ void parentFireForget(float* data) {\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        childKernel<<<1, 32>>>(data + i * 32);\n",
    "        // No explicit sync\n",
    "    }\n",
    "    // When parent ends, runtime syncs all children\n",
    "}\n",
    "\n",
    "// Host code:\n",
    "// parentFireForget<<<1, 1>>>(data);\n",
    "// cudaDeviceSynchronize();  // Waits for parent AND all children\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Error Checking (Primary)\n",
    "\n",
    "```cpp\n",
    "__global__ void errorCheckingPattern(float* data) {\n",
    "    // Launch child\n",
    "    childKernel<<<1000000, 1024>>>(data);  // Might fail!\n",
    "    \n",
    "    // Check for launch errors (device-side)\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"Launch error: %s\\n\", cudaGetErrorString(err));\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Sync and check for execution errors\n",
    "    err = cudaDeviceSynchronize();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"Execution error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d039cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Simple Recursion\n",
    "Write a kernel that recursively launches itself (with depth limit).\n",
    "\n",
    "### Exercise 2: Fan-Out Pattern\n",
    "One parent thread launches multiple children that process different data.\n",
    "\n",
    "### Exercise 3: Memory Communication\n",
    "Parent writes config to global memory, child reads it and writes results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a22b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           DYNAMIC PARALLELISM BASICS                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Launch from GPU:                                       â”‚\n",
    "â”‚  childKernel<<<blocks, threads>>>(args);                â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Synchronization:                                       â”‚\n",
    "â”‚  cudaDeviceSynchronize();  // Device-side               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Compile Flag:                                          â”‚\n",
    "â”‚  nvcc -rdc=true ...                                     â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Memory Rules:                                          â”‚\n",
    "â”‚  â€¢ Global: visible after sync                           â”‚\n",
    "â”‚  â€¢ Local/Shared: NOT visible to child                   â”‚\n",
    "â”‚  â€¢ Use __threadfence() for visibility                   â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Nesting Limit: 24 levels                               â”‚\n",
    "â”‚  Compute Capability: 3.5+                               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Next: Day 4 - Nested Kernel Patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
