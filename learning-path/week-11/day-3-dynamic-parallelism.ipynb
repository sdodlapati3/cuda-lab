{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a608f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"⚠️  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "print(\"⚠️  Dynamic Parallelism is a CUDA C++ feature (CC 3.5+)!\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e9f6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What is Dynamic Parallelism?\n",
    "\n",
    "### Concept\n",
    "\n",
    "```\n",
    "Traditional CUDA:\n",
    "━━━━━━━━━━━━━━━━━━\n",
    "Host (CPU) launches kernels on Device (GPU)\n",
    "\n",
    "[Host] ──launch──> [Kernel A]\n",
    "[Host] ──launch──> [Kernel B]\n",
    "[Host] ──launch──> [Kernel C]\n",
    "\n",
    "Dynamic Parallelism:\n",
    "━━━━━━━━━━━━━━━━━━━━━\n",
    "GPU kernels can launch other kernels!\n",
    "\n",
    "[Host] ──launch──> [Kernel A]\n",
    "                      │\n",
    "                      ├──launch──> [Child 1]\n",
    "                      ├──launch──> [Child 2]\n",
    "                      └──launch──> [Child 3]\n",
    "\n",
    "Use Cases:\n",
    "• Recursive algorithms (quicksort, tree traversal)\n",
    "• Adaptive mesh refinement\n",
    "• Work that generates more work\n",
    "• Data-dependent parallelism\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41428807",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Basic Child Kernel Launch\n",
    "\n",
    "### CUDA C++ Dynamic Parallelism (Primary)\n",
    "\n",
    "```cpp\n",
    "// dynamic_basic.cu - Basic child kernel launch\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// ============================================\n",
    "// Child Kernel - Launched from GPU\n",
    "// ============================================\n",
    "__global__ void childKernel(int parentId) {\n",
    "    int tid = threadIdx.x;\n",
    "    printf(\"  Child of parent %d: thread %d\\n\", parentId, tid);\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// Parent Kernel - Launches Children\n",
    "// ============================================\n",
    "__global__ void parentKernel() {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    printf(\"Parent thread %d launching child...\\n\", tid);\n",
    "    \n",
    "    // Launch child kernel from GPU!\n",
    "    childKernel<<<1, 4>>>(tid);\n",
    "    \n",
    "    // Wait for child to complete\n",
    "    cudaDeviceSynchronize();  // Device-side sync!\n",
    "    \n",
    "    printf(\"Parent thread %d: child completed\\n\", tid);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Host launching parent kernel...\\n\");\n",
    "    \n",
    "    parentKernel<<<1, 2>>>();\n",
    "    cudaDeviceSynchronize();  // Host-side sync\n",
    "    \n",
    "    printf(\"All done!\\n\");\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "**Compile with relocatable device code:**\n",
    "```bash\n",
    "nvcc -arch=sm_70 -rdc=true -o dynamic_basic dynamic_basic.cu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36d7b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Visibility\n",
    "\n",
    "### Memory Rules for Dynamic Parallelism\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│           MEMORY VISIBILITY RULES                       │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Global Memory:                                         │\n",
    "│  ✅ Parent writes visible to child after launch         │\n",
    "│  ✅ Child writes visible to parent after sync           │\n",
    "│  ⚠️  Must sync properly!                                │\n",
    "│                                                         │\n",
    "│  Local Memory:                                          │\n",
    "│  ❌ Parent's local memory NOT visible to child          │\n",
    "│  ❌ Child's local memory NOT visible to parent          │\n",
    "│                                                         │\n",
    "│  Shared Memory:                                         │\n",
    "│  ❌ Parent's shared NOT visible to child                │\n",
    "│  ❌ Each kernel has its own shared memory               │\n",
    "│                                                         │\n",
    "│  Constant Memory:                                       │\n",
    "│  ✅ Visible to all (set before host launch)             │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### CUDA C++ Memory Example (Primary)\n",
    "\n",
    "```cpp\n",
    "// dynamic_memory.cu - Memory visibility\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void childKernel(float* data, int idx) {\n",
    "    // Child can read parent's global memory writes\n",
    "    float parentValue = data[idx];\n",
    "    printf(\"Child: read %.1f from parent\\n\", parentValue);\n",
    "    \n",
    "    // Child writes to global memory\n",
    "    data[idx] = parentValue * 2.0f;\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(float* data) {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Parent writes to global memory\n",
    "    data[tid] = (float)(tid + 1) * 10.0f;\n",
    "    printf(\"Parent %d: wrote %.1f\\n\", tid, data[tid]);\n",
    "    \n",
    "    // __threadfence() ensures visibility before child launch\n",
    "    __threadfence();\n",
    "    \n",
    "    // Launch child\n",
    "    childKernel<<<1, 1>>>(data, tid);\n",
    "    \n",
    "    // Must sync to see child's writes!\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Now parent can see child's writes\n",
    "    printf(\"Parent %d: child wrote %.1f\\n\", tid, data[tid]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, 4 * sizeof(float));\n",
    "    \n",
    "    parentKernel<<<1, 4>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d104787",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Device-Side Streams\n",
    "\n",
    "### Streams in Dynamic Parallelism\n",
    "\n",
    "```cpp\n",
    "// dynamic_streams.cu - Device-side streams\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void childA(int* data) {\n",
    "    atomicAdd(data, 1);\n",
    "}\n",
    "\n",
    "__global__ void childB(int* data) {\n",
    "    atomicAdd(data, 10);\n",
    "}\n",
    "\n",
    "__global__ void parentKernel(int* data) {\n",
    "    // ============================================\n",
    "    // Default Stream (NULL)\n",
    "    // ============================================\n",
    "    // Children on default stream serialize with each other\n",
    "    // but are async with parent\n",
    "    \n",
    "    childA<<<1, 1>>>(data);  // On implicit NULL stream\n",
    "    childB<<<1, 1>>>(data);  // Waits for childA\n",
    "    \n",
    "    cudaDeviceSynchronize();  // Wait for both\n",
    "    printf(\"After default stream children: %d\\n\", *data);\n",
    "    \n",
    "    // ============================================\n",
    "    // Named Streams (Device-Side)\n",
    "    // ============================================\n",
    "    cudaStream_t stream1, stream2;\n",
    "    cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking);\n",
    "    cudaStreamCreateWithFlags(&stream2, cudaStreamNonBlocking);\n",
    "    \n",
    "    // Concurrent children on different streams\n",
    "    childA<<<1, 1, 0, stream1>>>(data);\n",
    "    childB<<<1, 1, 0, stream2>>>(data);\n",
    "    \n",
    "    // Sync specific streams\n",
    "    cudaStreamSynchronize(stream1);\n",
    "    cudaStreamSynchronize(stream2);\n",
    "    \n",
    "    printf(\"After named stream children: %d\\n\", *data);\n",
    "    \n",
    "    cudaStreamDestroy(stream1);\n",
    "    cudaStreamDestroy(stream2);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int* d_data;\n",
    "    cudaMalloc(&d_data, sizeof(int));\n",
    "    cudaMemset(d_data, 0, sizeof(int));\n",
    "    \n",
    "    parentKernel<<<1, 1>>>(d_data);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12bfa2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Synchronization Patterns\n",
    "\n",
    "### When to Sync\n",
    "\n",
    "```cpp\n",
    "// sync_patterns.cu - Synchronization patterns\n",
    "\n",
    "// Pattern 1: Sync before reading child results\n",
    "__global__ void pattern1(float* data) {\n",
    "    childKernel<<<1, 32>>>(data);\n",
    "    \n",
    "    cudaDeviceSynchronize();  // REQUIRED!\n",
    "    \n",
    "    float result = data[0];  // Safe to read\n",
    "}\n",
    "\n",
    "// Pattern 2: Fire and forget (parent returns)\n",
    "__global__ void pattern2(float* data) {\n",
    "    childKernel<<<1, 32>>>(data);\n",
    "    \n",
    "    // NO sync - parent returns immediately\n",
    "    // Child continues running\n",
    "    // Result visible to HOST after host sync\n",
    "}\n",
    "\n",
    "// Pattern 3: Implicit sync at parent completion\n",
    "// When parent kernel ends, all children are synced\n",
    "// before control returns to host\n",
    "\n",
    "__global__ void parentFireForget(float* data) {\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        childKernel<<<1, 32>>>(data + i * 32);\n",
    "        // No explicit sync\n",
    "    }\n",
    "    // When parent ends, runtime syncs all children\n",
    "}\n",
    "\n",
    "// Host code:\n",
    "// parentFireForget<<<1, 1>>>(data);\n",
    "// cudaDeviceSynchronize();  // Waits for parent AND all children\n",
    "```\n",
    "\n",
    "### Error Checking\n",
    "\n",
    "```cpp\n",
    "__global__ void errorCheckingPattern(float* data) {\n",
    "    // Launch child\n",
    "    childKernel<<<1000000, 1024>>>(data);  // Might fail!\n",
    "    \n",
    "    // Check for launch errors (device-side)\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"Launch error: %s\\n\", cudaGetErrorString(err));\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Sync and check for execution errors\n",
    "    err = cudaDeviceSynchronize();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"Execution error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d039cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Simple Recursion\n",
    "Write a kernel that recursively launches itself (with depth limit).\n",
    "\n",
    "### Exercise 2: Fan-Out Pattern\n",
    "One parent thread launches multiple children that process different data.\n",
    "\n",
    "### Exercise 3: Memory Communication\n",
    "Parent writes config to global memory, child reads it and writes results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a22b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│           DYNAMIC PARALLELISM BASICS                    │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Launch from GPU:                                       │\n",
    "│  childKernel<<<blocks, threads>>>(args);                │\n",
    "│                                                         │\n",
    "│  Synchronization:                                       │\n",
    "│  cudaDeviceSynchronize();  // Device-side               │\n",
    "│                                                         │\n",
    "│  Compile Flag:                                          │\n",
    "│  nvcc -rdc=true ...                                     │\n",
    "│                                                         │\n",
    "│  Memory Rules:                                          │\n",
    "│  • Global: visible after sync                           │\n",
    "│  • Local/Shared: NOT visible to child                   │\n",
    "│  • Use __threadfence() for visibility                   │\n",
    "│                                                         │\n",
    "│  Nesting Limit: 24 levels                               │\n",
    "│  Compute Capability: 3.5+                               │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Next: Day 4 - Nested Kernel Patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
