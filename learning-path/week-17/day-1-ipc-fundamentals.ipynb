{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2454ad6f",
   "metadata": {},
   "source": [
    "## Why IPC?\n",
    "\n",
    "GPU memory is normally **process-private**. But many scenarios need memory sharing:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    Use Cases for IPC                     │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ 1. Inference Server: Model in one process, requests     │\n",
    "│    handled by worker processes                          │\n",
    "│                                                         │\n",
    "│ 2. Pipeline Processing: Each stage in separate process  │\n",
    "│    (fault isolation, independent scaling)               │\n",
    "│                                                         │\n",
    "│ 3. MPI Applications: Processes on same node share data  │\n",
    "│    via GPU memory instead of copying through host       │\n",
    "│                                                         │\n",
    "│ 4. Multi-Tenant: Different users share GPU resources    │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0179fd",
   "metadata": {},
   "source": [
    "## IPC API Overview\n",
    "\n",
    "```cpp\n",
    "// Producer process: Export memory handle\n",
    "cudaIpcMemHandle_t handle;\n",
    "cudaMalloc(&d_ptr, size);\n",
    "cudaIpcGetMemHandle(&handle, d_ptr);  // Get shareable handle\n",
    "// Send 'handle' to consumer (via file, socket, shared memory, etc.)\n",
    "\n",
    "// Consumer process: Import memory handle  \n",
    "cudaIpcMemHandle_t handle;  // Received from producer\n",
    "void* d_ptr;\n",
    "cudaIpcOpenMemHandle(&d_ptr, handle, cudaIpcMemLazyEnablePeerAccess);\n",
    "// Now d_ptr points to SAME GPU memory as producer!\n",
    "\n",
    "// When done\n",
    "cudaIpcCloseMemHandle(d_ptr);  // Consumer closes handle\n",
    "cudaFree(d_ptr);               // Producer frees memory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and CUDA version\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv\n",
    "!nvcc --version | tail -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b5cf2",
   "metadata": {},
   "source": [
    "## Example 1: Basic IPC with File-Based Handle Transfer\n",
    "\n",
    "We'll create two programs:\n",
    "1. **Producer**: Allocates memory, writes data, exports handle to file\n",
    "2. **Consumer**: Reads handle from file, opens memory, reads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e545b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ipc_producer.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void initData(float* data, int n, float value) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = value + idx * 0.001f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    printf(\"=== IPC Producer ===\\n\");\n",
    "    \n",
    "    // Allocate GPU memory\n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, size));\n",
    "    printf(\"Allocated %zu bytes at %p\\n\", size, d_data);\n",
    "    \n",
    "    // Initialize data\n",
    "    initData<<<(N+255)/256, 256>>>(d_data, N, 42.0f);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    printf(\"Initialized data with pattern\\n\");\n",
    "    \n",
    "    // Get IPC handle\n",
    "    cudaIpcMemHandle_t handle;\n",
    "    CHECK_CUDA(cudaIpcGetMemHandle(&handle, d_data));\n",
    "    printf(\"Got IPC handle (64 bytes)\\n\");\n",
    "    \n",
    "    // Write handle to file\n",
    "    FILE* f = fopen(\"/tmp/cuda_ipc_handle.bin\", \"wb\");\n",
    "    if (!f) { perror(\"fopen\"); exit(1); }\n",
    "    fwrite(&handle, sizeof(handle), 1, f);\n",
    "    fclose(f);\n",
    "    printf(\"Wrote handle to /tmp/cuda_ipc_handle.bin\\n\");\n",
    "    \n",
    "    // Signal ready and wait\n",
    "    FILE* ready = fopen(\"/tmp/cuda_ipc_ready\", \"w\");\n",
    "    fclose(ready);\n",
    "    printf(\"Signaled ready. Waiting for consumer...\\n\");\n",
    "    printf(\"(Run ipc_consumer in another terminal, or press Ctrl+C to exit)\\n\");\n",
    "    \n",
    "    // Wait for consumer to finish\n",
    "    while (access(\"/tmp/cuda_ipc_done\", F_OK) != 0) {\n",
    "        sleep(1);\n",
    "    }\n",
    "    \n",
    "    printf(\"Consumer finished. Cleaning up...\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "    remove(\"/tmp/cuda_ipc_handle.bin\");\n",
    "    remove(\"/tmp/cuda_ipc_ready\");\n",
    "    remove(\"/tmp/cuda_ipc_done\");\n",
    "    \n",
    "    printf(\"Done!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ipc_consumer.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void verifyData(float* data, int n, float expected_base, int* errors) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float expected = expected_base + idx * 0.001f;\n",
    "        if (fabsf(data[idx] - expected) > 0.0001f) {\n",
    "            atomicAdd(errors, 1);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    \n",
    "    printf(\"=== IPC Consumer ===\\n\");\n",
    "    \n",
    "    // Wait for producer\n",
    "    printf(\"Waiting for producer...\\n\");\n",
    "    while (access(\"/tmp/cuda_ipc_ready\", F_OK) != 0) {\n",
    "        sleep(1);\n",
    "    }\n",
    "    printf(\"Producer ready!\\n\");\n",
    "    \n",
    "    // Read handle from file\n",
    "    cudaIpcMemHandle_t handle;\n",
    "    FILE* f = fopen(\"/tmp/cuda_ipc_handle.bin\", \"rb\");\n",
    "    if (!f) { perror(\"fopen\"); exit(1); }\n",
    "    fread(&handle, sizeof(handle), 1, f);\n",
    "    fclose(f);\n",
    "    printf(\"Read IPC handle from file\\n\");\n",
    "    \n",
    "    // Open the shared memory\n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaIpcOpenMemHandle((void**)&d_data, handle, \n",
    "                                     cudaIpcMemLazyEnablePeerAccess));\n",
    "    printf(\"Opened shared memory at %p\\n\", d_data);\n",
    "    \n",
    "    // Verify data\n",
    "    int* d_errors;\n",
    "    CHECK_CUDA(cudaMalloc(&d_errors, sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_errors, 0, sizeof(int)));\n",
    "    \n",
    "    verifyData<<<(N+255)/256, 256>>>(d_data, N, 42.0f, d_errors);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    int h_errors;\n",
    "    CHECK_CUDA(cudaMemcpy(&h_errors, d_errors, sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    if (h_errors == 0) {\n",
    "        printf(\"✓ Data verification PASSED! Shared memory works!\\n\");\n",
    "    } else {\n",
    "        printf(\"✗ Data verification FAILED: %d errors\\n\", h_errors);\n",
    "    }\n",
    "    \n",
    "    // Read first few values\n",
    "    float h_data[5];\n",
    "    CHECK_CUDA(cudaMemcpy(h_data, d_data, 5*sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    printf(\"First 5 values: %.3f, %.3f, %.3f, %.3f, %.3f\\n\",\n",
    "           h_data[0], h_data[1], h_data[2], h_data[3], h_data[4]);\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaFree(d_errors));\n",
    "    CHECK_CUDA(cudaIpcCloseMemHandle(d_data));\n",
    "    \n",
    "    // Signal done\n",
    "    FILE* done = fopen(\"/tmp/cuda_ipc_done\", \"w\");\n",
    "    fclose(done);\n",
    "    \n",
    "    printf(\"Done!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile both programs\n",
    "!nvcc -O3 -arch=sm_80 ipc_producer.cu -o ipc_producer\n",
    "!nvcc -O3 -arch=sm_80 ipc_consumer.cu -o ipc_consumer\n",
    "print(\"Compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2a10c",
   "metadata": {},
   "source": [
    "## Running the IPC Example\n",
    "\n",
    "On HPC cluster, open **two terminals**:\n",
    "\n",
    "**Terminal 1 (same GPU node):**\n",
    "```bash\n",
    "srun --partition=h100flex --gres=gpu:1 --time=00:10:00 --pty bash\n",
    "cd cuda-lab/learning-path/week-17\n",
    "./ipc_producer\n",
    "```\n",
    "\n",
    "**Terminal 2 (same GPU node - use srun with --jobid):**\n",
    "```bash\n",
    "srun --jobid=<JOBID> --pty bash\n",
    "cd cuda-lab/learning-path/week-17\n",
    "./ipc_consumer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d95d89",
   "metadata": {},
   "source": [
    "## Example 2: Single-Process IPC Demo\n",
    "\n",
    "For easier testing, here's a fork-based version that runs in one process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ipc_fork_demo.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <sys/wait.h>\n",
    "#include <unistd.h>\n",
    "#include <sys/mman.h>\n",
    "#include <fcntl.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"[PID %d] CUDA error: %s\\n\", getpid(), cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void producer_init(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = 100.0f + idx;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void consumer_transform(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= 2.0f;  // Consumer modifies producer's data!\n",
    "    }\n",
    "}\n",
    "\n",
    "struct SharedData {\n",
    "    cudaIpcMemHandle_t handle;\n",
    "    volatile int ready;\n",
    "    volatile int done;\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    const size_t size = N * sizeof(float);\n",
    "    \n",
    "    printf(\"=== IPC Fork Demo ===\\n\\n\");\n",
    "    \n",
    "    // Create shared memory for communication\n",
    "    SharedData* shared = (SharedData*)mmap(NULL, sizeof(SharedData),\n",
    "        PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n",
    "    shared->ready = 0;\n",
    "    shared->done = 0;\n",
    "    \n",
    "    pid_t pid = fork();\n",
    "    \n",
    "    if (pid == 0) {\n",
    "        // ========== CHILD (Consumer) ==========\n",
    "        printf(\"[Consumer PID %d] Started\\n\", getpid());\n",
    "        \n",
    "        // Wait for producer\n",
    "        while (!shared->ready) { usleep(1000); }\n",
    "        printf(\"[Consumer] Producer signaled ready\\n\");\n",
    "        \n",
    "        // Open shared GPU memory\n",
    "        float* d_data;\n",
    "        CHECK_CUDA(cudaIpcOpenMemHandle((void**)&d_data, shared->handle,\n",
    "                                         cudaIpcMemLazyEnablePeerAccess));\n",
    "        printf(\"[Consumer] Opened shared memory\\n\");\n",
    "        \n",
    "        // Transform the data (modifies producer's memory!)\n",
    "        consumer_transform<<<(N+255)/256, 256>>>(d_data, N);\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());\n",
    "        printf(\"[Consumer] Transformed data (multiplied by 2)\\n\");\n",
    "        \n",
    "        // Close handle\n",
    "        CHECK_CUDA(cudaIpcCloseMemHandle(d_data));\n",
    "        \n",
    "        shared->done = 1;\n",
    "        printf(\"[Consumer] Done!\\n\");\n",
    "        exit(0);\n",
    "        \n",
    "    } else {\n",
    "        // ========== PARENT (Producer) ==========\n",
    "        printf(\"[Producer PID %d] Started\\n\", getpid());\n",
    "        \n",
    "        // Allocate and initialize\n",
    "        float* d_data;\n",
    "        CHECK_CUDA(cudaMalloc(&d_data, size));\n",
    "        producer_init<<<(N+255)/256, 256>>>(d_data, N);\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());\n",
    "        printf(\"[Producer] Initialized data: [100, 101, 102, ...]\\n\");\n",
    "        \n",
    "        // Get IPC handle\n",
    "        CHECK_CUDA(cudaIpcGetMemHandle(&shared->handle, d_data));\n",
    "        printf(\"[Producer] Created IPC handle\\n\");\n",
    "        \n",
    "        // Signal consumer\n",
    "        shared->ready = 1;\n",
    "        printf(\"[Producer] Signaled ready, waiting for consumer...\\n\");\n",
    "        \n",
    "        // Wait for consumer\n",
    "        while (!shared->done) { usleep(1000); }\n",
    "        \n",
    "        // Verify consumer's transformation\n",
    "        float h_data[5];\n",
    "        CHECK_CUDA(cudaMemcpy(h_data, d_data, 5*sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        printf(\"[Producer] After consumer transform: [%.0f, %.0f, %.0f, %.0f, %.0f]\\n\",\n",
    "               h_data[0], h_data[1], h_data[2], h_data[3], h_data[4]);\n",
    "        printf(\"[Producer] Expected: [200, 202, 204, 206, 208] ✓\\n\");\n",
    "        \n",
    "        // Cleanup\n",
    "        CHECK_CUDA(cudaFree(d_data));\n",
    "        wait(NULL);  // Wait for child\n",
    "        munmap(shared, sizeof(SharedData));\n",
    "        \n",
    "        printf(\"\\n=== IPC Demo Complete! ===\\n\");\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 ipc_fork_demo.cu -o ipc_fork_demo && ./ipc_fork_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335090d1",
   "metadata": {},
   "source": [
    "## IPC Requirements and Limitations\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    IPC Requirements                      │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ ✓ Same GPU (or peer-accessible GPUs)                    │\n",
    "│ ✓ Same machine (no network IPC)                         │\n",
    "│ ✓ Memory allocated with cudaMalloc (not cudaMallocHost) │\n",
    "│ ✓ Producer must keep memory allocated until done        │\n",
    "│ ✓ 64-bit OS required                                    │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                      Limitations                         │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ ✗ Cannot share cudaMallocManaged() memory               │\n",
    "│ ✗ Cannot share across different GPU architectures       │\n",
    "│ ✗ Handle is only valid on same machine                  │\n",
    "│ ✗ Maximum open handles per process (OS dependent)       │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03307162",
   "metadata": {},
   "source": [
    "## Event IPC: Synchronization Across Processes\n",
    "\n",
    "CUDA also supports sharing **events** for cross-process synchronization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23845a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ipc_events_demo.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <sys/wait.h>\n",
    "#include <unistd.h>\n",
    "#include <sys/mman.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void slowKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Simulate work\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            val = sinf(val) + cosf(val);\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "struct SharedData {\n",
    "    cudaIpcMemHandle_t memHandle;\n",
    "    cudaIpcEventHandle_t eventHandle;\n",
    "    volatile int ready;\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024 * 1024;\n",
    "    \n",
    "    printf(\"=== IPC Events Demo ===\\n\\n\");\n",
    "    \n",
    "    SharedData* shared = (SharedData*)mmap(NULL, sizeof(SharedData),\n",
    "        PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n",
    "    shared->ready = 0;\n",
    "    \n",
    "    pid_t pid = fork();\n",
    "    \n",
    "    if (pid == 0) {\n",
    "        // ========== CHILD (Waiter) ==========\n",
    "        while (!shared->ready) usleep(1000);\n",
    "        \n",
    "        // Open event from handle\n",
    "        cudaEvent_t event;\n",
    "        CHECK_CUDA(cudaIpcOpenEventHandle(&event, shared->eventHandle));\n",
    "        \n",
    "        printf(\"[Child] Waiting for producer's kernel to complete...\\n\");\n",
    "        auto start = std::chrono::high_resolution_clock::now();\n",
    "        \n",
    "        // Wait for event (blocks until producer's kernel is done)\n",
    "        CHECK_CUDA(cudaEventSynchronize(event));\n",
    "        \n",
    "        auto end = std::chrono::high_resolution_clock::now();\n",
    "        float ms = std::chrono::duration<float, std::milli>(end - start).count();\n",
    "        printf(\"[Child] Event signaled! Waited %.2f ms\\n\", ms);\n",
    "        \n",
    "        exit(0);\n",
    "        \n",
    "    } else {\n",
    "        // ========== PARENT (Producer) ==========\n",
    "        float* d_data;\n",
    "        CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(float)));\n",
    "        \n",
    "        // Create IPC-capable event\n",
    "        cudaEvent_t event;\n",
    "        CHECK_CUDA(cudaEventCreate(&event, cudaEventInterprocess | cudaEventDisableTiming));\n",
    "        \n",
    "        // Get handles\n",
    "        CHECK_CUDA(cudaIpcGetMemHandle(&shared->memHandle, d_data));\n",
    "        CHECK_CUDA(cudaIpcGetEventHandle(&shared->eventHandle, event));\n",
    "        \n",
    "        shared->ready = 1;\n",
    "        printf(\"[Parent] Starting slow kernel...\\n\");\n",
    "        \n",
    "        // Launch kernel and record event when done\n",
    "        slowKernel<<<(N+255)/256, 256>>>(d_data, N);\n",
    "        CHECK_CUDA(cudaEventRecord(event));\n",
    "        \n",
    "        printf(\"[Parent] Kernel launched, doing other work...\\n\");\n",
    "        usleep(100000);  // Simulate other work\n",
    "        \n",
    "        CHECK_CUDA(cudaEventSynchronize(event));\n",
    "        printf(\"[Parent] Kernel complete\\n\");\n",
    "        \n",
    "        wait(NULL);\n",
    "        CHECK_CUDA(cudaEventDestroy(event));\n",
    "        CHECK_CUDA(cudaFree(d_data));\n",
    "        munmap(shared, sizeof(SharedData));\n",
    "        \n",
    "        printf(\"\\n=== Event IPC Complete! ===\\n\");\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95017d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 -std=c++14 ipc_events_demo.cu -o ipc_events_demo && ./ipc_events_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a30bc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `cudaIpcGetMemHandle()` | Export GPU memory to shareable handle |\n",
    "| `cudaIpcOpenMemHandle()` | Import handle to access shared memory |\n",
    "| `cudaIpcCloseMemHandle()` | Release imported handle |\n",
    "| `cudaIpcGetEventHandle()` | Export event for cross-process sync |\n",
    "| `cudaIpcOpenEventHandle()` | Import event handle |\n",
    "\n",
    "**Key Points:**\n",
    "1. IPC enables zero-copy memory sharing between processes\n",
    "2. Both memory and events can be shared\n",
    "3. Processes must be on the same machine and GPU\n",
    "4. Handle transfer mechanism is user's responsibility (file, socket, shared memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187a599",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Modify the producer to write a recognizable pattern (e.g., Fibonacci sequence)\n",
    "2. Implement a ring buffer using IPC for streaming data between processes\n",
    "3. Add error handling for the case where consumer opens handle before producer allocates"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
