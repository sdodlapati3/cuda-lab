{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b697b419",
   "metadata": {},
   "source": [
    "## Pattern 1: Producer-Consumer Pipeline\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   Producer-Consumer Pipeline                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                              â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚   â”‚Producer â”‚â”€â”€â”€â”€â”€â–¶â”‚ GPU Memory  â”‚â”€â”€â”€â”€â”€â–¶â”‚ Consumer â”‚        â”‚\n",
    "â”‚   â”‚ Process â”‚      â”‚ (Shared IPC)â”‚      â”‚ Process  â”‚        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "â”‚        â”‚                  â”‚                   â”‚              â”‚\n",
    "â”‚        â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”           â”‚              â”‚\n",
    "â”‚        â”‚           â”‚ Ring Buffer â”‚           â”‚              â”‚\n",
    "â”‚        â”‚           â”‚  N Slots    â”‚           â”‚              â”‚\n",
    "â”‚        â–¼           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â–¼              â”‚\n",
    "â”‚   Write slot 0         ....            Read slot 0          â”‚\n",
    "â”‚   Write slot 1                         Read slot 1          â”‚\n",
    "â”‚      ...                                  ...               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ipc_ring_buffer.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <sys/wait.h>\n",
    "#include <unistd.h>\n",
    "#include <sys/mman.h>\n",
    "#include <atomic>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "constexpr int NUM_SLOTS = 4;\n",
    "constexpr int SLOT_SIZE = 1024 * 1024;  // 1M floats per slot\n",
    "constexpr int NUM_ITERATIONS = 10;\n",
    "\n",
    "__global__ void produceData(float* slot, int slotIdx, int iteration) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < SLOT_SIZE) {\n",
    "        slot[idx] = (float)(iteration * 1000 + slotIdx * 100 + idx % 100);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void consumeData(float* slot, int slotIdx, int iteration, float* result) {\n",
    "    __shared__ float sdata[256];\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Compute sum of slot data (reduction)\n",
    "    float val = (idx < SLOT_SIZE) ? slot[idx] : 0.0f;\n",
    "    sdata[threadIdx.x] = val;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (threadIdx.x < s) sdata[threadIdx.x] += sdata[threadIdx.x + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (threadIdx.x == 0) atomicAdd(result, sdata[0]);\n",
    "}\n",
    "\n",
    "struct RingBuffer {\n",
    "    cudaIpcMemHandle_t handles[NUM_SLOTS];\n",
    "    std::atomic<int> writeIdx;  // Next slot to write\n",
    "    std::atomic<int> readIdx;   // Next slot to read\n",
    "    std::atomic<int> count;     // Number of filled slots\n",
    "    std::atomic<bool> done;\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== IPC Ring Buffer Demo ===\\n\");\n",
    "    printf(\"Slots: %d, Size per slot: %d floats\\n\\n\", NUM_SLOTS, SLOT_SIZE);\n",
    "    \n",
    "    // Create shared ring buffer metadata\n",
    "    RingBuffer* ring = (RingBuffer*)mmap(NULL, sizeof(RingBuffer),\n",
    "        PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n",
    "    ring->writeIdx = 0;\n",
    "    ring->readIdx = 0;\n",
    "    ring->count = 0;\n",
    "    ring->done = false;\n",
    "    \n",
    "    pid_t pid = fork();\n",
    "    \n",
    "    if (pid == 0) {\n",
    "        // ========== CONSUMER PROCESS ==========\n",
    "        usleep(100000);  // Let producer initialize\n",
    "        \n",
    "        // Open all slot handles\n",
    "        float* d_slots[NUM_SLOTS];\n",
    "        for (int i = 0; i < NUM_SLOTS; i++) {\n",
    "            CHECK_CUDA(cudaIpcOpenMemHandle((void**)&d_slots[i], ring->handles[i],\n",
    "                                             cudaIpcMemLazyEnablePeerAccess));\n",
    "        }\n",
    "        \n",
    "        float* d_result;\n",
    "        CHECK_CUDA(cudaMalloc(&d_result, sizeof(float)));\n",
    "        \n",
    "        int consumed = 0;\n",
    "        while (consumed < NUM_ITERATIONS) {\n",
    "            // Wait for data\n",
    "            while (ring->count == 0 && !ring->done) {\n",
    "                usleep(100);\n",
    "            }\n",
    "            \n",
    "            if (ring->count > 0) {\n",
    "                int slot = ring->readIdx % NUM_SLOTS;\n",
    "                \n",
    "                CHECK_CUDA(cudaMemset(d_result, 0, sizeof(float)));\n",
    "                consumeData<<<(SLOT_SIZE+255)/256, 256>>>(d_slots[slot], slot, consumed, d_result);\n",
    "                CHECK_CUDA(cudaDeviceSynchronize());\n",
    "                \n",
    "                float h_result;\n",
    "                CHECK_CUDA(cudaMemcpy(&h_result, d_result, sizeof(float), cudaMemcpyDeviceToHost));\n",
    "                printf(\"[Consumer] Iteration %d, Slot %d, Sum: %.2e\\n\", consumed, slot, h_result);\n",
    "                \n",
    "                ring->readIdx++;\n",
    "                ring->count--;\n",
    "                consumed++;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Cleanup\n",
    "        for (int i = 0; i < NUM_SLOTS; i++) {\n",
    "            CHECK_CUDA(cudaIpcCloseMemHandle(d_slots[i]));\n",
    "        }\n",
    "        CHECK_CUDA(cudaFree(d_result));\n",
    "        \n",
    "        printf(\"[Consumer] Done!\\n\");\n",
    "        exit(0);\n",
    "        \n",
    "    } else {\n",
    "        // ========== PRODUCER PROCESS ==========\n",
    "        \n",
    "        // Allocate ring buffer slots\n",
    "        float* d_slots[NUM_SLOTS];\n",
    "        for (int i = 0; i < NUM_SLOTS; i++) {\n",
    "            CHECK_CUDA(cudaMalloc(&d_slots[i], SLOT_SIZE * sizeof(float)));\n",
    "            CHECK_CUDA(cudaIpcGetMemHandle(&ring->handles[i], d_slots[i]));\n",
    "        }\n",
    "        printf(\"[Producer] Allocated %d slots\\n\", NUM_SLOTS);\n",
    "        \n",
    "        for (int iter = 0; iter < NUM_ITERATIONS; iter++) {\n",
    "            // Wait for free slot\n",
    "            while (ring->count >= NUM_SLOTS) {\n",
    "                usleep(100);\n",
    "            }\n",
    "            \n",
    "            int slot = ring->writeIdx % NUM_SLOTS;\n",
    "            produceData<<<(SLOT_SIZE+255)/256, 256>>>(d_slots[slot], slot, iter);\n",
    "            CHECK_CUDA(cudaDeviceSynchronize());\n",
    "            \n",
    "            printf(\"[Producer] Iteration %d, Slot %d filled\\n\", iter, slot);\n",
    "            \n",
    "            ring->writeIdx++;\n",
    "            ring->count++;\n",
    "        }\n",
    "        \n",
    "        ring->done = true;\n",
    "        \n",
    "        // Wait for consumer\n",
    "        wait(NULL);\n",
    "        \n",
    "        // Cleanup\n",
    "        for (int i = 0; i < NUM_SLOTS; i++) {\n",
    "            CHECK_CUDA(cudaFree(d_slots[i]));\n",
    "        }\n",
    "        munmap(ring, sizeof(RingBuffer));\n",
    "        \n",
    "        printf(\"\\n[Producer] Done! Pipeline complete.\\n\");\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 -std=c++14 ipc_ring_buffer.cu -o ipc_ring_buffer && ./ipc_ring_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c4847",
   "metadata": {},
   "source": [
    "## Pattern 2: Inference Server Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Inference Server Pattern                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                â”‚\n",
    "â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
    "â”‚                     â”‚  Model Server   â”‚                        â”‚\n",
    "â”‚                     â”‚ (Holds Weights) â”‚                        â”‚\n",
    "â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
    "â”‚                              â”‚                                 â”‚\n",
    "â”‚                    IPC Memory Handles                          â”‚\n",
    "â”‚                              â”‚                                 â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚         â–¼                    â–¼                    â–¼            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚   â”‚ Worker 1 â”‚        â”‚ Worker 2 â”‚        â”‚ Worker N â”‚        â”‚\n",
    "â”‚   â”‚ (Batch A)â”‚        â”‚ (Batch B)â”‚        â”‚ (Batch N)â”‚        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "â”‚                                                                â”‚\n",
    "â”‚   Workers share model weights via IPC                          â”‚\n",
    "â”‚   Each worker has own input/output buffers                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68552c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_server.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <sys/wait.h>\n",
    "#include <unistd.h>\n",
    "#include <sys/mman.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "constexpr int MODEL_SIZE = 10 * 1024 * 1024;  // 10M parameters\n",
    "constexpr int BATCH_SIZE = 1024;\n",
    "constexpr int NUM_WORKERS = 3;\n",
    "\n",
    "// Simple \"inference\" kernel - matrix-vector multiply\n",
    "__global__ void inference(const float* weights, const float* input, \n",
    "                          float* output, int modelSize, int batchSize) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < batchSize) {\n",
    "        float sum = 0.0f;\n",
    "        // Each output is dot product with subset of weights\n",
    "        int stride = modelSize / batchSize;\n",
    "        for (int i = 0; i < stride; i++) {\n",
    "            sum += weights[idx * stride + i] * input[idx];\n",
    "        }\n",
    "        output[idx] = tanhf(sum);  // Activation\n",
    "    }\n",
    "}\n",
    "\n",
    "struct ServerState {\n",
    "    cudaIpcMemHandle_t weightsHandle;\n",
    "    volatile bool ready;\n",
    "    volatile int workersFinished;\n",
    "};\n",
    "\n",
    "void runWorker(int workerId, ServerState* state) {\n",
    "    printf(\"[Worker %d] Started (PID %d)\\n\", workerId, getpid());\n",
    "    \n",
    "    // Wait for server\n",
    "    while (!state->ready) usleep(1000);\n",
    "    \n",
    "    // Open shared weights\n",
    "    float* d_weights;\n",
    "    CHECK_CUDA(cudaIpcOpenMemHandle((void**)&d_weights, state->weightsHandle,\n",
    "                                     cudaIpcMemLazyEnablePeerAccess));\n",
    "    printf(\"[Worker %d] Opened shared model weights\\n\", workerId);\n",
    "    \n",
    "    // Allocate local input/output\n",
    "    float *d_input, *d_output;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, BATCH_SIZE * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, BATCH_SIZE * sizeof(float)));\n",
    "    \n",
    "    // Prepare input (different for each worker)\n",
    "    float* h_input = new float[BATCH_SIZE];\n",
    "    for (int i = 0; i < BATCH_SIZE; i++) {\n",
    "        h_input[i] = (float)(workerId + 1) * 0.1f;\n",
    "    }\n",
    "    CHECK_CUDA(cudaMemcpy(d_input, h_input, BATCH_SIZE * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Run inference\n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 100; i++) {  // 100 inference runs\n",
    "        inference<<<(BATCH_SIZE+255)/256, 256>>>(d_weights, d_input, d_output, MODEL_SIZE, BATCH_SIZE);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    \n",
    "    float ms;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&ms, start, stop));\n",
    "    printf(\"[Worker %d] 100 inferences in %.2f ms (%.2f ms/inference)\\n\", \n",
    "           workerId, ms, ms/100);\n",
    "    \n",
    "    // Get sample output\n",
    "    float h_output[5];\n",
    "    CHECK_CUDA(cudaMemcpy(h_output, d_output, 5*sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    printf(\"[Worker %d] Sample outputs: [%.4f, %.4f, %.4f, %.4f, %.4f]\\n\",\n",
    "           workerId, h_output[0], h_output[1], h_output[2], h_output[3], h_output[4]);\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaIpcCloseMemHandle(d_weights));\n",
    "    CHECK_CUDA(cudaFree(d_input));\n",
    "    CHECK_CUDA(cudaFree(d_output));\n",
    "    delete[] h_input;\n",
    "    \n",
    "    __sync_fetch_and_add(&state->workersFinished, 1);\n",
    "    printf(\"[Worker %d] Done!\\n\", workerId);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Inference Server Demo ===\\n\");\n",
    "    printf(\"Model size: %d parameters (%.1f MB)\\n\", MODEL_SIZE, MODEL_SIZE * 4.0f / 1e6);\n",
    "    printf(\"Workers: %d\\n\\n\", NUM_WORKERS);\n",
    "    \n",
    "    ServerState* state = (ServerState*)mmap(NULL, sizeof(ServerState),\n",
    "        PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n",
    "    state->ready = false;\n",
    "    state->workersFinished = 0;\n",
    "    \n",
    "    // Fork workers\n",
    "    for (int w = 0; w < NUM_WORKERS; w++) {\n",
    "        if (fork() == 0) {\n",
    "            runWorker(w, state);\n",
    "            exit(0);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // ========== SERVER PROCESS ==========\n",
    "    printf(\"[Server] Allocating model weights...\\n\");\n",
    "    \n",
    "    float* d_weights;\n",
    "    CHECK_CUDA(cudaMalloc(&d_weights, MODEL_SIZE * sizeof(float)));\n",
    "    \n",
    "    // Initialize weights (normally would load from file)\n",
    "    float* h_weights = new float[MODEL_SIZE];\n",
    "    for (int i = 0; i < MODEL_SIZE; i++) {\n",
    "        h_weights[i] = ((float)rand() / RAND_MAX - 0.5f) * 0.1f;\n",
    "    }\n",
    "    CHECK_CUDA(cudaMemcpy(d_weights, h_weights, MODEL_SIZE * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    printf(\"[Server] Model weights initialized\\n\");\n",
    "    \n",
    "    // Create IPC handle\n",
    "    CHECK_CUDA(cudaIpcGetMemHandle(&state->weightsHandle, d_weights));\n",
    "    printf(\"[Server] IPC handle created\\n\");\n",
    "    \n",
    "    // Signal workers\n",
    "    state->ready = true;\n",
    "    printf(\"[Server] Workers notified, waiting for completion...\\n\\n\");\n",
    "    \n",
    "    // Wait for all workers\n",
    "    while (state->workersFinished < NUM_WORKERS) {\n",
    "        usleep(10000);\n",
    "    }\n",
    "    \n",
    "    // Reap children\n",
    "    for (int w = 0; w < NUM_WORKERS; w++) {\n",
    "        wait(NULL);\n",
    "    }\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaFree(d_weights));\n",
    "    delete[] h_weights;\n",
    "    munmap(state, sizeof(ServerState));\n",
    "    \n",
    "    printf(\"\\n[Server] All workers completed. Server shutdown.\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 inference_server.cu -o inference_server && ./inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f403a6",
   "metadata": {},
   "source": [
    "## Pattern 3: Multi-GPU IPC with Peer Access\n",
    "\n",
    "When processes use different GPUs, we need peer access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile multi_gpu_ipc.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <sys/wait.h>\n",
    "#include <unistd.h>\n",
    "#include <sys/mman.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void writePattern(float* data, int n, float value) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) data[idx] = value + idx;\n",
    "}\n",
    "\n",
    "__global__ void readAndVerify(float* data, int n, float expected, int* errors) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        if (fabsf(data[idx] - (expected + idx)) > 0.001f) {\n",
    "            atomicAdd(errors, 1);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "struct SharedState {\n",
    "    cudaIpcMemHandle_t handle;\n",
    "    volatile bool ready;\n",
    "    volatile bool done;\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    int numDevices;\n",
    "    CHECK_CUDA(cudaGetDeviceCount(&numDevices));\n",
    "    printf(\"=== Multi-GPU IPC Demo ===\\n\");\n",
    "    printf(\"Found %d GPU(s)\\n\\n\", numDevices);\n",
    "    \n",
    "    if (numDevices < 2) {\n",
    "        printf(\"Need at least 2 GPUs for this demo.\\n\");\n",
    "        printf(\"On HPC: srun --partition=h100dualflex --gres=gpu:2 ...\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    // Check peer access\n",
    "    int canAccess01, canAccess10;\n",
    "    CHECK_CUDA(cudaDeviceCanAccessPeer(&canAccess01, 0, 1));\n",
    "    CHECK_CUDA(cudaDeviceCanAccessPeer(&canAccess10, 1, 0));\n",
    "    printf(\"GPU 0 -> GPU 1 peer access: %s\\n\", canAccess01 ? \"YES\" : \"NO\");\n",
    "    printf(\"GPU 1 -> GPU 0 peer access: %s\\n\\n\", canAccess10 ? \"YES\" : \"NO\");\n",
    "    \n",
    "    SharedState* state = (SharedState*)mmap(NULL, sizeof(SharedState),\n",
    "        PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n",
    "    state->ready = false;\n",
    "    state->done = false;\n",
    "    \n",
    "    const int N = 1024 * 1024;\n",
    "    \n",
    "    pid_t pid = fork();\n",
    "    \n",
    "    if (pid == 0) {\n",
    "        // ========== PROCESS 2: GPU 1 ==========\n",
    "        CHECK_CUDA(cudaSetDevice(1));\n",
    "        printf(\"[Process 2] Using GPU 1\\n\");\n",
    "        \n",
    "        // Enable peer access\n",
    "        if (canAccess10) {\n",
    "            CHECK_CUDA(cudaDeviceEnablePeerAccess(0, 0));\n",
    "            printf(\"[Process 2] Enabled peer access to GPU 0\\n\");\n",
    "        }\n",
    "        \n",
    "        while (!state->ready) usleep(1000);\n",
    "        \n",
    "        // Open shared memory from GPU 0\n",
    "        float* d_data;\n",
    "        CHECK_CUDA(cudaIpcOpenMemHandle((void**)&d_data, state->handle,\n",
    "                                         cudaIpcMemLazyEnablePeerAccess));\n",
    "        printf(\"[Process 2] Opened GPU 0 memory\\n\");\n",
    "        \n",
    "        // Read and verify data written by GPU 0\n",
    "        int* d_errors;\n",
    "        CHECK_CUDA(cudaMalloc(&d_errors, sizeof(int)));\n",
    "        CHECK_CUDA(cudaMemset(d_errors, 0, sizeof(int)));\n",
    "        \n",
    "        readAndVerify<<<(N+255)/256, 256>>>(d_data, N, 42.0f, d_errors);\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());\n",
    "        \n",
    "        int errors;\n",
    "        CHECK_CUDA(cudaMemcpy(&errors, d_errors, sizeof(int), cudaMemcpyDeviceToHost));\n",
    "        printf(\"[Process 2] Verification: %s (%d errors)\\n\", \n",
    "               errors == 0 ? \"PASSED\" : \"FAILED\", errors);\n",
    "        \n",
    "        CHECK_CUDA(cudaIpcCloseMemHandle(d_data));\n",
    "        CHECK_CUDA(cudaFree(d_errors));\n",
    "        state->done = true;\n",
    "        \n",
    "        printf(\"[Process 2] Done!\\n\");\n",
    "        exit(0);\n",
    "        \n",
    "    } else {\n",
    "        // ========== PROCESS 1: GPU 0 ==========\n",
    "        CHECK_CUDA(cudaSetDevice(0));\n",
    "        printf(\"[Process 1] Using GPU 0\\n\");\n",
    "        \n",
    "        // Allocate and initialize\n",
    "        float* d_data;\n",
    "        CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(float)));\n",
    "        writePattern<<<(N+255)/256, 256>>>(d_data, N, 42.0f);\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());\n",
    "        printf(\"[Process 1] Data initialized\\n\");\n",
    "        \n",
    "        // Share via IPC\n",
    "        CHECK_CUDA(cudaIpcGetMemHandle(&state->handle, d_data));\n",
    "        state->ready = true;\n",
    "        printf(\"[Process 1] Handle shared, waiting...\\n\");\n",
    "        \n",
    "        while (!state->done) usleep(1000);\n",
    "        \n",
    "        wait(NULL);\n",
    "        CHECK_CUDA(cudaFree(d_data));\n",
    "        munmap(state, sizeof(SharedState));\n",
    "        \n",
    "        printf(\"\\n=== Multi-GPU IPC Complete! ===\\n\");\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1abfefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 multi_gpu_ipc.cu -o multi_gpu_ipc\n",
    "print(\"Run with: srun --partition=h100dualflex --gres=gpu:2 ./multi_gpu_ipc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e6fb2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to practice IPC patterns:\n",
    "\n",
    "**Exercise 1: Enhanced Ring Buffer**\n",
    "- Add support for variable-size messages\n",
    "- Implement a priority queue variant\n",
    "\n",
    "**Exercise 2: Multi-Consumer Pattern**\n",
    "- Extend the producer-consumer to support multiple consumers\n",
    "- Implement fair scheduling between consumers\n",
    "\n",
    "**Exercise 3: Bidirectional IPC**\n",
    "- Create a pattern where both processes can be producers and consumers\n",
    "- Implement request-response communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8695c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ipc_exercises.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <sys/wait.h>\n",
    "#include <unistd.h>\n",
    "#include <sys/mman.h>\n",
    "#include <string.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Enhanced Ring Buffer with Variable-Size Messages\n",
    "// TODO: Modify the ring buffer to support variable-size messages\n",
    "// Hint: Use a header that stores message size before each message\n",
    "// =============================================================================\n",
    "\n",
    "struct MessageHeader {\n",
    "    size_t size;      // Size of the following data\n",
    "    int priority;     // For priority queue extension\n",
    "};\n",
    "\n",
    "// TODO: Implement variable-size ring buffer here\n",
    "// - Each slot should contain a header + data\n",
    "// - Consumer should read header first to know data size\n",
    "\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Multi-Consumer Pattern\n",
    "// TODO: Extend to support multiple consumers reading different slots\n",
    "// Hint: Use atomic operations for slot claiming\n",
    "// =============================================================================\n",
    "\n",
    "struct MultiConsumerBuffer {\n",
    "    int* data;\n",
    "    int* read_index;   // Shared atomic read pointer\n",
    "    int* write_index;  // Producer write pointer\n",
    "    int capacity;\n",
    "};\n",
    "\n",
    "__device__ int atomicClaimSlot(int* read_index, int write_index, int capacity) {\n",
    "    // TODO: Atomically claim a slot for reading\n",
    "    // Return -1 if no data available\n",
    "    int old = atomicAdd(read_index, 1);\n",
    "    if (old < write_index) {\n",
    "        return old % capacity;\n",
    "    }\n",
    "    atomicSub(read_index, 1);  // Undo if nothing to read\n",
    "    return -1;\n",
    "}\n",
    "\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Bidirectional IPC (Request-Response)\n",
    "// TODO: Implement a pattern where processes can send requests and receive responses\n",
    "// =============================================================================\n",
    "\n",
    "struct BidirectionalChannel {\n",
    "    int* request_buffer;\n",
    "    int* response_buffer;\n",
    "    int* request_ready;   // Flag: 1 = request pending\n",
    "    int* response_ready;  // Flag: 1 = response ready\n",
    "};\n",
    "\n",
    "// TODO: Implement send_request() and wait_for_response() functions\n",
    "// TODO: Implement receive_request() and send_response() functions\n",
    "\n",
    "\n",
    "// =============================================================================\n",
    "// Main: Test your implementations\n",
    "// =============================================================================\n",
    "int main() {\n",
    "    printf(\"=== IPC Pattern Exercises ===\\n\\n\");\n",
    "    \n",
    "    // Exercise 1 placeholder\n",
    "    printf(\"Exercise 1: Enhanced Ring Buffer\\n\");\n",
    "    printf(\"TODO: Implement variable-size message support\\n\\n\");\n",
    "    \n",
    "    // Exercise 2 placeholder\n",
    "    printf(\"Exercise 2: Multi-Consumer Pattern\\n\");\n",
    "    printf(\"TODO: Implement fair scheduling between consumers\\n\\n\");\n",
    "    \n",
    "    // Exercise 3 placeholder\n",
    "    printf(\"Exercise 3: Bidirectional IPC\\n\");\n",
    "    printf(\"TODO: Implement request-response communication\\n\\n\");\n",
    "    \n",
    "    printf(\"Hint: Fork processes and use IPC memory handles for testing\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o ipc_exercises ipc_exercises.cu && ./ipc_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ff8db",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises use CuPy for IPC operations (Numba doesn't directly support IPC):\n",
    "\n",
    "1. **Process Communication with CuPy**: Use `cupy.cuda.runtime.ipcGetMemHandle()` to share memory between Python processes\n",
    "2. **Multiprocessing with GPU**: Create a producer-consumer pattern using Python's `multiprocessing` module with CuPy arrays\n",
    "3. **Performance Comparison**: Compare IPC overhead vs. copying data through CPU memory\n",
    "\n",
    "*Note: Full IPC functionality requires the `multiprocessing` module and careful handle management.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ced9a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Pattern | Use Case | Key Benefit |\n",
    "|---------|----------|-------------|\n",
    "| Ring Buffer | Streaming data | Decouples producer/consumer speed |\n",
    "| Inference Server | ML serving | Share large model weights |\n",
    "| Multi-GPU IPC | HPC/MPI | Cross-GPU zero-copy access |\n",
    "\n",
    "**Best Practices:**\n",
    "1. Keep producer alive while consumers use handles\n",
    "2. Use events for synchronization, not just polling\n",
    "3. Enable peer access for cross-GPU scenarios\n",
    "4. Consider NCCL for high-performance collective operations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
