{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"⚠️  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065b477",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Unified Memory Basics\n",
    "\n",
    "### What is Unified Memory?\n",
    "\n",
    "```\n",
    "Traditional CUDA:             Unified Memory:\n",
    "\n",
    "┌──────────────┐              ┌──────────────┐\n",
    "│   CPU        │              │   CPU        │\n",
    "│   Memory     │              │              │\n",
    "│   h_data     │              │              │\n",
    "└──────┬───────┘              │              │\n",
    "       │ cudaMemcpy()         │   Unified    │\n",
    "       ↓                      │   Address    │\n",
    "┌──────────────┐              │   Space      │\n",
    "│   GPU        │              │              │\n",
    "│   Memory     │              │   data       │ ← One pointer!\n",
    "│   d_data     │              │              │\n",
    "└──────────────┘              └──────────────┘\n",
    "\n",
    "Two pointers,                 One pointer,\n",
    "explicit copies               automatic migration\n",
    "```\n",
    "\n",
    "### CUDA C++ Unified Memory (Primary)\n",
    "\n",
    "```cpp\n",
    "// unified_memory.cu - Unified memory basics\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void addKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] += 1.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1 << 20;  // 1M elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // ========== TRADITIONAL APPROACH ==========\n",
    "    {\n",
    "        float *h_data = (float*)malloc(size);\n",
    "        float *d_data;\n",
    "        cudaMalloc(&d_data, size);\n",
    "        \n",
    "        // Initialize on host\n",
    "        for (int i = 0; i < n; i++) h_data[i] = i;\n",
    "        \n",
    "        // Copy to device\n",
    "        cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);\n",
    "        \n",
    "        // Launch kernel\n",
    "        addKernel<<<(n+255)/256, 256>>>(d_data, n);\n",
    "        \n",
    "        // Copy back\n",
    "        cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost);\n",
    "        \n",
    "        printf(\"Traditional: h_data[0] = %f\\n\", h_data[0]);\n",
    "        \n",
    "        free(h_data);\n",
    "        cudaFree(d_data);\n",
    "    }\n",
    "    \n",
    "    // ========== UNIFIED MEMORY APPROACH ==========\n",
    "    {\n",
    "        float *data;\n",
    "        cudaMallocManaged(&data, size);  // One allocation!\n",
    "        \n",
    "        // Initialize on host (no copy needed!)\n",
    "        for (int i = 0; i < n; i++) data[i] = i;\n",
    "        \n",
    "        // Launch kernel (no copy needed!)\n",
    "        addKernel<<<(n+255)/256, 256>>>(data, n);\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        // Use on host (no copy needed!)\n",
    "        printf(\"Unified: data[0] = %f\\n\", data[0]);\n",
    "        \n",
    "        cudaFree(data);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba - Managed memory example\n",
    "\n",
    "@cuda.jit\n",
    "def add_one(data):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < data.size:\n",
    "        data[idx] += 1.0\n",
    "\n",
    "# Using managed memory (simplified API)\n",
    "def unified_memory_demo():\n",
    "    n = 1_000_000\n",
    "    \n",
    "    # Create managed array\n",
    "    # Note: Numba handles this through cuda.to_device or \n",
    "    # cuda.managed_array (if available)\n",
    "    \n",
    "    # Standard approach (for comparison)\n",
    "    host_data = np.arange(n, dtype=np.float32)\n",
    "    device_data = cuda.to_device(host_data)\n",
    "    \n",
    "    block = 256\n",
    "    grid = (n + block - 1) // block\n",
    "    \n",
    "    add_one[grid, block](device_data)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    result = device_data.copy_to_host()\n",
    "    print(f\"First elements: {result[:5]}\")\n",
    "    print(f\"Expected: [1. 2. 3. 4. 5.]\")\n",
    "\n",
    "unified_memory_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c95c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Page Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75fe950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_page_migration():\n",
    "    \"\"\"Explain how unified memory page migration works.\"\"\"\n",
    "    print(\"Unified Memory Page Migration\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"How it works:\")\n",
    "    print(\"  1. Memory allocated as 'managed' pages\")\n",
    "    print(\"  2. Pages migrate on demand (page fault)\")\n",
    "    print(\"  3. OS/driver handles migration transparently\")\n",
    "    print()\n",
    "    print(\"Page fault flow:\")\n",
    "    print(\"  GPU kernel accesses page → Page not on GPU\")\n",
    "    print(\"  → Page fault triggered → Migration from CPU to GPU\")\n",
    "    print(\"  → Kernel resumes with page now on GPU\")\n",
    "    print()\n",
    "    print(\"Page sizes:\")\n",
    "    print(\"  CPU: 4 KB (standard) or 2 MB (huge pages)\")\n",
    "    print(\"  GPU: 64 KB (Pascal+) or 2 MB (large page mode)\")\n",
    "    print()\n",
    "    print(\"Migration overhead:\")\n",
    "    print(\"  - Page fault handling: ~20-50 µs\")\n",
    "    print(\"  - Data transfer: depends on page size and PCIe/NVLink\")\n",
    "    print(\"  - Can be significant for random access patterns!\")\n",
    "\n",
    "explain_page_migration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d0d2a",
   "metadata": {},
   "source": [
    "### Prefetching to Avoid Page Faults\n",
    "\n",
    "```cpp\n",
    "// prefetch.cu - Prefetching for better performance\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = sqrtf(data[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1 << 24;  // 16M elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *data;\n",
    "    cudaMallocManaged(&data, size);\n",
    "    \n",
    "    // Initialize on CPU\n",
    "    for (int i = 0; i < n; i++) data[i] = (float)i;\n",
    "    \n",
    "    int device;\n",
    "    cudaGetDevice(&device);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // ===== WITHOUT PREFETCH =====\n",
    "    cudaEventRecord(start);\n",
    "    processKernel<<<(n+255)/256, 256>>>(data, n);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms_no_prefetch;\n",
    "    cudaEventElapsedTime(&ms_no_prefetch, start, stop);\n",
    "    printf(\"Without prefetch: %.2f ms\\n\", ms_no_prefetch);\n",
    "    \n",
    "    // Reset data to CPU\n",
    "    cudaMemPrefetchAsync(data, size, cudaCpuDeviceId);\n",
    "    cudaDeviceSynchronize();\n",
    "    for (int i = 0; i < n; i++) data[i] = (float)i;\n",
    "    \n",
    "    // ===== WITH PREFETCH =====\n",
    "    cudaEventRecord(start);\n",
    "    cudaMemPrefetchAsync(data, size, device);  // Prefetch to GPU\n",
    "    processKernel<<<(n+255)/256, 256>>>(data, n);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms_with_prefetch;\n",
    "    cudaEventElapsedTime(&ms_with_prefetch, start, stop);\n",
    "    printf(\"With prefetch: %.2f ms\\n\", ms_with_prefetch);\n",
    "    \n",
    "    printf(\"Speedup: %.2fx\\n\", ms_no_prefetch / ms_with_prefetch);\n",
    "    \n",
    "    cudaFree(data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a190c4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Memory Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_hints():\n",
    "    \"\"\"Explain CUDA memory advise hints.\"\"\"\n",
    "    print(\"cudaMemAdvise Hints\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"cudaMemAdviseSetReadMostly\")\n",
    "    print(\"  - Hint: Data will be read, rarely written\")\n",
    "    print(\"  - Effect: May duplicate to avoid migration\")\n",
    "    print(\"  - Use: Lookup tables, constant data\")\n",
    "    print()\n",
    "    print(\"cudaMemAdviseSetPreferredLocation\")\n",
    "    print(\"  - Hint: Preferred location for data\")\n",
    "    print(\"  - Effect: Tries to keep data at specified location\")\n",
    "    print(\"  - Use: Data primarily used by one processor\")\n",
    "    print()\n",
    "    print(\"cudaMemAdviseSetAccessedBy\")\n",
    "    print(\"  - Hint: Which devices will access data\")\n",
    "    print(\"  - Effect: Creates direct mapping if possible\")\n",
    "    print(\"  - Use: Multi-GPU scenarios\")\n",
    "    print()\n",
    "    print(\"Example usage:\")\n",
    "    print(\"  cudaMemAdvise(ptr, size, cudaMemAdviseSetReadMostly, 0);\")\n",
    "    print(\"  cudaMemAdvise(ptr, size, cudaMemAdviseSetPreferredLocation, device);\")\n",
    "    print(\"  cudaMemAdvise(ptr, size, cudaMemAdviseSetAccessedBy, device);\")\n",
    "\n",
    "memory_hints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4fdf39",
   "metadata": {},
   "source": [
    "### CUDA C++ Memory Advise (Primary)\n",
    "\n",
    "```cpp\n",
    "// advise.cu - Using memory hints\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void lookupKernel(const float* table, const int* indices,\n",
    "                              float* output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        output[idx] = table[indices[idx]];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int tableSize = 1 << 20;   // 1M lookup table\n",
    "    int n = 1 << 24;           // 16M lookups\n",
    "    \n",
    "    float *table;\n",
    "    int *indices;\n",
    "    float *output;\n",
    "    \n",
    "    cudaMallocManaged(&table, tableSize * sizeof(float));\n",
    "    cudaMallocManaged(&indices, n * sizeof(int));\n",
    "    cudaMallocManaged(&output, n * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < tableSize; i++) table[i] = sqrtf(i);\n",
    "    for (int i = 0; i < n; i++) indices[i] = rand() % tableSize;\n",
    "    \n",
    "    int device;\n",
    "    cudaGetDevice(&device);\n",
    "    \n",
    "    // ===== APPLY HINTS =====\n",
    "    \n",
    "    // Table is read-only - can be duplicated\n",
    "    cudaMemAdvise(table, tableSize * sizeof(float),\n",
    "                  cudaMemAdviseSetReadMostly, 0);\n",
    "    \n",
    "    // Indices and output should prefer GPU\n",
    "    cudaMemAdvise(indices, n * sizeof(int),\n",
    "                  cudaMemAdviseSetPreferredLocation, device);\n",
    "    cudaMemAdvise(output, n * sizeof(float),\n",
    "                  cudaMemAdviseSetPreferredLocation, device);\n",
    "    \n",
    "    // Prefetch to GPU\n",
    "    cudaMemPrefetchAsync(table, tableSize * sizeof(float), device);\n",
    "    cudaMemPrefetchAsync(indices, n * sizeof(int), device);\n",
    "    \n",
    "    // Launch kernel\n",
    "    lookupKernel<<<(n+255)/256, 256>>>(table, indices, output, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Prefetch output back to CPU for verification\n",
    "    cudaMemPrefetchAsync(output, n * sizeof(float), cudaCpuDeviceId);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"output[0] = %f (expected: %f)\\n\", \n",
    "           output[0], table[indices[0]]);\n",
    "    \n",
    "    cudaFree(table);\n",
    "    cudaFree(indices);\n",
    "    cudaFree(output);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ab1d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: When to Use Unified Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_vs_explicit():\n",
    "    \"\"\"Compare unified vs explicit memory management.\"\"\"\n",
    "    print(\"Unified vs Explicit Memory\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"USE UNIFIED MEMORY WHEN:\")\n",
    "    print(\"  ✓ Prototyping and development\")\n",
    "    print(\"  ✓ Complex data structures (linked lists, trees)\")\n",
    "    print(\"  ✓ Oversubscription (data larger than GPU memory)\")\n",
    "    print(\"  ✓ Unclear access patterns\")\n",
    "    print(\"  ✓ Porting CPU code quickly\")\n",
    "    print()\n",
    "    print(\"USE EXPLICIT MEMORY WHEN:\")\n",
    "    print(\"  ✓ Maximum performance critical\")\n",
    "    print(\"  ✓ Predictable access patterns\")\n",
    "    print(\"  ✓ Frequent CPU-GPU ping-pong\")\n",
    "    print(\"  ✓ Fine-grained control needed\")\n",
    "    print(\"  ✓ Overlapping compute and transfer\")\n",
    "    print()\n",
    "    print(\"PERFORMANCE CONSIDERATIONS:\")\n",
    "    print(\"  - Page faults have ~20-50 µs overhead each\")\n",
    "    print(\"  - First access triggers migration\")\n",
    "    print(\"  - Random access patterns = many page faults\")\n",
    "    print(\"  - Prefetching mitigates most overhead\")\n",
    "    print(\"  - With proper hints, ~95%+ of explicit performance\")\n",
    "\n",
    "unified_vs_explicit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e4396",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Oversubscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6277713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_oversubscription():\n",
    "    \"\"\"Explain memory oversubscription with unified memory.\"\"\"\n",
    "    print(\"Memory Oversubscription\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"Traditional CUDA:\")\n",
    "    print(\"  GPU memory = hard limit\")\n",
    "    print(\"  cudaMalloc fails if not enough memory\")\n",
    "    print()\n",
    "    print(\"Unified Memory (Pascal+):\")\n",
    "    print(\"  Can allocate more than GPU memory!\")\n",
    "    print(\"  Pages migrate as needed\")\n",
    "    print(\"  Works like virtual memory\")\n",
    "    print()\n",
    "    print(\"Example:\")\n",
    "    print(\"  GPU has 8 GB memory\")\n",
    "    print(\"  Allocate 32 GB with cudaMallocManaged\")\n",
    "    print(\"  Process 8 GB at a time on GPU\")\n",
    "    print(\"  Pages swap automatically\")\n",
    "    print()\n",
    "    print(\"Caveats:\")\n",
    "    print(\"  - Performance degrades with thrashing\")\n",
    "    print(\"  - Need good access locality\")\n",
    "    print(\"  - Consider prefetch hints\")\n",
    "\n",
    "explain_oversubscription()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3daed0",
   "metadata": {},
   "source": [
    "### CUDA C++ Oversubscription Example\n",
    "\n",
    "```cpp\n",
    "// oversubscription.cu - Using more memory than GPU has\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void processChunk(float* data, int start, int chunk_size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < chunk_size) {\n",
    "        data[start + idx] = sqrtf(data[start + idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Query GPU memory\n",
    "    size_t free_mem, total_mem;\n",
    "    cudaMemGetInfo(&free_mem, &total_mem);\n",
    "    printf(\"GPU Memory: %.1f GB free, %.1f GB total\\n\",\n",
    "           free_mem / 1e9, total_mem / 1e9);\n",
    "    \n",
    "    // Allocate MORE than GPU memory\n",
    "    size_t n = total_mem / sizeof(float) * 2;  // 2x GPU memory\n",
    "    size_t size = n * sizeof(float);\n",
    "    printf(\"Allocating %.1f GB (2x GPU memory)\\n\", size / 1e9);\n",
    "    \n",
    "    float *data;\n",
    "    cudaError_t err = cudaMallocManaged(&data, size);\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"Allocation failed: %s\\n\", cudaGetErrorString(err));\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    // Initialize on CPU (pages stay on CPU)\n",
    "    for (size_t i = 0; i < n; i++) {\n",
    "        data[i] = (float)(i % 1000);\n",
    "    }\n",
    "    \n",
    "    // Process in chunks to demonstrate oversubscription\n",
    "    int device;\n",
    "    cudaGetDevice(&device);\n",
    "    \n",
    "    size_t chunk_size = n / 4;  // Process 1/4 at a time\n",
    "    \n",
    "    for (int chunk = 0; chunk < 4; chunk++) {\n",
    "        size_t start = chunk * chunk_size;\n",
    "        \n",
    "        // Prefetch this chunk to GPU\n",
    "        cudaMemPrefetchAsync(&data[start], chunk_size * sizeof(float), device);\n",
    "        \n",
    "        // Process chunk\n",
    "        processChunk<<<(chunk_size+255)/256, 256>>>(data, start, chunk_size);\n",
    "        \n",
    "        printf(\"Processed chunk %d\\n\", chunk);\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Prefetch result back to CPU\n",
    "    cudaMemPrefetchAsync(data, size, cudaCpuDeviceId);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"data[0] = %f (expected sqrt(0) = 0)\\n\", data[0]);\n",
    "    printf(\"data[1] = %f (expected sqrt(1) = 1)\\n\", data[1]);\n",
    "    \n",
    "    cudaFree(data);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8eb2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f638f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare unified memory (with/without prefetch) vs explicit memory\n",
    "\n",
    "@cuda.jit\n",
    "def compute_kernel(data, result):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < result.size:\n",
    "        x = data[idx]\n",
    "        result[idx] = x * x + x\n",
    "\n",
    "def benchmark_memory_approaches(n=10_000_000):\n",
    "    \"\"\"Compare different memory management approaches.\"\"\"\n",
    "    # TODO: Implement benchmarks for:\n",
    "    # 1. Explicit memory with cudaMemcpy\n",
    "    # 2. Unified memory without prefetch\n",
    "    # 3. Unified memory with prefetch\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800eeb28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Unified Memory API\n",
    "\n",
    "```cpp\n",
    "// Allocation\n",
    "cudaMallocManaged(&ptr, size);\n",
    "\n",
    "// Prefetching\n",
    "cudaMemPrefetchAsync(ptr, size, device);     // To GPU\n",
    "cudaMemPrefetchAsync(ptr, size, cudaCpuDeviceId); // To CPU\n",
    "\n",
    "// Hints\n",
    "cudaMemAdvise(ptr, size, cudaMemAdviseSetReadMostly, device);\n",
    "cudaMemAdvise(ptr, size, cudaMemAdviseSetPreferredLocation, device);\n",
    "cudaMemAdvise(ptr, size, cudaMemAdviseSetAccessedBy, device);\n",
    "```\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Prototyping | Unified memory |\n",
    "| Complex data structures | Unified memory |\n",
    "| Maximum performance | Explicit + overlapping |\n",
    "| Data > GPU memory | Unified + prefetch |\n",
    "| Production code | Unified + hints (or explicit) |\n",
    "\n",
    "### Week 7 Complete!\n",
    "Next week: Profiling & Analysis with Nsight tools."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
