{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafb4a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Register Basics\n",
    "\n",
    "### GPU Register File\n",
    "\n",
    "```\n",
    "Register Hierarchy:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  SM Register File (65536 Ã— 32-bit)      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Shared among all threads on SM         â”‚\n",
    "â”‚  ~256 regs/thread max, ~32-128 typical  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Access Speed:\n",
    "  Registers:      ~1 cycle\n",
    "  Shared Memory:  ~5-30 cycles\n",
    "  L1 Cache:       ~30-80 cycles\n",
    "  Global Memory:  ~300-800 cycles\n",
    "\n",
    "Key: Registers are THE fastest memory!\n",
    "```\n",
    "\n",
    "### Register Spilling\n",
    "\n",
    "```\n",
    "When kernel needs more registers than available:\n",
    "\n",
    "Compiler decision:\n",
    "  1. Reduce concurrent threads (lower occupancy)\n",
    "  2. Spill to local memory (slow!)\n",
    "\n",
    "Local Memory = DRAM with caching\n",
    "  - Uses same physical memory as global\n",
    "  - Has L1/L2 cache, but still ~100x slower than registers\n",
    "  - Automatic, transparent to programmer\n",
    "\n",
    "Symptoms of spilling:\n",
    "  - High local memory traffic in profiler\n",
    "  - \"spill stores\" and \"spill loads\" in ptxas output\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74577d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile registers.cu\n",
    "// registers.cu - Analyze register usage\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Kernel with moderate register usage\n",
    "__global__ void lowRegKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float x = data[idx];\n",
    "        data[idx] = x * x;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel with high register usage\n",
    "__global__ void highRegKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float a = data[idx];\n",
    "        float b = data[idx + n];\n",
    "        float c = data[idx + 2*n];\n",
    "        float d = data[idx + 3*n];\n",
    "        float e = data[idx + 4*n];\n",
    "        float f = data[idx + 5*n];\n",
    "        float g = data[idx + 6*n];\n",
    "        float h = data[idx + 7*n];\n",
    "        \n",
    "        // Keep all variables live\n",
    "        float r1 = a * b + c * d;\n",
    "        float r2 = e * f + g * h;\n",
    "        float r3 = a * c + b * d;\n",
    "        float r4 = e * g + f * h;\n",
    "        float r5 = r1 * r2 + r3 * r4;\n",
    "        \n",
    "        data[idx] = r5;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Query register usage\n",
    "void getKernelAttributes(const char* name, const void* kernel) {\n",
    "    cudaFuncAttributes attr;\n",
    "    cudaFuncGetAttributes(&attr, kernel);\n",
    "    \n",
    "    printf(\"%s:\\n\", name);\n",
    "    printf(\"  Registers per thread: %d\\n\", attr.numRegs);\n",
    "    printf(\"  Shared memory: %zu bytes\\n\", attr.sharedSizeBytes);\n",
    "    printf(\"  Local memory: %zu bytes\\n\", attr.localSizeBytes);\n",
    "    printf(\"  Max threads per block: %d\\n\", attr.maxThreadsPerBlock);\n",
    "    printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    getKernelAttributes(\"lowRegKernel\", (const void*)lowRegKernel);\n",
    "    getKernelAttributes(\"highRegKernel\", (const void*)highRegKernel);\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "// Compile with: nvcc -Xptxas -v registers.cu\n",
    "// to see register allocation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48690d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o registers registers.cu\n",
    "!./registers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26588946",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411dbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba - Demonstrate register impact\n",
    "\n",
    "@cuda.jit\n",
    "def low_register_kernel(data, result):\n",
    "    \"\"\"Low register usage kernel.\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < data.size:\n",
    "        x = data[idx]\n",
    "        result[idx] = x * x\n",
    "\n",
    "@cuda.jit\n",
    "def high_register_kernel(data, result):\n",
    "    \"\"\"Higher register usage kernel.\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < data.size:\n",
    "        # Many variables kept live simultaneously\n",
    "        a = data[idx]\n",
    "        b = a * 2.0\n",
    "        c = a * 3.0\n",
    "        d = a * 4.0\n",
    "        e = a * 5.0\n",
    "        f = a * 6.0\n",
    "        g = a * 7.0\n",
    "        h = a * 8.0\n",
    "        \n",
    "        # Use all variables\n",
    "        r1 = a + b + c + d\n",
    "        r2 = e + f + g + h\n",
    "        result[idx] = r1 * r2 o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kernels(n=10_000_000):\n",
    "    \"\"\"Compare low vs high register kernels.\"\"\"\n",
    "    data = np.random.rand(n).astype(np.float32)\n",
    "    result = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    d_data = cuda.to_device(data)\n",
    "    d_result = cuda.to_device(result)\n",
    "    \n",
    "    block = 256\n",
    "    grid = (n + block - 1) // block\n",
    "    \n",
    "    # Warmup and benchmark low register\n",
    "    low_register_kernel[grid, block](d_data, d_result)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(50):\n",
    "        low_register_kernel[grid, block](d_data, d_result)\n",
    "    cuda.synchronize()\n",
    "    low_time = (time.perf_counter() - start) / 50 * 1000\n",
    "    \n",
    "    # Warmup and benchmark high register\n",
    "    high_register_kernel[grid, block](d_data, d_result)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(50):\n",
    "        high_register_kernel[grid, block](d_data, d_result)\n",
    "    cuda.synchronize()\n",
    "    high_time = (time.perf_counter() - start) / 50 * 1000\n",
    "    \n",
    "    print(f\"Low register kernel:  {low_time:.3f} ms\")\n",
    "    print(f\"High register kernel: {high_time:.3f} ms\")\n",
    "    print(f\"Ratio: {high_time/low_time:.2f}x\")\n",
    "\n",
    "compare_kernels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d325add5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Launch Bounds\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b95a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile launch_bounds.cu\n",
    "// launch_bounds.cu - Controlling register allocation\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Without launch bounds - compiler chooses register allocation\n",
    "__global__ void unboundedKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Complex computation...\n",
    "        float x = data[idx];\n",
    "        float y = sinf(x) * cosf(x);\n",
    "        float z = expf(-x * x);\n",
    "        data[idx] = y + z;\n",
    "    }\n",
    "}\n",
    "\n",
    "// With launch bounds - hint to compiler\n",
    "__global__ void __launch_bounds__(256, 4)  // max 256 threads/block, min 4 blocks/SM\n",
    "boundedKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float x = data[idx];\n",
    "        float y = sinf(x) * cosf(x);\n",
    "        float z = expf(-x * x);\n",
    "        data[idx] = y + z;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Aggressive launch bounds for high occupancy\n",
    "__global__ void __launch_bounds__(1024, 2)\n",
    "highOccupancyKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = data[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "/*\n",
    "__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)\n",
    "\n",
    "Effects:\n",
    "  1. Compiler limits registers to fit minBlocks per SM\n",
    "  2. May spill to local memory if needed\n",
    "  3. Error if kernel launched with more threads than maxThreads\n",
    "\n",
    "Guidelines:\n",
    "  - maxThreadsPerBlock: your actual launch configuration\n",
    "  - minBlocksPerMultiprocessor: target occupancy (2-4 typical)\n",
    "*/\n",
    "\n",
    "int main() {\n",
    "    cudaFuncAttributes attr;\n",
    "    \n",
    "    cudaFuncGetAttributes(&attr, unboundedKernel);\n",
    "    printf(\"Unbounded: %d regs, max %d threads\\n\", \n",
    "           attr.numRegs, attr.maxThreadsPerBlock);\n",
    "    \n",
    "    cudaFuncGetAttributes(&attr, boundedKernel);\n",
    "    printf(\"Bounded:   %d regs, max %d threads\\n\", \n",
    "           attr.numRegs, attr.maxThreadsPerBlock);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c235313",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o launch_bounds launch_bounds.cu\n",
    "!./launch_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_launch_bounds():\n",
    "    \"\"\"Explain launch bounds usage.\"\"\"\n",
    "    print(\"__launch_bounds__ Explained\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"Syntax: __launch_bounds__(maxThreadsPerBlock, minBlocksPerSM)\")\n",
    "    print()\n",
    "    print(\"Parameters:\")\n",
    "    print(\"  maxThreadsPerBlock: Maximum threads you'll launch with\")\n",
    "    print(\"  minBlocksPerSM:     Minimum concurrent blocks per SM (optional)\")\n",
    "    print()\n",
    "    print(\"What it does:\")\n",
    "    print(\"  1. Tells compiler the launch configuration\")\n",
    "    print(\"  2. Compiler allocates registers to fit minBlocks\")\n",
    "    print(\"  3. May cause spilling if too aggressive\")\n",
    "    print()\n",
    "    print(\"Common patterns:\")\n",
    "    print(\"  __launch_bounds__(256)       // Just max threads\")\n",
    "    print(\"  __launch_bounds__(256, 2)    // 50% occupancy target\")\n",
    "    print(\"  __launch_bounds__(256, 4)    // 100% occupancy target\")\n",
    "    print(\"  __launch_bounds__(128, 8)    // High occupancy, small blocks\")\n",
    "    print()\n",
    "    print(\"Trade-offs:\")\n",
    "    print(\"  Higher minBlocks â†’ More occupancy, but may spill registers\")\n",
    "    print(\"  Lower minBlocks  â†’ More registers, but less occupancy\")\n",
    "\n",
    "explain_launch_bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356adab8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Register Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc442434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_optimization_techniques():\n",
    "    \"\"\"Explain techniques to reduce register usage.\"\"\"\n",
    "    print(\"Register Optimization Techniques\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"1. VARIABLE REUSE\")\n",
    "    print(\"   Bad:  float a = x; float b = y; float c = a + b;\")\n",
    "    print(\"   Good: float a = x + y;  // One variable instead of three\")\n",
    "    print()\n",
    "    print(\"2. REDUCE LIVE VARIABLES\")\n",
    "    print(\"   Bad:  Load all data, then process\")\n",
    "    print(\"   Good: Load, process, store - one at a time\")\n",
    "    print()\n",
    "    print(\"3. COMPUTATION VS STORAGE\")\n",
    "    print(\"   Sometimes recomputing is faster than storing:\")\n",
    "    print(\"   Bad:  float s = sinf(x); ... use s multiple times\")\n",
    "    print(\"   Maybe: Recompute sinf(x) if register-bound\")\n",
    "    print()\n",
    "    print(\"4. ARRAY INDEXING\")\n",
    "    print(\"   Local arrays often spill to local memory\")\n",
    "    print(\"   Use scalar variables when possible\")\n",
    "    print()\n",
    "    print(\"5. LOOP UNROLLING\")\n",
    "    print(\"   #pragma unroll can increase register pressure\")\n",
    "    print(\"   Use #pragma unroll N for partial unrolling\")\n",
    "    print()\n",
    "    print(\"6. SHARED MEMORY\")\n",
    "    print(\"   Move data from registers to shared memory\")\n",
    "    print(\"   Trade register pressure for shared memory pressure\")\n",
    "\n",
    "register_optimization_techniques()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746f6df",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Optimization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reg_optimization.cu\n",
    "// reg_optimization.cu - Register optimization examples\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// BAD: Many live variables\n",
    "__global__ void badRegisters(float* a, float* b, float* c, float* d, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float v1 = a[idx];  // Live\n",
    "        float v2 = b[idx];  // Live\n",
    "        float v3 = c[idx];  // Live\n",
    "        float v4 = d[idx];  // Live - 4 variables live!\n",
    "        \n",
    "        out[idx] = v1 + v2 + v3 + v4;\n",
    "    }\n",
    "}\n",
    "\n",
    "// GOOD: Accumulate to reduce live variables\n",
    "__global__ void goodRegisters(float* a, float* b, float* c, float* d, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float sum = a[idx];  // Only 1 variable live\n",
    "        sum += b[idx];\n",
    "        sum += c[idx];\n",
    "        sum += d[idx];\n",
    "        \n",
    "        out[idx] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// BAD: Local arrays spill\n",
    "__global__ void badArray(float* data, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float local[16];  // Likely spills to local memory!\n",
    "    \n",
    "    for (int i = 0; i < 16; i++) {\n",
    "        local[i] = data[idx + i * n];\n",
    "    }\n",
    "    \n",
    "    float sum = 0;\n",
    "    for (int i = 0; i < 16; i++) {\n",
    "        sum += local[i];\n",
    "    }\n",
    "    out[idx] = sum;\n",
    "}\n",
    "\n",
    "// GOOD: Process inline\n",
    "__global__ void goodInline(float* data, float* out, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    float sum = 0;\n",
    "    for (int i = 0; i < 16; i++) {\n",
    "        sum += data[idx + i * n];  // No local array needed\n",
    "    }\n",
    "    out[idx] = sum;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    cudaFuncAttributes attr;\n",
    "    \n",
    "    cudaFuncGetAttributes(&attr, badRegisters);\n",
    "    printf(\"badRegisters:  %d regs, %zu local bytes\\n\", attr.numRegs, attr.localSizeBytes);\n",
    "    \n",
    "    cudaFuncGetAttributes(&attr, goodRegisters);\n",
    "    printf(\"goodRegisters: %d regs, %zu local bytes\\n\", attr.numRegs, attr.localSizeBytes);\n",
    "    \n",
    "    cudaFuncGetAttributes(&attr, badArray);\n",
    "    printf(\"badArray:      %d regs, %zu local bytes\\n\", attr.numRegs, attr.localSizeBytes);\n",
    "    \n",
    "    cudaFuncGetAttributes(&attr, goodInline);\n",
    "    printf(\"goodInline:    %d regs, %zu local bytes\\n\", attr.numRegs, attr.localSizeBytes);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o reg_optimization reg_optimization.cu\n",
    "!./reg_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b63a2",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python demonstration of register optimization patterns\n",
    "\n",
    "@cuda.jit\n",
    "def bad_pattern(a, b, c, d, out):\n",
    "    \"\"\"Bad: Many live variables.\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        v1 = a[idx]\n",
    "        v2 = b[idx]\n",
    "        v3 = c[idx]\n",
    "        v4 = d[idx]\n",
    "        out[idx] = v1 + v2 + v3 + v4\n",
    "\n",
    "@cuda.jit\n",
    "def good_pattern(a, b, c, d, out):\n",
    "    \"\"\"Good: Accumulator pattern.\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        result = a[idx]\n",
    "        result += b[idx]\n",
    "        result += c[idx]\n",
    "        result += d[idx]\n",
    "        out[idx] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e48f95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Detecting Register Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a47c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_register_issues():\n",
    "    \"\"\"How to detect register problems.\"\"\"\n",
    "    print(\"Detecting Register Issues\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"1. COMPILE-TIME ANALYSIS\")\n",
    "    print(\"   nvcc -Xptxas -v kernel.cu\")\n",
    "    print(\"   Look for:\")\n",
    "    print(\"     - 'Used XX registers'\")\n",
    "    print(\"     - 'XX bytes spill stores'\")\n",
    "    print(\"     - 'XX bytes spill loads'\")\n",
    "    print()\n",
    "    print(\"2. RUNTIME QUERY\")\n",
    "    print(\"   cudaFuncGetAttributes(&attr, kernel);\")\n",
    "    print(\"   Check:\")\n",
    "    print(\"     - attr.numRegs (registers per thread)\")\n",
    "    print(\"     - attr.localSizeBytes (local memory per thread)\")\n",
    "    print()\n",
    "    print(\"3. PROFILER ANALYSIS\")\n",
    "    print(\"   Nsight Compute metrics:\")\n",
    "    print(\"     - launch__registers_per_thread\")\n",
    "    print(\"     - lts__t_sectors_srcunit_tex_op_read (local loads)\")\n",
    "    print(\"     - lts__t_sectors_srcunit_tex_op_write (local stores)\")\n",
    "    print()\n",
    "    print(\"4. OCCUPANCY CALCULATOR\")\n",
    "    print(\"   Use CUDA Occupancy Calculator spreadsheet\")\n",
    "    print(\"   Input register count, see occupancy impact\")\n",
    "\n",
    "detect_register_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae28d3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Reduce Register Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1004f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize this kernel to reduce register usage\n",
    "\n",
    "@cuda.jit\n",
    "def unoptimized_kernel(data, coeffs, result):\n",
    "    \"\"\"Polynomial evaluation - can you reduce registers?\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < result.size:\n",
    "        x = data[idx]\n",
    "        c0 = coeffs[0]\n",
    "        c1 = coeffs[1]\n",
    "        c2 = coeffs[2]\n",
    "        c3 = coeffs[3]\n",
    "        c4 = coeffs[4]\n",
    "        c5 = coeffs[5]\n",
    "        \n",
    "        # Evaluate polynomial: c0 + c1*x + c2*x^2 + ...\n",
    "        x2 = x * x\n",
    "        x3 = x2 * x\n",
    "        x4 = x3 * x\n",
    "        x5 = x4 * x\n",
    "        \n",
    "        result[idx] = c0 + c1*x + c2*x2 + c3*x3 + c4*x4 + c5*x5\n",
    "\n",
    "@cuda.jit\n",
    "def optimized_kernel(data, coeffs, result):\n",
    "    \"\"\"Your optimized version - use Horner's method!\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < result.size:\n",
    "        # TODO: Use Horner's method: c5 + x*(c4 + x*(c3 + x*(c2 + x*(c1 + x*c0))))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c65a2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Register Hierarchy\n",
    "\n",
    "| Level | Speed | Size |\n",
    "|-------|-------|------|\n",
    "| Registers | ~1 cycle | 255 per thread |\n",
    "| Local Memory | ~100+ cycles | Unlimited (spill) |\n",
    "\n",
    "### Launch Bounds\n",
    "\n",
    "```cpp\n",
    "__global__ void __launch_bounds__(maxThreads, minBlocks)\n",
    "kernel(...) {\n",
    "    // Compiler allocates registers to fit minBlocks per SM\n",
    "}\n",
    "```\n",
    "\n",
    "### Optimization Checklist\n",
    "\n",
    "1. âœ“ Check register count with nvcc -Xptxas -v\n",
    "2. âœ“ Look for spill loads/stores\n",
    "3. âœ“ Reduce live variables\n",
    "4. âœ“ Use accumulator patterns\n",
    "5. âœ“ Avoid local arrays when possible\n",
    "6. âœ“ Consider launch bounds\n",
    "\n",
    "### Tomorrow: Cache Optimization\n",
    "We'll explore L1/L2 cache and memory access patterns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
