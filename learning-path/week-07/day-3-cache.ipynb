{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3de5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: GPU Cache Hierarchy\n",
    "\n",
    "### Memory Hierarchy\n",
    "\n",
    "```\n",
    "GPU Memory Hierarchy:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     Registers (per thread)                   â”‚\n",
    "â”‚                     ~1 cycle, 255 max                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              Shared Memory / L1 Cache (per SM)               â”‚\n",
    "â”‚              ~5-30 cycles, 48-228KB configurable             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    L2 Cache (shared)                         â”‚\n",
    "â”‚                    ~100-200 cycles, 1.5-80MB                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 Global Memory (DRAM)                         â”‚\n",
    "â”‚                 ~400-800 cycles, 8-80GB                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### L1 Cache Details\n",
    "\n",
    "```\n",
    "L1 Cache (per SM):\n",
    "  - Unified with shared memory (128-228KB total)\n",
    "  - Configurable split: more shared OR more L1\n",
    "  - Cache line: 128 bytes\n",
    "  - Write-through (writes go to L2)\n",
    "  - Automatic caching of global loads\n",
    "\n",
    "Configuration options (cudaFuncSetCacheConfig):\n",
    "  cudaFuncCachePreferNone     Default\n",
    "  cudaFuncCachePreferShared   Prefer shared memory\n",
    "  cudaFuncCachePreferL1       Prefer L1 cache\n",
    "  cudaFuncCachePreferEqual    Equal split\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa653da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cache_config.cu\n",
    "// cache_config.cu - Configure L1/shared memory split\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void memoryIntensiveKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Read many times from different locations\n",
    "        float sum = 0;\n",
    "        for (int i = 0; i < 16; i++) {\n",
    "            int offset = (idx + i * 1024) % n;\n",
    "            sum += data[offset];\n",
    "        }\n",
    "        data[idx] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Query current cache config\n",
    "    cudaFuncCache currentConfig;\n",
    "    cudaDeviceGetCacheConfig(&currentConfig);\n",
    "    printf(\"Current config: %d\\n\", currentConfig);\n",
    "    \n",
    "    // Set cache preference for kernel\n",
    "    cudaFuncSetCacheConfig(memoryIntensiveKernel, cudaFuncCachePreferL1);\n",
    "    \n",
    "    // For kernels that use shared memory heavily:\n",
    "    // cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferShared);\n",
    "    \n",
    "    // Query device L1/shared config\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Shared memory per block: %zu KB\\n\", prop.sharedMemPerBlock / 1024);\n",
    "    printf(\"Shared memory per SM: %zu KB\\n\", prop.sharedMemPerMultiprocessor / 1024);\n",
    "    printf(\"L2 cache size: %d KB\\n\", prop.l2CacheSize / 1024);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cache_config cache_config.cu\n",
    "!./cache_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006382a",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query cache information\n",
    "device = cuda.get_current_device()\n",
    "\n",
    "print(\"GPU Cache Information\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Device: {device.name}\")\n",
    "print(f\"Max shared memory per block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.0f} KB\")\n",
    "\n",
    "# Get more detailed info via context\n",
    "ctx = cuda.current_context()\n",
    "try:\n",
    "    print(f\"\\nNote: L1/L2 cache sizes vary by GPU architecture\")\n",
    "    print(f\"Typical L1: 48-128 KB per SM\")\n",
    "    print(f\"Typical L2: 1.5-40 MB shared\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca8cb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Coalescing Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_coalescing():\n",
    "    \"\"\"Deep dive into memory coalescing.\"\"\"\n",
    "    print(\"Memory Coalescing\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"Definition: Combining multiple memory requests into fewer\")\n",
    "    print(\"            transactions at cache-line granularity (128 bytes)\")\n",
    "    print()\n",
    "    print(\"COALESCED ACCESS (GOOD):\")\n",
    "    print(\"  Warp threads 0-31 access addresses 0-124 (consecutive floats)\")\n",
    "    print(\"  â†’ 1 cache line transaction (128 bytes = 32 floats)\")\n",
    "    print()\n",
    "    print(\"  Thread 0:  addr 0\")\n",
    "    print(\"  Thread 1:  addr 4\")\n",
    "    print(\"  Thread 2:  addr 8\")\n",
    "    print(\"  ...\")\n",
    "    print(\"  Thread 31: addr 124\")\n",
    "    print(\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(\"  Result: ONE 128-byte transaction\")\n",
    "    print()\n",
    "    print(\"STRIDED ACCESS (BAD):\")\n",
    "    print(\"  Warp threads access every 16th element (stride = 64 bytes)\")\n",
    "    print(\"  â†’ Multiple cache line transactions\")\n",
    "    print()\n",
    "    print(\"  Thread 0:  addr 0    â†’ cache line 0\")\n",
    "    print(\"  Thread 1:  addr 64   â†’ cache line 0\")\n",
    "    print(\"  Thread 2:  addr 128  â†’ cache line 1\")\n",
    "    print(\"  Thread 3:  addr 192  â†’ cache line 1\")\n",
    "    print(\"  ...\")\n",
    "    print(\"  Result: 16+ cache line transactions!\")\n",
    "\n",
    "explain_coalescing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate coalescing impact\n",
    "\n",
    "@cuda.jit\n",
    "def coalesced_access(data, result, stride):\n",
    "    \"\"\"Stride-1 access (coalesced).\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < result.size:\n",
    "        result[idx] = data[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def strided_access(data, result, stride):\n",
    "    \"\"\"Strided access (not coalesced).\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < result.size:\n",
    "        # Strided read - bad for coalescing\n",
    "        src_idx = (idx * stride) % data.size\n",
    "        result[idx] = data[src_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a600b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_coalescing(n=1_000_000):\n",
    "    \"\"\"Benchmark coalesced vs strided access.\"\"\"\n",
    "    data = np.random.rand(n * 32).astype(np.float32)  # Extra space for strides\n",
    "    result = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    d_data = cuda.to_device(data)\n",
    "    d_result = cuda.to_device(result)\n",
    "    \n",
    "    block = 256\n",
    "    grid = (n + block - 1) // block\n",
    "    \n",
    "    print(f\"Coalescing Benchmark ({n:,} elements)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for stride in [1, 2, 4, 8, 16, 32]:\n",
    "        # Warmup\n",
    "        strided_access[grid, block](d_data, d_result, stride)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        iterations = 100\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            strided_access[grid, block](d_data, d_result, stride)\n",
    "        cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / iterations * 1000\n",
    "        \n",
    "        # Calculate bandwidth\n",
    "        bytes_moved = n * 4 * 2  # read + write\n",
    "        bandwidth = bytes_moved / (elapsed / 1000) / 1e9\n",
    "        \n",
    "        print(f\"Stride {stride:2d}: {elapsed:.3f} ms, {bandwidth:.1f} GB/s\")\n",
    "\n",
    "benchmark_coalescing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e777b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: L2 Cache Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_cache_strategies():\n",
    "    \"\"\"Strategies for L2 cache optimization.\"\"\"\n",
    "    print(\"L2 Cache Optimization Strategies\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"1. DATA LOCALITY\")\n",
    "    print(\"   Keep working set smaller than L2 cache\")\n",
    "    print(\"   Typical L2: 1.5-40 MB\")\n",
    "    print()\n",
    "    print(\"2. PERSISTENCE (Ampere+)\")\n",
    "    print(\"   cudaAccessPropertyPersisting: keep in L2\")\n",
    "    print(\"   cudaAccessPropertyStreaming: don't cache\")\n",
    "    print()\n",
    "    print(\"3. CACHE PARTITIONING (Ampere+)\")\n",
    "    print(\"   Reserve portion of L2 for specific data\")\n",
    "    print(\"   cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, bytes)\")\n",
    "    print()\n",
    "    print(\"4. ACCESS PATTERNS\")\n",
    "    print(\"   Sequential: Best cache utilization\")\n",
    "    print(\"   Random: Poor cache utilization\")\n",
    "    print(\"   Blocked/Tiled: Good cache reuse\")\n",
    "\n",
    "l2_cache_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6831c6fa",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ L2 Cache Control (Ampere+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile l2_cache.cu\n",
    "// l2_cache.cu - L2 cache persistence (Ampere+)\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    \n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"L2 cache size: %d MB\\n\", prop.l2CacheSize / (1024 * 1024));\n",
    "    printf(\"Compute capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "    \n",
    "    // On Ampere+, you can control L2 persistence\n",
    "    if (prop.major >= 8) {\n",
    "        printf(\"\\nAmpere+ detected - L2 persistence available\\n\");\n",
    "        \n",
    "        // Reserve some L2 cache for persistent data\n",
    "        size_t persistingL2 = prop.l2CacheSize / 2;  // Reserve half\n",
    "        cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, persistingL2);\n",
    "        printf(\"Reserved %zu KB for L2 persistence\\n\", persistingL2 / 1024);\n",
    "        \n",
    "        // Set access policy for a memory range\n",
    "        float* d_persistent;\n",
    "        size_t size = 1024 * 1024;  // 1MB\n",
    "        cudaMalloc(&d_persistent, size);\n",
    "        \n",
    "        cudaStreamAttrValue attr;\n",
    "        attr.accessPolicyWindow.base_ptr = d_persistent;\n",
    "        attr.accessPolicyWindow.num_bytes = size;\n",
    "        attr.accessPolicyWindow.hitProp = cudaAccessPropertyPersisting;\n",
    "        attr.accessPolicyWindow.missProp = cudaAccessPropertyStreaming;\n",
    "        attr.accessPolicyWindow.hitRatio = 1.0f;\n",
    "        \n",
    "        cudaStream_t stream;\n",
    "        cudaStreamCreate(&stream);\n",
    "        cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &attr);\n",
    "        \n",
    "        printf(\"Set up L2 persistence policy for 1MB buffer\\n\");\n",
    "        \n",
    "        // Now kernels on this stream will try to keep d_persistent in L2\n",
    "        \n",
    "        cudaStreamDestroy(stream);\n",
    "        cudaFree(d_persistent);\n",
    "    } else {\n",
    "        printf(\"\\nPre-Ampere GPU - L2 persistence not available\\n\");\n",
    "        printf(\"But automatic L2 caching still works!\\n\");\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9538fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o l2_cache l2_cache.cu\n",
    "!./l2_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2bb18e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Texture and Constant Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47039dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_caches():\n",
    "    \"\"\"Explain texture and constant memory caches.\"\"\"\n",
    "    print(\"Special GPU Caches\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"CONSTANT MEMORY\")\n",
    "    print(\"  Size: 64 KB total, 8 KB cache per SM\")\n",
    "    print(\"  Access: Broadcast to all threads in warp\")\n",
    "    print(\"  Best for: Same value read by all threads\")\n",
    "    print(\"  Declare: __constant__ float data[1024];\")\n",
    "    print()\n",
    "    print(\"TEXTURE MEMORY\")\n",
    "    print(\"  Cache: ~48 KB per SM\")\n",
    "    print(\"  Access: Optimized for 2D spatial locality\")\n",
    "    print(\"  Features:\")\n",
    "    print(\"    - Hardware interpolation\")\n",
    "    print(\"    - Boundary handling (clamp/wrap)\")\n",
    "    print(\"    - Normalized coordinates\")\n",
    "    print(\"  Best for: Image processing, lookup tables\")\n",
    "    print()\n",
    "    print(\"READ-ONLY CACHE (via __ldg)\")\n",
    "    print(\"  Uses texture cache for global loads\")\n",
    "    print(\"  float x = __ldg(&data[idx]);\")\n",
    "    print(\"  Automatic with 'const __restrict__' pointers\")\n",
    "\n",
    "special_caches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a96e5",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Constant Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constant_memory.cu\n",
    "// constant_memory.cu - Using constant memory\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Declare constant memory (at file scope)\n",
    "__constant__ float coefficients[256];\n",
    "\n",
    "__global__ void applyCoefficients(float* data, float* result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float x = data[idx];\n",
    "        int coef_idx = idx % 256;\n",
    "        \n",
    "        // All threads in warp likely read same coefficient\n",
    "        // â†’ Constant cache broadcasts efficiently\n",
    "        result[idx] = x * coefficients[coef_idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1 << 20;  // 1M elements\n",
    "    \n",
    "    // Copy to constant memory\n",
    "    float h_coefs[256];\n",
    "    for (int i = 0; i < 256; i++) {\n",
    "        h_coefs[i] = i * 0.1f;\n",
    "    }\n",
    "    cudaMemcpyToSymbol(coefficients, h_coefs, sizeof(h_coefs));\n",
    "    \n",
    "    // Allocate and initialize data\n",
    "    float *h_data = (float*)malloc(n * sizeof(float));\n",
    "    float *h_result = (float*)malloc(n * sizeof(float));\n",
    "    for (int i = 0; i < n; i++) h_data[i] = 1.0f;\n",
    "    \n",
    "    float *d_data, *d_result;\n",
    "    cudaMalloc(&d_data, n * sizeof(float));\n",
    "    cudaMalloc(&d_result, n * sizeof(float));\n",
    "    cudaMemcpy(d_data, h_data, n * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch kernel\n",
    "    applyCoefficients<<<(n+255)/256, 256>>>(d_data, d_result, n);\n",
    "    \n",
    "    cudaMemcpy(h_result, d_result, n * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"result[0] = %f (expected: %f)\\n\", h_result[0], h_coefs[0]);\n",
    "    printf(\"result[1] = %f (expected: %f)\\n\", h_result[1], h_coefs[1]);\n",
    "    printf(\"result[255] = %f (expected: %f)\\n\", h_result[255], h_coefs[255]);\n",
    "    \n",
    "    free(h_data);\n",
    "    free(h_result);\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o constant_memory constant_memory.cu\n",
    "!./constant_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0ebd7",
   "metadata": {},
   "source": [
    "### ğŸ”· Using __ldg for Read-Only Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1aff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ldg_readonly.cu\n",
    "// ldg_readonly.cu - Using __ldg for read-only data\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Option 1: Explicit __ldg intrinsic\n",
    "__global__ void withLdg(const float* data, float* result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Uses texture cache path\n",
    "        float x = __ldg(&data[idx]);\n",
    "        result[idx] = x * x;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Option 2: const __restrict__ (compiler may use __ldg)\n",
    "__global__ void withRestrict(const float* __restrict__ data, \n",
    "                             float* __restrict__ result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Compiler knows data is read-only and non-aliasing\n",
    "        result[idx] = data[idx] * data[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1 << 20;\n",
    "    \n",
    "    float *h_data = (float*)malloc(n * sizeof(float));\n",
    "    float *h_result = (float*)malloc(n * sizeof(float));\n",
    "    for (int i = 0; i < n; i++) h_data[i] = (float)i;\n",
    "    \n",
    "    float *d_data, *d_result;\n",
    "    cudaMalloc(&d_data, n * sizeof(float));\n",
    "    cudaMalloc(&d_result, n * sizeof(float));\n",
    "    cudaMemcpy(d_data, h_data, n * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Test both kernels\n",
    "    withLdg<<<(n+255)/256, 256>>>(d_data, d_result, n);\n",
    "    cudaMemcpy(h_result, d_result, n * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"withLdg: result[2] = %f (expected: %f)\\n\", h_result[2], 4.0f);\n",
    "    \n",
    "    withRestrict<<<(n+255)/256, 256>>>(d_data, d_result, n);\n",
    "    cudaMemcpy(h_result, d_result, n * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"withRestrict: result[2] = %f (expected: %f)\\n\", h_result[2], 4.0f);\n",
    "    \n",
    "    free(h_data);\n",
    "    free(h_result);\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e257aa",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea15a58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Cache-Aware Algorithm Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cache-friendly vs cache-unfriendly matrix access\n",
    "\n",
    "@cuda.jit\n",
    "def row_major_sum(matrix, result, rows, cols):\n",
    "    \"\"\"Row-major traversal - cache friendly.\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < rows:\n",
    "        total = 0.0\n",
    "        for j in range(cols):\n",
    "            total += matrix[idx, j]  # Sequential in memory\n",
    "        result[idx] = total\n",
    "\n",
    "@cuda.jit\n",
    "def col_major_sum(matrix, result, rows, cols):\n",
    "    \"\"\"Column-major traversal - cache unfriendly for row-major layout.\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < cols:\n",
    "        total = 0.0\n",
    "        for i in range(rows):\n",
    "            total += matrix[i, idx]  # Strided in memory\n",
    "        result[idx] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matrix_access(rows=1024, cols=1024):\n",
    "    \"\"\"Compare row vs column traversal.\"\"\"\n",
    "    matrix = np.random.rand(rows, cols).astype(np.float32)\n",
    "    result_row = np.zeros(rows, dtype=np.float32)\n",
    "    result_col = np.zeros(cols, dtype=np.float32)\n",
    "    \n",
    "    d_matrix = cuda.to_device(matrix)\n",
    "    d_result_row = cuda.to_device(result_row)\n",
    "    d_result_col = cuda.to_device(result_col)\n",
    "    \n",
    "    block = 256\n",
    "    grid_row = (rows + block - 1) // block\n",
    "    grid_col = (cols + block - 1) // block\n",
    "    \n",
    "    # Warmup\n",
    "    row_major_sum[grid_row, block](d_matrix, d_result_row, rows, cols)\n",
    "    col_major_sum[grid_col, block](d_matrix, d_result_col, rows, cols)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark row-major\n",
    "    iterations = 50\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        row_major_sum[grid_row, block](d_matrix, d_result_row, rows, cols)\n",
    "    cuda.synchronize()\n",
    "    row_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # Benchmark column-major\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        col_major_sum[grid_col, block](d_matrix, d_result_col, rows, cols)\n",
    "    cuda.synchronize()\n",
    "    col_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    print(f\"Matrix Access Pattern Comparison ({rows}Ã—{cols})\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Row-major (cache-friendly):   {row_time:.3f} ms\")\n",
    "    print(f\"Column-major (cache-unfriendly): {col_time:.3f} ms\")\n",
    "    print(f\"Ratio: {col_time/row_time:.2f}x slower\")\n",
    "\n",
    "benchmark_matrix_access()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaaea30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cache_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Cache Line Utilization Analysis\n",
    "// ============================================================\n",
    "\n",
    "// Coalesced access (stride 1) - good cache utilization\n",
    "__global__ void coalescedAccess(const float* data, float* result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        result[idx] = data[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Strided access - poor cache utilization\n",
    "__global__ void stridedAccess(const float* data, float* result, int n, int stride) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int dataIdx = idx * stride;\n",
    "    if (dataIdx < n) {\n",
    "        result[idx] = data[dataIdx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: L1 vs L2 Cache Behavior\n",
    "// ============================================================\n",
    "\n",
    "// Small working set - fits in L1\n",
    "__global__ void l1CacheHit(const float* data, float* result, int workingSetSize, int iters) {\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = tid % workingSetSize;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < iters; i++) {\n",
    "        sum += data[idx];  // Same small set repeatedly - L1 cache hits\n",
    "    }\n",
    "    result[tid] = sum;\n",
    "}\n",
    "\n",
    "// Large working set - spills to L2\n",
    "__global__ void l2CacheMiss(const float* data, float* result, int dataSize, int iters) {\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < iters; i++) {\n",
    "        int idx = (tid * 1024 + i * 32) % dataSize;  // Random-ish access\n",
    "        sum += data[idx];\n",
    "    }\n",
    "    result[tid] = sum;\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Read-Only Cache (__ldg)\n",
    "// ============================================================\n",
    "\n",
    "// Normal load\n",
    "__global__ void normalLoad(const float* __restrict__ data, float* result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        result[idx] = data[idx] * 2.0f + data[(idx + 1) % n] * 0.5f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Using __ldg (read-only cache path)\n",
    "__global__ void ldgLoad(const float* __restrict__ data, float* result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        result[idx] = __ldg(&data[idx]) * 2.0f + __ldg(&data[(idx + 1) % n]) * 0.5f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Benchmarks\n",
    "// ============================================================\n",
    "\n",
    "void benchmarkCoalescing() {\n",
    "    printf(\"=== Exercise 1: Cache Line Utilization ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 24;  // 16M elements\n",
    "    const int numAccessed = 1 << 20;  // 1M actual accesses\n",
    "    \n",
    "    float *d_data, *d_result;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_result, n * sizeof(float)));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (numAccessed + blockSize - 1) / blockSize;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int strides[] = {1, 2, 4, 8, 16, 32};\n",
    "    int numStrides = sizeof(strides) / sizeof(strides[0]);\n",
    "    \n",
    "    printf(\"%-10s %-15s %-15s %-15s\\n\", \"Stride\", \"Time (ms)\", \"Bandwidth\", \"Efficiency\");\n",
    "    printf(\"----------------------------------------------------------\\n\");\n",
    "    \n",
    "    for (int s = 0; s < numStrides; s++) {\n",
    "        int stride = strides[s];\n",
    "        \n",
    "        // Warmup\n",
    "        stridedAccess<<<gridSize, blockSize>>>(d_data, d_result, n, stride);\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            stridedAccess<<<gridSize, blockSize>>>(d_data, d_result, n, stride);\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        \n",
    "        // Calculate effective bandwidth\n",
    "        float dataBytes = 2.0f * numAccessed * sizeof(float) * 100;  // read + write\n",
    "        float gbps = dataBytes / (ms * 1e6);\n",
    "        float efficiency = 100.0f / stride;  // Simplified efficiency estimate\n",
    "        \n",
    "        printf(\"%-10d %-15.2f %-12.2f GB/s %-10.1f%%\\n\", stride, ms, gbps, efficiency);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "}\n",
    "\n",
    "void benchmarkCacheLevels() {\n",
    "    printf(\"=== Exercise 2: L1 vs L2 Cache ===\\n\");\n",
    "    \n",
    "    const int dataSize = 1 << 24;\n",
    "    const int iters = 1000;\n",
    "    \n",
    "    float *d_data, *d_result;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, dataSize * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_result, 256 * sizeof(float)));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Small working set (L1)\n",
    "    int l1Size = 64;  // 64 floats = 256 bytes, fits in L1\n",
    "    cudaEventRecord(start);\n",
    "    l1CacheHit<<<1, 256>>>(d_data, d_result, l1Size, iters);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float l1Ms;\n",
    "    cudaEventElapsedTime(&l1Ms, start, stop);\n",
    "    \n",
    "    // Large working set (L2/memory)\n",
    "    cudaEventRecord(start);\n",
    "    l2CacheMiss<<<1, 256>>>(d_data, d_result, dataSize, iters);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float l2Ms;\n",
    "    cudaEventElapsedTime(&l2Ms, start, stop);\n",
    "    \n",
    "    printf(\"Small working set (L1): %.3f ms\\n\", l1Ms);\n",
    "    printf(\"Large working set (L2): %.3f ms\\n\", l2Ms);\n",
    "    printf(\"Ratio: %.2fx\\n\\n\", l2Ms / l1Ms);\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "}\n",
    "\n",
    "void benchmarkLdg() {\n",
    "    printf(\"=== Exercise 3: __ldg Read-Only Cache ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 24;\n",
    "    \n",
    "    float *d_data, *d_result;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_result, n * sizeof(float)));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Warmup\n",
    "    normalLoad<<<gridSize, blockSize>>>(d_data, d_result, n);\n",
    "    ldgLoad<<<gridSize, blockSize>>>(d_data, d_result, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark normal\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        normalLoad<<<gridSize, blockSize>>>(d_data, d_result, n);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float normalMs;\n",
    "    cudaEventElapsedTime(&normalMs, start, stop);\n",
    "    \n",
    "    // Benchmark __ldg\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        ldgLoad<<<gridSize, blockSize>>>(d_data, d_result, n);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float ldgMs;\n",
    "    cudaEventElapsedTime(&ldgMs, start, stop);\n",
    "    \n",
    "    printf(\"Normal load: %.2f ms\\n\", normalMs);\n",
    "    printf(\"__ldg load:  %.2f ms\\n\", ldgMs);\n",
    "    printf(\"Speedup: %.2fx\\n\\n\", normalMs / ldgMs);\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_result);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘              Cache Optimization Exercises                    â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"L2 Cache Size: %d KB\\n\\n\", prop.l2CacheSize / 1024);\n",
    "    \n",
    "    benchmarkCoalescing();\n",
    "    benchmarkCacheLevels();\n",
    "    benchmarkLdg();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cache_exercises cache_exercises.cu && ./cache_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da96b4a",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Cache Line Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af430692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate cache line utilization for different access patterns\n",
    "\n",
    "def cache_line_efficiency(element_size, stride, warp_size=32, cache_line=128):\n",
    "    \"\"\"\n",
    "    Calculate cache line efficiency.\n",
    "    \n",
    "    Args:\n",
    "        element_size: Size of each element in bytes\n",
    "        stride: Access stride (1 = consecutive)\n",
    "        warp_size: Threads per warp\n",
    "        cache_line: Cache line size in bytes\n",
    "    \n",
    "    Returns:\n",
    "        Efficiency (0.0 - 1.0)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # Hint: Calculate how many cache lines are needed\n",
    "    #       vs how much data is actually used\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# cache_line_efficiency(4, 1)   # float, stride 1 â†’ should be 100%\n",
    "# cache_line_efficiency(4, 32)  # float, stride 32 â†’ should be low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f791a76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Cache Hierarchy\n",
    "\n",
    "| Level | Size | Latency | Scope |\n",
    "|-------|------|---------|-------|\n",
    "| L1 | 48-128 KB | ~30 cycles | Per SM |\n",
    "| L2 | 1.5-40 MB | ~200 cycles | Shared |\n",
    "| Texture | ~48 KB | ~100 cycles | Per SM |\n",
    "| Constant | 8 KB | ~5 cycles | Per SM |\n",
    "\n",
    "### Coalescing Rules\n",
    "\n",
    "1. **Consecutive threads â†’ Consecutive addresses** (ideal)\n",
    "2. **Cache line = 128 bytes**\n",
    "3. **Stride-1 access = 100% efficiency**\n",
    "4. **Stride-32+ = ~3% efficiency**\n",
    "\n",
    "### CUDA C++ Patterns\n",
    "\n",
    "```cpp\n",
    "// Set cache preference\n",
    "cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferL1);\n",
    "\n",
    "// Use read-only cache\n",
    "float x = __ldg(&data[idx]);\n",
    "\n",
    "// Constant memory\n",
    "__constant__ float coefs[256];\n",
    "cudaMemcpyToSymbol(coefs, h_coefs, sizeof(coefs));\n",
    "```\n",
    "\n",
    "### Tomorrow: Unified Memory\n",
    "We'll explore simplified memory management with unified memory."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
