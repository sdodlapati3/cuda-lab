{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"⚠️  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3de5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: GPU Cache Hierarchy\n",
    "\n",
    "### Memory Hierarchy\n",
    "\n",
    "```\n",
    "GPU Memory Hierarchy:\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     Registers (per thread)                   │\n",
    "│                     ~1 cycle, 255 max                        │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              Shared Memory / L1 Cache (per SM)               │\n",
    "│              ~5-30 cycles, 48-228KB configurable             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    L2 Cache (shared)                         │\n",
    "│                    ~100-200 cycles, 1.5-80MB                 │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 Global Memory (DRAM)                         │\n",
    "│                 ~400-800 cycles, 8-80GB                      │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### L1 Cache Details\n",
    "\n",
    "```\n",
    "L1 Cache (per SM):\n",
    "  - Unified with shared memory (128-228KB total)\n",
    "  - Configurable split: more shared OR more L1\n",
    "  - Cache line: 128 bytes\n",
    "  - Write-through (writes go to L2)\n",
    "  - Automatic caching of global loads\n",
    "\n",
    "Configuration options (cudaFuncSetCacheConfig):\n",
    "  cudaFuncCachePreferNone     Default\n",
    "  cudaFuncCachePreferShared   Prefer shared memory\n",
    "  cudaFuncCachePreferL1       Prefer L1 cache\n",
    "  cudaFuncCachePreferEqual    Equal split\n",
    "```\n",
    "\n",
    "### CUDA C++ Cache Configuration (Primary)\n",
    "\n",
    "```cpp\n",
    "// cache_config.cu - Configure L1/shared memory split\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void memoryIntensiveKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Read many times from different locations\n",
    "        float sum = 0;\n",
    "        for (int i = 0; i < 16; i++) {\n",
    "            int offset = (idx + i * 1024) % n;\n",
    "            sum += data[offset];\n",
    "        }\n",
    "        data[idx] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Query current cache config\n",
    "    cudaFuncCache currentConfig;\n",
    "    cudaDeviceGetCacheConfig(&currentConfig);\n",
    "    printf(\"Current config: %d\\n\", currentConfig);\n",
    "    \n",
    "    // Set cache preference for kernel\n",
    "    cudaFuncSetCacheConfig(memoryIntensiveKernel, cudaFuncCachePreferL1);\n",
    "    \n",
    "    // For kernels that use shared memory heavily:\n",
    "    // cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferShared);\n",
    "    \n",
    "    // Query device L1/shared config\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Shared memory per block: %zu KB\\n\", prop.sharedMemPerBlock / 1024);\n",
    "    printf(\"Shared memory per SM: %zu KB\\n\", prop.sharedMemPerMultiprocessor / 1024);\n",
    "    printf(\"L2 cache size: %d KB\\n\", prop.l2CacheSize / 1024);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query cache information\n",
    "device = cuda.get_current_device()\n",
    "\n",
    "print(\"GPU Cache Information\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Device: {device.name}\")\n",
    "print(f\"Max shared memory per block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.0f} KB\")\n",
    "\n",
    "# Get more detailed info via context\n",
    "ctx = cuda.current_context()\n",
    "try:\n",
    "    print(f\"\\nNote: L1/L2 cache sizes vary by GPU architecture\")\n",
    "    print(f\"Typical L1: 48-128 KB per SM\")\n",
    "    print(f\"Typical L2: 1.5-40 MB shared\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca8cb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Coalescing Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_coalescing():\n",
    "    \"\"\"Deep dive into memory coalescing.\"\"\"\n",
    "    print(\"Memory Coalescing\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"Definition: Combining multiple memory requests into fewer\")\n",
    "    print(\"            transactions at cache-line granularity (128 bytes)\")\n",
    "    print()\n",
    "    print(\"COALESCED ACCESS (GOOD):\")\n",
    "    print(\"  Warp threads 0-31 access addresses 0-124 (consecutive floats)\")\n",
    "    print(\"  → 1 cache line transaction (128 bytes = 32 floats)\")\n",
    "    print()\n",
    "    print(\"  Thread 0:  addr 0\")\n",
    "    print(\"  Thread 1:  addr 4\")\n",
    "    print(\"  Thread 2:  addr 8\")\n",
    "    print(\"  ...\")\n",
    "    print(\"  Thread 31: addr 124\")\n",
    "    print(\"  ────────────────────\")\n",
    "    print(\"  Result: ONE 128-byte transaction\")\n",
    "    print()\n",
    "    print(\"STRIDED ACCESS (BAD):\")\n",
    "    print(\"  Warp threads access every 16th element (stride = 64 bytes)\")\n",
    "    print(\"  → Multiple cache line transactions\")\n",
    "    print()\n",
    "    print(\"  Thread 0:  addr 0    → cache line 0\")\n",
    "    print(\"  Thread 1:  addr 64   → cache line 0\")\n",
    "    print(\"  Thread 2:  addr 128  → cache line 1\")\n",
    "    print(\"  Thread 3:  addr 192  → cache line 1\")\n",
    "    print(\"  ...\")\n",
    "    print(\"  Result: 16+ cache line transactions!\")\n",
    "\n",
    "explain_coalescing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate coalescing impact\n",
    "\n",
    "@cuda.jit\n",
    "def coalesced_access(data, result, stride):\n",
    "    \"\"\"Stride-1 access (coalesced).\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < result.size:\n",
    "        result[idx] = data[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def strided_access(data, result, stride):\n",
    "    \"\"\"Strided access (not coalesced).\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < result.size:\n",
    "        # Strided read - bad for coalescing\n",
    "        src_idx = (idx * stride) % data.size\n",
    "        result[idx] = data[src_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a600b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_coalescing(n=1_000_000):\n",
    "    \"\"\"Benchmark coalesced vs strided access.\"\"\"\n",
    "    data = np.random.rand(n * 32).astype(np.float32)  # Extra space for strides\n",
    "    result = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    d_data = cuda.to_device(data)\n",
    "    d_result = cuda.to_device(result)\n",
    "    \n",
    "    block = 256\n",
    "    grid = (n + block - 1) // block\n",
    "    \n",
    "    print(f\"Coalescing Benchmark ({n:,} elements)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for stride in [1, 2, 4, 8, 16, 32]:\n",
    "        # Warmup\n",
    "        strided_access[grid, block](d_data, d_result, stride)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        iterations = 100\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            strided_access[grid, block](d_data, d_result, stride)\n",
    "        cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / iterations * 1000\n",
    "        \n",
    "        # Calculate bandwidth\n",
    "        bytes_moved = n * 4 * 2  # read + write\n",
    "        bandwidth = bytes_moved / (elapsed / 1000) / 1e9\n",
    "        \n",
    "        print(f\"Stride {stride:2d}: {elapsed:.3f} ms, {bandwidth:.1f} GB/s\")\n",
    "\n",
    "benchmark_coalescing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e777b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: L2 Cache Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_cache_strategies():\n",
    "    \"\"\"Strategies for L2 cache optimization.\"\"\"\n",
    "    print(\"L2 Cache Optimization Strategies\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"1. DATA LOCALITY\")\n",
    "    print(\"   Keep working set smaller than L2 cache\")\n",
    "    print(\"   Typical L2: 1.5-40 MB\")\n",
    "    print()\n",
    "    print(\"2. PERSISTENCE (Ampere+)\")\n",
    "    print(\"   cudaAccessPropertyPersisting: keep in L2\")\n",
    "    print(\"   cudaAccessPropertyStreaming: don't cache\")\n",
    "    print()\n",
    "    print(\"3. CACHE PARTITIONING (Ampere+)\")\n",
    "    print(\"   Reserve portion of L2 for specific data\")\n",
    "    print(\"   cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, bytes)\")\n",
    "    print()\n",
    "    print(\"4. ACCESS PATTERNS\")\n",
    "    print(\"   Sequential: Best cache utilization\")\n",
    "    print(\"   Random: Poor cache utilization\")\n",
    "    print(\"   Blocked/Tiled: Good cache reuse\")\n",
    "\n",
    "l2_cache_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6831c6fa",
   "metadata": {},
   "source": [
    "### CUDA C++ L2 Cache Control (Ampere+)\n",
    "\n",
    "```cpp\n",
    "// l2_cache.cu - L2 cache persistence (Ampere+)\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int main() {\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    \n",
    "    printf(\"L2 cache size: %d MB\\n\", prop.l2CacheSize / (1024 * 1024));\n",
    "    \n",
    "    // On Ampere+, you can control L2 persistence\n",
    "    if (prop.major >= 8) {\n",
    "        // Reserve some L2 cache for persistent data\n",
    "        size_t persistingL2 = prop.l2CacheSize / 2;  // Reserve half\n",
    "        cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, persistingL2);\n",
    "        \n",
    "        // Set access policy for a memory range\n",
    "        float* d_persistent;\n",
    "        size_t size = 1024 * 1024;  // 1MB\n",
    "        cudaMalloc(&d_persistent, size);\n",
    "        \n",
    "        cudaStreamAttrValue attr;\n",
    "        attr.accessPolicyWindow.base_ptr = d_persistent;\n",
    "        attr.accessPolicyWindow.num_bytes = size;\n",
    "        attr.accessPolicyWindow.hitProp = cudaAccessPropertyPersisting;\n",
    "        attr.accessPolicyWindow.missProp = cudaAccessPropertyStreaming;\n",
    "        attr.accessPolicyWindow.hitRatio = 1.0f;\n",
    "        \n",
    "        cudaStream_t stream;\n",
    "        cudaStreamCreate(&stream);\n",
    "        cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &attr);\n",
    "        \n",
    "        // Now kernels on this stream will try to keep d_persistent in L2\n",
    "        \n",
    "        cudaStreamDestroy(stream);\n",
    "        cudaFree(d_persistent);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2bb18e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Texture and Constant Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47039dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_caches():\n",
    "    \"\"\"Explain texture and constant memory caches.\"\"\"\n",
    "    print(\"Special GPU Caches\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"CONSTANT MEMORY\")\n",
    "    print(\"  Size: 64 KB total, 8 KB cache per SM\")\n",
    "    print(\"  Access: Broadcast to all threads in warp\")\n",
    "    print(\"  Best for: Same value read by all threads\")\n",
    "    print(\"  Declare: __constant__ float data[1024];\")\n",
    "    print()\n",
    "    print(\"TEXTURE MEMORY\")\n",
    "    print(\"  Cache: ~48 KB per SM\")\n",
    "    print(\"  Access: Optimized for 2D spatial locality\")\n",
    "    print(\"  Features:\")\n",
    "    print(\"    - Hardware interpolation\")\n",
    "    print(\"    - Boundary handling (clamp/wrap)\")\n",
    "    print(\"    - Normalized coordinates\")\n",
    "    print(\"  Best for: Image processing, lookup tables\")\n",
    "    print()\n",
    "    print(\"READ-ONLY CACHE (via __ldg)\")\n",
    "    print(\"  Uses texture cache for global loads\")\n",
    "    print(\"  float x = __ldg(&data[idx]);\")\n",
    "    print(\"  Automatic with 'const __restrict__' pointers\")\n",
    "\n",
    "special_caches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a96e5",
   "metadata": {},
   "source": [
    "### CUDA C++ Constant Memory (Primary)\n",
    "\n",
    "```cpp\n",
    "// constant_memory.cu - Using constant memory\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Declare constant memory (at file scope)\n",
    "__constant__ float coefficients[256];\n",
    "\n",
    "__global__ void applyCoefficients(float* data, float* result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float x = data[idx];\n",
    "        int coef_idx = idx % 256;\n",
    "        \n",
    "        // All threads in warp likely read same coefficient\n",
    "        // → Constant cache broadcasts efficiently\n",
    "        result[idx] = x * coefficients[coef_idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Copy to constant memory\n",
    "    float h_coefs[256];\n",
    "    for (int i = 0; i < 256; i++) {\n",
    "        h_coefs[i] = i * 0.1f;\n",
    "    }\n",
    "    \n",
    "    cudaMemcpyToSymbol(coefficients, h_coefs, sizeof(h_coefs));\n",
    "    \n",
    "    // ... launch kernel ...\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Using __ldg for Read-Only Data\n",
    "\n",
    "```cpp\n",
    "// Option 1: Explicit __ldg intrinsic\n",
    "__global__ void withLdg(const float* data, float* result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Uses texture cache path\n",
    "        float x = __ldg(&data[idx]);\n",
    "        result[idx] = x * x;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Option 2: const __restrict__ (compiler may use __ldg)\n",
    "__global__ void withRestrict(const float* __restrict__ data, \n",
    "                             float* __restrict__ result, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Compiler knows data is read-only and non-aliasing\n",
    "        result[idx] = data[idx] * data[idx];\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea15a58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Cache-Aware Algorithm Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cache-friendly vs cache-unfriendly matrix access\n",
    "\n",
    "@cuda.jit\n",
    "def row_major_sum(matrix, result, rows, cols):\n",
    "    \"\"\"Row-major traversal - cache friendly.\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < rows:\n",
    "        total = 0.0\n",
    "        for j in range(cols):\n",
    "            total += matrix[idx, j]  # Sequential in memory\n",
    "        result[idx] = total\n",
    "\n",
    "@cuda.jit\n",
    "def col_major_sum(matrix, result, rows, cols):\n",
    "    \"\"\"Column-major traversal - cache unfriendly for row-major layout.\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < cols:\n",
    "        total = 0.0\n",
    "        for i in range(rows):\n",
    "            total += matrix[i, idx]  # Strided in memory\n",
    "        result[idx] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matrix_access(rows=1024, cols=1024):\n",
    "    \"\"\"Compare row vs column traversal.\"\"\"\n",
    "    matrix = np.random.rand(rows, cols).astype(np.float32)\n",
    "    result_row = np.zeros(rows, dtype=np.float32)\n",
    "    result_col = np.zeros(cols, dtype=np.float32)\n",
    "    \n",
    "    d_matrix = cuda.to_device(matrix)\n",
    "    d_result_row = cuda.to_device(result_row)\n",
    "    d_result_col = cuda.to_device(result_col)\n",
    "    \n",
    "    block = 256\n",
    "    grid_row = (rows + block - 1) // block\n",
    "    grid_col = (cols + block - 1) // block\n",
    "    \n",
    "    # Warmup\n",
    "    row_major_sum[grid_row, block](d_matrix, d_result_row, rows, cols)\n",
    "    col_major_sum[grid_col, block](d_matrix, d_result_col, rows, cols)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark row-major\n",
    "    iterations = 50\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        row_major_sum[grid_row, block](d_matrix, d_result_row, rows, cols)\n",
    "    cuda.synchronize()\n",
    "    row_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    # Benchmark column-major\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        col_major_sum[grid_col, block](d_matrix, d_result_col, rows, cols)\n",
    "    cuda.synchronize()\n",
    "    col_time = (time.perf_counter() - start) / iterations * 1000\n",
    "    \n",
    "    print(f\"Matrix Access Pattern Comparison ({rows}×{cols})\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Row-major (cache-friendly):   {row_time:.3f} ms\")\n",
    "    print(f\"Column-major (cache-unfriendly): {col_time:.3f} ms\")\n",
    "    print(f\"Ratio: {col_time/row_time:.2f}x slower\")\n",
    "\n",
    "benchmark_matrix_access()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaaea30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Cache Line Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af430692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate cache line utilization for different access patterns\n",
    "\n",
    "def cache_line_efficiency(element_size, stride, warp_size=32, cache_line=128):\n",
    "    \"\"\"\n",
    "    Calculate cache line efficiency.\n",
    "    \n",
    "    Args:\n",
    "        element_size: Size of each element in bytes\n",
    "        stride: Access stride (1 = consecutive)\n",
    "        warp_size: Threads per warp\n",
    "        cache_line: Cache line size in bytes\n",
    "    \n",
    "    Returns:\n",
    "        Efficiency (0.0 - 1.0)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # Hint: Calculate how many cache lines are needed\n",
    "    #       vs how much data is actually used\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# cache_line_efficiency(4, 1)   # float, stride 1 → should be 100%\n",
    "# cache_line_efficiency(4, 32)  # float, stride 32 → should be low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f791a76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Cache Hierarchy\n",
    "\n",
    "| Level | Size | Latency | Scope |\n",
    "|-------|------|---------|-------|\n",
    "| L1 | 48-128 KB | ~30 cycles | Per SM |\n",
    "| L2 | 1.5-40 MB | ~200 cycles | Shared |\n",
    "| Texture | ~48 KB | ~100 cycles | Per SM |\n",
    "| Constant | 8 KB | ~5 cycles | Per SM |\n",
    "\n",
    "### Coalescing Rules\n",
    "\n",
    "1. **Consecutive threads → Consecutive addresses** (ideal)\n",
    "2. **Cache line = 128 bytes**\n",
    "3. **Stride-1 access = 100% efficiency**\n",
    "4. **Stride-32+ = ~3% efficiency**\n",
    "\n",
    "### CUDA C++ Patterns\n",
    "\n",
    "```cpp\n",
    "// Set cache preference\n",
    "cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferL1);\n",
    "\n",
    "// Use read-only cache\n",
    "float x = __ldg(&data[idx]);\n",
    "\n",
    "// Constant memory\n",
    "__constant__ float coefs[256];\n",
    "cudaMemcpyToSymbol(coefs, h_coefs, sizeof(coefs));\n",
    "```\n",
    "\n",
    "### Tomorrow: Unified Memory\n",
    "We'll explore simplified memory management with unified memory."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
