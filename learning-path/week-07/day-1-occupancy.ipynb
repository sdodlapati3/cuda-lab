{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9dd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bd3b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 1: GPU Occupancy - Maximizing SM Utilization\n",
    "\n",
    "## ğŸ¯ Why This Matters\n",
    "\n",
    "Ever wonder why some CUDA kernels leave the GPU partially idle even with thousands of threads? Just like a restaurant kitchen needs enough chefs to keep cooking while others wait for ingredients, your GPU needs enough active warps to stay productive while some wait for memory. **Occupancy** is the secret to hiding memory latency and keeping your SMs humming.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. **Calculate occupancy** using the formula: Active Warps / Max Warps per SM\n",
    "2. **Identify the three occupancy limiters**: threads, registers, and shared memory\n",
    "3. **Use CUDA occupancy APIs** to query and optimize block sizes\n",
    "4. **Understand when to prioritize occupancy** vs. other optimization strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: What is Occupancy?\n",
    "\n",
    "### Definition\n",
    "\n",
    "```\n",
    "                    Active Warps per SM\n",
    "Occupancy (%) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã— 100\n",
    "                 Maximum Warps per SM\n",
    "\n",
    "Example: 32 active warps / 64 max warps = 50% occupancy\n",
    "```\n",
    "\n",
    "### Why Occupancy Matters\n",
    "\n",
    "```\n",
    "GPU hides memory latency through parallelism:\n",
    "\n",
    "Low Occupancy (25%):\n",
    "  Warp 0: [COMPUTE]â”€â”€[WAIT FOR MEMORY]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[COMPUTE]\n",
    "  Warp 1: â”€â”€[COMPUTE]â”€â”€[WAIT FOR MEMORY]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[COMPUTE]\n",
    "  Warp 2: â”€â”€â”€â”€[COMPUTE]â”€â”€[WAIT FOR MEMORY]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[COMPUTE]\n",
    "  Warp 3: â”€â”€â”€â”€â”€â”€[COMPUTE]â”€â”€[WAIT FOR MEMORY]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[COMPUTE]\n",
    "  SM:     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "                          ^^ SM idle, waiting for memory\n",
    "\n",
    "High Occupancy (100%):\n",
    "  Warp 0:  [COMP][WAIT....][COMP]\n",
    "  Warp 1:  [COMP][WAIT....][COMP]\n",
    "  ...many more warps...\n",
    "  Warp 63: [COMP][WAIT....][COMP]\n",
    "  SM:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "                    ^^ SM always has work to do\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c9364",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸƒ Concept Card: Restaurant Kitchen Analogy\n",
    "\n",
    "```\n",
    "OCCUPANCY = How many chefs are actively working in the kitchen\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    THE RESTAURANT KITCHEN                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  Kitchen = SM (Streaming Multiprocessor)                         â”‚\n",
    "â”‚  Chefs = Warps (groups of 32 threads)                           â”‚\n",
    "â”‚  Cooking = Computing                                             â”‚\n",
    "â”‚  Waiting for ingredients = Waiting for memory                    â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚  LOW OCCUPANCY (2 chefs, 25%):                          â”‚    â”‚\n",
    "â”‚  â”‚                                                          â”‚    â”‚\n",
    "â”‚  â”‚  Chef 1: ğŸ³ cooking... â³ waiting for ingredients...    â”‚    â”‚\n",
    "â”‚  â”‚  Chef 2: â³ waiting... ğŸ³ cooking...                    â”‚    â”‚\n",
    "â”‚  â”‚  Kitchen: Often idle while both wait!                   â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚  HIGH OCCUPANCY (8 chefs, 100%):                        â”‚    â”‚\n",
    "â”‚  â”‚                                                          â”‚    â”‚\n",
    "â”‚  â”‚  Chef 1: ğŸ³â³ğŸ³â³  Chef 5: â³ğŸ³â³ğŸ³                      â”‚    â”‚\n",
    "â”‚  â”‚  Chef 2: â³ğŸ³â³ğŸ³  Chef 6: ğŸ³â³ğŸ³â³                      â”‚    â”‚\n",
    "â”‚  â”‚  Chef 3: ğŸ³â³ğŸ³â³  Chef 7: â³ğŸ³â³ğŸ³                      â”‚    â”‚\n",
    "â”‚  â”‚  Chef 4: â³ğŸ³â³ğŸ³  Chef 8: ğŸ³â³ğŸ³â³                      â”‚    â”‚\n",
    "â”‚  â”‚  Kitchen: Always someone cooking! ğŸ”¥                    â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  WHY MORE CHEFS HELP:                                            â”‚\n",
    "â”‚  â€¢ Memory latency = chef waiting for ingredients (~300 cycles)   â”‚\n",
    "â”‚  â€¢ More chefs = always someone ready to cook                     â”‚\n",
    "â”‚  â€¢ GPU switches between warps instantly (zero overhead)          â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  KITCHEN CAPACITY LIMITS:                                        â”‚\n",
    "â”‚  â€¢ Only so many stoves (registers per thread)                    â”‚\n",
    "â”‚  â€¢ Only so much counter space (shared memory)                    â”‚\n",
    "â”‚  â€¢ Only so many chefs can fit (threads per SM)                   â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  ğŸ’¡ KEY INSIGHT: You don't need 100% occupancy!                  â”‚\n",
    "â”‚     50-75% is often enough to hide memory latency.               â”‚\n",
    "â”‚     Too many chefs = each gets fewer tools (registers)!          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile occupancy.cu\n",
    "// occupancy.cu - Query and optimize occupancy\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void sampleKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float x = data[idx];\n",
    "        data[idx] = x * x + x;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Get device properties\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    \n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Max threads per SM: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
    "    printf(\"Max threads per block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "    printf(\"Registers per SM: %d\\n\", prop.regsPerMultiprocessor);\n",
    "    printf(\"Shared memory per SM: %zu KB\\n\", prop.sharedMemPerMultiprocessor / 1024);\n",
    "    printf(\"Number of SMs: %d\\n\", prop.multiProcessorCount);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    // Calculate occupancy for different block sizes\n",
    "    printf(\"Block Size | Active Blocks/SM | Occupancy\\n\");\n",
    "    printf(\"----------------------------------------\\n\");\n",
    "    \n",
    "    for (int blockSize = 64; blockSize <= 1024; blockSize *= 2) {\n",
    "        int minGridSize, optBlockSize;\n",
    "        \n",
    "        // Query occupancy\n",
    "        cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "            &minGridSize, sampleKernel, blockSize, 0);\n",
    "        \n",
    "        int maxWarpsPerSM = prop.maxThreadsPerMultiProcessor / 32;\n",
    "        int activeWarps = minGridSize * (blockSize / 32);\n",
    "        float occupancy = 100.0f * activeWarps / maxWarpsPerSM;\n",
    "        \n",
    "        printf(\"%10d | %16d | %6.1f%%\\n\", blockSize, minGridSize, occupancy);\n",
    "    }\n",
    "    \n",
    "    // Get optimal block size\n",
    "    int minGridSize, optBlockSize;\n",
    "    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &optBlockSize, \n",
    "                                        sampleKernel, 0, 0);\n",
    "    printf(\"\\nOptimal block size: %d\\n\", optBlockSize);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o occupancy occupancy.cu\n",
    "!./occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0e06e",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query device properties\n",
    "device = cuda.get_current_device()\n",
    "\n",
    "print(\"=== GPU Occupancy Properties ===\")\n",
    "print(f\"Device: {device.name}\")\n",
    "print(f\"Compute Capability: {device.compute_capability}\")\n",
    "print(f\"Max threads per block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "print(f\"Max block dimensions: {device.MAX_BLOCK_DIM_X} x {device.MAX_BLOCK_DIM_Y} x {device.MAX_BLOCK_DIM_Z}\")\n",
    "print(f\"Max grid dimensions: {device.MAX_GRID_DIM_X} x {device.MAX_GRID_DIM_Y} x {device.MAX_GRID_DIM_Z}\")\n",
    "print(f\"Max shared memory per block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.0f} KB\")\n",
    "print(f\"Multiprocessor count: {device.MULTIPROCESSOR_COUNT}\")\n",
    "print(f\"Warp size: {device.WARP_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719482e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Occupancy Limiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbcd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_occupancy_limiters():\n",
    "    \"\"\"Explain the three occupancy limiters.\"\"\"\n",
    "    print(\"The Three Occupancy Limiters\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"1. THREADS (Block Size)\")\n",
    "    print(\"   Max threads per SM: typically 1536-2048\")\n",
    "    print(\"   Max threads per block: 1024\")\n",
    "    print(\"   Issue: Too small block â†’ not enough threads\")\n",
    "    print(\"          Too large block â†’ can't fit enough blocks\")\n",
    "    print()\n",
    "    print(\"2. REGISTERS\")\n",
    "    print(\"   Registers per SM: 65536 (typical)\")\n",
    "    print(\"   Max per thread: 255\")\n",
    "    print(\"   Issue: High register usage â†’ fewer concurrent threads\")\n",
    "    print(\"   Formula: max_threads = regs_per_SM / regs_per_thread\")\n",
    "    print()\n",
    "    print(\"3. SHARED MEMORY\")\n",
    "    print(\"   Shared memory per SM: 48-164 KB\")\n",
    "    print(\"   Configurable with L1 cache\")\n",
    "    print(\"   Issue: Large shared blocks â†’ fewer concurrent blocks\")\n",
    "    print(\"   Formula: max_blocks = smem_per_SM / smem_per_block\")\n",
    "\n",
    "explain_occupancy_limiters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173579fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_occupancy(threads_per_block, regs_per_thread, shared_per_block,\n",
    "                        max_threads_per_sm=2048, max_regs_per_sm=65536,\n",
    "                        max_shared_per_sm=49152, max_blocks_per_sm=32):\n",
    "    \"\"\"Calculate theoretical occupancy.\"\"\"\n",
    "    \n",
    "    # Threads limit\n",
    "    blocks_by_threads = max_threads_per_sm // threads_per_block\n",
    "    \n",
    "    # Registers limit\n",
    "    regs_per_block = threads_per_block * regs_per_thread\n",
    "    if regs_per_block > 0:\n",
    "        blocks_by_regs = max_regs_per_sm // regs_per_block\n",
    "    else:\n",
    "        blocks_by_regs = max_blocks_per_sm\n",
    "    \n",
    "    # Shared memory limit\n",
    "    if shared_per_block > 0:\n",
    "        blocks_by_shared = max_shared_per_sm // shared_per_block\n",
    "    else:\n",
    "        blocks_by_shared = max_blocks_per_sm\n",
    "    \n",
    "    # Block limit\n",
    "    max_blocks = min(blocks_by_threads, blocks_by_regs, blocks_by_shared, max_blocks_per_sm)\n",
    "    \n",
    "    # Calculate occupancy\n",
    "    warps_per_block = threads_per_block // 32\n",
    "    active_warps = max_blocks * warps_per_block\n",
    "    max_warps = max_threads_per_sm // 32\n",
    "    occupancy = 100 * active_warps / max_warps\n",
    "    \n",
    "    print(f\"Occupancy Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Input: {threads_per_block} threads/block, {regs_per_thread} regs/thread, {shared_per_block} bytes shared\")\n",
    "    print(f\"\\nLimiters:\")\n",
    "    print(f\"  By threads:      {blocks_by_threads} blocks\")\n",
    "    print(f\"  By registers:    {blocks_by_regs} blocks\")\n",
    "    print(f\"  By shared mem:   {blocks_by_shared} blocks\")\n",
    "    print(f\"  By block limit:  {max_blocks_per_sm} blocks\")\n",
    "    print(f\"\\nResult:\")\n",
    "    print(f\"  Active blocks:   {max_blocks}\")\n",
    "    print(f\"  Active warps:    {active_warps}/{max_warps}\")\n",
    "    print(f\"  Occupancy:       {occupancy:.1f}%\")\n",
    "    \n",
    "    # Identify limiter\n",
    "    limiter = \"blocks\"\n",
    "    if max_blocks == blocks_by_threads:\n",
    "        limiter = \"threads\"\n",
    "    elif max_blocks == blocks_by_regs:\n",
    "        limiter = \"registers\"\n",
    "    elif max_blocks == blocks_by_shared:\n",
    "        limiter = \"shared memory\"\n",
    "    print(f\"  Limiter:         {limiter}\")\n",
    "    \n",
    "    return occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: High occupancy kernel\n",
    "print(\"Example 1: Simple kernel\")\n",
    "calculate_occupancy(threads_per_block=256, regs_per_thread=32, shared_per_block=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a33b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Register-limited kernel\n",
    "print(\"\\nExample 2: Register-heavy kernel\")\n",
    "calculate_occupancy(threads_per_block=256, regs_per_thread=128, shared_per_block=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Shared-memory limited kernel\n",
    "print(\"\\nExample 3: Shared memory heavy kernel\")\n",
    "calculate_occupancy(threads_per_block=256, regs_per_thread=32, shared_per_block=16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b130c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Occupancy vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb088983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupancy_performance_relationship():\n",
    "    \"\"\"Explain when high occupancy matters.\"\"\"\n",
    "    print(\"Occupancy vs Performance\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"Key Insight: Higher occupancy â‰  always better performance\")\n",
    "    print()\n",
    "    print(\"When HIGH OCCUPANCY helps:\")\n",
    "    print(\"  âœ“ Memory-bound kernels (need to hide latency)\")\n",
    "    print(\"  âœ“ Simple arithmetic operations\")\n",
    "    print(\"  âœ“ Irregular memory access patterns\")\n",
    "    print()\n",
    "    print(\"When HIGH OCCUPANCY may NOT help:\")\n",
    "    print(\"  âœ— Compute-bound kernels (already saturated)\")\n",
    "    print(\"  âœ— High instruction-level parallelism (ILP)\")\n",
    "    print(\"  âœ— When reducing occupancy enables better optimizations\")\n",
    "    print()\n",
    "    print(\"Rule of Thumb:\")\n",
    "    print(\"  - Start with ~50% occupancy\")\n",
    "    print(\"  - Profile to determine if occupancy is the bottleneck\")\n",
    "    print(\"  - Memory-bound: aim for higher occupancy\")\n",
    "    print(\"  - Compute-bound: focus on instruction throughput\")\n",
    "\n",
    "occupancy_performance_relationship()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c51124",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Practical Occupancy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test different block sizes\n",
    "@cuda.jit\n",
    "def simple_kernel(data, result):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < data.size:\n",
    "        x = data[idx]\n",
    "        result[idx] = x * x + x\n",
    "\n",
    "def benchmark_block_sizes(n=10_000_000):\n",
    "    \"\"\"Benchmark different block sizes.\"\"\"\n",
    "    data = np.random.rand(n).astype(np.float32)\n",
    "    result = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    d_data = cuda.to_device(data)\n",
    "    d_result = cuda.to_device(result)\n",
    "    \n",
    "    print(f\"Benchmarking with {n:,} elements\")\n",
    "    print(f\"{'Block Size':<12} {'Grid Size':<12} {'Time (ms)':<12} {'Throughput':<15}\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    for block_size in [32, 64, 128, 256, 512, 1024]:\n",
    "        grid_size = (n + block_size - 1) // block_size\n",
    "        \n",
    "        # Warmup\n",
    "        simple_kernel[grid_size, block_size](d_data, d_result)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        iterations = 50\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            simple_kernel[grid_size, block_size](d_data, d_result)\n",
    "        cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / iterations * 1000\n",
    "        \n",
    "        throughput = n / (elapsed / 1000) / 1e9  # billion elements/sec\n",
    "        print(f\"{block_size:<12} {grid_size:<12} {elapsed:<12.3f} {throughput:.2f} B elem/s\")\n",
    "\n",
    "benchmark_block_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161be117",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be12da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile occupancy_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Compare Memory-Bound vs Compute-Bound at Different Occupancies\n",
    "// ============================================================\n",
    "\n",
    "// Memory-bound kernel: simple read-modify-write\n",
    "__global__ void memoryBoundKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        data[i] = data[i] * 2.0f + 1.0f;  // Minimal compute\n",
    "    }\n",
    "}\n",
    "\n",
    "// Compute-bound kernel: heavy arithmetic\n",
    "__global__ void computeBoundKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        float val = data[i];\n",
    "        // Heavy computation\n",
    "        for (int j = 0; j < 100; j++) {\n",
    "            val = sinf(val) * cosf(val) + sqrtf(fabsf(val));\n",
    "        }\n",
    "        data[i] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Occupancy Query API Demo\n",
    "// ============================================================\n",
    "\n",
    "template<typename Func>\n",
    "void printOccupancyInfo(Func kernel, int blockSize, const char* name) {\n",
    "    int maxActiveBlocksPerSM;\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "        &maxActiveBlocksPerSM, kernel, blockSize, 0);\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    \n",
    "    int warpsPerBlock = blockSize / 32;\n",
    "    int activeWarps = maxActiveBlocksPerSM * warpsPerBlock;\n",
    "    int maxWarpsPerSM = prop.maxThreadsPerMultiProcessor / 32;\n",
    "    float occupancy = 100.0f * activeWarps / maxWarpsPerSM;\n",
    "    \n",
    "    printf(\"%s (block=%d):\\n\", name, blockSize);\n",
    "    printf(\"  Max blocks/SM: %d\\n\", maxActiveBlocksPerSM);\n",
    "    printf(\"  Active warps: %d / %d\\n\", activeWarps, maxWarpsPerSM);\n",
    "    printf(\"  Occupancy: %.1f%%\\n\\n\", occupancy);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: Auto-Tuning Block Size\n",
    "// ============================================================\n",
    "\n",
    "void testAutoTuning() {\n",
    "    printf(\"=== Exercise 3: Auto-Tuning Block Size ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 24;  // 16M elements\n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    int minGridSize, optBlockSize;\n",
    "    \n",
    "    // Query optimal block size for memory-bound kernel\n",
    "    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &optBlockSize,\n",
    "        memoryBoundKernel, 0, 0);\n",
    "    printf(\"Memory-bound kernel:\\n\");\n",
    "    printf(\"  Optimal block size: %d\\n\", optBlockSize);\n",
    "    printf(\"  Minimum grid size: %d\\n\", minGridSize);\n",
    "    \n",
    "    // Query for compute-bound kernel\n",
    "    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &optBlockSize,\n",
    "        computeBoundKernel, 0, 0);\n",
    "    printf(\"Compute-bound kernel:\\n\");\n",
    "    printf(\"  Optimal block size: %d\\n\", optBlockSize);\n",
    "    printf(\"  Minimum grid size: %d\\n\\n\", minGridSize);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 4: Block Size Impact Benchmark\n",
    "// ============================================================\n",
    "\n",
    "void benchmarkOccupancyImpact() {\n",
    "    printf(\"=== Exercise 4: Occupancy Impact Benchmark ===\\n\");\n",
    "    \n",
    "    const int n = 1 << 24;\n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    \n",
    "    int blockSizes[] = {32, 64, 128, 256, 512, 1024};\n",
    "    int numSizes = sizeof(blockSizes) / sizeof(blockSizes[0]);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    printf(\"Memory-bound kernel:\\n\");\n",
    "    printf(\"%-15s %-15s %-15s %-15s\\n\", \"BlockSize\", \"Occupancy(%)\", \"Time(ms)\", \"GB/s\");\n",
    "    printf(\"------------------------------------------------------------\\n\");\n",
    "    \n",
    "    for (int s = 0; s < numSizes; s++) {\n",
    "        int blockSize = blockSizes[s];\n",
    "        int gridSize = (n + blockSize - 1) / blockSize;\n",
    "        \n",
    "        // Get occupancy\n",
    "        int maxActiveBlocks;\n",
    "        cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "            &maxActiveBlocks, memoryBoundKernel, blockSize, 0);\n",
    "        \n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, 0);\n",
    "        float occupancy = 100.0f * maxActiveBlocks * blockSize / prop.maxThreadsPerMultiProcessor;\n",
    "        \n",
    "        // Warmup\n",
    "        memoryBoundKernel<<<gridSize, blockSize>>>(d_data, n);\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        // Benchmark\n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < 10; i++) {\n",
    "            memoryBoundKernel<<<gridSize, blockSize>>>(d_data, n);\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        \n",
    "        float gbps = (2.0f * n * sizeof(float) * 10) / (ms * 1e6);  // read + write\n",
    "        printf(\"%-15d %-15.1f %-15.2f %-15.2f\\n\", blockSize, occupancy, ms, gbps);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘              Occupancy Optimization Exercises                â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Max threads/SM: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
    "    printf(\"Max threads/block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "    printf(\"Registers/SM: %d\\n\\n\", prop.regsPerMultiprocessor);\n",
    "    \n",
    "    printf(\"=== Exercise 1 & 2: Occupancy Info ===\\n\");\n",
    "    printOccupancyInfo(memoryBoundKernel, 256, \"Memory-bound\");\n",
    "    printOccupancyInfo(computeBoundKernel, 256, \"Compute-bound\");\n",
    "    \n",
    "    testAutoTuning();\n",
    "    benchmarkOccupancyImpact();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o occupancy_exercises occupancy_exercises.cu && ./occupancy_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa1d61",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Occupancy Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an interactive occupancy calculator\n",
    "# that takes kernel properties and outputs:\n",
    "# 1. Theoretical occupancy\n",
    "# 2. The limiting factor\n",
    "# 3. Suggestions for improvement\n",
    "\n",
    "def occupancy_advisor(threads, regs, shared):\n",
    "    \"\"\"Provide occupancy advice for given kernel parameters.\"\"\"\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae213ba",
   "metadata": {},
   "source": [
    "### Exercise 2: Occupancy Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create two kernels:\n",
    "# 1. Memory-bound kernel (should benefit from high occupancy)\n",
    "# 2. Compute-bound kernel (may not benefit as much)\n",
    "# Compare performance at different block sizes\n",
    "\n",
    "@cuda.jit\n",
    "def memory_bound_kernel(data, result):\n",
    "    \"\"\"Memory-bound: simple read-modify-write.\"\"\"\n",
    "    pass  # Your implementation\n",
    "\n",
    "@cuda.jit\n",
    "def compute_bound_kernel(data, result):\n",
    "    \"\"\"Compute-bound: heavy arithmetic.\"\"\"\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de60f35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Occupancy Formula\n",
    "\n",
    "```\n",
    "Occupancy = Active Warps / Max Warps per SM\n",
    "```\n",
    "\n",
    "### Three Limiters\n",
    "\n",
    "| Limiter | Resource | Typical Limit |\n",
    "|---------|----------|---------------|\n",
    "| Threads | Threads per SM | 1536-2048 |\n",
    "| Registers | Registers per SM | 65536 |\n",
    "| Shared Memory | Bytes per SM | 48-164 KB |\n",
    "\n",
    "### CUDA C++ APIs\n",
    "\n",
    "```cpp\n",
    "// Query occupancy\n",
    "cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "    &numBlocks, kernel, blockSize, sharedMem);\n",
    "\n",
    "// Get optimal block size\n",
    "cudaOccupancyMaxPotentialBlockSize(\n",
    "    &minGridSize, &blockSize, kernel, 0, 0);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### âœ… Occupancy Optimization Checklist\n",
    "\n",
    "1. â˜ **Calculate theoretical occupancy** - Use the occupancy API or CUDA Occupancy Calculator\n",
    "2. â˜ **Profile achieved occupancy** - Nsight Compute shows actual vs. theoretical\n",
    "3. â˜ **Identify the limiter** - Is it threads, registers, or shared memory?\n",
    "4. â˜ **Consider trade-offs** - Higher occupancy may mean fewer registers per thread\n",
    "5. â˜ **Test at 50%, 75%, 100%** - Sometimes lower occupancy performs better!\n",
    "\n",
    "### ğŸ’¡ Remember\n",
    "\n",
    "- **50% occupancy is often sufficient** for hiding memory latency\n",
    "- **Memory-bound kernels** benefit most from high occupancy\n",
    "- **Compute-bound kernels** may prefer more registers over more warps\n",
    "- **Profile first** - don't blindly maximize occupancy\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® What's Next?\n",
    "\n",
    "**Tomorrow: Register Optimization** - Now that you understand occupancy, we'll dive into the first limiter: **registers**. You'll learn why each thread's \"toolbelt\" of registers affects how many warps can run, and how to use `__launch_bounds__` to control register allocation.\n",
    "\n",
    "*Week 07 Progress: Day 1 âœ… | Day 2 â†’ Register Optimization | Day 3 â†’ Cache | Day 4 â†’ Unified Memory*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
