{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc608c4",
   "metadata": {},
   "source": [
    "## What is MIG?\n",
    "\n",
    "Multi-Instance GPU (MIG) partitions a single GPU into multiple isolated instances:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 Full H100 GPU                   â”‚\n",
    "â”‚  80GB HBM3  |  132 SMs  |  Full Memory BW       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "            MIG Partitioning\n",
    "                      â”‚\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â–¼                       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   MIG 3g.40gb   â”‚     â”‚   MIG 3g.40gb   â”‚\n",
    "â”‚   40GB | 66 SMs â”‚     â”‚   40GB | 66 SMs â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     Instance 0              Instance 1\n",
    "```\n",
    "\n",
    "**Key benefits:**\n",
    "- Hardware-level isolation (memory, SMs, cache)\n",
    "- Quality of Service (QoS) guarantees\n",
    "- Multi-tenant GPU sharing\n",
    "- Right-sizing GPU for workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdeb31",
   "metadata": {},
   "source": [
    "## Querying MIG Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722729cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mig_query.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <nvml.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "#define CHECK_NVML(call) \\\n",
    "    do { \\\n",
    "        nvmlReturn_t result = call; \\\n",
    "        if (result != NVML_SUCCESS) { \\\n",
    "            printf(\"NVML error: %s\\n\", nvmlErrorString(result)); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "void printMIGCapabilities(int device) {\n",
    "    cudaDeviceProp prop;\n",
    "    CHECK_CUDA(cudaGetDeviceProperties(&prop, device));\n",
    "    \n",
    "    printf(\"\\n=== MIG Capabilities for %s ===\\n\", prop.name);\n",
    "    \n",
    "    // Check architecture\n",
    "    int major = prop.major, minor = prop.minor;\n",
    "    printf(\"Compute Capability: %d.%d\\n\", major, minor);\n",
    "    \n",
    "    // MIG requires Ampere (8.0) or newer\n",
    "    if (major < 8) {\n",
    "        printf(\"MIG NOT SUPPORTED - Requires Ampere (8.0) or newer\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Check specific GPU models\n",
    "    const char* name = prop.name;\n",
    "    bool isMIGCapable = false;\n",
    "    \n",
    "    // Known MIG-capable GPUs\n",
    "    if (strstr(name, \"A100\") || strstr(name, \"H100\") || \n",
    "        strstr(name, \"H200\") || strstr(name, \"B100\") ||\n",
    "        strstr(name, \"B200\") || strstr(name, \"A30\")) {\n",
    "        isMIGCapable = true;\n",
    "    }\n",
    "    \n",
    "    printf(\"GPU: %s\\n\", name);\n",
    "    printf(\"MIG Capable: %s\\n\", isMIGCapable ? \"YES\" : \"NO (consumer GPU)\");\n",
    "    \n",
    "    if (!isMIGCapable) {\n",
    "        printf(\"  Note: Only datacenter GPUs (A100, H100, etc.) support MIG\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // Query MIG mode via NVML\n",
    "    nvmlInit();\n",
    "    nvmlDevice_t nvmlDevice;\n",
    "    CHECK_NVML(nvmlDeviceGetHandleByIndex(device, &nvmlDevice));\n",
    "    \n",
    "    unsigned int currentMode, pendingMode;\n",
    "    nvmlReturn_t migResult = nvmlDeviceGetMigMode(nvmlDevice, &currentMode, &pendingMode);\n",
    "    \n",
    "    if (migResult == NVML_SUCCESS) {\n",
    "        printf(\"\\nMIG Mode:\\n\");\n",
    "        printf(\"  Current: %s\\n\", currentMode ? \"ENABLED\" : \"DISABLED\");\n",
    "        printf(\"  Pending: %s\\n\", pendingMode ? \"ENABLED\" : \"DISABLED\");\n",
    "        \n",
    "        if (currentMode) {\n",
    "            // List GPU instances\n",
    "            unsigned int count;\n",
    "            nvmlGpuInstance_t gpuInstances[8];\n",
    "            nvmlReturn_t giResult = nvmlDeviceGetGpuInstances(nvmlDevice, \n",
    "                                                               NVML_GPU_INSTANCE_PROFILE_1_SLICE,\n",
    "                                                               gpuInstances, &count);\n",
    "            printf(\"  GPU Instances found: %u\\n\", count);\n",
    "        }\n",
    "    } else if (migResult == NVML_ERROR_NOT_SUPPORTED) {\n",
    "        printf(\"\\nMIG query not supported on this configuration\\n\");\n",
    "    }\n",
    "    \n",
    "    // Memory info\n",
    "    printf(\"\\nGPU Resources:\\n\");\n",
    "    printf(\"  Total Memory: %.2f GB\\n\", prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));\n",
    "    printf(\"  SMs: %d\\n\", prop.multiProcessorCount);\n",
    "    printf(\"  Memory Bus Width: %d bits\\n\", prop.memoryBusWidth);\n",
    "    \n",
    "    nvmlShutdown();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== MIG Query Tool ===\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CHECK_CUDA(cudaGetDeviceCount(&deviceCount));\n",
    "    printf(\"Found %d CUDA device(s)\\n\", deviceCount);\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        printMIGCapabilities(i);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ba708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with NVML library\n",
    "!nvcc -O3 -arch=sm_80 mig_query.cu -o mig_query -lnvidia-ml && ./mig_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7452cb",
   "metadata": {},
   "source": [
    "## nvidia-smi MIG Commands\n",
    "\n",
    "Check MIG status using command line (typically requires admin access to modify):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adea654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query GPU info including MIG mode\n",
    "!nvidia-smi --query-gpu=name,mig.mode.current,mig.mode.pending --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99845051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available MIG profiles (if on MIG-capable GPU)\n",
    "!nvidia-smi mig -lgip 2>/dev/null || echo \"MIG not available or not enabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing GPU instances\n",
    "!nvidia-smi mig -lgi 2>/dev/null || echo \"No GPU instances or MIG not enabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6b822",
   "metadata": {},
   "source": [
    "## MIG Instance Profiles\n",
    "\n",
    "H100 SXM (80GB) supports these profiles:\n",
    "\n",
    "| Profile | GPCs | Memory | Use Case |\n",
    "|---------|------|--------|----------|\n",
    "| 1g.10gb | 1/7 | 10GB | Small inference |\n",
    "| 2g.20gb | 2/7 | 20GB | Medium workloads |\n",
    "| 3g.40gb | 3/7 | 40GB | Training |\n",
    "| 4g.40gb | 4/7 | 40GB | Large training |\n",
    "| 7g.80gb | 7/7 | 80GB | Full GPU (no isolation) |\n",
    "\n",
    "A100 (80GB) profiles:\n",
    "\n",
    "| Profile | GPCs | Memory |\n",
    "|---------|------|--------|\n",
    "| 1g.10gb | 1/7 | 10GB |\n",
    "| 2g.20gb | 2/7 | 20GB |\n",
    "| 3g.40gb | 3/7 | 40GB |\n",
    "| 7g.80gb | 7/7 | 80GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6841068",
   "metadata": {},
   "source": [
    "## Detecting MIG Instance in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a92220",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile detect_mig_instance.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== MIG Instance Detection ===\\n\\n\");\n",
    "    \n",
    "    // Check CUDA_VISIBLE_DEVICES\n",
    "    const char* visibleDevices = getenv(\"CUDA_VISIBLE_DEVICES\");\n",
    "    printf(\"CUDA_VISIBLE_DEVICES: %s\\n\", visibleDevices ? visibleDevices : \"(not set)\");\n",
    "    \n",
    "    // Check for MIG UUID format\n",
    "    if (visibleDevices && strstr(visibleDevices, \"MIG-\")) {\n",
    "        printf(\"Running on MIG instance!\\n\");\n",
    "        printf(\"MIG UUID: %s\\n\", visibleDevices);\n",
    "    }\n",
    "    \n",
    "    int deviceCount;\n",
    "    CHECK_CUDA(cudaGetDeviceCount(&deviceCount));\n",
    "    printf(\"\\nVisible devices: %d\\n\", deviceCount);\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        cudaDeviceProp prop;\n",
    "        CHECK_CUDA(cudaGetDeviceProperties(&prop, i));\n",
    "        \n",
    "        printf(\"\\nDevice %d: %s\\n\", i, prop.name);\n",
    "        printf(\"  UUID: \");\n",
    "        for (int j = 0; j < 16; j++) {\n",
    "            printf(\"%02x\", (unsigned char)prop.uuid.bytes[j]);\n",
    "            if (j == 3 || j == 5 || j == 7 || j == 9) printf(\"-\");\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "        \n",
    "        printf(\"  Total Memory: %.2f GB\\n\", \n",
    "               prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));\n",
    "        printf(\"  SMs: %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"  Compute: %d.%d\\n\", prop.major, prop.minor);\n",
    "        \n",
    "        // Detect if this looks like a MIG partition\n",
    "        // MIG instances have fewer SMs than the full GPU\n",
    "        // H100: 132 SMs full, A100: 108 SMs full\n",
    "        bool likelyMIG = false;\n",
    "        if (strstr(prop.name, \"H100\") && prop.multiProcessorCount < 100) {\n",
    "            likelyMIG = true;\n",
    "        } else if (strstr(prop.name, \"A100\") && prop.multiProcessorCount < 100) {\n",
    "            likelyMIG = true;\n",
    "        }\n",
    "        \n",
    "        if (likelyMIG) {\n",
    "            printf(\"  >>> Appears to be a MIG partition (reduced SM count)\\n\");\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Run a simple kernel to verify\n",
    "    printf(\"\\nRunning verification kernel...\\n\");\n",
    "    int* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemset(d_data, 0, sizeof(int)));\n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "    printf(\"Kernel execution successful!\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 detect_mig_instance.cu -o detect_mig_instance && ./detect_mig_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9480f20e",
   "metadata": {},
   "source": [
    "## MIG Architecture Deep Dive\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          H100 Full GPU                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  GPC 0  â”‚  GPC 1  â”‚  GPC 2  â”‚  GPC 3  â”‚  GPC 4  â”‚  GPC 5  â”‚ GPC 6â”‚\n",
    "â”‚ (16 SM) â”‚ (16 SM) â”‚ (16 SM) â”‚ (16 SM) â”‚ (16 SM) â”‚ (16 SM) â”‚(20SM)â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                    L2 Cache (50 MB)                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  HBM3 Stack 0  â”‚  Stack 1  â”‚  Stack 2  â”‚  Stack 3  â”‚  Stack 4   â”‚\n",
    "â”‚    (16 GB)     â”‚  (16 GB)  â”‚  (16 GB)  â”‚  (16 GB)  â”‚  (16 GB)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "MIG creates isolated \"slices\" with:\n",
    "- Dedicated GPCs (Graphics Processing Clusters)\n",
    "- Dedicated memory bandwidth\n",
    "- Isolated L2 cache partition\n",
    "- Separate error isolation domain\n",
    "```\n",
    "\n",
    "**Key isolation properties:**\n",
    "1. **Memory isolation** - Each instance has private memory\n",
    "2. **SM isolation** - Dedicated compute resources\n",
    "3. **Bandwidth isolation** - QoS guaranteed memory BW\n",
    "4. **Error isolation** - Faults don't affect other instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec489e52",
   "metadata": {},
   "source": [
    "## Benchmark on MIG Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mig_benchmark.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void computeKernel(float* data, int n, int iterations) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < iterations; i++) {\n",
    "            val = sinf(val) * cosf(val) + 0.1f;\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== MIG Instance Benchmark ===\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    CHECK_CUDA(cudaGetDeviceProperties(&prop, 0));\n",
    "    \n",
    "    printf(\"GPU: %s\\n\", prop.name);\n",
    "    printf(\"SMs: %d\\n\", prop.multiProcessorCount);\n",
    "    printf(\"Memory: %.2f GB\\n\\n\", prop.totalGlobalMem / (1024.0*1024.0*1024.0));\n",
    "    \n",
    "    // Use available memory (scale to instance size)\n",
    "    size_t freeMemory, totalMemory;\n",
    "    CHECK_CUDA(cudaMemGetInfo(&freeMemory, &totalMemory));\n",
    "    \n",
    "    // Use 50% of available memory\n",
    "    size_t dataSize = freeMemory / 2;\n",
    "    int n = dataSize / sizeof(float);\n",
    "    \n",
    "    printf(\"Allocating %.2f GB (%.2f GB available)\\n\", \n",
    "           dataSize / (1024.0*1024.0*1024.0),\n",
    "           freeMemory / (1024.0*1024.0*1024.0));\n",
    "    \n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, dataSize));\n",
    "    CHECK_CUDA(cudaMemset(d_data, 0, dataSize));\n",
    "    \n",
    "    // Benchmark compute throughput\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "    int iterations = 100;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA(cudaEventCreate(&start));\n",
    "    CHECK_CUDA(cudaEventCreate(&stop));\n",
    "    \n",
    "    // Warmup\n",
    "    computeKernel<<<numBlocks, blockSize>>>(d_data, n, iterations);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Benchmark\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        computeKernel<<<numBlocks, blockSize>>>(d_data, n, iterations);\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    \n",
    "    float ms;\n",
    "    CHECK_CUDA(cudaEventElapsedTime(&ms, start, stop));\n",
    "    \n",
    "    double throughput = (double)n * 10 * iterations / (ms / 1000.0) / 1e9;\n",
    "    printf(\"\\nCompute: %.2f ms for 10 iterations\\n\", ms);\n",
    "    printf(\"Throughput: %.2f billion ops/sec\\n\", throughput);\n",
    "    printf(\"Per-SM throughput: %.2f billion ops/sec/SM\\n\", \n",
    "           throughput / prop.multiProcessorCount);\n",
    "    \n",
    "    // Memory bandwidth test\n",
    "    CHECK_CUDA(cudaEventRecord(start));\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        CHECK_CUDA(cudaMemset(d_data, i, dataSize));\n",
    "    }\n",
    "    CHECK_CUDA(cudaEventRecord(stop));\n",
    "    CHECK_CUDA(cudaEventSynchronize(stop));\n",
    "    \n",
    "    CHECK_CUDA(cudaEventElapsedTime(&ms, start, stop));\n",
    "    double bw = (double)dataSize * 10 / (ms / 1000.0) / 1e9;\n",
    "    printf(\"\\nMemory Bandwidth: %.2f GB/s\\n\", bw);\n",
    "    \n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 mig_benchmark.cu -o mig_benchmark && ./mig_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e4ad7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to practice MIG fundamentals concepts:\n",
    "\n",
    "1. **Exercise 1: MIG Device Detection** - Query and display MIG-specific device properties\n",
    "2. **Exercise 2: Memory Partition Awareness** - Adapt memory allocation to MIG partition sizes\n",
    "3. **Exercise 3: SM-Aware Kernel Launch** - Configure kernels based on available SMs in MIG instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cd06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mig_fundamentals_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t error = call; \\\n",
    "        if (error != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n",
    "                    cudaGetErrorString(error), __FILE__, __LINE__); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: MIG Device Detection\n",
    "// Query and display MIG-specific device properties\n",
    "// =============================================================================\n",
    "\n",
    "void exercise1_mig_detection() {\n",
    "    printf(\"=== Exercise 1: MIG Device Detection ===\\n\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    printf(\"Number of CUDA devices: %d\\n\\n\", deviceCount);\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        cudaDeviceProp prop;\n",
    "        CUDA_CHECK(cudaGetDeviceProperties(&prop, i));\n",
    "        \n",
    "        printf(\"Device %d: %s\\n\", i, prop.name);\n",
    "        printf(\"  Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"  Total Global Memory: %.2f GB\\n\", \n",
    "               prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));\n",
    "        printf(\"  Multiprocessors (SMs): %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"  Max Threads per SM: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
    "        printf(\"  Total Cores (estimated): %d\\n\", \n",
    "               prop.multiProcessorCount * 128); // Approximate for modern GPUs\n",
    "        \n",
    "        // Check if this might be a MIG instance (limited SMs, specific memory sizes)\n",
    "        // MIG instances typically have fewer SMs than full GPU\n",
    "        printf(\"  Possible MIG Instance: %s\\n\\n\", \n",
    "               (prop.multiProcessorCount < 50) ? \"Yes (reduced SM count)\" : \"No (full GPU)\");\n",
    "        \n",
    "        // TODO: Add check for CUDA_VISIBLE_DEVICES with MIG-UUID format\n",
    "        // Hint: MIG UUIDs contain \"MIG-\" prefix\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Memory Partition Awareness\n",
    "// Adapt memory allocation to MIG partition sizes\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void memoryTestKernel(float* data, size_t n) {\n",
    "    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (size_t i = idx; i < n; i += stride) {\n",
    "        data[i] = (float)i * 0.001f;\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise2_memory_partition_awareness() {\n",
    "    printf(\"=== Exercise 2: Memory Partition Awareness ===\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
    "    \n",
    "    size_t freeMemory, totalMemory;\n",
    "    CUDA_CHECK(cudaMemGetInfo(&freeMemory, &totalMemory));\n",
    "    \n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Total Memory: %.2f GB\\n\", totalMemory / (1024.0 * 1024.0 * 1024.0));\n",
    "    printf(\"Free Memory: %.2f GB\\n\", freeMemory / (1024.0 * 1024.0 * 1024.0));\n",
    "    \n",
    "    // Adaptive allocation: use percentage of available memory\n",
    "    // This works for both full GPU and MIG instances\n",
    "    size_t safeAllocation = (size_t)(freeMemory * 0.7);  // Use 70% of free memory\n",
    "    size_t numElements = safeAllocation / sizeof(float);\n",
    "    \n",
    "    printf(\"\\nAdaptive Allocation Strategy:\\n\");\n",
    "    printf(\"  Safe allocation size: %.2f GB\\n\", safeAllocation / (1024.0 * 1024.0 * 1024.0));\n",
    "    printf(\"  Number of float elements: %zu\\n\", numElements);\n",
    "    \n",
    "    float* d_data;\n",
    "    cudaError_t allocResult = cudaMalloc(&d_data, safeAllocation);\n",
    "    \n",
    "    if (allocResult == cudaSuccess) {\n",
    "        printf(\"  Allocation: SUCCESS\\n\");\n",
    "        \n",
    "        // Launch kernel with appropriate grid size for available SMs\n",
    "        int blockSize = 256;\n",
    "        int numBlocks = min((int)((numElements + blockSize - 1) / blockSize), \n",
    "                           prop.multiProcessorCount * 32);\n",
    "        \n",
    "        memoryTestKernel<<<numBlocks, blockSize>>>(d_data, numElements);\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "        printf(\"  Kernel execution: SUCCESS\\n\");\n",
    "        \n",
    "        CUDA_CHECK(cudaFree(d_data));\n",
    "    } else {\n",
    "        printf(\"  Allocation: FAILED - %s\\n\", cudaGetErrorString(allocResult));\n",
    "        printf(\"  Consider reducing allocation for MIG instance\\n\");\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: SM-Aware Kernel Launch\n",
    "// Configure kernels based on available SMs in MIG instance\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void computeKernel(float* output, int n, int iterations) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        float val = (float)i;\n",
    "        for (int j = 0; j < iterations; j++) {\n",
    "            val = sinf(val) * cosf(val) + 0.1f;\n",
    "        }\n",
    "        output[i] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise3_sm_aware_launch() {\n",
    "    printf(\"=== Exercise 3: SM-Aware Kernel Launch ===\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
    "    \n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Available SMs: %d\\n\", prop.multiProcessorCount);\n",
    "    printf(\"Max Threads per SM: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
    "    \n",
    "    // Calculate optimal launch configuration based on available SMs\n",
    "    int blockSize = 256;\n",
    "    int maxActiveBlocks;\n",
    "    \n",
    "    CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "        &maxActiveBlocks, computeKernel, blockSize, 0));\n",
    "    \n",
    "    printf(\"Max Active Blocks per SM: %d\\n\", maxActiveBlocks);\n",
    "    \n",
    "    // Optimal grid size = SMs * blocks_per_SM\n",
    "    int optimalGridSize = prop.multiProcessorCount * maxActiveBlocks;\n",
    "    printf(\"Optimal Grid Size: %d blocks\\n\", optimalGridSize);\n",
    "    \n",
    "    // Allocate test data\n",
    "    int n = 1024 * 1024;\n",
    "    float* d_output;\n",
    "    CUDA_CHECK(cudaMalloc(&d_output, n * sizeof(float)));\n",
    "    \n",
    "    // Benchmark with different grid sizes\n",
    "    printf(\"\\nBenchmarking different grid configurations:\\n\");\n",
    "    \n",
    "    int gridSizes[] = {\n",
    "        optimalGridSize / 4,      // Under-utilizing SMs\n",
    "        optimalGridSize,          // Optimal for this device\n",
    "        optimalGridSize * 2,      // Slightly over\n",
    "        optimalGridSize * 4       // Way over\n",
    "    };\n",
    "    \n",
    "    for (int g = 0; g < 4; g++) {\n",
    "        int gridSize = gridSizes[g];\n",
    "        \n",
    "        cudaEvent_t start, stop;\n",
    "        CUDA_CHECK(cudaEventCreate(&start));\n",
    "        CUDA_CHECK(cudaEventCreate(&stop));\n",
    "        \n",
    "        // Warmup\n",
    "        computeKernel<<<gridSize, blockSize>>>(d_output, n, 100);\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "        \n",
    "        // Timed run\n",
    "        CUDA_CHECK(cudaEventRecord(start));\n",
    "        computeKernel<<<gridSize, blockSize>>>(d_output, n, 100);\n",
    "        CUDA_CHECK(cudaEventRecord(stop));\n",
    "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
    "        \n",
    "        float ms;\n",
    "        CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "        \n",
    "        const char* label = (gridSize == optimalGridSize) ? \" (OPTIMAL)\" :\n",
    "                           (gridSize < optimalGridSize) ? \" (under)\" : \" (over)\";\n",
    "        printf(\"  Grid=%4d blocks: %.3f ms%s\\n\", gridSize, ms, label);\n",
    "        \n",
    "        CUDA_CHECK(cudaEventDestroy(start));\n",
    "        CUDA_CHECK(cudaEventDestroy(stop));\n",
    "    }\n",
    "    \n",
    "    CUDA_CHECK(cudaFree(d_output));\n",
    "    printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘     Week 18 Day 1: MIG Fundamentals - CUDA C++ Exercises     â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    exercise1_mig_detection();\n",
    "    exercise2_memory_partition_awareness();\n",
    "    exercise3_sm_aware_launch();\n",
    "    \n",
    "    printf(\"All exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_75 -o mig_fundamentals_exercises mig_fundamentals_exercises.cu && ./mig_fundamentals_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45a377",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises use Python with PyCUDA/Numba for those who prefer Python-based GPU programming. Note that MIG detection and configuration concepts work similarly in Python using the CUDA runtime APIs.\n",
    "\n",
    "**Python Exercise Ideas:**\n",
    "1. Use `pycuda.driver` to enumerate devices and detect MIG instances\n",
    "2. Implement adaptive memory allocation based on available GPU memory\n",
    "3. Create a Python class that wraps MIG-aware resource management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0291a82",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **MIG provides hardware isolation** - Not just time-slicing\n",
    "2. **Available on datacenter GPUs** - A100, H100, B100, B200\n",
    "3. **Check CUDA_VISIBLE_DEVICES** - MIG instances have MIG-UUID format\n",
    "4. **SM count indicates partition size** - Fewer SMs = smaller MIG instance\n",
    "5. **Memory is proportionally allocated** - 1g.10gb gets ~10GB"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
