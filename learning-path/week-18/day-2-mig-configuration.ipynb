{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1430ea0f",
   "metadata": {},
   "source": [
    "## MIG Configuration Workflow (Admin)\n",
    "\n",
    "```bash\n",
    "# Step 1: Enable MIG mode (requires GPU reset)\n",
    "sudo nvidia-smi -i 0 -mig 1\n",
    "sudo nvidia-smi --gpu-reset\n",
    "\n",
    "# Step 2: List available profiles\n",
    "nvidia-smi mig -lgip\n",
    "\n",
    "# Step 3: Create GPU instances\n",
    "# Example: Create two 3g.40gb instances on H100\n",
    "sudo nvidia-smi mig -cgi 9,9 -i 0\n",
    "\n",
    "# Step 4: Create compute instances for each GPU instance\n",
    "sudo nvidia-smi mig -cci -gi 0\n",
    "sudo nvidia-smi mig -cci -gi 1\n",
    "\n",
    "# Step 5: List created instances\n",
    "nvidia-smi mig -lgi\n",
    "nvidia-smi mig -lci\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e1064",
   "metadata": {},
   "source": [
    "## Query MIG Configuration with NVML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431336b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mig_config_query.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <nvml.h>\n",
    "\n",
    "#define CHECK_NVML(call) \\\n",
    "    do { \\\n",
    "        nvmlReturn_t result = call; \\\n",
    "        if (result != NVML_SUCCESS) { \\\n",
    "            printf(\"NVML: %s\\n\", nvmlErrorString(result)); \\\n",
    "            return; \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "void queryMIGConfiguration(unsigned int deviceIdx) {\n",
    "    nvmlDevice_t device;\n",
    "    CHECK_NVML(nvmlDeviceGetHandleByIndex(deviceIdx, &device));\n",
    "    \n",
    "    char name[NVML_DEVICE_NAME_BUFFER_SIZE];\n",
    "    CHECK_NVML(nvmlDeviceGetName(device, name, sizeof(name)));\n",
    "    printf(\"\\n=== Device %u: %s ===\\n\", deviceIdx, name);\n",
    "    \n",
    "    // Check MIG mode\n",
    "    unsigned int currentMode, pendingMode;\n",
    "    nvmlReturn_t migResult = nvmlDeviceGetMigMode(device, &currentMode, &pendingMode);\n",
    "    \n",
    "    if (migResult == NVML_ERROR_NOT_SUPPORTED) {\n",
    "        printf(\"MIG not supported on this GPU\\n\");\n",
    "        return;\n",
    "    }\n",
    "    CHECK_NVML(migResult);\n",
    "    \n",
    "    printf(\"MIG Mode: %s (pending: %s)\\n\",\n",
    "           currentMode ? \"ENABLED\" : \"DISABLED\",\n",
    "           pendingMode ? \"ENABLED\" : \"DISABLED\");\n",
    "    \n",
    "    if (!currentMode) {\n",
    "        printf(\"MIG is disabled. Enable with: sudo nvidia-smi -mig 1\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // List GPU instance profiles\n",
    "    printf(\"\\nAvailable GPU Instance Profiles:\\n\");\n",
    "    for (unsigned int profileId = 0; profileId < 20; profileId++) {\n",
    "        nvmlGpuInstanceProfileInfo_t profileInfo;\n",
    "        nvmlReturn_t result = nvmlDeviceGetGpuInstanceProfileInfo(\n",
    "            device, profileId, &profileInfo);\n",
    "        \n",
    "        if (result == NVML_SUCCESS) {\n",
    "            printf(\"  Profile %u: memory=%llu MB, slices=%u, instances=%u\\n\",\n",
    "                   profileId,\n",
    "                   profileInfo.memorySizeMB,\n",
    "                   profileInfo.sliceCount,\n",
    "                   profileInfo.instanceCount);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // List existing GPU instances\n",
    "    printf(\"\\nExisting GPU Instances:\\n\");\n",
    "    unsigned int count = 0;\n",
    "    nvmlGpuInstance_t gpuInstances[NVML_MAX_GPU_INSTANCES];\n",
    "    \n",
    "    // Try to get instances for each profile\n",
    "    for (unsigned int profileId = 0; profileId < 20; profileId++) {\n",
    "        nvmlGpuInstanceProfileInfo_t profileInfo;\n",
    "        if (nvmlDeviceGetGpuInstanceProfileInfo(device, profileId, &profileInfo) == NVML_SUCCESS) {\n",
    "            unsigned int instCount;\n",
    "            if (nvmlDeviceGetGpuInstances(device, profileId, gpuInstances, &instCount) == NVML_SUCCESS) {\n",
    "                for (unsigned int i = 0; i < instCount; i++) {\n",
    "                    nvmlGpuInstanceInfo_t info;\n",
    "                    if (nvmlGpuInstanceGetInfo(gpuInstances[i], &info) == NVML_SUCCESS) {\n",
    "                        printf(\"  Instance %u: id=%u, profileId=%u\\n\",\n",
    "                               count++, info.id, info.profileId);\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (count == 0) {\n",
    "        printf(\"  No GPU instances created\\n\");\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== MIG Configuration Query ===\\n\");\n",
    "    \n",
    "    nvmlReturn_t result = nvmlInit();\n",
    "    if (result != NVML_SUCCESS) {\n",
    "        printf(\"Failed to initialize NVML: %s\\n\", nvmlErrorString(result));\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    unsigned int deviceCount;\n",
    "    CHECK_NVML(nvmlDeviceGetCount(&deviceCount));\n",
    "    \n",
    "    for (unsigned int i = 0; i < deviceCount; i++) {\n",
    "        queryMIGConfiguration(i);\n",
    "    }\n",
    "    \n",
    "    nvmlShutdown();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e92b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 mig_config_query.cu -o mig_config_query -lnvidia-ml && ./mig_config_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb317f3",
   "metadata": {},
   "source": [
    "## Targeting Specific MIG Instances\n",
    "\n",
    "When MIG is enabled, each instance has a unique UUID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List MIG devices with UUIDs\n",
    "!nvidia-smi -L 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b12f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile target_mig_instance.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void simpleKernel(int* result) {\n",
    "    *result = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    printf(\"=== Target MIG Instance Demo ===\\n\\n\");\n",
    "    \n",
    "    // Check environment\n",
    "    const char* cvd = getenv(\"CUDA_VISIBLE_DEVICES\");\n",
    "    printf(\"CUDA_VISIBLE_DEVICES: %s\\n\", cvd ? cvd : \"(not set)\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CHECK_CUDA(cudaGetDeviceCount(&deviceCount));\n",
    "    printf(\"Available CUDA devices: %d\\n\\n\", deviceCount);\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        cudaDeviceProp prop;\n",
    "        CHECK_CUDA(cudaGetDeviceProperties(&prop, i));\n",
    "        \n",
    "        printf(\"Device %d: %s\\n\", i, prop.name);\n",
    "        printf(\"  Memory: %.2f GB\\n\", \n",
    "               prop.totalGlobalMem / (1024.0*1024.0*1024.0));\n",
    "        printf(\"  SMs: %d\\n\", prop.multiProcessorCount);\n",
    "        \n",
    "        // Set device and run kernel\n",
    "        CHECK_CUDA(cudaSetDevice(i));\n",
    "        \n",
    "        int* d_result;\n",
    "        int h_result = -1;\n",
    "        CHECK_CUDA(cudaMalloc(&d_result, sizeof(int)));\n",
    "        \n",
    "        simpleKernel<<<1, 1>>>(d_result);\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());\n",
    "        CHECK_CUDA(cudaMemcpy(&h_result, d_result, sizeof(int), \n",
    "                               cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"  Kernel result: %d\\n\", h_result);\n",
    "        CHECK_CUDA(cudaFree(d_result));\n",
    "    }\n",
    "    \n",
    "    printf(\"\\nTo target a specific MIG instance:\\n\");\n",
    "    printf(\"  export CUDA_VISIBLE_DEVICES=MIG-<uuid>\\n\");\n",
    "    printf(\"  ./program\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f7125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 target_mig_instance.cu -o target_mig_instance && ./target_mig_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ae033",
   "metadata": {},
   "source": [
    "## MIG with CUDA MPS\n",
    "\n",
    "MPS (Multi-Process Service) can run on MIG instances for even finer sharing:\n",
    "\n",
    "```bash\n",
    "# Start MPS on a specific MIG instance\n",
    "export CUDA_VISIBLE_DEVICES=MIG-<uuid>\n",
    "export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps-mig\n",
    "export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-mps-log\n",
    "\n",
    "nvidia-cuda-mps-control -d\n",
    "\n",
    "# Run multiple processes sharing the MIG instance\n",
    "./app1 &\n",
    "./app2 &\n",
    "./app3 &\n",
    "wait\n",
    "\n",
    "# Stop MPS\n",
    "echo quit | nvidia-cuda-mps-control\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbdf0d3",
   "metadata": {},
   "source": [
    "## Compute Instance Profiles\n",
    "\n",
    "Each GPU Instance can be further divided into Compute Instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile compute_instance_query.cu\n",
    "#include <stdio.h>\n",
    "#include <nvml.h>\n",
    "\n",
    "#define CHECK_NVML(call) \\\n",
    "    do { \\\n",
    "        nvmlReturn_t result = call; \\\n",
    "        if (result != NVML_SUCCESS) { \\\n",
    "            printf(\"NVML: %s\\n\", nvmlErrorString(result)); \\\n",
    "            return; \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "void listComputeInstanceProfiles(nvmlGpuInstance_t gpuInstance, unsigned int giId) {\n",
    "    printf(\"  Compute Instance Profiles for GPU Instance %u:\\n\", giId);\n",
    "    \n",
    "    for (unsigned int profileId = 0; profileId < 10; profileId++) {\n",
    "        nvmlComputeInstanceProfileInfo_t profileInfo;\n",
    "        nvmlReturn_t result = nvmlGpuInstanceGetComputeInstanceProfileInfo(\n",
    "            gpuInstance, profileId, NVML_COMPUTE_INSTANCE_ENGINE_PROFILE_SHARED,\n",
    "            &profileInfo);\n",
    "        \n",
    "        if (result == NVML_SUCCESS) {\n",
    "            printf(\"    Profile %u: SMs=%u, instances=%u\\n\",\n",
    "                   profileId,\n",
    "                   profileInfo.smCount,\n",
    "                   profileInfo.instanceCount);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Compute Instance Profiles ===\\n\\n\");\n",
    "    \n",
    "    if (nvmlInit() != NVML_SUCCESS) {\n",
    "        printf(\"Failed to initialize NVML\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    nvmlDevice_t device;\n",
    "    CHECK_NVML(nvmlDeviceGetHandleByIndex(0, &device));\n",
    "    \n",
    "    unsigned int currentMode, pendingMode;\n",
    "    if (nvmlDeviceGetMigMode(device, &currentMode, &pendingMode) != NVML_SUCCESS ||\n",
    "        !currentMode) {\n",
    "        printf(\"MIG not enabled. Showing theoretical profiles...\\n\\n\");\n",
    "        \n",
    "        // Show what would be available\n",
    "        printf(\"For 3g.40gb GPU Instance:\\n\");\n",
    "        printf(\"  - 3c.3g.40gb: All 3 slices (full)\\n\");\n",
    "        printf(\"  - 2c.3g.40gb: 2/3 compute slices\\n\");\n",
    "        printf(\"  - 1c.3g.40gb: 1/3 compute slices\\n\");\n",
    "        printf(\"\\nFor 7g.80gb GPU Instance (A100/H100):\\n\");\n",
    "        printf(\"  - 7c.7g.80gb: All compute (full)\\n\");\n",
    "        printf(\"  - 4c.7g.80gb: 4/7 compute\\n\");\n",
    "        printf(\"  - 3c.7g.80gb: 3/7 compute\\n\");\n",
    "        printf(\"  - 2c.7g.80gb: 2/7 compute\\n\");\n",
    "        printf(\"  - 1c.7g.80gb: 1/7 compute\\n\");\n",
    "    } else {\n",
    "        // Query actual instances\n",
    "        printf(\"MIG enabled. Querying actual configuration...\\n\\n\");\n",
    "        \n",
    "        nvmlGpuInstance_t gpuInstances[8];\n",
    "        for (unsigned int profileId = 0; profileId < 20; profileId++) {\n",
    "            unsigned int count;\n",
    "            if (nvmlDeviceGetGpuInstances(device, profileId, gpuInstances, &count) == NVML_SUCCESS) {\n",
    "                for (unsigned int i = 0; i < count; i++) {\n",
    "                    nvmlGpuInstanceInfo_t info;\n",
    "                    if (nvmlGpuInstanceGetInfo(gpuInstances[i], &info) == NVML_SUCCESS) {\n",
    "                        listComputeInstanceProfiles(gpuInstances[i], info.id);\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    nvmlShutdown();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 compute_instance_query.cu -o compute_instance_query -lnvidia-ml && ./compute_instance_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300344f",
   "metadata": {},
   "source": [
    "## MIG Configuration Reference\n",
    "\n",
    "### H100 80GB MIG Configurations\n",
    "\n",
    "| Configuration | GPU Instances | Memory per Instance |\n",
    "|--------------|---------------|---------------------|\n",
    "| 7 x 1g.10gb | 7 | 10GB each |\n",
    "| 3 x 2g.20gb + 1g.10gb | 4 | 20GB, 20GB, 20GB, 10GB |\n",
    "| 2 x 3g.40gb | 2 | 40GB each |\n",
    "| 1 x 4g.40gb + 1 x 3g.40gb | 2 | 40GB, 40GB |\n",
    "| 1 x 7g.80gb | 1 | 80GB (full) |\n",
    "\n",
    "### A100 80GB MIG Configurations\n",
    "\n",
    "| Configuration | GPU Instances |\n",
    "|--------------|---------------|\n",
    "| 7 x 1g.10gb | 7 instances |\n",
    "| 3 x 2g.20gb + 1g.10gb | 4 instances |\n",
    "| 2 x 3g.40gb | 2 instances |\n",
    "| 1 x 7g.80gb | Full GPU |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76491d34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercises\n",
    "\n",
    "### ðŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to practice MIG configuration concepts:\n",
    "\n",
    "1. **Exercise 1: Instance-Aware Resource Management** - Allocate resources based on MIG instance capabilities\n",
    "2. **Exercise 2: Multi-Instance Coordination** - Design patterns for running across MIG instances\n",
    "3. **Exercise 3: Configuration Validation** - Verify and report MIG configuration programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mig_configuration_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <string.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t error = call; \\\n",
    "        if (error != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n",
    "                    cudaGetErrorString(error), __FILE__, __LINE__); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Instance-Aware Resource Management\n",
    "// Allocate resources based on MIG instance capabilities\n",
    "// =============================================================================\n",
    "\n",
    "struct MIGResourceProfile {\n",
    "    size_t maxMemory;\n",
    "    int maxSMs;\n",
    "    int recommendedBlockSize;\n",
    "    int recommendedGridSize;\n",
    "    float memoryReserveFactor;\n",
    "};\n",
    "\n",
    "MIGResourceProfile detectResourceProfile() {\n",
    "    MIGResourceProfile profile;\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
    "    \n",
    "    size_t freeMemory, totalMemory;\n",
    "    CUDA_CHECK(cudaMemGetInfo(&freeMemory, &totalMemory));\n",
    "    \n",
    "    profile.maxMemory = totalMemory;\n",
    "    profile.maxSMs = prop.multiProcessorCount;\n",
    "    \n",
    "    // Classify instance size based on SM count (approximate)\n",
    "    if (prop.multiProcessorCount <= 14) {\n",
    "        // Small MIG instance (1g.10gb equivalent)\n",
    "        profile.recommendedBlockSize = 128;\n",
    "        profile.recommendedGridSize = prop.multiProcessorCount * 4;\n",
    "        profile.memoryReserveFactor = 0.6f;\n",
    "    } else if (prop.multiProcessorCount <= 28) {\n",
    "        // Medium MIG instance (2g.20gb equivalent)\n",
    "        profile.recommendedBlockSize = 256;\n",
    "        profile.recommendedGridSize = prop.multiProcessorCount * 8;\n",
    "        profile.memoryReserveFactor = 0.7f;\n",
    "    } else if (prop.multiProcessorCount <= 56) {\n",
    "        // Large MIG instance (3g.40gb or 4g.40gb)\n",
    "        profile.recommendedBlockSize = 256;\n",
    "        profile.recommendedGridSize = prop.multiProcessorCount * 12;\n",
    "        profile.memoryReserveFactor = 0.75f;\n",
    "    } else {\n",
    "        // Full GPU or 7g.80gb\n",
    "        profile.recommendedBlockSize = 256;\n",
    "        profile.recommendedGridSize = prop.multiProcessorCount * 16;\n",
    "        profile.memoryReserveFactor = 0.8f;\n",
    "    }\n",
    "    \n",
    "    return profile;\n",
    "}\n",
    "\n",
    "void exercise1_resource_management() {\n",
    "    printf(\"=== Exercise 1: Instance-Aware Resource Management ===\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
    "    \n",
    "    MIGResourceProfile profile = detectResourceProfile();\n",
    "    \n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Detected Profile:\\n\");\n",
    "    printf(\"  Max Memory: %.2f GB\\n\", profile.maxMemory / (1024.0 * 1024.0 * 1024.0));\n",
    "    printf(\"  Available SMs: %d\\n\", profile.maxSMs);\n",
    "    printf(\"  Recommended Block Size: %d\\n\", profile.recommendedBlockSize);\n",
    "    printf(\"  Recommended Grid Size: %d\\n\", profile.recommendedGridSize);\n",
    "    printf(\"  Memory Reserve Factor: %.0f%%\\n\", profile.memoryReserveFactor * 100);\n",
    "    \n",
    "    // Calculate safe allocation\n",
    "    size_t safeAlloc = (size_t)(profile.maxMemory * profile.memoryReserveFactor * 0.5);\n",
    "    printf(\"\\nSafe allocation for workload: %.2f GB\\n\\n\", \n",
    "           safeAlloc / (1024.0 * 1024.0 * 1024.0));\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Multi-Instance Coordination\n",
    "// Design patterns for running across MIG instances\n",
    "// =============================================================================\n",
    "\n",
    "__global__ void workloadKernel(float* data, int n, int instanceId) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        // Simulate work with instance-specific computation\n",
    "        data[i] = sinf((float)i + instanceId * 1000.0f);\n",
    "    }\n",
    "}\n",
    "\n",
    "void exercise2_multi_instance_coordination() {\n",
    "    printf(\"=== Exercise 2: Multi-Instance Coordination ===\\n\\n\");\n",
    "    \n",
    "    // In a real MIG setup, each instance would be a separate device\n",
    "    // Here we simulate the pattern for a single device\n",
    "    \n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    printf(\"Available CUDA devices (or MIG instances): %d\\n\\n\", deviceCount);\n",
    "    \n",
    "    // Pattern: Work distribution across instances\n",
    "    printf(\"Multi-Instance Work Distribution Pattern:\\n\\n\");\n",
    "    \n",
    "    for (int dev = 0; dev < deviceCount; dev++) {\n",
    "        CUDA_CHECK(cudaSetDevice(dev));\n",
    "        \n",
    "        cudaDeviceProp prop;\n",
    "        CUDA_CHECK(cudaGetDeviceProperties(&prop, dev));\n",
    "        \n",
    "        size_t freeMemory, totalMemory;\n",
    "        CUDA_CHECK(cudaMemGetInfo(&freeMemory, &totalMemory));\n",
    "        \n",
    "        printf(\"Instance %d: %s\\n\", dev, prop.name);\n",
    "        printf(\"  SMs: %d, Memory: %.2f GB\\n\", \n",
    "               prop.multiProcessorCount, \n",
    "               totalMemory / (1024.0 * 1024.0 * 1024.0));\n",
    "        \n",
    "        // Calculate proportional workload based on SM count\n",
    "        // (In multi-MIG setup, different instances may have different sizes)\n",
    "        int workloadSize = prop.multiProcessorCount * 1024 * 256;\n",
    "        printf(\"  Assigned workload: %d elements\\n\\n\", workloadSize);\n",
    "        \n",
    "        // Allocate and process\n",
    "        float* d_data;\n",
    "        CUDA_CHECK(cudaMalloc(&d_data, workloadSize * sizeof(float)));\n",
    "        \n",
    "        int blockSize = 256;\n",
    "        int gridSize = (workloadSize + blockSize - 1) / blockSize;\n",
    "        gridSize = min(gridSize, prop.multiProcessorCount * 8);\n",
    "        \n",
    "        workloadKernel<<<gridSize, blockSize>>>(d_data, workloadSize, dev);\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "        \n",
    "        printf(\"  Workload completed on instance %d\\n\\n\", dev);\n",
    "        \n",
    "        CUDA_CHECK(cudaFree(d_data));\n",
    "    }\n",
    "    \n",
    "    CUDA_CHECK(cudaSetDevice(0));  // Reset to device 0\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Configuration Validation\n",
    "// Verify and report MIG configuration programmatically\n",
    "// =============================================================================\n",
    "\n",
    "struct DeviceCapabilityReport {\n",
    "    char name[256];\n",
    "    int smCount;\n",
    "    size_t totalMemory;\n",
    "    size_t freeMemory;\n",
    "    int computeCapabilityMajor;\n",
    "    int computeCapabilityMinor;\n",
    "    bool isMIGCapable;\n",
    "    bool isLikelyMIGInstance;\n",
    "};\n",
    "\n",
    "DeviceCapabilityReport validateConfiguration(int deviceId) {\n",
    "    DeviceCapabilityReport report;\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, deviceId));\n",
    "    \n",
    "    strncpy(report.name, prop.name, 255);\n",
    "    report.name[255] = '\\0';\n",
    "    report.smCount = prop.multiProcessorCount;\n",
    "    report.computeCapabilityMajor = prop.major;\n",
    "    report.computeCapabilityMinor = prop.minor;\n",
    "    \n",
    "    CUDA_CHECK(cudaSetDevice(deviceId));\n",
    "    CUDA_CHECK(cudaMemGetInfo(&report.freeMemory, &report.totalMemory));\n",
    "    \n",
    "    // MIG is available on A100 (8.0), H100 (9.0), and newer datacenter GPUs\n",
    "    report.isMIGCapable = (prop.major >= 8);\n",
    "    \n",
    "    // Heuristic: MIG instances typically have reduced SM count\n",
    "    // A100 has 108 SMs, H100 has 132 SMs - MIG instances have less\n",
    "    report.isLikelyMIGInstance = (prop.major >= 8 && prop.multiProcessorCount < 60);\n",
    "    \n",
    "    return report;\n",
    "}\n",
    "\n",
    "void exercise3_configuration_validation() {\n",
    "    printf(\"=== Exercise 3: Configuration Validation ===\\n\\n\");\n",
    "    \n",
    "    int deviceCount;\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘                    MIG CONFIGURATION REPORT                       â•‘\\n\");\n",
    "    printf(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\\n\");\n",
    "    \n",
    "    for (int dev = 0; dev < deviceCount; dev++) {\n",
    "        DeviceCapabilityReport report = validateConfiguration(dev);\n",
    "        \n",
    "        printf(\"â•‘ Device %d: %-55s â•‘\\n\", dev, report.name);\n",
    "        printf(\"â•‘   Compute Capability: %d.%d                                       â•‘\\n\",\n",
    "               report.computeCapabilityMajor, report.computeCapabilityMinor);\n",
    "        printf(\"â•‘   SM Count: %-3d                                                  â•‘\\n\",\n",
    "               report.smCount);\n",
    "        printf(\"â•‘   Total Memory: %6.2f GB                                        â•‘\\n\",\n",
    "               report.totalMemory / (1024.0 * 1024.0 * 1024.0));\n",
    "        printf(\"â•‘   Free Memory:  %6.2f GB                                        â•‘\\n\",\n",
    "               report.freeMemory / (1024.0 * 1024.0 * 1024.0));\n",
    "        printf(\"â•‘   MIG Capable: %-3s                                               â•‘\\n\",\n",
    "               report.isMIGCapable ? \"Yes\" : \"No\");\n",
    "        printf(\"â•‘   Likely MIG Instance: %-3s                                       â•‘\\n\",\n",
    "               report.isLikelyMIGInstance ? \"Yes\" : \"No\");\n",
    "        \n",
    "        // Provide recommendations\n",
    "        printf(\"â•‘                                                                   â•‘\\n\");\n",
    "        printf(\"â•‘   Recommendations:                                                â•‘\\n\");\n",
    "        \n",
    "        if (report.isLikelyMIGInstance) {\n",
    "            printf(\"â•‘   - Use adaptive memory allocation (check free memory first)     â•‘\\n\");\n",
    "            printf(\"â•‘   - Limit grid size to %d blocks for optimal performance        â•‘\\n\",\n",
    "                   report.smCount * 8);\n",
    "        } else {\n",
    "            printf(\"â•‘   - Full GPU resources available                                 â•‘\\n\");\n",
    "            printf(\"â•‘   - Consider enabling MIG for multi-tenant workloads             â•‘\\n\");\n",
    "        }\n",
    "        printf(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\\n\");\n",
    "    }\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘    Week 18 Day 2: MIG Configuration - CUDA C++ Exercises     â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    exercise1_resource_management();\n",
    "    exercise2_multi_instance_coordination();\n",
    "    exercise3_configuration_validation();\n",
    "    \n",
    "    printf(\"All exercises completed!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_75 -o mig_configuration_exercises mig_configuration_exercises.cu && ./mig_configuration_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ccf4aa",
   "metadata": {},
   "source": [
    "### ðŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises use Python with PyCUDA/Numba for those who prefer Python-based GPU programming. MIG configuration concepts translate well to Python using subprocess calls to nvidia-smi or NVML Python bindings.\n",
    "\n",
    "**Python Exercise Ideas:**\n",
    "1. Use `py3nvml` to query MIG configuration and GPU/Compute instances\n",
    "2. Implement a Python context manager for MIG-aware device selection\n",
    "3. Create a workload scheduler that distributes tasks across MIG instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1419b6",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **GPU Instance** - Memory and SM partition\n",
    "2. **Compute Instance** - Further subdivides SMs within a GPU Instance\n",
    "3. **Use CUDA_VISIBLE_DEVICES** - Target specific MIG instances via UUID\n",
    "4. **NVML API** - Query MIG configuration programmatically\n",
    "5. **Admin required** - MIG configuration needs root access"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
