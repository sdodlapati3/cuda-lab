{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cec834",
   "metadata": {},
   "source": [
    "## Pattern 1: GPU Health Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fab485",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gpu_health_check.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <nvml.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            return false; \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "struct GPUHealth {\n",
    "    int deviceId;\n",
    "    bool cudaAccessible;\n",
    "    bool memoryOk;\n",
    "    bool computeOk;\n",
    "    bool temperatureOk;\n",
    "    bool eccOk;\n",
    "    \n",
    "    // Metrics\n",
    "    size_t freeMemory;\n",
    "    size_t totalMemory;\n",
    "    unsigned int temperature;\n",
    "    unsigned int powerUsage;\n",
    "    unsigned int gpuUtilization;\n",
    "};\n",
    "\n",
    "__global__ void healthCheckKernel(int* result) {\n",
    "    // Simple compute test\n",
    "    int val = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    val = val * 2 + 1;\n",
    "    if (threadIdx.x == 0) {\n",
    "        *result = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "bool checkGPUHealth(int deviceId, GPUHealth& health) {\n",
    "    health.deviceId = deviceId;\n",
    "    health.cudaAccessible = false;\n",
    "    health.memoryOk = false;\n",
    "    health.computeOk = false;\n",
    "    health.temperatureOk = false;\n",
    "    health.eccOk = true;  // Assume OK unless detected otherwise\n",
    "    \n",
    "    // Test CUDA accessibility\n",
    "    CHECK_CUDA(cudaSetDevice(deviceId));\n",
    "    health.cudaAccessible = true;\n",
    "    \n",
    "    // Memory check\n",
    "    CHECK_CUDA(cudaMemGetInfo(&health.freeMemory, &health.totalMemory));\n",
    "    health.memoryOk = (health.freeMemory > 0.1 * health.totalMemory);  // >10% free\n",
    "    \n",
    "    // Compute check\n",
    "    int* d_result;\n",
    "    int h_result = -1;\n",
    "    CHECK_CUDA(cudaMalloc(&d_result, sizeof(int)));\n",
    "    \n",
    "    healthCheckKernel<<<1, 32>>>(d_result);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    CHECK_CUDA(cudaMemcpy(&h_result, d_result, sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    CHECK_CUDA(cudaFree(d_result));\n",
    "    \n",
    "    health.computeOk = (h_result == 1);  // Expected: 0 * 2 + 1 = 1\n",
    "    \n",
    "    // NVML checks\n",
    "    nvmlDevice_t nvmlDevice;\n",
    "    if (nvmlDeviceGetHandleByIndex(deviceId, &nvmlDevice) == NVML_SUCCESS) {\n",
    "        // Temperature\n",
    "        if (nvmlDeviceGetTemperature(nvmlDevice, NVML_TEMPERATURE_GPU, \n",
    "                                      &health.temperature) == NVML_SUCCESS) {\n",
    "            health.temperatureOk = (health.temperature < 85);  // <85°C\n",
    "        }\n",
    "        \n",
    "        // Power\n",
    "        nvmlDeviceGetPowerUsage(nvmlDevice, &health.powerUsage);\n",
    "        health.powerUsage /= 1000;  // Convert to watts\n",
    "        \n",
    "        // Utilization\n",
    "        nvmlUtilization_t util;\n",
    "        if (nvmlDeviceGetUtilizationRates(nvmlDevice, &util) == NVML_SUCCESS) {\n",
    "            health.gpuUtilization = util.gpu;\n",
    "        }\n",
    "        \n",
    "        // ECC errors\n",
    "        unsigned long long eccErrors;\n",
    "        if (nvmlDeviceGetTotalEccErrors(nvmlDevice, NVML_MEMORY_ERROR_TYPE_UNCORRECTED,\n",
    "                                         NVML_VOLATILE_ECC, &eccErrors) == NVML_SUCCESS) {\n",
    "            health.eccOk = (eccErrors == 0);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return health.cudaAccessible && health.memoryOk && \n",
    "           health.computeOk && health.temperatureOk && health.eccOk;\n",
    "}\n",
    "\n",
    "void printHealth(const GPUHealth& h) {\n",
    "    printf(\"GPU %d Health:\\n\", h.deviceId);\n",
    "    printf(\"  CUDA Accessible: %s\\n\", h.cudaAccessible ? \"OK\" : \"FAIL\");\n",
    "    printf(\"  Memory: %s (%.1f/%.1f GB free)\\n\", \n",
    "           h.memoryOk ? \"OK\" : \"LOW\",\n",
    "           h.freeMemory / (1024.0*1024.0*1024.0),\n",
    "           h.totalMemory / (1024.0*1024.0*1024.0));\n",
    "    printf(\"  Compute: %s\\n\", h.computeOk ? \"OK\" : \"FAIL\");\n",
    "    printf(\"  Temperature: %s (%u°C)\\n\", \n",
    "           h.temperatureOk ? \"OK\" : \"HIGH\", h.temperature);\n",
    "    printf(\"  ECC: %s\\n\", h.eccOk ? \"OK\" : \"ERRORS DETECTED\");\n",
    "    printf(\"  Power: %u W, Utilization: %u%%\\n\", \n",
    "           h.powerUsage, h.gpuUtilization);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== GPU Health Check ===\\n\\n\");\n",
    "    \n",
    "    nvmlInit();\n",
    "    \n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    int healthyGPUs = 0;\n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        GPUHealth health;\n",
    "        bool isHealthy = checkGPUHealth(i, health);\n",
    "        \n",
    "        printHealth(health);\n",
    "        printf(\"  Overall: %s\\n\\n\", isHealthy ? \"HEALTHY\" : \"UNHEALTHY\");\n",
    "        \n",
    "        if (isHealthy) healthyGPUs++;\n",
    "    }\n",
    "    \n",
    "    printf(\"=== Summary: %d/%d GPUs healthy ===\\n\", healthyGPUs, deviceCount);\n",
    "    \n",
    "    nvmlShutdown();\n",
    "    return (healthyGPUs == deviceCount) ? 0 : 1;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 gpu_health_check.cu -o gpu_health_check -lnvidia-ml && ./gpu_health_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9bb91",
   "metadata": {},
   "source": [
    "## Pattern 2: Graceful GPU Failover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35379441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gpu_failover.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <vector>\n",
    "\n",
    "class GPUManager {\n",
    "public:\n",
    "    struct GPUInfo {\n",
    "        int id;\n",
    "        bool available;\n",
    "        size_t memory;\n",
    "        int smCount;\n",
    "    };\n",
    "    \n",
    "private:\n",
    "    std::vector<GPUInfo> gpus_;\n",
    "    int currentDevice_ = -1;\n",
    "    \n",
    "public:\n",
    "    bool initialize() {\n",
    "        int count;\n",
    "        if (cudaGetDeviceCount(&count) != cudaSuccess) return false;\n",
    "        \n",
    "        for (int i = 0; i < count; i++) {\n",
    "            GPUInfo info;\n",
    "            info.id = i;\n",
    "            info.available = true;\n",
    "            \n",
    "            cudaDeviceProp prop;\n",
    "            if (cudaGetDeviceProperties(&prop, i) == cudaSuccess) {\n",
    "                info.memory = prop.totalGlobalMem;\n",
    "                info.smCount = prop.multiProcessorCount;\n",
    "            }\n",
    "            \n",
    "            gpus_.push_back(info);\n",
    "        }\n",
    "        \n",
    "        return !gpus_.empty();\n",
    "    }\n",
    "    \n",
    "    int selectBestGPU() {\n",
    "        int best = -1;\n",
    "        size_t bestMemory = 0;\n",
    "        \n",
    "        for (auto& gpu : gpus_) {\n",
    "            if (!gpu.available) continue;\n",
    "            \n",
    "            // Check actual free memory\n",
    "            cudaSetDevice(gpu.id);\n",
    "            size_t free, total;\n",
    "            if (cudaMemGetInfo(&free, &total) == cudaSuccess) {\n",
    "                if (free > bestMemory) {\n",
    "                    bestMemory = free;\n",
    "                    best = gpu.id;\n",
    "                }\n",
    "            } else {\n",
    "                gpu.available = false;  // Mark as unavailable\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return best;\n",
    "    }\n",
    "    \n",
    "    bool setDevice(int deviceId) {\n",
    "        if (cudaSetDevice(deviceId) != cudaSuccess) {\n",
    "            markUnavailable(deviceId);\n",
    "            return false;\n",
    "        }\n",
    "        currentDevice_ = deviceId;\n",
    "        return true;\n",
    "    }\n",
    "    \n",
    "    void markUnavailable(int deviceId) {\n",
    "        for (auto& gpu : gpus_) {\n",
    "            if (gpu.id == deviceId) {\n",
    "                gpu.available = false;\n",
    "                printf(\"[FAILOVER] GPU %d marked unavailable\\n\", deviceId);\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    int failover() {\n",
    "        printf(\"[FAILOVER] Attempting failover from GPU %d\\n\", currentDevice_);\n",
    "        \n",
    "        // Mark current as unavailable\n",
    "        if (currentDevice_ >= 0) {\n",
    "            markUnavailable(currentDevice_);\n",
    "        }\n",
    "        \n",
    "        // Reset device to clear any errors\n",
    "        cudaDeviceReset();\n",
    "        \n",
    "        // Find next best GPU\n",
    "        int next = selectBestGPU();\n",
    "        if (next >= 0) {\n",
    "            if (setDevice(next)) {\n",
    "                printf(\"[FAILOVER] Switched to GPU %d\\n\", next);\n",
    "                return next;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        printf(\"[FAILOVER] No available GPUs!\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "    \n",
    "    void printStatus() {\n",
    "        printf(\"\\n=== GPU Status ===\\n\");\n",
    "        for (const auto& gpu : gpus_) {\n",
    "            printf(\"GPU %d: %s (%.1f GB, %d SMs)\\n\",\n",
    "                   gpu.id, \n",
    "                   gpu.available ? \"Available\" : \"Unavailable\",\n",
    "                   gpu.memory / (1024.0*1024.0*1024.0),\n",
    "                   gpu.smCount);\n",
    "        }\n",
    "        printf(\"Current: GPU %d\\n\\n\", currentDevice_);\n",
    "    }\n",
    "};\n",
    "\n",
    "__global__ void workloadKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = sqrtf((float)idx);\n",
    "    }\n",
    "}\n",
    "\n",
    "bool executeWithFailover(GPUManager& mgr, float* d_data, int n) {\n",
    "    const int maxRetries = 3;\n",
    "    \n",
    "    for (int retry = 0; retry < maxRetries; retry++) {\n",
    "        int blockSize = 256;\n",
    "        int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "        \n",
    "        workloadKernel<<<numBlocks, blockSize>>>(d_data, n);\n",
    "        \n",
    "        cudaError_t err = cudaDeviceSynchronize();\n",
    "        if (err == cudaSuccess) {\n",
    "            return true;  // Success!\n",
    "        }\n",
    "        \n",
    "        printf(\"[ERROR] Kernel failed: %s\\n\", cudaGetErrorString(err));\n",
    "        \n",
    "        // Attempt failover\n",
    "        int newDevice = mgr.failover();\n",
    "        if (newDevice < 0) {\n",
    "            return false;  // No GPUs left\n",
    "        }\n",
    "        \n",
    "        // Reallocate on new device\n",
    "        cudaMalloc(&d_data, n * sizeof(float));\n",
    "        printf(\"[RECOVERY] Retrying on GPU %d (attempt %d/%d)\\n\", \n",
    "               newDevice, retry + 2, maxRetries);\n",
    "    }\n",
    "    \n",
    "    return false;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== GPU Failover Demo ===\\n\\n\");\n",
    "    \n",
    "    GPUManager mgr;\n",
    "    if (!mgr.initialize()) {\n",
    "        printf(\"No GPUs found!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    mgr.printStatus();\n",
    "    \n",
    "    // Select best GPU\n",
    "    int device = mgr.selectBestGPU();\n",
    "    if (device < 0) {\n",
    "        printf(\"No available GPUs!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    mgr.setDevice(device);\n",
    "    printf(\"Selected GPU %d\\n\\n\", device);\n",
    "    \n",
    "    // Allocate and run\n",
    "    const int N = 1024 * 1024;\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    \n",
    "    printf(\"Executing workload...\\n\");\n",
    "    if (executeWithFailover(mgr, d_data, N)) {\n",
    "        printf(\"Workload completed successfully!\\n\");\n",
    "    } else {\n",
    "        printf(\"Workload failed after all retries!\\n\");\n",
    "    }\n",
    "    \n",
    "    mgr.printStatus();\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 -std=c++14 gpu_failover.cu -o gpu_failover && ./gpu_failover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af932298",
   "metadata": {},
   "source": [
    "## Pattern 3: Resource RAII Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_raii.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <memory>\n",
    "#include <stdexcept>\n",
    "\n",
    "// RAII wrapper for device memory\n",
    "template<typename T>\n",
    "class DeviceBuffer {\n",
    "private:\n",
    "    T* ptr_ = nullptr;\n",
    "    size_t size_ = 0;\n",
    "    \n",
    "public:\n",
    "    DeviceBuffer() = default;\n",
    "    \n",
    "    explicit DeviceBuffer(size_t count) : size_(count) {\n",
    "        if (cudaMalloc(&ptr_, count * sizeof(T)) != cudaSuccess) {\n",
    "            throw std::runtime_error(\"cudaMalloc failed\");\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ~DeviceBuffer() {\n",
    "        if (ptr_) cudaFree(ptr_);\n",
    "    }\n",
    "    \n",
    "    // Move only\n",
    "    DeviceBuffer(DeviceBuffer&& other) : ptr_(other.ptr_), size_(other.size_) {\n",
    "        other.ptr_ = nullptr;\n",
    "        other.size_ = 0;\n",
    "    }\n",
    "    \n",
    "    DeviceBuffer& operator=(DeviceBuffer&& other) {\n",
    "        if (this != &other) {\n",
    "            if (ptr_) cudaFree(ptr_);\n",
    "            ptr_ = other.ptr_;\n",
    "            size_ = other.size_;\n",
    "            other.ptr_ = nullptr;\n",
    "            other.size_ = 0;\n",
    "        }\n",
    "        return *this;\n",
    "    }\n",
    "    \n",
    "    // No copy\n",
    "    DeviceBuffer(const DeviceBuffer&) = delete;\n",
    "    DeviceBuffer& operator=(const DeviceBuffer&) = delete;\n",
    "    \n",
    "    T* get() { return ptr_; }\n",
    "    const T* get() const { return ptr_; }\n",
    "    size_t size() const { return size_; }\n",
    "    size_t bytes() const { return size_ * sizeof(T); }\n",
    "    \n",
    "    void copyFrom(const T* host) {\n",
    "        cudaMemcpy(ptr_, host, bytes(), cudaMemcpyHostToDevice);\n",
    "    }\n",
    "    \n",
    "    void copyTo(T* host) const {\n",
    "        cudaMemcpy(host, ptr_, bytes(), cudaMemcpyDeviceToHost);\n",
    "    }\n",
    "};\n",
    "\n",
    "// RAII wrapper for streams\n",
    "class CudaStream {\n",
    "private:\n",
    "    cudaStream_t stream_ = nullptr;\n",
    "    \n",
    "public:\n",
    "    CudaStream() {\n",
    "        cudaStreamCreate(&stream_);\n",
    "    }\n",
    "    \n",
    "    explicit CudaStream(unsigned int flags) {\n",
    "        cudaStreamCreateWithFlags(&stream_, flags);\n",
    "    }\n",
    "    \n",
    "    ~CudaStream() {\n",
    "        if (stream_) cudaStreamDestroy(stream_);\n",
    "    }\n",
    "    \n",
    "    CudaStream(CudaStream&& other) : stream_(other.stream_) {\n",
    "        other.stream_ = nullptr;\n",
    "    }\n",
    "    \n",
    "    cudaStream_t get() { return stream_; }\n",
    "    operator cudaStream_t() { return stream_; }\n",
    "    \n",
    "    void synchronize() { cudaStreamSynchronize(stream_); }\n",
    "    bool query() { return cudaStreamQuery(stream_) == cudaSuccess; }\n",
    "};\n",
    "\n",
    "// RAII wrapper for events\n",
    "class CudaEvent {\n",
    "private:\n",
    "    cudaEvent_t event_ = nullptr;\n",
    "    \n",
    "public:\n",
    "    CudaEvent() {\n",
    "        cudaEventCreate(&event_);\n",
    "    }\n",
    "    \n",
    "    explicit CudaEvent(unsigned int flags) {\n",
    "        cudaEventCreateWithFlags(&event_, flags);\n",
    "    }\n",
    "    \n",
    "    ~CudaEvent() {\n",
    "        if (event_) cudaEventDestroy(event_);\n",
    "    }\n",
    "    \n",
    "    cudaEvent_t get() { return event_; }\n",
    "    operator cudaEvent_t() { return event_; }\n",
    "    \n",
    "    void record(cudaStream_t stream = 0) {\n",
    "        cudaEventRecord(event_, stream);\n",
    "    }\n",
    "    \n",
    "    void synchronize() { cudaEventSynchronize(event_); }\n",
    "    \n",
    "    float elapsedMs(const CudaEvent& start) const {\n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start.event_, event_);\n",
    "        return ms;\n",
    "    }\n",
    "};\n",
    "\n",
    "// Demo kernel\n",
    "__global__ void addKernel(float* c, const float* a, const float* b, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) c[idx] = a[idx] + b[idx];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== CUDA RAII Wrappers Demo ===\\n\\n\");\n",
    "    \n",
    "    const int N = 1024 * 1024;\n",
    "    \n",
    "    try {\n",
    "        // Automatic cleanup on scope exit!\n",
    "        DeviceBuffer<float> d_a(N);\n",
    "        DeviceBuffer<float> d_b(N);\n",
    "        DeviceBuffer<float> d_c(N);\n",
    "        \n",
    "        printf(\"Allocated 3 buffers: %.2f MB each\\n\", \n",
    "               d_a.bytes() / (1024.0*1024.0));\n",
    "        \n",
    "        // Prepare host data\n",
    "        std::unique_ptr<float[]> h_a(new float[N]);\n",
    "        std::unique_ptr<float[]> h_b(new float[N]);\n",
    "        std::unique_ptr<float[]> h_c(new float[N]);\n",
    "        \n",
    "        for (int i = 0; i < N; i++) {\n",
    "            h_a[i] = 1.0f;\n",
    "            h_b[i] = 2.0f;\n",
    "        }\n",
    "        \n",
    "        d_a.copyFrom(h_a.get());\n",
    "        d_b.copyFrom(h_b.get());\n",
    "        \n",
    "        // Timing with RAII events\n",
    "        CudaEvent start, stop;\n",
    "        CudaStream stream;\n",
    "        \n",
    "        start.record(stream);\n",
    "        \n",
    "        int blockSize = 256;\n",
    "        int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "        addKernel<<<numBlocks, blockSize, 0, stream>>>(d_c.get(), d_a.get(), d_b.get(), N);\n",
    "        \n",
    "        stop.record(stream);\n",
    "        stream.synchronize();\n",
    "        \n",
    "        printf(\"Kernel time: %.3f ms\\n\", stop.elapsedMs(start));\n",
    "        \n",
    "        // Verify\n",
    "        d_c.copyTo(h_c.get());\n",
    "        bool correct = (h_c[0] == 3.0f && h_c[N-1] == 3.0f);\n",
    "        printf(\"Result: %s\\n\", correct ? \"CORRECT\" : \"INCORRECT\");\n",
    "        \n",
    "    } catch (const std::exception& e) {\n",
    "        printf(\"Error: %s\\n\", e.what());\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    // All resources automatically freed here!\n",
    "    printf(\"\\nResources automatically cleaned up.\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbafc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -arch=sm_80 -std=c++14 cuda_raii.cu -o cuda_raii && ./cuda_raii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39687ab1",
   "metadata": {},
   "source": [
    "## Production Deployment Checklist\n",
    "\n",
    "### Pre-Deployment\n",
    "- [ ] All error checks in place\n",
    "- [ ] Memory leak testing complete\n",
    "- [ ] Stress testing passed\n",
    "- [ ] Logging configured\n",
    "- [ ] Metrics collection ready\n",
    "\n",
    "### Runtime\n",
    "- [ ] GPU health monitoring\n",
    "- [ ] Memory usage alerts\n",
    "- [ ] Temperature thresholds\n",
    "- [ ] ECC error tracking\n",
    "- [ ] Failover procedures\n",
    "\n",
    "### Recovery\n",
    "- [ ] Sticky error handling\n",
    "- [ ] Device reset procedures\n",
    "- [ ] State checkpointing\n",
    "- [ ] Graceful degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34412624",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Always monitor GPU health** - Temperature, memory, ECC\n",
    "2. **Implement failover** - Handle GPU failures gracefully\n",
    "3. **Use RAII wrappers** - Automatic resource cleanup\n",
    "4. **Log everything** - Errors, warnings, metrics\n",
    "5. **Test error paths** - Verify recovery procedures work"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
