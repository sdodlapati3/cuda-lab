{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd31aaf",
   "metadata": {},
   "source": [
    "# üöÄ Day 4: Special Memory Types - Constant & Texture Memory\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-02/day-4-special-memory.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why Special Memory Types?\n",
    "\n",
    "> **The Problem:** Not all data access patterns are created equal. Some data is read by every thread (like a filter kernel), while other data has strong 2D spatial locality (like image pixels). Using only global memory for these patterns leaves performance on the table.\n",
    "\n",
    "**Real-World Impact:**\n",
    "- üé® **Image processing** - Convolution filters read by millions of threads simultaneously\n",
    "- üî¨ **Scientific computing** - Physical constants and lookup tables accessed uniformly  \n",
    "- üéÆ **Graphics** - Texture sampling with automatic interpolation\n",
    "\n",
    "**Today's Mission:** Learn when and how to use CUDA's specialized memory types to match your data access patterns for maximum performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "\n",
    "| # | Objective | Skill Level |\n",
    "|---|-----------|-------------|\n",
    "| 1 | Understand when constant memory provides broadcast benefits | üîµ Core |\n",
    "| 2 | Implement convolution with constant memory filters | üîµ Core |\n",
    "| 3 | Understand texture memory's 2D spatial locality optimization | üîµ Core |\n",
    "| 4 | Choose the right memory type for different access patterns | üü¢ Essential |\n",
    "| 5 | Combine multiple memory optimizations in a complete solution | üü° Advanced |\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15fb12",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**For ODU HPC (Wahab):**\n",
    "```bash\n",
    "module load container_env cuda-12.3.0\n",
    "crun -p ~/envs/cuda python -m jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")\n",
    "print(\"\\n‚ö†Ô∏è  Remember: CUDA C++ code is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151b051",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Complete CUDA Memory Hierarchy Review\n",
    "\n",
    "Before diving into special memory types, let's review the complete memory hierarchy:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    CUDA Memory Hierarchy                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ              Global Memory (GB, slowest)            ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ All threads can read/write                       ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ Persists for application lifetime                ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ ~400-900 GB/s bandwidth                          ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ                          ‚îÇ                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ\n",
    "‚îÇ  ‚îÇ                      ‚îÇ                        ‚îÇ          ‚îÇ\n",
    "‚îÇ  ‚ñº                      ‚ñº                        ‚ñº          ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
    "‚îÇ  ‚îÇ   Constant   ‚îÇ  ‚îÇ   Texture    ‚îÇ  ‚îÇ    Shared    ‚îÇ      ‚îÇ\n",
    "‚îÇ  ‚îÇ    Memory    ‚îÇ  ‚îÇ    Memory    ‚îÇ  ‚îÇ    Memory    ‚îÇ      ‚îÇ\n",
    "‚îÇ  ‚îÇ  (64KB,      ‚îÇ  ‚îÇ  (cached,    ‚îÇ  ‚îÇ  (48-164KB   ‚îÇ      ‚îÇ\n",
    "‚îÇ  ‚îÇ   cached)    ‚îÇ  ‚îÇ   read-only) ‚îÇ  ‚îÇ   per SM)    ‚îÇ      ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
    "‚îÇ         ‚îÇ                 ‚îÇ                  ‚îÇ              ‚îÇ\n",
    "‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ\n",
    "‚îÇ                           ‚îÇ                                  ‚îÇ\n",
    "‚îÇ                           ‚ñº                                  ‚îÇ\n",
    "‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ\n",
    "‚îÇ            ‚îÇ  Registers (fastest, private) ‚îÇ                 ‚îÇ\n",
    "‚îÇ            ‚îÇ  ‚Ä¢ Per-thread, ~256 per thread‚îÇ                 ‚îÇ\n",
    "‚îÇ            ‚îÇ  ‚Ä¢ ~12 TB/s equivalent        ‚îÇ                 ‚îÇ\n",
    "‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Memory Types Summary\n",
    "\n",
    "| Memory Type | Location | Scope | Lifetime | Cache | Speed |\n",
    "|-------------|----------|-------|----------|-------|-------|\n",
    "| Register | On-chip | Thread | Thread | N/A | Fastest |\n",
    "| Local | Off-chip | Thread | Thread | L1/L2 | Slow |\n",
    "| Shared | On-chip | Block | Block | N/A | Fast |\n",
    "| Global | Off-chip | Grid | Application | L1/L2 | Slow |\n",
    "| Constant | Off-chip | Grid | Application | Constant cache | Fast (broadcast) |\n",
    "| Texture | Off-chip | Grid | Application | Texture cache | Fast (spatial) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ca997",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üÉè Concept Card: The Complete Memory Hierarchy\n",
    "\n",
    "> **Analogy: A City's Communication Systems**\n",
    ">\n",
    "> Think of GPU memory like different communication systems in a city:\n",
    ">\n",
    "> | Memory Type | City Analogy | Best For |\n",
    "> |-------------|--------------|----------|\n",
    "> | **Registers** | Person's own thoughts | Private calculations |\n",
    "> | **Shared Memory** | Conference room | Team collaboration |\n",
    "> | **Global Memory** | Public library | Large shared data |\n",
    "> | **Constant Memory** | üìª **Radio broadcast** | Same info to everyone |\n",
    "> | **Texture Memory** | üó∫Ô∏è **GPS navigation** | Spatial lookups with interpolation |\n",
    ">\n",
    "> **Today's Focus:** The specialized \"broadcast\" and \"spatial\" systems that match specific access patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69704e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Constant Memory\n",
    "\n",
    "Now that we understand where constant memory fits in the hierarchy, let's explore it in depth. Remember our radio broadcast analogy‚Äîconstant memory shines when **all threads need the same data**.\n",
    "\n",
    "### What is Constant Memory?\n",
    "\n",
    "Constant memory is a **read-only** memory space that:\n",
    "- Has 64KB total capacity per GPU\n",
    "- Is cached in a dedicated **constant cache**\n",
    "- Optimized for **broadcasting** same value to all threads\n",
    "- Initialized by the host before kernel launch\n",
    "\n",
    "### When Constant Memory Excels\n",
    "\n",
    "```\n",
    "BEST CASE: All threads read the SAME address\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Thread 0  Thread 1  Thread 2  ...  Thread 31    ‚îÇ\n",
    "‚îÇ     ‚îÇ         ‚îÇ         ‚îÇ              ‚îÇ         ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n",
    "‚îÇ                     ‚îÇ                             ‚îÇ\n",
    "‚îÇ                     ‚ñº                             ‚îÇ\n",
    "‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ\n",
    "‚îÇ           ‚îÇ Constant Cache  ‚îÇ                    ‚îÇ\n",
    "‚îÇ           ‚îÇ   (1 read)      ‚îÇ                    ‚îÇ\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ\n",
    "‚îÇ  ‚Üí Result broadcast to all 32 threads in 1 cycle ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "WORST CASE: All threads read DIFFERENT addresses\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Thread 0  Thread 1  Thread 2  ...  Thread 31    ‚îÇ\n",
    "‚îÇ     ‚îÇ         ‚îÇ         ‚îÇ              ‚îÇ         ‚îÇ\n",
    "‚îÇ     ‚ñº         ‚ñº         ‚ñº              ‚ñº         ‚îÇ\n",
    "‚îÇ   addr[0]  addr[1]  addr[2]  ...  addr[31]      ‚îÇ\n",
    "‚îÇ     ‚îÇ         ‚îÇ         ‚îÇ              ‚îÇ         ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n",
    "‚îÇ                     ‚îÇ                             ‚îÇ\n",
    "‚îÇ                     ‚ñº                             ‚îÇ\n",
    "‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ\n",
    "‚îÇ           ‚îÇ 32 serial reads ‚îÇ ‚Üê SLOW!            ‚îÇ\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Ideal Use Cases for Constant Memory\n",
    "\n",
    "1. **Filter kernels/masks** (convolution, Gaussian blur)\n",
    "2. **Lookup tables** accessed uniformly\n",
    "3. **Configuration parameters** (dimensions, coefficients)\n",
    "4. **Mathematical constants** (œÄ, e, conversion factors)\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)\n",
    "\n",
    "Let's see constant memory in action with a classic use case: **image convolution**. Every thread applies the same filter, making this a perfect broadcast scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a6c06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üÉè Concept Card: Constant Memory - The Broadcast Channel\n",
    "\n",
    "> **Analogy: Radio Station Broadcasting**\n",
    ">\n",
    "> Imagine a radio station broadcasting the weather report:\n",
    "> - üìª **One transmission** reaches **thousands of listeners simultaneously**\n",
    "> - Each listener doesn't need their own phone call to the station\n",
    "> - The information is the **same for everyone**\n",
    ">\n",
    "> **Constant memory works the same way:**\n",
    "> ```\n",
    "> Traditional (Global Memory):          Constant Memory (Broadcast):\n",
    "> ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "> ‚îÇ Thread 0: \"Give me data\"‚îÇ          ‚îÇ Thread 0 ‚îÄ‚îê             ‚îÇ\n",
    "> ‚îÇ Thread 1: \"Give me data\"‚îÇ          ‚îÇ Thread 1 ‚îÄ‚îº‚îÄ‚îÄ üìª ‚îÄ‚îÄ‚îÄ‚Üí data‚îÇ\n",
    "> ‚îÇ Thread 2: \"Give me data\"‚îÇ          ‚îÇ Thread 2 ‚îÄ‚î§  (1 read)   ‚îÇ\n",
    "> ‚îÇ    ...32 separate reads ‚îÇ          ‚îÇ   ...     ‚îÄ‚îò             ‚îÇ\n",
    "> ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    ">         32 reads!                           1 read, broadcast!\n",
    "> ```\n",
    ">\n",
    "> **Perfect For:** Filter kernels, lookup tables, constants that ALL threads need\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83945205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constant_mem_demo.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Constant memory declaration (64KB max)\n",
    "__constant__ float d_filter[9];\n",
    "\n",
    "// Convolution using constant memory for filter\n",
    "__global__ void convolution_constant(const float* input, float* output, \n",
    "                                      int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x > 0 && x < width - 1 && y > 0 && y < height - 1) {\n",
    "        float result = 0.0f;\n",
    "        \n",
    "        // 3x3 convolution using constant memory filter\n",
    "        // All threads read the SAME filter values -> broadcast!\n",
    "        for (int i = -1; i <= 1; i++) {\n",
    "            for (int j = -1; j <= 1; j++) {\n",
    "                int idx = (y + i) * width + (x + j);\n",
    "                int fIdx = (i + 1) * 3 + (j + 1);\n",
    "                result += input[idx] * d_filter[fIdx];  // Constant memory access\n",
    "            }\n",
    "        }\n",
    "        output[y * width + x] = result;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Convolution using global memory for filter (for comparison)\n",
    "__global__ void convolution_global(const float* input, const float* filter,\n",
    "                                   float* output, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x > 0 && x < width - 1 && y > 0 && y < height - 1) {\n",
    "        float result = 0.0f;\n",
    "        \n",
    "        for (int i = -1; i <= 1; i++) {\n",
    "            for (int j = -1; j <= 1; j++) {\n",
    "                int idx = (y + i) * width + (x + j);\n",
    "                int fIdx = (i + 1) * 3 + (j + 1);\n",
    "                result += input[idx] * filter[fIdx];  // Global memory access\n",
    "            }\n",
    "        }\n",
    "        output[y * width + x] = result;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int width = 2048, height = 2048;\n",
    "    int size = width * height;\n",
    "    \n",
    "    // Sobel edge detection filter\n",
    "    float h_filter[9] = {\n",
    "        -1, 0, 1,\n",
    "        -2, 0, 2,\n",
    "        -1, 0, 1\n",
    "    };\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float* h_input = (float*)malloc(size * sizeof(float));\n",
    "    float* h_output = (float*)malloc(size * sizeof(float));\n",
    "    \n",
    "    // Initialize with test pattern\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        h_input[i] = (float)(rand() % 256) / 255.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_output, *d_filter_global;\n",
    "    cudaMalloc(&d_input, size * sizeof(float));\n",
    "    cudaMalloc(&d_output, size * sizeof(float));\n",
    "    cudaMalloc(&d_filter_global, 9 * sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_input, h_input, size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_filter_global, h_filter, 9 * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Copy filter to constant memory\n",
    "    cudaMemcpyToSymbol(d_filter, h_filter, 9 * sizeof(float));\n",
    "    \n",
    "    printf(\"=== Constant Memory Demonstration ===\\n\");\n",
    "    printf(\"Image size: %dx%d, Filter: 3x3 Sobel\\n\\n\", width, height);\n",
    "    \n",
    "    dim3 threads(16, 16);\n",
    "    dim3 blocks((width + 15) / 16, (height + 15) / 16);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark constant memory version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        convolution_constant<<<blocks, threads>>>(d_input, d_output, width, height);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float const_time;\n",
    "    cudaEventElapsedTime(&const_time, start, stop);\n",
    "    \n",
    "    // Benchmark global memory version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        convolution_global<<<blocks, threads>>>(d_input, d_filter_global, d_output, width, height);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float global_time;\n",
    "    cudaEventElapsedTime(&global_time, start, stop);\n",
    "    \n",
    "    printf(\"Constant memory filter: %.3f ms\\n\", const_time / 100);\n",
    "    printf(\"Global memory filter:   %.3f ms\\n\", global_time / 100);\n",
    "    printf(\"Speedup:                %.2fx\\n\", global_time / const_time);\n",
    "    printf(\"\\nüí° Constant memory broadcasts filter values to all threads efficiently!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_filter_global);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o constant_mem_demo constant_mem_demo.cu\n",
    "!./constant_mem_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109d209",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)\n",
    "\n",
    "Example: Convolution with Constant Memory\n",
    "\n",
    "Image convolution is the **classic** use case for constant memory because all threads apply the **same filter kernel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a60eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 Sobel edge detection filter\n",
    "SOBEL_X = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(\"Sobel X filter (edge detection):\")\n",
    "print(SOBEL_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9dfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Passing filter as regular global memory\n",
    "@cuda.jit\n",
    "def convolution_global(image, filter_kernel, output):\n",
    "    \"\"\"Convolution using global memory for filter.\"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    height, width = image.shape\n",
    "    \n",
    "    if row > 0 and row < height - 1 and col > 0 and col < width - 1:\n",
    "        result = 0.0\n",
    "        for i in range(-1, 2):\n",
    "            for j in range(-1, 2):\n",
    "                # Each read of filter_kernel goes to global memory\n",
    "                result += image[row + i, col + j] * filter_kernel[i + 1, j + 1]\n",
    "        output[row, col] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f050319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Numba, we simulate constant memory behavior by:\n",
    "# 1. Using a closure to capture the filter at compile time\n",
    "# 2. Or placing filter in shared memory (next best thing)\n",
    "\n",
    "# Version 2: Filter in shared memory (simulating constant memory)\n",
    "@cuda.jit\n",
    "def convolution_shared_filter(image, filter_kernel, output):\n",
    "    \"\"\"Convolution with filter loaded to shared memory.\"\"\"\n",
    "    # Load filter to shared memory (done once per block)\n",
    "    shared_filter = cuda.shared.array((3, 3), dtype=np.float32)\n",
    "    \n",
    "    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y\n",
    "    \n",
    "    # First 9 threads load the filter\n",
    "    linear_tid = ty * cuda.blockDim.x + tx\n",
    "    if linear_tid < 9:\n",
    "        fi, fj = linear_tid // 3, linear_tid % 3\n",
    "        shared_filter[fi, fj] = filter_kernel[fi, fj]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    row, col = cuda.grid(2)\n",
    "    height, width = image.shape\n",
    "    \n",
    "    if row > 0 and row < height - 1 and col > 0 and col < width - 1:\n",
    "        result = 0.0\n",
    "        for i in range(-1, 2):\n",
    "            for j in range(-1, 2):\n",
    "                # Filter reads now from fast shared memory\n",
    "                result += image[row + i, col + j] * shared_filter[i + 1, j + 1]\n",
    "        output[row, col] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both versions\n",
    "SIZE = 2048\n",
    "image = np.random.rand(SIZE, SIZE).astype(np.float32)\n",
    "output = np.zeros_like(image)\n",
    "\n",
    "d_image = cuda.to_device(image)\n",
    "d_filter = cuda.to_device(SOBEL_X)\n",
    "d_output = cuda.to_device(output)\n",
    "\n",
    "threads = (16, 16)\n",
    "blocks = ((SIZE + 15) // 16, (SIZE + 15) // 16)\n",
    "\n",
    "# Warm up\n",
    "convolution_global[blocks, threads](d_image, d_filter, d_output)\n",
    "convolution_shared_filter[blocks, threads](d_image, d_filter, d_output)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Benchmark global memory filter\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    convolution_global[blocks, threads](d_image, d_filter, d_output)\n",
    "cuda.synchronize()\n",
    "global_time = (time.perf_counter() - start) / 100 * 1000\n",
    "\n",
    "# Benchmark shared memory filter\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    convolution_shared_filter[blocks, threads](d_image, d_filter, d_output)\n",
    "cuda.synchronize()\n",
    "shared_time = (time.perf_counter() - start) / 100 * 1000\n",
    "\n",
    "print(f\"Image size: {SIZE}x{SIZE}\")\n",
    "print(f\"Global memory filter: {global_time:.3f} ms\")\n",
    "print(f\"Shared memory filter: {shared_time:.3f} ms\")\n",
    "print(f\"Speedup: {global_time/shared_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65583b03",
   "metadata": {},
   "source": [
    "### Understanding Constant Memory Benefits\n",
    "\n",
    "Even though Numba doesn't have direct constant memory syntax, the concept matters:\n",
    "\n",
    "```\n",
    "Why constant memory is fast for filters:\n",
    "\n",
    "For a 3x3 convolution:\n",
    "- Each thread reads the SAME 9 filter values\n",
    "- 1 warp = 32 threads all reading filter[0,0]\n",
    "- With constant cache: 1 read, broadcast to 32\n",
    "- Without: 32 reads to global memory\n",
    "\n",
    "Result: 32x reduction in filter memory traffic!\n",
    "```\n",
    "\n",
    "In native CUDA C++:\n",
    "```cpp\n",
    "__constant__ float filter[9];  // Declared at file scope\n",
    "\n",
    "// Copied before kernel launch\n",
    "cudaMemcpyToSymbol(filter, host_filter, 9 * sizeof(float));\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Transition:** Now that we've mastered constant memory for uniform broadcast access, let's explore texture memory‚Äîdesigned for a completely different pattern: **2D spatial locality**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aaed1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Texture Memory\n",
    "\n",
    "While constant memory broadcasts the same value to all threads, texture memory excels at a different pattern: **2D spatial access with hardware interpolation**.\n",
    "\n",
    "### What is Texture Memory?\n",
    "\n",
    "Texture memory is another **read-only** memory that:\n",
    "- Is optimized for **2D spatial locality**\n",
    "- Has a dedicated **texture cache**\n",
    "- Supports **hardware interpolation** (free bilinear/trilinear)\n",
    "- Supports **automatic boundary handling** (clamp, wrap, mirror)\n",
    "- Originally designed for graphics, but useful for compute\n",
    "\n",
    "### Texture Cache vs L1 Cache\n",
    "\n",
    "```\n",
    "L1 Cache: Optimized for 1D linear access (coalescing)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ [0][1][2][3][4][5][6][7] ‚Üí cache line  ‚îÇ\n",
    "‚îÇ Linear memory layout                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Texture Cache: Optimized for 2D spatial access\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ [0,0][0,1]‚îÇ[0,2][0,3]                  ‚îÇ\n",
    "‚îÇ [1,0][1,1]‚îÇ[1,2][1,3]  ‚Üí 2D tiles      ‚îÇ\n",
    "‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                  ‚îÇ\n",
    "‚îÇ [2,0][2,1]‚îÇ[2,2][2,3]                  ‚îÇ\n",
    "‚îÇ [3,0][3,1]‚îÇ[3,2][3,3]                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### When Texture Memory Excels\n",
    "\n",
    "1. **Image processing** (resizing, rotation, warping)\n",
    "2. **Interpolation** operations\n",
    "3. **Random 2D access patterns**\n",
    "4. **Volume rendering** (3D textures)\n",
    "5. **Data with spatial locality** in 2D/3D\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29727bb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üÉè Concept Card: Texture Memory - The Image Cache\n",
    "\n",
    "> **Analogy: GPS Navigation with Smart Caching**\n",
    ">\n",
    "> Your GPS doesn't load the entire world map‚Äîit loads **tiles around your location**:\n",
    "> - üó∫Ô∏è Moving **north**? Nearby northern tiles are likely needed next\n",
    "> - üó∫Ô∏è Moving **east**? Eastern tiles are pre-cached\n",
    "> - üó∫Ô∏è Need a point **between** grid points? GPS **interpolates** automatically\n",
    ">\n",
    "> **Texture memory works the same way:**\n",
    "> ```\n",
    "> Regular L1 Cache (1D optimized):     Texture Cache (2D optimized):\n",
    "> ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "> ‚îÇ Cache line: [0][1][2][3] ‚îÇ         ‚îÇ 2D Tile: [0,0][0,1]      ‚îÇ\n",
    "> ‚îÇ Great for linear access  ‚îÇ         ‚îÇ          [1,0][1,1]      ‚îÇ\n",
    "> ‚îÇ Poor for 2D neighbors    ‚îÇ         ‚îÇ Great for 2D neighbors!  ‚îÇ\n",
    "> ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "> ```\n",
    ">\n",
    "> **Bonus Features (Free in Hardware!):**\n",
    "> - ‚ú® **Bilinear interpolation** between pixels\n",
    "> - ‚ú® **Boundary handling** (clamp, wrap, mirror)\n",
    "> - ‚ú® **Normalized coordinates** (0.0 to 1.0)\n",
    ">\n",
    "> **Perfect For:** Image resizing, rotation, texture mapping, any 2D spatial access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36a8f6",
   "metadata": {},
   "source": [
    "### Texture Memory in Modern CUDA\n",
    "\n",
    "Modern GPUs use **texture objects** (introduced in CUDA 5.0). Texture memory provides automatic interpolation and boundary handling.\n",
    "\n",
    "Unfortunately, Numba CUDA doesn't directly support texture objects. For texture-like benefits in Numba:\n",
    "1. Use shared memory tiling for 2D spatial locality\n",
    "2. Implement manual interpolation\n",
    "3. For advanced cases, use CuPy or raw CUDA\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile texture_demo.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Texture object reference\n",
    "cudaTextureObject_t texObj;\n",
    "\n",
    "// Kernel using texture memory for bilinear interpolation\n",
    "__global__ void resizeWithTexture(cudaTextureObject_t tex, float* output,\n",
    "                                   int outWidth, int outHeight,\n",
    "                                   int inWidth, int inHeight) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < outWidth && y < outHeight) {\n",
    "        // Map output coords to input coords (normalized)\n",
    "        float u = (float)x / (outWidth - 1);\n",
    "        float v = (float)y / (outHeight - 1);\n",
    "        \n",
    "        // tex2D with linear filtering does bilinear interpolation for free!\n",
    "        float value = tex2D<float>(tex, u * inWidth, v * inHeight);\n",
    "        output[y * outWidth + x] = value;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel without texture (manual bilinear interpolation)\n",
    "__global__ void resizeManual(float* input, float* output,\n",
    "                              int outWidth, int outHeight,\n",
    "                              int inWidth, int inHeight) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < outWidth && y < outHeight) {\n",
    "        // Map output coords to input coords\n",
    "        float srcX = (float)x * (inWidth - 1) / (outWidth - 1);\n",
    "        float srcY = (float)y * (inHeight - 1) / (outHeight - 1);\n",
    "        \n",
    "        // Manual bilinear interpolation\n",
    "        int x0 = (int)srcX;\n",
    "        int y0 = (int)srcY;\n",
    "        int x1 = min(x0 + 1, inWidth - 1);\n",
    "        int y1 = min(y0 + 1, inHeight - 1);\n",
    "        \n",
    "        float fx = srcX - x0;\n",
    "        float fy = srcY - y0;\n",
    "        \n",
    "        float v00 = input[y0 * inWidth + x0];\n",
    "        float v01 = input[y0 * inWidth + x1];\n",
    "        float v10 = input[y1 * inWidth + x0];\n",
    "        float v11 = input[y1 * inWidth + x1];\n",
    "        \n",
    "        float v0 = v00 * (1 - fx) + v01 * fx;\n",
    "        float v1 = v10 * (1 - fx) + v11 * fx;\n",
    "        \n",
    "        output[y * outWidth + x] = v0 * (1 - fy) + v1 * fy;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int inWidth = 256, inHeight = 256;\n",
    "    int outWidth = 512, outHeight = 512;\n",
    "    \n",
    "    // Allocate and initialize input\n",
    "    float* h_input = (float*)malloc(inWidth * inHeight * sizeof(float));\n",
    "    for (int i = 0; i < inHeight; i++) {\n",
    "        for (int j = 0; j < inWidth; j++) {\n",
    "            h_input[i * inWidth + j] = (float)(i + j) / (inWidth + inHeight);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_output_tex, *d_output_manual;\n",
    "    cudaMalloc(&d_input, inWidth * inHeight * sizeof(float));\n",
    "    cudaMalloc(&d_output_tex, outWidth * outHeight * sizeof(float));\n",
    "    cudaMalloc(&d_output_manual, outWidth * outHeight * sizeof(float));\n",
    "    cudaMemcpy(d_input, h_input, inWidth * inHeight * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Create CUDA array for texture\n",
    "    cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc<float>();\n",
    "    cudaArray* cuArray;\n",
    "    cudaMallocArray(&cuArray, &channelDesc, inWidth, inHeight);\n",
    "    cudaMemcpy2DToArray(cuArray, 0, 0, h_input, inWidth * sizeof(float),\n",
    "                        inWidth * sizeof(float), inHeight, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Create texture object\n",
    "    cudaResourceDesc resDesc = {};\n",
    "    resDesc.resType = cudaResourceTypeArray;\n",
    "    resDesc.res.array.array = cuArray;\n",
    "    \n",
    "    cudaTextureDesc texDesc = {};\n",
    "    texDesc.addressMode[0] = cudaAddressModeClamp;\n",
    "    texDesc.addressMode[1] = cudaAddressModeClamp;\n",
    "    texDesc.filterMode = cudaFilterModeLinear;  // Bilinear interpolation!\n",
    "    texDesc.normalizedCoords = false;\n",
    "    \n",
    "    cudaTextureObject_t texObj;\n",
    "    cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);\n",
    "    \n",
    "    printf(\"=== Texture Memory Demonstration ===\\n\");\n",
    "    printf(\"Resizing %dx%d -> %dx%d with bilinear interpolation\\n\\n\", \n",
    "           inWidth, inHeight, outWidth, outHeight);\n",
    "    \n",
    "    dim3 threads(16, 16);\n",
    "    dim3 blocks((outWidth + 15) / 16, (outHeight + 15) / 16);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark texture version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 1000; i++) {\n",
    "        resizeWithTexture<<<blocks, threads>>>(texObj, d_output_tex, \n",
    "                                                outWidth, outHeight, inWidth, inHeight);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float tex_time;\n",
    "    cudaEventElapsedTime(&tex_time, start, stop);\n",
    "    \n",
    "    // Benchmark manual version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 1000; i++) {\n",
    "        resizeManual<<<blocks, threads>>>(d_input, d_output_manual,\n",
    "                                          outWidth, outHeight, inWidth, inHeight);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float manual_time;\n",
    "    cudaEventElapsedTime(&manual_time, start, stop);\n",
    "    \n",
    "    printf(\"Texture memory (hw interpolation): %.3f ms\\n\", tex_time);\n",
    "    printf(\"Manual bilinear interpolation:     %.3f ms\\n\", manual_time);\n",
    "    printf(\"Speedup from texture:              %.2fx\\n\", manual_time / tex_time);\n",
    "    printf(\"\\n‚úÖ Texture memory provides FREE hardware interpolation!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaDestroyTextureObject(texObj);\n",
    "    cudaFreeArray(cuArray);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output_tex);\n",
    "    cudaFree(d_output_manual);\n",
    "    free(h_input);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o texture_demo texture_demo.cu\n",
    "!./texture_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bad322",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)\n",
    "\n",
    "Simulating Texture Benefits: Image Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual bilinear interpolation (what texture memory does for free)\n",
    "@cuda.jit(device=True)\n",
    "def bilinear_sample(image, x, y, height, width):\n",
    "    \"\"\"Bilinear interpolation at floating-point coordinates.\"\"\"\n",
    "    # Clamp to valid range\n",
    "    x = max(0.0, min(x, width - 1.001))\n",
    "    y = max(0.0, min(y, height - 1.001))\n",
    "    \n",
    "    # Get integer coordinates\n",
    "    x0 = int(x)\n",
    "    y0 = int(y)\n",
    "    x1 = min(x0 + 1, width - 1)\n",
    "    y1 = min(y0 + 1, height - 1)\n",
    "    \n",
    "    # Fractional parts\n",
    "    fx = x - x0\n",
    "    fy = y - y0\n",
    "    \n",
    "    # Bilinear interpolation\n",
    "    v00 = image[y0, x0]\n",
    "    v01 = image[y0, x1]\n",
    "    v10 = image[y1, x0]\n",
    "    v11 = image[y1, x1]\n",
    "    \n",
    "    v0 = v00 * (1 - fx) + v01 * fx\n",
    "    v1 = v10 * (1 - fx) + v11 * fx\n",
    "    \n",
    "    return v0 * (1 - fy) + v1 * fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24318b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image resize using bilinear interpolation\n",
    "@cuda.jit\n",
    "def resize_bilinear(src, dst):\n",
    "    \"\"\"Resize image using bilinear interpolation.\"\"\"\n",
    "    dst_y, dst_x = cuda.grid(2)\n",
    "    dst_h, dst_w = dst.shape\n",
    "    src_h, src_w = src.shape\n",
    "    \n",
    "    if dst_y < dst_h and dst_x < dst_w:\n",
    "        # Map destination coords to source coords\n",
    "        src_x = dst_x * (src_w - 1) / (dst_w - 1)\n",
    "        src_y = dst_y * (src_h - 1) / (dst_h - 1)\n",
    "        \n",
    "        dst[dst_y, dst_x] = bilinear_sample(src, src_x, src_y, src_h, src_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec42413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image resizing\n",
    "src_size = 256\n",
    "dst_size = 512\n",
    "\n",
    "# Create a simple test pattern\n",
    "src_image = np.zeros((src_size, src_size), dtype=np.float32)\n",
    "for i in range(src_size):\n",
    "    for j in range(src_size):\n",
    "        src_image[i, j] = (i + j) / (2 * src_size)\n",
    "\n",
    "dst_image = np.zeros((dst_size, dst_size), dtype=np.float32)\n",
    "\n",
    "d_src = cuda.to_device(src_image)\n",
    "d_dst = cuda.to_device(dst_image)\n",
    "\n",
    "threads = (16, 16)\n",
    "blocks = ((dst_size + 15) // 16, (dst_size + 15) // 16)\n",
    "\n",
    "resize_bilinear[blocks, threads](d_src, d_dst)\n",
    "result = d_dst.copy_to_host()\n",
    "\n",
    "print(f\"Resized from {src_size}x{src_size} to {dst_size}x{dst_size}\")\n",
    "print(f\"Source range: [{src_image.min():.3f}, {src_image.max():.3f}]\")\n",
    "print(f\"Result range: [{result.min():.3f}, {result.max():.3f}]\")\n",
    "print(\"\\nNote: With texture memory, interpolation would be automatic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f077b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Memory Type Decision Guide\n",
    "\n",
    "### Decision Flowchart\n",
    "\n",
    "```\n",
    "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                 ‚îÇ Need to store data   ‚îÇ\n",
    "                 ‚îÇ   for GPU kernel?    ‚îÇ\n",
    "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚ñº                           ‚ñº\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ Read-only data?    ‚îÇ    ‚îÇ Read-write data?   ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ                          ‚îÇ\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚ñº                 ‚ñº         ‚ñº                ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Small ‚îÇ       ‚îÇ Large  ‚îÇ  ‚îÇPrivate ‚îÇ   ‚îÇ  Shared   ‚îÇ\n",
    "‚îÇ <64KB ‚îÇ       ‚îÇdata or ‚îÇ  ‚îÇto each ‚îÇ   ‚îÇamong block‚îÇ\n",
    "‚îÇuniform‚îÇ       ‚îÇspatial ‚îÇ  ‚îÇthread  ‚îÇ   ‚îÇthreads    ‚îÇ\n",
    "‚îÇaccess ‚îÇ       ‚îÇaccess  ‚îÇ  ‚îÇ        ‚îÇ   ‚îÇ           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îÇ               ‚îÇ           ‚îÇ              ‚îÇ\n",
    "    ‚ñº               ‚ñº           ‚ñº              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇCONSTANT‚îÇ   ‚îÇ TEXTURE  ‚îÇ  ‚îÇREGISTER‚îÇ   ‚îÇ  SHARED   ‚îÇ\n",
    "‚îÇ MEMORY ‚îÇ   ‚îÇ  MEMORY  ‚îÇ  ‚îÇ(auto)  ‚îÇ   ‚îÇ  MEMORY   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Default: GLOBAL MEMORY (with coalescing optimizations)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c3378",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üÉè Concept Card: When to Use Each Memory Type\n",
    "\n",
    "> **The Decision Tree**\n",
    ">\n",
    "> Ask yourself these questions in order:\n",
    ">\n",
    "> ```\n",
    "> ‚ùì Is the data READ-ONLY during kernel execution?\n",
    ">    ‚îÇ\n",
    ">    ‚îú‚îÄ NO ‚Üí Use Global Memory (read-write) or Shared Memory (block-local)\n",
    ">    ‚îÇ\n",
    ">    ‚îî‚îÄ YES ‚Üí Continue...\n",
    ">        ‚îÇ\n",
    ">        ‚ùì Is the data < 64KB AND accessed UNIFORMLY by all threads?\n",
    ">           ‚îÇ\n",
    ">           ‚îú‚îÄ YES ‚Üí üìª CONSTANT MEMORY\n",
    ">           ‚îÇ        Examples: filter kernels, LUTs, config parameters\n",
    ">           ‚îÇ\n",
    ">           ‚îî‚îÄ NO ‚Üí Continue...\n",
    ">               ‚îÇ\n",
    ">               ‚ùì Does the data have 2D/3D SPATIAL LOCALITY?\n",
    ">                  ‚îÇ\n",
    ">                  ‚îú‚îÄ YES ‚Üí üó∫Ô∏è TEXTURE MEMORY\n",
    ">                  ‚îÇ        Examples: image processing, volume rendering\n",
    ">                  ‚îÇ\n",
    ">                  ‚îî‚îÄ NO ‚Üí Use GLOBAL MEMORY with coalescing\n",
    "> ```\n",
    ">\n",
    "> **Quick Cheat Sheet:**\n",
    "> | Access Pattern | Memory Choice | Why |\n",
    "> |----------------|---------------|-----|\n",
    "> | Same value ‚Üí all threads | Constant | Broadcast efficiency |\n",
    "> | 2D neighborhood reads | Texture | 2D cache + interpolation |\n",
    "> | Linear streaming | Global | Coalescing works well |\n",
    "> | Block-local reuse | Shared | Fastest for collaboration |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd368d32",
   "metadata": {},
   "source": [
    "### Quick Reference Table\n",
    "\n",
    "| Scenario | Best Memory | Why |\n",
    "|----------|-------------|-----|\n",
    "| Convolution kernel/filter | Constant | Same values read by all threads |\n",
    "| Configuration parameters | Constant | Small, uniform read access |\n",
    "| Lookup table (uniform access) | Constant | Broadcast efficiency |\n",
    "| Image processing (resize, rotate) | Texture | 2D spatial locality + interpolation |\n",
    "| Volume rendering | Texture | 3D spatial locality |\n",
    "| Random 2D reads | Texture | 2D cache optimization |\n",
    "| Thread-local accumulator | Register | Fastest, private to thread |\n",
    "| Block-wide reduction | Shared | Threads need to communicate |\n",
    "| Tiled matrix multiply | Shared | Data reuse within block |\n",
    "| Histogram (atomic updates) | Shared ‚Üí Global | Reduce atomic contention |\n",
    "| Large arrays with streaming | Global | Only option for large data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e213a15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Practical Example - Optimized Gaussian Blur\n",
    "\n",
    "Now let's bring together everything we've learned this week! We'll combine:\n",
    "- **Shared memory** for image tile caching (Day 2)\n",
    "- **Constant-like behavior** for the filter kernel (Day 4)\n",
    "- **Coalesced access** patterns (Day 1)\n",
    "\n",
    "This is what real-world CUDA optimization looks like‚Äîlayering multiple techniques for maximum performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15956c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_2d(size, sigma):\n",
    "    \"\"\"Generate 2D Gaussian kernel.\"\"\"\n",
    "    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    return (kernel / kernel.sum()).astype(np.float32)\n",
    "\n",
    "# 5x5 Gaussian kernel\n",
    "KERNEL_SIZE = 5\n",
    "GAUSSIAN = gaussian_kernel_2d(KERNEL_SIZE, 1.0)\n",
    "print(\"5x5 Gaussian kernel:\")\n",
    "print(np.round(GAUSSIAN, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300cecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive implementation: Global memory only\n",
    "@cuda.jit\n",
    "def gaussian_blur_naive(image, kernel, output, ksize):\n",
    "    \"\"\"Naive Gaussian blur - all global memory.\"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    height, width = image.shape\n",
    "    half_k = ksize // 2\n",
    "    \n",
    "    if row >= half_k and row < height - half_k and col >= half_k and col < width - half_k:\n",
    "        result = 0.0\n",
    "        for i in range(-half_k, half_k + 1):\n",
    "            for j in range(-half_k, half_k + 1):\n",
    "                result += image[row + i, col + j] * kernel[i + half_k, j + half_k]\n",
    "        output[row, col] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized implementation: Shared memory tiling + kernel in shared memory\n",
    "TILE_SIZE = 16\n",
    "BLOCK_SIZE = TILE_SIZE + KERNEL_SIZE - 1  # Tile + halo\n",
    "\n",
    "@cuda.jit\n",
    "def gaussian_blur_optimized(image, kernel, output, ksize):\n",
    "    \"\"\"Optimized Gaussian blur with shared memory tiling.\"\"\"\n",
    "    # Shared memory for image tile (with halo) and kernel\n",
    "    shared_tile = cuda.shared.array((BLOCK_SIZE, BLOCK_SIZE), dtype=np.float32)\n",
    "    shared_kernel = cuda.shared.array((5, 5), dtype=np.float32)\n",
    "    \n",
    "    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y\n",
    "    bx, by = cuda.blockIdx.x, cuda.blockIdx.y\n",
    "    height, width = image.shape\n",
    "    half_k = ksize // 2\n",
    "    \n",
    "    # Load kernel to shared memory (first 25 threads)\n",
    "    linear_tid = ty * cuda.blockDim.x + tx\n",
    "    if linear_tid < ksize * ksize:\n",
    "        ki, kj = linear_tid // ksize, linear_tid % ksize\n",
    "        shared_kernel[ki, kj] = kernel[ki, kj]\n",
    "    \n",
    "    # Calculate tile starting position (with halo offset)\n",
    "    tile_start_row = by * TILE_SIZE - half_k\n",
    "    tile_start_col = bx * TILE_SIZE - half_k\n",
    "    \n",
    "    # Load tile with halo into shared memory\n",
    "    # Each thread may need to load multiple elements\n",
    "    for i in range(0, BLOCK_SIZE, TILE_SIZE):\n",
    "        for j in range(0, BLOCK_SIZE, TILE_SIZE):\n",
    "            si = ty + i\n",
    "            sj = tx + j\n",
    "            if si < BLOCK_SIZE and sj < BLOCK_SIZE:\n",
    "                gi = tile_start_row + si\n",
    "                gj = tile_start_col + sj\n",
    "                if 0 <= gi < height and 0 <= gj < width:\n",
    "                    shared_tile[si, sj] = image[gi, gj]\n",
    "                else:\n",
    "                    shared_tile[si, sj] = 0.0\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Compute output\n",
    "    out_row = by * TILE_SIZE + ty\n",
    "    out_col = bx * TILE_SIZE + tx\n",
    "    \n",
    "    if out_row < height and out_col < width:\n",
    "        result = 0.0\n",
    "        for i in range(ksize):\n",
    "            for j in range(ksize):\n",
    "                result += shared_tile[ty + i, tx + j] * shared_kernel[i, j]\n",
    "        output[out_row, out_col] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark comparison\n",
    "SIZE = 2048\n",
    "image = np.random.rand(SIZE, SIZE).astype(np.float32)\n",
    "output = np.zeros_like(image)\n",
    "\n",
    "d_image = cuda.to_device(image)\n",
    "d_kernel = cuda.to_device(GAUSSIAN)\n",
    "d_output = cuda.to_device(output)\n",
    "\n",
    "# Naive version\n",
    "threads_naive = (16, 16)\n",
    "blocks_naive = ((SIZE + 15) // 16, (SIZE + 15) // 16)\n",
    "\n",
    "gaussian_blur_naive[blocks_naive, threads_naive](d_image, d_kernel, d_output, KERNEL_SIZE)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(50):\n",
    "    gaussian_blur_naive[blocks_naive, threads_naive](d_image, d_kernel, d_output, KERNEL_SIZE)\n",
    "cuda.synchronize()\n",
    "naive_time = (time.perf_counter() - start) / 50 * 1000\n",
    "\n",
    "# Optimized version\n",
    "threads_opt = (TILE_SIZE, TILE_SIZE)\n",
    "blocks_opt = ((SIZE + TILE_SIZE - 1) // TILE_SIZE, (SIZE + TILE_SIZE - 1) // TILE_SIZE)\n",
    "\n",
    "gaussian_blur_optimized[blocks_opt, threads_opt](d_image, d_kernel, d_output, KERNEL_SIZE)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(50):\n",
    "    gaussian_blur_optimized[blocks_opt, threads_opt](d_image, d_kernel, d_output, KERNEL_SIZE)\n",
    "cuda.synchronize()\n",
    "optimized_time = (time.perf_counter() - start) / 50 * 1000\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Gaussian Blur Performance ({SIZE}x{SIZE} image)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Naive (global memory):     {naive_time:.3f} ms\")\n",
    "print(f\"Optimized (shared memory): {optimized_time:.3f} ms\")\n",
    "print(f\"Speedup:                   {naive_time/optimized_time:.2f}x\")\n",
    "print(f\"\\nOptimizations applied:\")\n",
    "print(\"  ‚úì Kernel loaded to shared memory (constant-like behavior)\")\n",
    "print(\"  ‚úì Image tile with halo in shared memory\")\n",
    "print(\"  ‚úì Coalesced global memory loads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4181320",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "Now it's your turn! These exercises will help you internalize constant and texture memory patterns through hands-on practice.\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises using constant and texture memory in CUDA C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c63ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile special_memory_exercises.cu\n",
    "// special_memory_exercises.cu - Constant and texture memory exercises\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Separable Gaussian Blur using Constant Memory\n",
    "// =============================================================================\n",
    "\n",
    "// Constant memory for 1D Gaussian kernel (shared by all threads)\n",
    "#define MAX_KERNEL_SIZE 25\n",
    "__constant__ float c_gaussianKernel[MAX_KERNEL_SIZE];\n",
    "__constant__ int c_kernelRadius;\n",
    "\n",
    "// Generate 1D Gaussian kernel on host\n",
    "void generateGaussianKernel(float* kernel, int size, float sigma) {\n",
    "    int radius = size / 2;\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        int x = i - radius;\n",
    "        kernel[i] = expf(-(x * x) / (2.0f * sigma * sigma));\n",
    "        sum += kernel[i];\n",
    "    }\n",
    "    // Normalize\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        kernel[i] /= sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Horizontal 1D convolution\n",
    "__global__ void gaussianBlurHorizontal(const float* input, float* output, \n",
    "                                        int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x >= width || y >= height) return;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    int radius = c_kernelRadius;\n",
    "    \n",
    "    for (int k = -radius; k <= radius; k++) {\n",
    "        int nx = min(max(x + k, 0), width - 1);  // Clamp to border\n",
    "        sum += input[y * width + nx] * c_gaussianKernel[k + radius];\n",
    "    }\n",
    "    \n",
    "    output[y * width + x] = sum;\n",
    "}\n",
    "\n",
    "// Vertical 1D convolution\n",
    "__global__ void gaussianBlurVertical(const float* input, float* output, \n",
    "                                      int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x >= width || y >= height) return;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    int radius = c_kernelRadius;\n",
    "    \n",
    "    for (int k = -radius; k <= radius; k++) {\n",
    "        int ny = min(max(y + k, 0), height - 1);  // Clamp to border\n",
    "        sum += input[ny * width + x] * c_gaussianKernel[k + radius];\n",
    "    }\n",
    "    \n",
    "    output[y * width + x] = sum;\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Lookup Table with Constant Memory\n",
    "// =============================================================================\n",
    "\n",
    "// Heatmap colormap (256 RGB entries)\n",
    "__constant__ float c_heatmapLUT[256 * 3];\n",
    "\n",
    "void generateHeatmapLUT(float* lut) {\n",
    "    for (int i = 0; i < 256; i++) {\n",
    "        float t = i / 255.0f;\n",
    "        float r, g, b;\n",
    "        \n",
    "        // Blue -> Cyan -> Green -> Yellow -> Red\n",
    "        if (t < 0.25f) {\n",
    "            r = 0; g = t * 4; b = 1;\n",
    "        } else if (t < 0.5f) {\n",
    "            r = 0; g = 1; b = 1 - (t - 0.25f) * 4;\n",
    "        } else if (t < 0.75f) {\n",
    "            r = (t - 0.5f) * 4; g = 1; b = 0;\n",
    "        } else {\n",
    "            r = 1; g = 1 - (t - 0.75f) * 4; b = 0;\n",
    "        }\n",
    "        \n",
    "        lut[i * 3 + 0] = r;\n",
    "        lut[i * 3 + 1] = g;\n",
    "        lut[i * 3 + 2] = b;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void applyHeatmap(const unsigned char* grayscale, float* output_rgb,\n",
    "                              int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x >= width || y >= height) return;\n",
    "    \n",
    "    int idx = y * width + x;\n",
    "    int gray = grayscale[idx];\n",
    "    \n",
    "    // Read from constant memory LUT (broadcast to all threads)\n",
    "    output_rgb[idx * 3 + 0] = c_heatmapLUT[gray * 3 + 0];\n",
    "    output_rgb[idx * 3 + 1] = c_heatmapLUT[gray * 3 + 1];\n",
    "    output_rgb[idx * 3 + 2] = c_heatmapLUT[gray * 3 + 2];\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Test harness\n",
    "// =============================================================================\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Special Memory Exercises ===\\n\\n\");\n",
    "    \n",
    "    // Exercise 1: Separable Gaussian Blur\n",
    "    printf(\"Exercise 1: Separable Gaussian Blur\\n\");\n",
    "    printf(\"-\" \"-----------------------------------\\n\");\n",
    "    {\n",
    "        const int WIDTH = 512;\n",
    "        const int HEIGHT = 512;\n",
    "        const int KERNEL_SIZE = 5;\n",
    "        const float SIGMA = 1.0f;\n",
    "        size_t imageSize = WIDTH * HEIGHT * sizeof(float);\n",
    "        \n",
    "        // Generate Gaussian kernel and copy to constant memory\n",
    "        float h_kernel[MAX_KERNEL_SIZE];\n",
    "        generateGaussianKernel(h_kernel, KERNEL_SIZE, SIGMA);\n",
    "        CUDA_CHECK(cudaMemcpyToSymbol(c_gaussianKernel, h_kernel, \n",
    "                                       KERNEL_SIZE * sizeof(float)));\n",
    "        int radius = KERNEL_SIZE / 2;\n",
    "        CUDA_CHECK(cudaMemcpyToSymbol(c_kernelRadius, &radius, sizeof(int)));\n",
    "        \n",
    "        printf(\"Gaussian kernel (size=%d, sigma=%.1f):\\n  \", KERNEL_SIZE, SIGMA);\n",
    "        for (int i = 0; i < KERNEL_SIZE; i++) printf(\"%.3f \", h_kernel[i]);\n",
    "        printf(\"\\n\\n\");\n",
    "        \n",
    "        // Create test image (gradient)\n",
    "        float* h_image = (float*)malloc(imageSize);\n",
    "        for (int y = 0; y < HEIGHT; y++) {\n",
    "            for (int x = 0; x < WIDTH; x++) {\n",
    "                h_image[y * WIDTH + x] = (float)((x + y) % 256) / 255.0f;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Allocate device memory\n",
    "        float *d_input, *d_temp, *d_output;\n",
    "        CUDA_CHECK(cudaMalloc(&d_input, imageSize));\n",
    "        CUDA_CHECK(cudaMalloc(&d_temp, imageSize));\n",
    "        CUDA_CHECK(cudaMalloc(&d_output, imageSize));\n",
    "        CUDA_CHECK(cudaMemcpy(d_input, h_image, imageSize, cudaMemcpyHostToDevice));\n",
    "        \n",
    "        // Launch separable blur\n",
    "        dim3 block(16, 16);\n",
    "        dim3 grid((WIDTH + 15) / 16, (HEIGHT + 15) / 16);\n",
    "        \n",
    "        cudaEvent_t start, stop;\n",
    "        CUDA_CHECK(cudaEventCreate(&start));\n",
    "        CUDA_CHECK(cudaEventCreate(&stop));\n",
    "        \n",
    "        CUDA_CHECK(cudaEventRecord(start));\n",
    "        gaussianBlurHorizontal<<<grid, block>>>(d_input, d_temp, WIDTH, HEIGHT);\n",
    "        gaussianBlurVertical<<<grid, block>>>(d_temp, d_output, WIDTH, HEIGHT);\n",
    "        CUDA_CHECK(cudaEventRecord(stop));\n",
    "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
    "        \n",
    "        float ms;\n",
    "        CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "        \n",
    "        CUDA_CHECK(cudaMemcpy(h_image, d_output, imageSize, cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Separable blur: %.3f ms\\n\", ms);\n",
    "        printf(\"Operations per pixel: 2 √ó %d = %d (vs %d for 2D)\\n\", \n",
    "               KERNEL_SIZE, 2 * KERNEL_SIZE, KERNEL_SIZE * KERNEL_SIZE);\n",
    "        printf(\"Sample output: [0,0]=%.3f, [255,255]=%.3f\\n\\n\", \n",
    "               h_image[0], h_image[255 * WIDTH + 255]);\n",
    "        \n",
    "        cudaFree(d_input); cudaFree(d_temp); cudaFree(d_output);\n",
    "        free(h_image);\n",
    "    }\n",
    "    \n",
    "    // Exercise 2: Heatmap Colorization\n",
    "    printf(\"Exercise 2: Heatmap Colorization\\n\");\n",
    "    printf(\"-\" \"--------------------------------\\n\");\n",
    "    {\n",
    "        const int WIDTH = 1024;\n",
    "        const int HEIGHT = 1024;\n",
    "        \n",
    "        // Generate and copy LUT to constant memory\n",
    "        float h_lut[256 * 3];\n",
    "        generateHeatmapLUT(h_lut);\n",
    "        CUDA_CHECK(cudaMemcpyToSymbol(c_heatmapLUT, h_lut, 256 * 3 * sizeof(float)));\n",
    "        \n",
    "        // Create grayscale test image\n",
    "        unsigned char* h_gray = (unsigned char*)malloc(WIDTH * HEIGHT);\n",
    "        float* h_rgb = (float*)malloc(WIDTH * HEIGHT * 3 * sizeof(float));\n",
    "        \n",
    "        for (int y = 0; y < HEIGHT; y++) {\n",
    "            for (int x = 0; x < WIDTH; x++) {\n",
    "                h_gray[y * WIDTH + x] = (unsigned char)((x + y) % 256);\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Allocate device memory\n",
    "        unsigned char* d_gray;\n",
    "        float* d_rgb;\n",
    "        CUDA_CHECK(cudaMalloc(&d_gray, WIDTH * HEIGHT));\n",
    "        CUDA_CHECK(cudaMalloc(&d_rgb, WIDTH * HEIGHT * 3 * sizeof(float)));\n",
    "        CUDA_CHECK(cudaMemcpy(d_gray, h_gray, WIDTH * HEIGHT, cudaMemcpyHostToDevice));\n",
    "        \n",
    "        // Launch kernel\n",
    "        dim3 block(16, 16);\n",
    "        dim3 grid((WIDTH + 15) / 16, (HEIGHT + 15) / 16);\n",
    "        \n",
    "        cudaEvent_t start, stop;\n",
    "        CUDA_CHECK(cudaEventCreate(&start));\n",
    "        CUDA_CHECK(cudaEventCreate(&stop));\n",
    "        \n",
    "        CUDA_CHECK(cudaEventRecord(start));\n",
    "        applyHeatmap<<<grid, block>>>(d_gray, d_rgb, WIDTH, HEIGHT);\n",
    "        CUDA_CHECK(cudaEventRecord(stop));\n",
    "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
    "        \n",
    "        float ms;\n",
    "        CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "        \n",
    "        CUDA_CHECK(cudaMemcpy(h_rgb, d_rgb, WIDTH * HEIGHT * 3 * sizeof(float), \n",
    "                              cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"Colorization: %.3f ms (%.2f Mpixels/s)\\n\", \n",
    "               ms, (WIDTH * HEIGHT / 1e6) / (ms / 1000));\n",
    "        printf(\"Sample: gray[0]=%d ‚Üí RGB=(%.2f, %.2f, %.2f)\\n\",\n",
    "               h_gray[0], h_rgb[0], h_rgb[1], h_rgb[2]);\n",
    "        printf(\"Sample: gray[128]=%d ‚Üí RGB=(%.2f, %.2f, %.2f)\\n\",\n",
    "               h_gray[128], h_rgb[128*3], h_rgb[128*3+1], h_rgb[128*3+2]);\n",
    "        \n",
    "        cudaFree(d_gray); cudaFree(d_rgb);\n",
    "        free(h_gray); free(h_rgb);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n=== All exercises complete! ===\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c50081",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o special_memory_exercises special_memory_exercises.cu && ./special_memory_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186a39a",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Separable Gaussian Blur\n",
    "\n",
    "A 2D Gaussian is **separable**: it can be computed as two 1D passes (horizontal then vertical). This reduces operations from O(k¬≤) to O(2k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Gaussian kernel\n",
    "def gaussian_kernel_1d(size, sigma):\n",
    "    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n",
    "    kernel = np.exp(-ax**2 / (2 * sigma**2))\n",
    "    return (kernel / kernel.sum()).astype(np.float32)\n",
    "\n",
    "GAUSSIAN_1D = gaussian_kernel_1d(5, 1.0)\n",
    "print(\"1D Gaussian kernel:\", GAUSSIAN_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement horizontal and vertical 1D convolution kernels\n",
    "\n",
    "@cuda.jit\n",
    "def gaussian_blur_horizontal(image, kernel, output, ksize):\n",
    "    \"\"\"Apply 1D Gaussian blur horizontally.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "@cuda.jit\n",
    "def gaussian_blur_vertical(image, kernel, output, ksize):\n",
    "    \"\"\"Apply 1D Gaussian blur vertically.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Compare:\n",
    "# 1. 5x5 2D convolution: 25 multiplications per pixel\n",
    "# 2. Two 1x5 1D convolutions: 10 multiplications per pixel\n",
    "# Expected speedup: ~2.5x from reduced arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04521f5a",
   "metadata": {},
   "source": [
    "### Exercise 2: Lookup Table with Constant Memory Pattern\n",
    "\n",
    "Implement a color mapping kernel where all threads read from the same lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a colormap lookup table (256 entries)\n",
    "# Maps grayscale values to \"heat\" colors\n",
    "def create_heatmap_lut():\n",
    "    lut = np.zeros((256, 3), dtype=np.float32)\n",
    "    for i in range(256):\n",
    "        t = i / 255.0\n",
    "        # Blue -> Cyan -> Green -> Yellow -> Red\n",
    "        if t < 0.25:\n",
    "            lut[i] = [0, t * 4, 1]\n",
    "        elif t < 0.5:\n",
    "            lut[i] = [0, 1, 1 - (t - 0.25) * 4]\n",
    "        elif t < 0.75:\n",
    "            lut[i] = [(t - 0.5) * 4, 1, 0]\n",
    "        else:\n",
    "            lut[i] = [1, 1 - (t - 0.75) * 4, 0]\n",
    "    return lut\n",
    "\n",
    "HEATMAP_LUT = create_heatmap_lut()\n",
    "print(f\"Lookup table shape: {HEATMAP_LUT.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement color mapping with LUT in shared memory\n",
    "\n",
    "@cuda.jit\n",
    "def apply_heatmap(grayscale, lut, output_rgb):\n",
    "    \"\"\"\n",
    "    Apply heatmap colorization using lookup table.\n",
    "    \n",
    "    Args:\n",
    "        grayscale: 2D input (H, W), values 0-255\n",
    "        lut: Lookup table (256, 3)\n",
    "        output_rgb: 3D output (H, W, 3)\n",
    "    \n",
    "    Hint: Load LUT to shared memory for constant-memory-like behavior\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e154ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### üìã Quick Reference Card: Special Memory Types\n",
    "\n",
    "| Memory Type | Size Limit | Best Access Pattern | Hardware Feature | Use Case |\n",
    "|-------------|------------|---------------------|------------------|----------|\n",
    "| **Constant** | 64 KB | Uniform (all threads same address) | Broadcast to warp | Filter kernels, LUTs, config |\n",
    "| **Texture** | Large | 2D/3D spatial locality | Interpolation, boundary handling | Image processing, volume rendering |\n",
    "| **Shared** | 48-164 KB/SM | Block-local reuse | Low latency, high bandwidth | Tiled algorithms, reductions |\n",
    "| **Global** | GB | Coalesced linear | L1/L2 cache | General purpose |\n",
    "\n",
    "### üß† Three Things to Remember\n",
    "\n",
    "1. **Constant Memory = Radio Broadcast**\n",
    "   - One read serves an entire warp when all threads access the same address\n",
    "   - Perfect for filter kernels, lookup tables, and configuration parameters\n",
    "   - ‚ö†Ô∏è Different addresses per thread ‚Üí serialized reads (slow!)\n",
    "\n",
    "2. **Texture Memory = 2D GPS Cache**\n",
    "   - Optimized for 2D spatial locality (nearby pixels likely accessed together)\n",
    "   - FREE hardware interpolation and boundary handling\n",
    "   - Great for image resizing, rotation, and any 2D neighborhood access\n",
    "\n",
    "3. **Match Memory to Access Pattern**\n",
    "   - There's no universally \"best\" memory type\n",
    "   - Profile and measure‚Äîthe right choice depends on YOUR data access pattern\n",
    "   - Layering techniques (shared + constant-like filter) yields best results\n",
    "\n",
    "### Week 2 Summary: Memory Mastery\n",
    "\n",
    "| Day | Topic | Key Insight |\n",
    "|-----|-------|-------------|\n",
    "| 1 | Memory Coalescing | Adjacent threads ‚Üí adjacent memory = single transaction |\n",
    "| 2 | Shared Memory | On-chip cache for data reuse within a block |\n",
    "| 3 | Bank Conflicts | 32 banks, same-bank access ‚Üí serialization |\n",
    "| 4 | Special Memory | Constant (broadcast) + Texture (2D spatial) |\n",
    "\n",
    "### üîß Optimization Hierarchy\n",
    "\n",
    "```\n",
    "1. First: Choose right algorithm (parallelizable)\n",
    "2. Then:  Ensure coalesced global memory access\n",
    "3. Then:  Use shared memory for data reuse\n",
    "4. Then:  Avoid bank conflicts (padding)\n",
    "5. Then:  Consider special memory types\n",
    "6. Then:  Fine-tune thread/block configuration\n",
    "```\n",
    "\n",
    "### Memory Selection Quick Guide\n",
    "\n",
    "```\n",
    "Small read-only + uniform access ‚Üí Constant\n",
    "2D spatial access + interpolation ‚Üí Texture  \n",
    "Block-local data reuse           ‚Üí Shared\n",
    "Thread-private temporary         ‚Üí Register\n",
    "Everything else                  ‚Üí Global (with coalescing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c66b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "### ‚úÖ Week 2 Complete: Memory Patterns & Optimization\n",
    "\n",
    "You've mastered the memory hierarchy! You now understand:\n",
    "- How to achieve coalesced memory access\n",
    "- When and how to use shared memory for data reuse\n",
    "- How to avoid bank conflicts\n",
    "- Which special memory types to choose for different patterns\n",
    "\n",
    "### üìã Before Moving On\n",
    "- Complete the **Day 5 Review & Checkpoint Quiz**\n",
    "- Try the exercises above to solidify your understanding\n",
    "\n",
    "### üîÆ Week 3 Preview: Synchronization & Atomics\n",
    "\n",
    "Next week, we tackle **thread coordination**‚Äîwhat happens when threads need to work together:\n",
    "\n",
    "| Day | Topic | Why It Matters |\n",
    "|-----|-------|----------------|\n",
    "| 1 | Thread Synchronization | Coordinate threads within a block |\n",
    "| 2 | Atomic Operations | Safe concurrent updates to shared data |\n",
    "| 3 | Warp-Level Programming | Leverage warp-level primitives |\n",
    "| 4 | Parallel Reduction | Efficient patterns for aggregation |\n",
    "\n",
    "**Key Question for Week 3:** *How do we safely combine results from thousands of threads?*\n",
    "\n",
    "---\n",
    "\n",
    "*Great work completing Week 2! You now have the memory optimization skills that separate efficient CUDA code from naive implementations.* üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
