{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd31aaf",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 4: Special Memory Types - Constant & Texture Memory\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-02/day-4-special-memory.ipynb)\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15fb12",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**For ODU HPC (Wahab):**\n",
    "```bash\n",
    "module load container_env cuda-12.3.0\n",
    "crun -p ~/envs/cuda python -m jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"\\nCUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")\n",
    "print(\"\\nâš ï¸  Remember: CUDA C++ code is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151b051",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Complete CUDA Memory Hierarchy Review\n",
    "\n",
    "Before diving into special memory types, let's review the complete memory hierarchy:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    CUDA Memory Hierarchy                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚              Global Memory (GB, slowest)            â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ All threads can read/write                       â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Persists for application lifetime                â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ ~400-900 GB/s bandwidth                          â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                          â”‚                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚                      â”‚                        â”‚          â”‚\n",
    "â”‚  â–¼                      â–¼                        â–¼          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚   Constant   â”‚  â”‚   Texture    â”‚  â”‚    Shared    â”‚      â”‚\n",
    "â”‚  â”‚    Memory    â”‚  â”‚    Memory    â”‚  â”‚    Memory    â”‚      â”‚\n",
    "â”‚  â”‚  (64KB,      â”‚  â”‚  (cached,    â”‚  â”‚  (48-164KB   â”‚      â”‚\n",
    "â”‚  â”‚   cached)    â”‚  â”‚   read-only) â”‚  â”‚   per SM)    â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚         â”‚                 â”‚                  â”‚              â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                           â”‚                                  â”‚\n",
    "â”‚                           â–¼                                  â”‚\n",
    "â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚            â”‚  Registers (fastest, private) â”‚                 â”‚\n",
    "â”‚            â”‚  â€¢ Per-thread, ~256 per threadâ”‚                 â”‚\n",
    "â”‚            â”‚  â€¢ ~12 TB/s equivalent        â”‚                 â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Memory Types Summary\n",
    "\n",
    "| Memory Type | Location | Scope | Lifetime | Cache | Speed |\n",
    "|-------------|----------|-------|----------|-------|-------|\n",
    "| Register | On-chip | Thread | Thread | N/A | Fastest |\n",
    "| Local | Off-chip | Thread | Thread | L1/L2 | Slow |\n",
    "| Shared | On-chip | Block | Block | N/A | Fast |\n",
    "| Global | Off-chip | Grid | Application | L1/L2 | Slow |\n",
    "| Constant | Off-chip | Grid | Application | Constant cache | Fast (broadcast) |\n",
    "| Texture | Off-chip | Grid | Application | Texture cache | Fast (spatial) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69704e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Constant Memory\n",
    "\n",
    "### What is Constant Memory?\n",
    "\n",
    "Constant memory is a **read-only** memory space that:\n",
    "- Has 64KB total capacity per GPU\n",
    "- Is cached in a dedicated **constant cache**\n",
    "- Optimized for **broadcasting** same value to all threads\n",
    "- Initialized by the host before kernel launch\n",
    "\n",
    "### When Constant Memory Excels\n",
    "\n",
    "```\n",
    "BEST CASE: All threads read the SAME address\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Thread 0  Thread 1  Thread 2  ...  Thread 31    â”‚\n",
    "â”‚     â”‚         â”‚         â”‚              â”‚         â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚                     â”‚                             â”‚\n",
    "â”‚                     â–¼                             â”‚\n",
    "â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
    "â”‚           â”‚ Constant Cache  â”‚                    â”‚\n",
    "â”‚           â”‚   (1 read)      â”‚                    â”‚\n",
    "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â”‚  â†’ Result broadcast to all 32 threads in 1 cycle â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "WORST CASE: All threads read DIFFERENT addresses\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Thread 0  Thread 1  Thread 2  ...  Thread 31    â”‚\n",
    "â”‚     â”‚         â”‚         â”‚              â”‚         â”‚\n",
    "â”‚     â–¼         â–¼         â–¼              â–¼         â”‚\n",
    "â”‚   addr[0]  addr[1]  addr[2]  ...  addr[31]      â”‚\n",
    "â”‚     â”‚         â”‚         â”‚              â”‚         â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚                     â”‚                             â”‚\n",
    "â”‚                     â–¼                             â”‚\n",
    "â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
    "â”‚           â”‚ 32 serial reads â”‚ â† SLOW!            â”‚\n",
    "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Ideal Use Cases for Constant Memory\n",
    "\n",
    "1. **Filter kernels/masks** (convolution, Gaussian blur)\n",
    "2. **Lookup tables** accessed uniformly\n",
    "3. **Configuration parameters** (dimensions, coefficients)\n",
    "4. **Mathematical constants** (Ï€, e, conversion factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83945205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constant_mem_demo.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Constant memory declaration (64KB max)\n",
    "__constant__ float d_filter[9];\n",
    "\n",
    "// Convolution using constant memory for filter\n",
    "__global__ void convolution_constant(const float* input, float* output, \n",
    "                                      int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x > 0 && x < width - 1 && y > 0 && y < height - 1) {\n",
    "        float result = 0.0f;\n",
    "        \n",
    "        // 3x3 convolution using constant memory filter\n",
    "        // All threads read the SAME filter values -> broadcast!\n",
    "        for (int i = -1; i <= 1; i++) {\n",
    "            for (int j = -1; j <= 1; j++) {\n",
    "                int idx = (y + i) * width + (x + j);\n",
    "                int fIdx = (i + 1) * 3 + (j + 1);\n",
    "                result += input[idx] * d_filter[fIdx];  // Constant memory access\n",
    "            }\n",
    "        }\n",
    "        output[y * width + x] = result;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Convolution using global memory for filter (for comparison)\n",
    "__global__ void convolution_global(const float* input, const float* filter,\n",
    "                                   float* output, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x > 0 && x < width - 1 && y > 0 && y < height - 1) {\n",
    "        float result = 0.0f;\n",
    "        \n",
    "        for (int i = -1; i <= 1; i++) {\n",
    "            for (int j = -1; j <= 1; j++) {\n",
    "                int idx = (y + i) * width + (x + j);\n",
    "                int fIdx = (i + 1) * 3 + (j + 1);\n",
    "                result += input[idx] * filter[fIdx];  // Global memory access\n",
    "            }\n",
    "        }\n",
    "        output[y * width + x] = result;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int width = 2048, height = 2048;\n",
    "    int size = width * height;\n",
    "    \n",
    "    // Sobel edge detection filter\n",
    "    float h_filter[9] = {\n",
    "        -1, 0, 1,\n",
    "        -2, 0, 2,\n",
    "        -1, 0, 1\n",
    "    };\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float* h_input = (float*)malloc(size * sizeof(float));\n",
    "    float* h_output = (float*)malloc(size * sizeof(float));\n",
    "    \n",
    "    // Initialize with test pattern\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        h_input[i] = (float)(rand() % 256) / 255.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_output, *d_filter_global;\n",
    "    cudaMalloc(&d_input, size * sizeof(float));\n",
    "    cudaMalloc(&d_output, size * sizeof(float));\n",
    "    cudaMalloc(&d_filter_global, 9 * sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_input, h_input, size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_filter_global, h_filter, 9 * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Copy filter to constant memory\n",
    "    cudaMemcpyToSymbol(d_filter, h_filter, 9 * sizeof(float));\n",
    "    \n",
    "    printf(\"=== Constant Memory Demonstration ===\\n\");\n",
    "    printf(\"Image size: %dx%d, Filter: 3x3 Sobel\\n\\n\", width, height);\n",
    "    \n",
    "    dim3 threads(16, 16);\n",
    "    dim3 blocks((width + 15) / 16, (height + 15) / 16);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark constant memory version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        convolution_constant<<<blocks, threads>>>(d_input, d_output, width, height);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float const_time;\n",
    "    cudaEventElapsedTime(&const_time, start, stop);\n",
    "    \n",
    "    // Benchmark global memory version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        convolution_global<<<blocks, threads>>>(d_input, d_filter_global, d_output, width, height);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float global_time;\n",
    "    cudaEventElapsedTime(&global_time, start, stop);\n",
    "    \n",
    "    printf(\"Constant memory filter: %.3f ms\\n\", const_time / 100);\n",
    "    printf(\"Global memory filter:   %.3f ms\\n\", global_time / 100);\n",
    "    printf(\"Speedup:                %.2fx\\n\", global_time / const_time);\n",
    "    printf(\"\\nğŸ’¡ Constant memory broadcasts filter values to all threads efficiently!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_filter_global);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o constant_mem_demo constant_mem_demo.cu\n",
    "!./constant_mem_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109d209",
   "metadata": {},
   "source": [
    "### Example: Convolution with Constant Memory\n",
    "\n",
    "Image convolution is the **classic** use case for constant memory because all threads apply the **same filter kernel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a60eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 Sobel edge detection filter\n",
    "SOBEL_X = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(\"Sobel X filter (edge detection):\")\n",
    "print(SOBEL_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9dfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Passing filter as regular global memory\n",
    "@cuda.jit\n",
    "def convolution_global(image, filter_kernel, output):\n",
    "    \"\"\"Convolution using global memory for filter.\"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    height, width = image.shape\n",
    "    \n",
    "    if row > 0 and row < height - 1 and col > 0 and col < width - 1:\n",
    "        result = 0.0\n",
    "        for i in range(-1, 2):\n",
    "            for j in range(-1, 2):\n",
    "                # Each read of filter_kernel goes to global memory\n",
    "                result += image[row + i, col + j] * filter_kernel[i + 1, j + 1]\n",
    "        output[row, col] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f050319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Numba, we simulate constant memory behavior by:\n",
    "# 1. Using a closure to capture the filter at compile time\n",
    "# 2. Or placing filter in shared memory (next best thing)\n",
    "\n",
    "# Version 2: Filter in shared memory (simulating constant memory)\n",
    "@cuda.jit\n",
    "def convolution_shared_filter(image, filter_kernel, output):\n",
    "    \"\"\"Convolution with filter loaded to shared memory.\"\"\"\n",
    "    # Load filter to shared memory (done once per block)\n",
    "    shared_filter = cuda.shared.array((3, 3), dtype=np.float32)\n",
    "    \n",
    "    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y\n",
    "    \n",
    "    # First 9 threads load the filter\n",
    "    linear_tid = ty * cuda.blockDim.x + tx\n",
    "    if linear_tid < 9:\n",
    "        fi, fj = linear_tid // 3, linear_tid % 3\n",
    "        shared_filter[fi, fj] = filter_kernel[fi, fj]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    row, col = cuda.grid(2)\n",
    "    height, width = image.shape\n",
    "    \n",
    "    if row > 0 and row < height - 1 and col > 0 and col < width - 1:\n",
    "        result = 0.0\n",
    "        for i in range(-1, 2):\n",
    "            for j in range(-1, 2):\n",
    "                # Filter reads now from fast shared memory\n",
    "                result += image[row + i, col + j] * shared_filter[i + 1, j + 1]\n",
    "        output[row, col] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both versions\n",
    "SIZE = 2048\n",
    "image = np.random.rand(SIZE, SIZE).astype(np.float32)\n",
    "output = np.zeros_like(image)\n",
    "\n",
    "d_image = cuda.to_device(image)\n",
    "d_filter = cuda.to_device(SOBEL_X)\n",
    "d_output = cuda.to_device(output)\n",
    "\n",
    "threads = (16, 16)\n",
    "blocks = ((SIZE + 15) // 16, (SIZE + 15) // 16)\n",
    "\n",
    "# Warm up\n",
    "convolution_global[blocks, threads](d_image, d_filter, d_output)\n",
    "convolution_shared_filter[blocks, threads](d_image, d_filter, d_output)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Benchmark global memory filter\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    convolution_global[blocks, threads](d_image, d_filter, d_output)\n",
    "cuda.synchronize()\n",
    "global_time = (time.perf_counter() - start) / 100 * 1000\n",
    "\n",
    "# Benchmark shared memory filter\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    convolution_shared_filter[blocks, threads](d_image, d_filter, d_output)\n",
    "cuda.synchronize()\n",
    "shared_time = (time.perf_counter() - start) / 100 * 1000\n",
    "\n",
    "print(f\"Image size: {SIZE}x{SIZE}\")\n",
    "print(f\"Global memory filter: {global_time:.3f} ms\")\n",
    "print(f\"Shared memory filter: {shared_time:.3f} ms\")\n",
    "print(f\"Speedup: {global_time/shared_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65583b03",
   "metadata": {},
   "source": [
    "### Understanding Constant Memory Benefits\n",
    "\n",
    "Even though Numba doesn't have direct constant memory syntax, the concept matters:\n",
    "\n",
    "```\n",
    "Why constant memory is fast for filters:\n",
    "\n",
    "For a 3x3 convolution:\n",
    "- Each thread reads the SAME 9 filter values\n",
    "- 1 warp = 32 threads all reading filter[0,0]\n",
    "- With constant cache: 1 read, broadcast to 32\n",
    "- Without: 32 reads to global memory\n",
    "\n",
    "Result: 32x reduction in filter memory traffic!\n",
    "```\n",
    "\n",
    "In native CUDA C++:\n",
    "```cpp\n",
    "__constant__ float filter[9];  // Declared at file scope\n",
    "\n",
    "// Copied before kernel launch\n",
    "cudaMemcpyToSymbol(filter, host_filter, 9 * sizeof(float));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aaed1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Texture Memory\n",
    "\n",
    "### What is Texture Memory?\n",
    "\n",
    "Texture memory is another **read-only** memory that:\n",
    "- Is optimized for **2D spatial locality**\n",
    "- Has a dedicated **texture cache**\n",
    "- Supports **hardware interpolation** (free bilinear/trilinear)\n",
    "- Supports **automatic boundary handling** (clamp, wrap, mirror)\n",
    "- Originally designed for graphics, but useful for compute\n",
    "\n",
    "### Texture Cache vs L1 Cache\n",
    "\n",
    "```\n",
    "L1 Cache: Optimized for 1D linear access (coalescing)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ [0][1][2][3][4][5][6][7] â†’ cache line  â”‚\n",
    "â”‚ Linear memory layout                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Texture Cache: Optimized for 2D spatial access\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ [0,0][0,1]â”‚[0,2][0,3]                  â”‚\n",
    "â”‚ [1,0][1,1]â”‚[1,2][1,3]  â†’ 2D tiles      â”‚\n",
    "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚\n",
    "â”‚ [2,0][2,1]â”‚[2,2][2,3]                  â”‚\n",
    "â”‚ [3,0][3,1]â”‚[3,2][3,3]                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### When Texture Memory Excels\n",
    "\n",
    "1. **Image processing** (resizing, rotation, warping)\n",
    "2. **Interpolation** operations\n",
    "3. **Random 2D access patterns**\n",
    "4. **Volume rendering** (3D textures)\n",
    "5. **Data with spatial locality** in 2D/3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36a8f6",
   "metadata": {},
   "source": [
    "### Texture Memory in Modern CUDA\n",
    "\n",
    "Modern GPUs use **texture objects** (introduced in CUDA 5.0). Texture memory provides automatic interpolation and boundary handling.\n",
    "\n",
    "Unfortunately, Numba CUDA doesn't directly support texture objects. For texture-like benefits in Numba:\n",
    "1. Use shared memory tiling for 2D spatial locality\n",
    "2. Implement manual interpolation\n",
    "3. For advanced cases, use CuPy or raw CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile texture_demo.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Texture object reference\n",
    "cudaTextureObject_t texObj;\n",
    "\n",
    "// Kernel using texture memory for bilinear interpolation\n",
    "__global__ void resizeWithTexture(cudaTextureObject_t tex, float* output,\n",
    "                                   int outWidth, int outHeight,\n",
    "                                   int inWidth, int inHeight) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < outWidth && y < outHeight) {\n",
    "        // Map output coords to input coords (normalized)\n",
    "        float u = (float)x / (outWidth - 1);\n",
    "        float v = (float)y / (outHeight - 1);\n",
    "        \n",
    "        // tex2D with linear filtering does bilinear interpolation for free!\n",
    "        float value = tex2D<float>(tex, u * inWidth, v * inHeight);\n",
    "        output[y * outWidth + x] = value;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel without texture (manual bilinear interpolation)\n",
    "__global__ void resizeManual(float* input, float* output,\n",
    "                              int outWidth, int outHeight,\n",
    "                              int inWidth, int inHeight) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < outWidth && y < outHeight) {\n",
    "        // Map output coords to input coords\n",
    "        float srcX = (float)x * (inWidth - 1) / (outWidth - 1);\n",
    "        float srcY = (float)y * (inHeight - 1) / (outHeight - 1);\n",
    "        \n",
    "        // Manual bilinear interpolation\n",
    "        int x0 = (int)srcX;\n",
    "        int y0 = (int)srcY;\n",
    "        int x1 = min(x0 + 1, inWidth - 1);\n",
    "        int y1 = min(y0 + 1, inHeight - 1);\n",
    "        \n",
    "        float fx = srcX - x0;\n",
    "        float fy = srcY - y0;\n",
    "        \n",
    "        float v00 = input[y0 * inWidth + x0];\n",
    "        float v01 = input[y0 * inWidth + x1];\n",
    "        float v10 = input[y1 * inWidth + x0];\n",
    "        float v11 = input[y1 * inWidth + x1];\n",
    "        \n",
    "        float v0 = v00 * (1 - fx) + v01 * fx;\n",
    "        float v1 = v10 * (1 - fx) + v11 * fx;\n",
    "        \n",
    "        output[y * outWidth + x] = v0 * (1 - fy) + v1 * fy;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int inWidth = 256, inHeight = 256;\n",
    "    int outWidth = 512, outHeight = 512;\n",
    "    \n",
    "    // Allocate and initialize input\n",
    "    float* h_input = (float*)malloc(inWidth * inHeight * sizeof(float));\n",
    "    for (int i = 0; i < inHeight; i++) {\n",
    "        for (int j = 0; j < inWidth; j++) {\n",
    "            h_input[i * inWidth + j] = (float)(i + j) / (inWidth + inHeight);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_output_tex, *d_output_manual;\n",
    "    cudaMalloc(&d_input, inWidth * inHeight * sizeof(float));\n",
    "    cudaMalloc(&d_output_tex, outWidth * outHeight * sizeof(float));\n",
    "    cudaMalloc(&d_output_manual, outWidth * outHeight * sizeof(float));\n",
    "    cudaMemcpy(d_input, h_input, inWidth * inHeight * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Create CUDA array for texture\n",
    "    cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc<float>();\n",
    "    cudaArray* cuArray;\n",
    "    cudaMallocArray(&cuArray, &channelDesc, inWidth, inHeight);\n",
    "    cudaMemcpy2DToArray(cuArray, 0, 0, h_input, inWidth * sizeof(float),\n",
    "                        inWidth * sizeof(float), inHeight, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Create texture object\n",
    "    cudaResourceDesc resDesc = {};\n",
    "    resDesc.resType = cudaResourceTypeArray;\n",
    "    resDesc.res.array.array = cuArray;\n",
    "    \n",
    "    cudaTextureDesc texDesc = {};\n",
    "    texDesc.addressMode[0] = cudaAddressModeClamp;\n",
    "    texDesc.addressMode[1] = cudaAddressModeClamp;\n",
    "    texDesc.filterMode = cudaFilterModeLinear;  // Bilinear interpolation!\n",
    "    texDesc.normalizedCoords = false;\n",
    "    \n",
    "    cudaTextureObject_t texObj;\n",
    "    cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);\n",
    "    \n",
    "    printf(\"=== Texture Memory Demonstration ===\\n\");\n",
    "    printf(\"Resizing %dx%d -> %dx%d with bilinear interpolation\\n\\n\", \n",
    "           inWidth, inHeight, outWidth, outHeight);\n",
    "    \n",
    "    dim3 threads(16, 16);\n",
    "    dim3 blocks((outWidth + 15) / 16, (outHeight + 15) / 16);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark texture version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 1000; i++) {\n",
    "        resizeWithTexture<<<blocks, threads>>>(texObj, d_output_tex, \n",
    "                                                outWidth, outHeight, inWidth, inHeight);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float tex_time;\n",
    "    cudaEventElapsedTime(&tex_time, start, stop);\n",
    "    \n",
    "    // Benchmark manual version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 1000; i++) {\n",
    "        resizeManual<<<blocks, threads>>>(d_input, d_output_manual,\n",
    "                                          outWidth, outHeight, inWidth, inHeight);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float manual_time;\n",
    "    cudaEventElapsedTime(&manual_time, start, stop);\n",
    "    \n",
    "    printf(\"Texture memory (hw interpolation): %.3f ms\\n\", tex_time);\n",
    "    printf(\"Manual bilinear interpolation:     %.3f ms\\n\", manual_time);\n",
    "    printf(\"Speedup from texture:              %.2fx\\n\", manual_time / tex_time);\n",
    "    printf(\"\\nâœ… Texture memory provides FREE hardware interpolation!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaDestroyTextureObject(texObj);\n",
    "    cudaFreeArray(cuArray);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output_tex);\n",
    "    cudaFree(d_output_manual);\n",
    "    free(h_input);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o texture_demo texture_demo.cu\n",
    "!./texture_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bad322",
   "metadata": {},
   "source": [
    "### Simulating Texture Benefits: Image Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual bilinear interpolation (what texture memory does for free)\n",
    "@cuda.jit(device=True)\n",
    "def bilinear_sample(image, x, y, height, width):\n",
    "    \"\"\"Bilinear interpolation at floating-point coordinates.\"\"\"\n",
    "    # Clamp to valid range\n",
    "    x = max(0.0, min(x, width - 1.001))\n",
    "    y = max(0.0, min(y, height - 1.001))\n",
    "    \n",
    "    # Get integer coordinates\n",
    "    x0 = int(x)\n",
    "    y0 = int(y)\n",
    "    x1 = min(x0 + 1, width - 1)\n",
    "    y1 = min(y0 + 1, height - 1)\n",
    "    \n",
    "    # Fractional parts\n",
    "    fx = x - x0\n",
    "    fy = y - y0\n",
    "    \n",
    "    # Bilinear interpolation\n",
    "    v00 = image[y0, x0]\n",
    "    v01 = image[y0, x1]\n",
    "    v10 = image[y1, x0]\n",
    "    v11 = image[y1, x1]\n",
    "    \n",
    "    v0 = v00 * (1 - fx) + v01 * fx\n",
    "    v1 = v10 * (1 - fx) + v11 * fx\n",
    "    \n",
    "    return v0 * (1 - fy) + v1 * fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24318b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image resize using bilinear interpolation\n",
    "@cuda.jit\n",
    "def resize_bilinear(src, dst):\n",
    "    \"\"\"Resize image using bilinear interpolation.\"\"\"\n",
    "    dst_y, dst_x = cuda.grid(2)\n",
    "    dst_h, dst_w = dst.shape\n",
    "    src_h, src_w = src.shape\n",
    "    \n",
    "    if dst_y < dst_h and dst_x < dst_w:\n",
    "        # Map destination coords to source coords\n",
    "        src_x = dst_x * (src_w - 1) / (dst_w - 1)\n",
    "        src_y = dst_y * (src_h - 1) / (dst_h - 1)\n",
    "        \n",
    "        dst[dst_y, dst_x] = bilinear_sample(src, src_x, src_y, src_h, src_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec42413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image resizing\n",
    "src_size = 256\n",
    "dst_size = 512\n",
    "\n",
    "# Create a simple test pattern\n",
    "src_image = np.zeros((src_size, src_size), dtype=np.float32)\n",
    "for i in range(src_size):\n",
    "    for j in range(src_size):\n",
    "        src_image[i, j] = (i + j) / (2 * src_size)\n",
    "\n",
    "dst_image = np.zeros((dst_size, dst_size), dtype=np.float32)\n",
    "\n",
    "d_src = cuda.to_device(src_image)\n",
    "d_dst = cuda.to_device(dst_image)\n",
    "\n",
    "threads = (16, 16)\n",
    "blocks = ((dst_size + 15) // 16, (dst_size + 15) // 16)\n",
    "\n",
    "resize_bilinear[blocks, threads](d_src, d_dst)\n",
    "result = d_dst.copy_to_host()\n",
    "\n",
    "print(f\"Resized from {src_size}x{src_size} to {dst_size}x{dst_size}\")\n",
    "print(f\"Source range: [{src_image.min():.3f}, {src_image.max():.3f}]\")\n",
    "print(f\"Result range: [{result.min():.3f}, {result.max():.3f}]\")\n",
    "print(\"\\nNote: With texture memory, interpolation would be automatic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f077b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Memory Type Decision Guide\n",
    "\n",
    "### Decision Flowchart\n",
    "\n",
    "```\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚ Need to store data   â”‚\n",
    "                 â”‚   for GPU kernel?    â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â–¼                           â–¼\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Read-only data?    â”‚    â”‚ Read-write data?   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â”‚                          â”‚\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â–¼                 â–¼         â–¼                â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Small â”‚       â”‚ Large  â”‚  â”‚Private â”‚   â”‚  Shared   â”‚\n",
    "â”‚ <64KB â”‚       â”‚data or â”‚  â”‚to each â”‚   â”‚among blockâ”‚\n",
    "â”‚uniformâ”‚       â”‚spatial â”‚  â”‚thread  â”‚   â”‚threads    â”‚\n",
    "â”‚access â”‚       â”‚access  â”‚  â”‚        â”‚   â”‚           â”‚\n",
    "â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚               â”‚           â”‚              â”‚\n",
    "    â–¼               â–¼           â–¼              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚CONSTANTâ”‚   â”‚ TEXTURE  â”‚  â”‚REGISTERâ”‚   â”‚  SHARED   â”‚\n",
    "â”‚ MEMORY â”‚   â”‚  MEMORY  â”‚  â”‚(auto)  â”‚   â”‚  MEMORY   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Default: GLOBAL MEMORY (with coalescing optimizations)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd368d32",
   "metadata": {},
   "source": [
    "### Quick Reference Table\n",
    "\n",
    "| Scenario | Best Memory | Why |\n",
    "|----------|-------------|-----|\n",
    "| Convolution kernel/filter | Constant | Same values read by all threads |\n",
    "| Configuration parameters | Constant | Small, uniform read access |\n",
    "| Lookup table (uniform access) | Constant | Broadcast efficiency |\n",
    "| Image processing (resize, rotate) | Texture | 2D spatial locality + interpolation |\n",
    "| Volume rendering | Texture | 3D spatial locality |\n",
    "| Random 2D reads | Texture | 2D cache optimization |\n",
    "| Thread-local accumulator | Register | Fastest, private to thread |\n",
    "| Block-wide reduction | Shared | Threads need to communicate |\n",
    "| Tiled matrix multiply | Shared | Data reuse within block |\n",
    "| Histogram (atomic updates) | Shared â†’ Global | Reduce atomic contention |\n",
    "| Large arrays with streaming | Global | Only option for large data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e213a15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Practical Example - Optimized Gaussian Blur\n",
    "\n",
    "Let's combine all Week 2 concepts into a complete optimized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15956c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_2d(size, sigma):\n",
    "    \"\"\"Generate 2D Gaussian kernel.\"\"\"\n",
    "    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    return (kernel / kernel.sum()).astype(np.float32)\n",
    "\n",
    "# 5x5 Gaussian kernel\n",
    "KERNEL_SIZE = 5\n",
    "GAUSSIAN = gaussian_kernel_2d(KERNEL_SIZE, 1.0)\n",
    "print(\"5x5 Gaussian kernel:\")\n",
    "print(np.round(GAUSSIAN, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300cecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive implementation: Global memory only\n",
    "@cuda.jit\n",
    "def gaussian_blur_naive(image, kernel, output, ksize):\n",
    "    \"\"\"Naive Gaussian blur - all global memory.\"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    height, width = image.shape\n",
    "    half_k = ksize // 2\n",
    "    \n",
    "    if row >= half_k and row < height - half_k and col >= half_k and col < width - half_k:\n",
    "        result = 0.0\n",
    "        for i in range(-half_k, half_k + 1):\n",
    "            for j in range(-half_k, half_k + 1):\n",
    "                result += image[row + i, col + j] * kernel[i + half_k, j + half_k]\n",
    "        output[row, col] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized implementation: Shared memory tiling + kernel in shared memory\n",
    "TILE_SIZE = 16\n",
    "BLOCK_SIZE = TILE_SIZE + KERNEL_SIZE - 1  # Tile + halo\n",
    "\n",
    "@cuda.jit\n",
    "def gaussian_blur_optimized(image, kernel, output, ksize):\n",
    "    \"\"\"Optimized Gaussian blur with shared memory tiling.\"\"\"\n",
    "    # Shared memory for image tile (with halo) and kernel\n",
    "    shared_tile = cuda.shared.array((BLOCK_SIZE, BLOCK_SIZE), dtype=np.float32)\n",
    "    shared_kernel = cuda.shared.array((5, 5), dtype=np.float32)\n",
    "    \n",
    "    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y\n",
    "    bx, by = cuda.blockIdx.x, cuda.blockIdx.y\n",
    "    height, width = image.shape\n",
    "    half_k = ksize // 2\n",
    "    \n",
    "    # Load kernel to shared memory (first 25 threads)\n",
    "    linear_tid = ty * cuda.blockDim.x + tx\n",
    "    if linear_tid < ksize * ksize:\n",
    "        ki, kj = linear_tid // ksize, linear_tid % ksize\n",
    "        shared_kernel[ki, kj] = kernel[ki, kj]\n",
    "    \n",
    "    # Calculate tile starting position (with halo offset)\n",
    "    tile_start_row = by * TILE_SIZE - half_k\n",
    "    tile_start_col = bx * TILE_SIZE - half_k\n",
    "    \n",
    "    # Load tile with halo into shared memory\n",
    "    # Each thread may need to load multiple elements\n",
    "    for i in range(0, BLOCK_SIZE, TILE_SIZE):\n",
    "        for j in range(0, BLOCK_SIZE, TILE_SIZE):\n",
    "            si = ty + i\n",
    "            sj = tx + j\n",
    "            if si < BLOCK_SIZE and sj < BLOCK_SIZE:\n",
    "                gi = tile_start_row + si\n",
    "                gj = tile_start_col + sj\n",
    "                if 0 <= gi < height and 0 <= gj < width:\n",
    "                    shared_tile[si, sj] = image[gi, gj]\n",
    "                else:\n",
    "                    shared_tile[si, sj] = 0.0\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Compute output\n",
    "    out_row = by * TILE_SIZE + ty\n",
    "    out_col = bx * TILE_SIZE + tx\n",
    "    \n",
    "    if out_row < height and out_col < width:\n",
    "        result = 0.0\n",
    "        for i in range(ksize):\n",
    "            for j in range(ksize):\n",
    "                result += shared_tile[ty + i, tx + j] * shared_kernel[i, j]\n",
    "        output[out_row, out_col] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark comparison\n",
    "SIZE = 2048\n",
    "image = np.random.rand(SIZE, SIZE).astype(np.float32)\n",
    "output = np.zeros_like(image)\n",
    "\n",
    "d_image = cuda.to_device(image)\n",
    "d_kernel = cuda.to_device(GAUSSIAN)\n",
    "d_output = cuda.to_device(output)\n",
    "\n",
    "# Naive version\n",
    "threads_naive = (16, 16)\n",
    "blocks_naive = ((SIZE + 15) // 16, (SIZE + 15) // 16)\n",
    "\n",
    "gaussian_blur_naive[blocks_naive, threads_naive](d_image, d_kernel, d_output, KERNEL_SIZE)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(50):\n",
    "    gaussian_blur_naive[blocks_naive, threads_naive](d_image, d_kernel, d_output, KERNEL_SIZE)\n",
    "cuda.synchronize()\n",
    "naive_time = (time.perf_counter() - start) / 50 * 1000\n",
    "\n",
    "# Optimized version\n",
    "threads_opt = (TILE_SIZE, TILE_SIZE)\n",
    "blocks_opt = ((SIZE + TILE_SIZE - 1) // TILE_SIZE, (SIZE + TILE_SIZE - 1) // TILE_SIZE)\n",
    "\n",
    "gaussian_blur_optimized[blocks_opt, threads_opt](d_image, d_kernel, d_output, KERNEL_SIZE)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(50):\n",
    "    gaussian_blur_optimized[blocks_opt, threads_opt](d_image, d_kernel, d_output, KERNEL_SIZE)\n",
    "cuda.synchronize()\n",
    "optimized_time = (time.perf_counter() - start) / 50 * 1000\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Gaussian Blur Performance ({SIZE}x{SIZE} image)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Naive (global memory):     {naive_time:.3f} ms\")\n",
    "print(f\"Optimized (shared memory): {optimized_time:.3f} ms\")\n",
    "print(f\"Speedup:                   {naive_time/optimized_time:.2f}x\")\n",
    "print(f\"\\nOptimizations applied:\")\n",
    "print(\"  âœ“ Kernel loaded to shared memory (constant-like behavior)\")\n",
    "print(\"  âœ“ Image tile with halo in shared memory\")\n",
    "print(\"  âœ“ Coalesced global memory loads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4181320",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Separable Gaussian Blur\n",
    "\n",
    "A 2D Gaussian is **separable**: it can be computed as two 1D passes (horizontal then vertical). This reduces operations from O(kÂ²) to O(2k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Gaussian kernel\n",
    "def gaussian_kernel_1d(size, sigma):\n",
    "    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n",
    "    kernel = np.exp(-ax**2 / (2 * sigma**2))\n",
    "    return (kernel / kernel.sum()).astype(np.float32)\n",
    "\n",
    "GAUSSIAN_1D = gaussian_kernel_1d(5, 1.0)\n",
    "print(\"1D Gaussian kernel:\", GAUSSIAN_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement horizontal and vertical 1D convolution kernels\n",
    "\n",
    "@cuda.jit\n",
    "def gaussian_blur_horizontal(image, kernel, output, ksize):\n",
    "    \"\"\"Apply 1D Gaussian blur horizontally.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "@cuda.jit\n",
    "def gaussian_blur_vertical(image, kernel, output, ksize):\n",
    "    \"\"\"Apply 1D Gaussian blur vertically.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Compare:\n",
    "# 1. 5x5 2D convolution: 25 multiplications per pixel\n",
    "# 2. Two 1x5 1D convolutions: 10 multiplications per pixel\n",
    "# Expected speedup: ~2.5x from reduced arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04521f5a",
   "metadata": {},
   "source": [
    "### Exercise 2: Lookup Table with Constant Memory Pattern\n",
    "\n",
    "Implement a color mapping kernel where all threads read from the same lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a colormap lookup table (256 entries)\n",
    "# Maps grayscale values to \"heat\" colors\n",
    "def create_heatmap_lut():\n",
    "    lut = np.zeros((256, 3), dtype=np.float32)\n",
    "    for i in range(256):\n",
    "        t = i / 255.0\n",
    "        # Blue -> Cyan -> Green -> Yellow -> Red\n",
    "        if t < 0.25:\n",
    "            lut[i] = [0, t * 4, 1]\n",
    "        elif t < 0.5:\n",
    "            lut[i] = [0, 1, 1 - (t - 0.25) * 4]\n",
    "        elif t < 0.75:\n",
    "            lut[i] = [(t - 0.5) * 4, 1, 0]\n",
    "        else:\n",
    "            lut[i] = [1, 1 - (t - 0.75) * 4, 0]\n",
    "    return lut\n",
    "\n",
    "HEATMAP_LUT = create_heatmap_lut()\n",
    "print(f\"Lookup table shape: {HEATMAP_LUT.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement color mapping with LUT in shared memory\n",
    "\n",
    "@cuda.jit\n",
    "def apply_heatmap(grayscale, lut, output_rgb):\n",
    "    \"\"\"\n",
    "    Apply heatmap colorization using lookup table.\n",
    "    \n",
    "    Args:\n",
    "        grayscale: 2D input (H, W), values 0-255\n",
    "        lut: Lookup table (256, 3)\n",
    "        output_rgb: 3D output (H, W, 3)\n",
    "    \n",
    "    Hint: Load LUT to shared memory for constant-memory-like behavior\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e154ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Week 2 Summary\n",
    "\n",
    "### Key Concepts Mastered\n",
    "\n",
    "| Day | Topic | Key Takeaway |\n",
    "|-----|-------|-------------|\n",
    "| 1 | Memory Coalescing | Adjacent threads â†’ adjacent memory = fast |\n",
    "| 2 | Shared Memory | On-chip memory for data reuse |\n",
    "| 3 | Bank Conflicts | 32 banks, avoid same-bank access |\n",
    "| 4 | Special Memory | Constant (broadcast), Texture (2D spatial) |\n",
    "\n",
    "### Optimization Hierarchy\n",
    "\n",
    "```\n",
    "1. First: Choose right algorithm (parallelizable)\n",
    "2. Then:  Ensure coalesced global memory access\n",
    "3. Then:  Use shared memory for data reuse\n",
    "4. Then:  Avoid bank conflicts (padding)\n",
    "5. Then:  Consider special memory types\n",
    "6. Then:  Fine-tune thread/block configuration\n",
    "```\n",
    "\n",
    "### Memory Selection Quick Guide\n",
    "\n",
    "```\n",
    "Small read-only + uniform access â†’ Constant\n",
    "2D spatial access + interpolation â†’ Texture  \n",
    "Block-local data reuse           â†’ Shared\n",
    "Thread-private temporary         â†’ Register\n",
    "Everything else                  â†’ Global (with coalescing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c66b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "âœ… Completed Week 2: Memory Patterns & Optimization\n",
    "\n",
    "ğŸ“‹ Day 5: Review & complete checkpoint quiz\n",
    "\n",
    "ğŸ“‹ Week 3 Preview: **Synchronization & Atomics**\n",
    "- Thread synchronization primitives\n",
    "- Atomic operations\n",
    "- Warp-level programming\n",
    "- Parallel reduction patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
