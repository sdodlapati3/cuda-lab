{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a066f073",
   "metadata": {},
   "source": [
    "# üöÄ Day 3: Bank Conflicts - The Hidden Performance Killer\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-02/day-3-bank-conflicts.ipynb)\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Remember: CUDA C++ code is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79be61",
   "metadata": {},
   "source": [
    "# Day 3: Bank Conflicts - The Hidden Performance Killer\n",
    "\n",
    "Yesterday you learned how shared memory provides ~100x faster access than global memory. But there's a catch: **bank conflicts** can serialize your access and destroy that speedup!\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how shared memory is organized into banks\n",
    "- Identify patterns that cause bank conflicts\n",
    "- Apply techniques to avoid conflicts (padding, access patterns)\n",
    "- Optimize the matrix transpose to avoid conflicts\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Shared Memory Bank Architecture\n",
    "\n",
    "Shared memory is divided into **32 banks** (one per warp thread). Each bank can serve one address per cycle.\n",
    "\n",
    "```\n",
    "SHARED MEMORY BANKS (32 banks):\n",
    "\n",
    "Address:  0   1   2   3   4   5  ...  30  31  32  33  34  ...\n",
    "Bank:     0   1   2   3   4   5  ...  30  31   0   1   2  ...\n",
    "          ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ\n",
    "          ‚ñº   ‚ñº   ‚ñº   ‚ñº   ‚ñº   ‚ñº       ‚ñº   ‚ñº   ‚ñº   ‚ñº   ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ 0 ‚îÇ 1 ‚îÇ 2 ‚îÇ 3 ‚îÇ 4 ‚îÇ 5 ‚îÇ...‚îÇ30 ‚îÇ31 ‚îÇ 0 ‚îÇ 1 ‚îÇ 2 ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Bank number = (byte_address / 4) % 32    (for 4-byte words)\n",
    "```\n",
    "\n",
    "### Bank Conflict Rules:\n",
    "\n",
    "| Scenario | Result |\n",
    "|----------|--------|\n",
    "| Threads access different banks | Parallel (fast!) |\n",
    "| Threads access same address (broadcast) | Parallel (fast!) |\n",
    "| Threads access different addresses in same bank | **Serialized (slow!)** |\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bank_conflict_demo.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// GOOD: Each thread accesses a different bank (no conflict)\n",
    "__global__ void noBankConflict(float* out) {\n",
    "    __shared__ float smem[256];\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Thread 0‚ÜíBank 0, Thread 1‚ÜíBank 1, ...\n",
    "    smem[tid] = (float)tid;\n",
    "    __syncthreads();\n",
    "    out[tid] = smem[tid];\n",
    "}\n",
    "\n",
    "// BAD: 32-way bank conflict! All threads hit same bank\n",
    "__global__ void bankConflict32Way(float* out) {\n",
    "    __shared__ float smem[8192];\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // All threads access Bank 0: addr 0, 32, 64, 96...\n",
    "    int idx = tid * 32;\n",
    "    smem[idx] = (float)tid;\n",
    "    __syncthreads();\n",
    "    out[tid] = smem[idx];\n",
    "}\n",
    "\n",
    "// 2-way bank conflict: stride of 2\n",
    "__global__ void bankConflict2Way(float* out) {\n",
    "    __shared__ float smem[512];\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Thread 0‚ÜíBank 0, Thread 1‚ÜíBank 2, ..., Thread 16‚ÜíBank 0 (conflict!)\n",
    "    int idx = tid * 2;\n",
    "    smem[idx] = (float)tid;\n",
    "    __syncthreads();\n",
    "    out[tid] = smem[idx];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 256;\n",
    "    float *d_out, *h_out;\n",
    "    \n",
    "    h_out = (float*)malloc(N * sizeof(float));\n",
    "    cudaMalloc(&d_out, N * sizeof(float));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    printf(\"=== Bank Conflict Demonstration ===\\n\\n\");\n",
    "    \n",
    "    // Benchmark no conflict\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 10000; i++) {\n",
    "        noBankConflict<<<1, 256>>>(d_out);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float no_conflict_ms;\n",
    "    cudaEventElapsedTime(&no_conflict_ms, start, stop);\n",
    "    printf(\"No bank conflict (stride=1):     %.3f ms\\n\", no_conflict_ms);\n",
    "    \n",
    "    // Benchmark 2-way conflict\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 10000; i++) {\n",
    "        bankConflict2Way<<<1, 256>>>(d_out);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float conflict_2way_ms;\n",
    "    cudaEventElapsedTime(&conflict_2way_ms, start, stop);\n",
    "    printf(\"2-way bank conflict (stride=2):  %.3f ms (%.1fx slower)\\n\", \n",
    "           conflict_2way_ms, conflict_2way_ms / no_conflict_ms);\n",
    "    \n",
    "    // Benchmark 32-way conflict\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 10000; i++) {\n",
    "        bankConflict32Way<<<1, 256>>>(d_out);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float conflict_32way_ms;\n",
    "    cudaEventElapsedTime(&conflict_32way_ms, start, stop);\n",
    "    printf(\"32-way bank conflict (stride=32): %.3f ms (%.1fx slower)\\n\", \n",
    "           conflict_32way_ms, conflict_32way_ms / no_conflict_ms);\n",
    "    \n",
    "    printf(\"\\nüí° 32-way conflict serializes all 32 threads in a warp!\\n\");\n",
    "    \n",
    "    cudaFree(d_out);\n",
    "    free(h_out);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a6734",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o bank_conflict_demo bank_conflict_demo.cu\n",
    "!./bank_conflict_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587e2fc",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b29b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"GPU:\", cuda.get_current_device().name.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d5065",
   "metadata": {},
   "source": [
    "## 2. Common Bank Conflict Patterns\n",
    "\n",
    "### ‚úÖ No Conflict: Sequential Access\n",
    "```\n",
    "Thread 0 ‚Üí Bank 0 (addr 0)\n",
    "Thread 1 ‚Üí Bank 1 (addr 1)\n",
    "Thread 2 ‚Üí Bank 2 (addr 2)\n",
    "...all different banks, parallel access!\n",
    "```\n",
    "\n",
    "### ‚ùå 2-way Conflict: Stride of 2\n",
    "```\n",
    "Thread 0 ‚Üí Bank 0 (addr 0)\n",
    "Thread 1 ‚Üí Bank 2 (addr 2)\n",
    "Thread 2 ‚Üí Bank 4 (addr 4)\n",
    "...\n",
    "Thread 16 ‚Üí Bank 0 (addr 32)  ‚Üê CONFLICT with Thread 0!\n",
    "```\n",
    "\n",
    "### ‚ùå‚ùå 32-way Conflict: Stride of 32\n",
    "```\n",
    "Thread 0 ‚Üí Bank 0 (addr 0)\n",
    "Thread 1 ‚Üí Bank 0 (addr 32)  ‚Üê All same bank!\n",
    "Thread 2 ‚Üí Bank 0 (addr 64)\n",
    "...32x serialization!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5beb0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bank_access(stride, num_threads=32):\n",
    "    \"\"\"Visualize which bank each thread accesses\"\"\"\n",
    "    banks = [0] * 32  # Count accesses per bank\n",
    "    \n",
    "    print(f\"Stride = {stride}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for tid in range(num_threads):\n",
    "        addr = tid * stride\n",
    "        bank = addr % 32\n",
    "        banks[bank] += 1\n",
    "        if tid < 8:\n",
    "            print(f\"  Thread {tid} ‚Üí Address {addr:3d} ‚Üí Bank {bank:2d}\")\n",
    "    \n",
    "    if num_threads > 8:\n",
    "        print(\"  ...\")\n",
    "    \n",
    "    max_conflicts = max(banks)\n",
    "    print(f\"\\nMax accesses to single bank: {max_conflicts} ({max_conflicts}-way conflict)\")\n",
    "    print(f\"Banks with conflicts: {sum(1 for b in banks if b > 1)}\")\n",
    "    return max_conflicts\n",
    "\n",
    "# Demonstrate different strides\n",
    "print(\"=\" * 60)\n",
    "print(\"BANK CONFLICT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for stride in [1, 2, 8, 16, 32]:\n",
    "    print()\n",
    "    visualize_bank_access(stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc686a",
   "metadata": {},
   "source": [
    "## 3. Measuring Bank Conflict Impact\n",
    "\n",
    "Let's create kernels with different access patterns and measure the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 256\n",
    "ITERATIONS_PER_THREAD = 1000  # Repeat to make timing measurable\n",
    "\n",
    "@cuda.jit\n",
    "def no_conflict_access(output):\n",
    "    \"\"\"Sequential access: No bank conflicts\"\"\"\n",
    "    shared = cuda.shared.array(shape=256, dtype=float32)\n",
    "    tid = cuda.threadIdx.x\n",
    "    \n",
    "    # Initialize\n",
    "    shared[tid] = float(tid)\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Repeated access pattern: stride 1 (no conflict)\n",
    "    total = 0.0\n",
    "    for i in range(ITERATIONS_PER_THREAD):\n",
    "        idx = tid  # Stride 1: Thread 0‚Üí0, Thread 1‚Üí1, ...\n",
    "        total += shared[idx]\n",
    "    \n",
    "    output[cuda.grid(1)] = total\n",
    "\n",
    "@cuda.jit\n",
    "def strided_access(output, stride):\n",
    "    \"\"\"Strided access: May cause bank conflicts\"\"\"\n",
    "    shared = cuda.shared.array(shape=8192, dtype=float32)  # Large enough for any stride\n",
    "    tid = cuda.threadIdx.x\n",
    "    \n",
    "    # Initialize more elements\n",
    "    for i in range(32):\n",
    "        if tid + i * 256 < 8192:\n",
    "            shared[tid + i * 256] = float(tid)\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Repeated access pattern with stride\n",
    "    total = 0.0\n",
    "    for i in range(ITERATIONS_PER_THREAD):\n",
    "        idx = (tid * stride) % 8192\n",
    "        total += shared[idx]\n",
    "    \n",
    "    output[cuda.grid(1)] = total\n",
    "\n",
    "def benchmark_bank_conflicts():\n",
    "    \"\"\"Benchmark different stride patterns\"\"\"\n",
    "    output = cuda.device_array(BLOCK_SIZE, dtype=np.float32)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for stride in [1, 2, 4, 8, 16, 32]:\n",
    "        # Warmup\n",
    "        strided_access[1, BLOCK_SIZE](output, stride)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(100):\n",
    "            strided_access[1, BLOCK_SIZE](output, stride)\n",
    "        cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / 100\n",
    "        \n",
    "        results.append((stride, elapsed * 1000))\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = benchmark_bank_conflicts()\n",
    "\n",
    "print(\"Bank Conflict Performance Impact\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Stride':<10} | {'Time (ms)':<12} | {'Slowdown'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "baseline = results[0][1]\n",
    "for stride, time_ms in results:\n",
    "    slowdown = time_ms / baseline\n",
    "    conflict_type = \"none\" if stride == 1 else f\"{min(32, 32 // math.gcd(32, stride))}-way\"\n",
    "    print(f\"{stride:<10} | {time_ms:<12.4f} | {slowdown:.2f}x ({conflict_type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d32a9b",
   "metadata": {},
   "source": [
    "## 4. Matrix Transpose: The Bank Conflict Problem\n",
    "\n",
    "In yesterday's shared memory transpose, we had a hidden problem:\n",
    "\n",
    "```\n",
    "tile[ty][tx] = input[row][col]  // Write: ty varies, tx varies ‚Üí OK\n",
    "output[...] = tile[tx][ty]      // Read: tx varies, ty fixed ‚Üí CONFLICT!\n",
    "```\n",
    "\n",
    "When we read `tile[tx][ty]` with fixed `ty`, all threads in a warp access the same column. In a 32√ó32 tile, consecutive columns are 32 elements apart = **32-way bank conflict!**\n",
    "\n",
    "### The Fix: Padding\n",
    "\n",
    "Add 1 extra element per row to shift the bank pattern:\n",
    "\n",
    "```\n",
    "Without padding (32 columns):        With padding (33 columns):\n",
    "Row 0: Banks 0,1,2,...,31            Row 0: Banks 0,1,2,...,31,0\n",
    "Row 1: Banks 0,1,2,...,31            Row 1: Banks 1,2,3,...,0,1\n",
    "Row 2: Banks 0,1,2,...,31            Row 2: Banks 2,3,4,...,1,2\n",
    "                                     \n",
    "Column access: All Bank 0! ‚ùå        Column access: Banks 0,1,2...! ‚úÖ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_DIM = 32\n",
    "\n",
    "@cuda.jit\n",
    "def transpose_shared_conflict(input_mat, output_mat):\n",
    "    \"\"\"\n",
    "    Shared memory transpose WITH bank conflicts.\n",
    "    tile[32][32] - column access causes 32-way conflicts!\n",
    "    \"\"\"\n",
    "    tile = cuda.shared.array(shape=(TILE_DIM, TILE_DIM), dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    row = cuda.blockIdx.y * TILE_DIM + ty\n",
    "    col = cuda.blockIdx.x * TILE_DIM + tx\n",
    "    \n",
    "    rows, cols = input_mat.shape\n",
    "    \n",
    "    if row < rows and col < cols:\n",
    "        tile[ty, tx] = input_mat[row, col]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    out_row = cuda.blockIdx.x * TILE_DIM + ty\n",
    "    out_col = cuda.blockIdx.y * TILE_DIM + tx\n",
    "    \n",
    "    if out_row < cols and out_col < rows:\n",
    "        # Reading tile[tx, ty] with tx varying = column access = CONFLICT!\n",
    "        output_mat[out_row, out_col] = tile[tx, ty]\n",
    "\n",
    "@cuda.jit\n",
    "def transpose_shared_padded(input_mat, output_mat):\n",
    "    \"\"\"\n",
    "    Shared memory transpose WITHOUT bank conflicts.\n",
    "    tile[32][33] - padding eliminates conflicts!\n",
    "    \"\"\"\n",
    "    # Add 1 to column dimension to avoid bank conflicts\n",
    "    tile = cuda.shared.array(shape=(TILE_DIM, TILE_DIM + 1), dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    row = cuda.blockIdx.y * TILE_DIM + ty\n",
    "    col = cuda.blockIdx.x * TILE_DIM + tx\n",
    "    \n",
    "    rows, cols = input_mat.shape\n",
    "    \n",
    "    if row < rows and col < cols:\n",
    "        tile[ty, tx] = input_mat[row, col]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    out_row = cuda.blockIdx.x * TILE_DIM + ty\n",
    "    out_col = cuda.blockIdx.y * TILE_DIM + tx\n",
    "    \n",
    "    if out_row < cols and out_col < rows:\n",
    "        # Now column access goes through different banks!\n",
    "        output_mat[out_row, out_col] = tile[tx, ty]\n",
    "\n",
    "def benchmark_transpose_conflicts(size):\n",
    "    \"\"\"Compare transpose with and without bank conflict avoidance\"\"\"\n",
    "    input_mat = cuda.to_device(np.random.randn(size, size).astype(np.float32))\n",
    "    output_conflict = cuda.device_array((size, size), dtype=np.float32)\n",
    "    output_padded = cuda.device_array((size, size), dtype=np.float32)\n",
    "    \n",
    "    threads = (TILE_DIM, TILE_DIM)\n",
    "    blocks = (math.ceil(size / TILE_DIM), math.ceil(size / TILE_DIM))\n",
    "    \n",
    "    iterations = 100\n",
    "    \n",
    "    # Warmup\n",
    "    transpose_shared_conflict[blocks, threads](input_mat, output_conflict)\n",
    "    transpose_shared_padded[blocks, threads](input_mat, output_padded)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark with conflicts\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        transpose_shared_conflict[blocks, threads](input_mat, output_conflict)\n",
    "    cuda.synchronize()\n",
    "    conflict_time = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Benchmark without conflicts (padded)\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        transpose_shared_padded[blocks, threads](input_mat, output_padded)\n",
    "    cuda.synchronize()\n",
    "    padded_time = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Verify correctness\n",
    "    input_host = input_mat.copy_to_host()\n",
    "    conflict_result = output_conflict.copy_to_host()\n",
    "    padded_result = output_padded.copy_to_host()\n",
    "    \n",
    "    correct = (np.allclose(conflict_result, input_host.T) and \n",
    "               np.allclose(padded_result, input_host.T))\n",
    "    \n",
    "    bytes_moved = 2 * size * size * 4\n",
    "    conflict_bw = bytes_moved / conflict_time / 1e9\n",
    "    padded_bw = bytes_moved / padded_time / 1e9\n",
    "    \n",
    "    return conflict_time, padded_time, conflict_bw, padded_bw, correct\n",
    "\n",
    "# Benchmark\n",
    "size = 4096\n",
    "conflict_t, padded_t, conflict_bw, padded_bw, correct = benchmark_transpose_conflicts(size)\n",
    "\n",
    "print(f\"Matrix Transpose: {size} √ó {size}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"With bank conflicts:    {conflict_t*1000:.3f} ms  ({conflict_bw:.1f} GB/s)\")\n",
    "print(f\"Padded (no conflicts):  {padded_t*1000:.3f} ms  ({padded_bw:.1f} GB/s)\")\n",
    "print(f\"Speedup from padding:   {conflict_t/padded_t:.2f}x\")\n",
    "print(f\"Results correct: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6309640",
   "metadata": {},
   "source": [
    "## 5. Bank Conflict Avoidance Techniques\n",
    "\n",
    "### Technique 1: Padding\n",
    "```python\n",
    "# Instead of:\n",
    "shared = cuda.shared.array((32, 32), float32)  # Conflicts!\n",
    "\n",
    "# Use:\n",
    "shared = cuda.shared.array((32, 33), float32)  # No conflicts\n",
    "```\n",
    "\n",
    "### Technique 2: Change Access Pattern\n",
    "```python\n",
    "# Instead of column access:\n",
    "val = shared[col, row]  # Conflicts if row is fixed\n",
    "\n",
    "# Restructure to row access:\n",
    "val = shared[row, col]  # No conflicts if row varies\n",
    "```\n",
    "\n",
    "### Technique 3: Broadcast (same address = OK)\n",
    "```python\n",
    "# All threads reading the SAME address is fine:\n",
    "val = shared[0]  # Broadcast, no conflict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c48ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Parallel reduction with bank conflict consideration\n",
    "\n",
    "@cuda.jit\n",
    "def reduce_sum_conflict_free(arr, partial_sums):\n",
    "    \"\"\"\n",
    "    Reduction that avoids bank conflicts by using sequential addressing.\n",
    "    \"\"\"\n",
    "    shared = cuda.shared.array(shape=256, dtype=float32)\n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    \n",
    "    # Load\n",
    "    shared[tid] = arr[gid] if gid < arr.size else 0.0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Reduce with sequential addressing (conflict-free)\n",
    "    stride = 128  # Start with half block size\n",
    "    while stride > 0:\n",
    "        if tid < stride:\n",
    "            # Adjacent threads access adjacent memory = no conflict!\n",
    "            shared[tid] += shared[tid + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "    \n",
    "    # Thread 0 writes result\n",
    "    if tid == 0:\n",
    "        partial_sums[cuda.blockIdx.x] = shared[0]\n",
    "\n",
    "# Test reduction\n",
    "n = 1024\n",
    "arr = np.ones(n, dtype=np.float32)\n",
    "arr_d = cuda.to_device(arr)\n",
    "\n",
    "blocks = n // 256\n",
    "partial_sums = cuda.device_array(blocks, dtype=np.float32)\n",
    "\n",
    "reduce_sum_conflict_free[blocks, 256](arr_d, partial_sums)\n",
    "result = partial_sums.copy_to_host()\n",
    "\n",
    "print(f\"Reduction of {n} ones:\")\n",
    "print(f\"  Partial sums: {result}\")\n",
    "print(f\"  Total sum: {result.sum()} (expected: {n})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69485b2b",
   "metadata": {},
   "source": [
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete the exercises below in CUDA C++. The code demonstrates bank conflict analysis and avoidance techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccdc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bank_conflicts_exercises.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 32\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Analyze Bank Conflicts\n",
    "// For each pattern, calculate the number of bank conflicts\n",
    "// =============================================================================\n",
    "\n",
    "// Pattern A: shared[threadIdx.x] - Stride 1\n",
    "// Answer: No conflict (each thread accesses different bank)\n",
    "\n",
    "// Pattern B: shared[threadIdx.x * 2] - Stride 2  \n",
    "// Answer: 2-way conflict (threads 0,16 access bank 0; threads 1,17 access bank 2, etc.)\n",
    "\n",
    "// Pattern C: shared[threadIdx.x * 33] - Stride 33\n",
    "// Answer: No conflict! (33 % 32 = 1, so effective stride is 1)\n",
    "\n",
    "// Pattern D: tile[32][32], accessing tile[threadIdx.y][threadIdx.x]\n",
    "// Answer: No conflict (within a warp, tx varies 0-31, each accesses different bank)\n",
    "\n",
    "// Pattern E: tile[32][32], accessing tile[threadIdx.x][threadIdx.y]\n",
    "// Answer: 32-way conflict! (stride of 32, all threads access same bank)\n",
    "\n",
    "__global__ void demonstrate_patterns() {\n",
    "    __shared__ float shared[1024];\n",
    "    __shared__ float tile[32][32];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    \n",
    "    // Pattern A: Stride 1 - NO CONFLICT\n",
    "    shared[tx] = 1.0f;\n",
    "    \n",
    "    // Pattern B: Stride 2 - 2-WAY CONFLICT\n",
    "    // shared[tx * 2] = 1.0f;\n",
    "    \n",
    "    // Pattern C: Stride 33 - NO CONFLICT (33 % 32 = 1)\n",
    "    // shared[tx * 33] = 1.0f;\n",
    "    \n",
    "    // Pattern D: Row-major access - NO CONFLICT\n",
    "    tile[ty][tx] = 1.0f;\n",
    "    \n",
    "    // Pattern E: Column-major - 32-WAY CONFLICT\n",
    "    // tile[tx][ty] = 1.0f;\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Fix Bank Conflicts with Padding\n",
    "// The column sum kernel has conflicts - fix it!\n",
    "// =============================================================================\n",
    "\n",
    "// PROBLEM VERSION: Has bank conflicts on column access\n",
    "__global__ void matrix_column_sum_conflict(float* matrix, float* col_sums, \n",
    "                                            int rows, int cols) {\n",
    "    __shared__ float tile[32][32];  // CONFLICTS when accessing columns!\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int col = blockIdx.x * 32 + tx;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    for (int row_offset = 0; row_offset < rows; row_offset += 32) {\n",
    "        int row = row_offset + ty;\n",
    "        \n",
    "        // Load tile (row-major: no conflict)\n",
    "        if (row < rows && col < cols) {\n",
    "            tile[ty][tx] = matrix[row * cols + col];\n",
    "        } else {\n",
    "            tile[ty][tx] = 0.0f;\n",
    "        }\n",
    "        __syncthreads();\n",
    "        \n",
    "        // Sum down columns - each thread reads tile[0..31][tx]\n",
    "        // ty varies within warp, accessing same tx = CONFLICT!\n",
    "        if (tx < cols) {\n",
    "            for (int i = 0; i < 32 && (row_offset + i) < rows; i++) {\n",
    "                sum += tile[i][tx];  // BANK CONFLICT: stride 32\n",
    "            }\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (ty == 0 && col < cols) {\n",
    "        col_sums[col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// FIXED VERSION: Use padding to avoid bank conflicts\n",
    "__global__ void matrix_column_sum_fixed(float* matrix, float* col_sums, \n",
    "                                         int rows, int cols) {\n",
    "    // PADDED: 33 columns instead of 32\n",
    "    __shared__ float tile[32][33];  // +1 padding eliminates conflicts!\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int col = blockIdx.x * 32 + tx;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    for (int row_offset = 0; row_offset < rows; row_offset += 32) {\n",
    "        int row = row_offset + ty;\n",
    "        \n",
    "        // Load tile\n",
    "        if (row < rows && col < cols) {\n",
    "            tile[ty][tx] = matrix[row * cols + col];\n",
    "        } else {\n",
    "            tile[ty][tx] = 0.0f;\n",
    "        }\n",
    "        __syncthreads();\n",
    "        \n",
    "        // Sum down columns - now stride is 33, which gives stride 1 (33%32=1)\n",
    "        if (ty == 0 && col < cols) {\n",
    "            for (int i = 0; i < 32 && (row_offset + i) < rows; i++) {\n",
    "                sum += tile[i][tx];  // NO CONFLICT: stride 33 = stride 1\n",
    "            }\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (ty == 0 && col < cols) {\n",
    "        col_sums[col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Helper function for timing\n",
    "__global__ void warmup() {\n",
    "    // Empty kernel for GPU warmup\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Bank Conflict Exercises ===\\n\\n\");\n",
    "    \n",
    "    // Exercise 1: Print pattern analysis\n",
    "    printf(\"Exercise 1: Bank Conflict Analysis\\n\");\n",
    "    printf(\"==================================\\n\");\n",
    "    printf(\"Pattern A: shared[threadIdx.x]\\n\");\n",
    "    printf(\"  ‚Üí Stride = 1, Bank = tid %% 32\\n\");\n",
    "    printf(\"  ‚Üí Answer: NO CONFLICT (each thread, different bank)\\n\\n\");\n",
    "    \n",
    "    printf(\"Pattern B: shared[threadIdx.x * 2]\\n\");\n",
    "    printf(\"  ‚Üí Stride = 2, Bank = (tid * 2) %% 32\\n\");\n",
    "    printf(\"  ‚Üí Answer: 2-WAY CONFLICT (threads 0,16 ‚Üí bank 0)\\n\\n\");\n",
    "    \n",
    "    printf(\"Pattern C: shared[threadIdx.x * 33]\\n\");\n",
    "    printf(\"  ‚Üí Stride = 33, Bank = (tid * 33) %% 32 = tid %% 32\\n\");\n",
    "    printf(\"  ‚Üí Answer: NO CONFLICT (33 %% 32 = 1)\\n\\n\");\n",
    "    \n",
    "    printf(\"Pattern D: tile[32][32], access tile[threadIdx.y][threadIdx.x]\\n\");\n",
    "    printf(\"  ‚Üí Within warp: ty constant, tx varies 0-31\\n\");\n",
    "    printf(\"  ‚Üí Answer: NO CONFLICT (row-major access)\\n\\n\");\n",
    "    \n",
    "    printf(\"Pattern E: tile[32][32], access tile[threadIdx.x][threadIdx.y]\\n\");\n",
    "    printf(\"  ‚Üí Within warp: tx varies, ty constant\\n\");\n",
    "    printf(\"  ‚Üí Address = tx * 32 + ty, stride = 32\\n\");\n",
    "    printf(\"  ‚Üí Answer: 32-WAY CONFLICT (all threads, same bank!)\\n\\n\");\n",
    "    \n",
    "    // Exercise 2: Benchmark conflict vs no-conflict\n",
    "    printf(\"Exercise 2: Column Sum - Conflict vs Fixed\\n\");\n",
    "    printf(\"==========================================\\n\");\n",
    "    \n",
    "    const int rows = 1024;\n",
    "    const int cols = 1024;\n",
    "    \n",
    "    // Allocate memory\n",
    "    float *h_matrix = new float[rows * cols];\n",
    "    float *h_col_sums = new float[cols];\n",
    "    float *d_matrix, *d_col_sums;\n",
    "    \n",
    "    cudaMalloc(&d_matrix, rows * cols * sizeof(float));\n",
    "    cudaMalloc(&d_col_sums, cols * sizeof(float));\n",
    "    \n",
    "    // Initialize matrix\n",
    "    for (int i = 0; i < rows * cols; i++) {\n",
    "        h_matrix[i] = 1.0f;  // Each element is 1, so column sum = rows\n",
    "    }\n",
    "    cudaMemcpy(d_matrix, h_matrix, rows * cols * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 block(32, 32);\n",
    "    dim3 grid((cols + 31) / 32, 1);\n",
    "    \n",
    "    // Warmup\n",
    "    warmup<<<1, 1>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Create events for timing\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Time version with conflicts\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        matrix_column_sum_conflict<<<grid, block>>>(d_matrix, d_col_sums, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms_conflict;\n",
    "    cudaEventElapsedTime(&ms_conflict, start, stop);\n",
    "    \n",
    "    // Verify result\n",
    "    cudaMemcpy(h_col_sums, d_col_sums, cols * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Version with conflicts:\\n\");\n",
    "    printf(\"  Time: %.3f ms (100 iterations)\\n\", ms_conflict);\n",
    "    printf(\"  Result check: col_sums[0] = %.0f (expected: %d)\\n\\n\", h_col_sums[0], rows);\n",
    "    \n",
    "    // Time fixed version\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        matrix_column_sum_fixed<<<grid, block>>>(d_matrix, d_col_sums, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms_fixed;\n",
    "    cudaEventElapsedTime(&ms_fixed, start, stop);\n",
    "    \n",
    "    cudaMemcpy(h_col_sums, d_col_sums, cols * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Version with padding (fixed):\\n\");\n",
    "    printf(\"  Time: %.3f ms (100 iterations)\\n\", ms_fixed);\n",
    "    printf(\"  Result check: col_sums[0] = %.0f (expected: %d)\\n\", h_col_sums[0], rows);\n",
    "    printf(\"  Speedup: %.2fx\\n\\n\", ms_conflict / ms_fixed);\n",
    "    \n",
    "    printf(\"Key Insight: Adding 1 element of padding per row eliminates\\n\");\n",
    "    printf(\"32-way bank conflicts, significantly improving performance!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    delete[] h_matrix;\n",
    "    delete[] h_col_sums;\n",
    "    cudaFree(d_matrix);\n",
    "    cudaFree(d_col_sums);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5db078",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o bank_conflicts_exercises bank_conflicts_exercises.cu && ./bank_conflicts_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb3cfb",
   "metadata": {},
   "source": [
    "### üî∂ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Identify Bank Conflicts\n",
    "\n",
    "For each access pattern, calculate the number of bank conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd474cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Analyze these patterns\n",
    "\n",
    "# Pattern A: shared[threadIdx.x]\n",
    "# ‚Üí Stride = 1, Bank = tid % 32\n",
    "# Answer: ?\n",
    "\n",
    "# Pattern B: shared[threadIdx.x * 2]\n",
    "# ‚Üí Stride = 2, Bank = (tid * 2) % 32\n",
    "# Answer: ?\n",
    "\n",
    "# Pattern C: shared[threadIdx.x * 33]\n",
    "# ‚Üí Stride = 33, Bank = (tid * 33) % 32 = (tid * 1) % 32\n",
    "# Answer: ?\n",
    "\n",
    "# Pattern D: 2D array tile[32][32], accessing tile[threadIdx.y][threadIdx.x]\n",
    "# ‚Üí Within a warp (32 threads), ty is same, tx varies 0-31\n",
    "# Answer: ?\n",
    "\n",
    "# Pattern E: 2D array tile[32][32], accessing tile[threadIdx.x][threadIdx.y]\n",
    "# ‚Üí Within a warp (32 threads), tx varies 0-31, ty is same\n",
    "# ‚Üí Address = tx * 32 + ty, stride of 32\n",
    "# Answer: ?\n",
    "\n",
    "print(\"Analyze each pattern for bank conflicts!\")\n",
    "print(\"Hint: Bank = (element_index * sizeof(element)) / 4 % 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d81504",
   "metadata": {},
   "source": [
    "### Exercise 2: Fix the Bank Conflicts\n",
    "\n",
    "The kernel below has bank conflicts. Fix it using padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Fix the bank conflicts\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_column_sum_conflict(matrix, col_sums, rows, cols):\n",
    "    \"\"\"\n",
    "    Sum each column of a matrix.\n",
    "    PROBLEM: Column access pattern causes bank conflicts!\n",
    "    \"\"\"\n",
    "    # Each block handles one tile of columns\n",
    "    shared = cuda.shared.array(shape=(32, 32), dtype=float32)  # CONFLICTS!\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    col = cuda.blockIdx.x * 32 + tx\n",
    "    \n",
    "    # Load tile\n",
    "    for row_offset in range(0, rows, 32):\n",
    "        row = row_offset + ty\n",
    "        if row < rows and col < cols:\n",
    "            shared[ty, tx] = matrix[row, col]\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Sum down columns (ty varies = conflicts!)\n",
    "        # TODO: This has conflicts - how to fix?\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "\n",
    "@cuda.jit  \n",
    "def matrix_column_sum_fixed(matrix, col_sums, rows, cols):\n",
    "    \"\"\"\n",
    "    TODO: Fix the bank conflicts using padding.\n",
    "    \"\"\"\n",
    "    # TODO: Change shared array dimensions\n",
    "    shared = cuda.shared.array(shape=(32, 32), dtype=float32)  # FIX THIS\n",
    "    \n",
    "    # Rest of the implementation...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b98201",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "### Bank Conflict Rules:\n",
    "\n",
    "1. **32 banks, 4 bytes per bank**\n",
    "   - Bank = (byte_address / 4) % 32\n",
    "\n",
    "2. **Conflict = multiple threads accessing different addresses in same bank**\n",
    "   - N-way conflict ‚Üí N sequential accesses\n",
    "\n",
    "3. **Broadcast is OK**\n",
    "   - All threads reading SAME address = no conflict\n",
    "\n",
    "### Avoidance Techniques:\n",
    "\n",
    "| Technique | When to Use |\n",
    "|-----------|-------------|\n",
    "| **Padding** | 2D arrays with column access |\n",
    "| **Access restructuring** | When you can change the algorithm |\n",
    "| **Sequential addressing** | Reductions, scans |\n",
    "\n",
    "### Quick Check:\n",
    "- Stride 1: No conflict ‚úÖ\n",
    "- Stride 2,4,8,16: 2,4,8,16-way conflict ‚ùå\n",
    "- Stride 32: 32-way conflict ‚ùå‚ùå‚ùå\n",
    "- Stride 33: No conflict! (33 % 32 = 1) ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Up: Day 4 - Special Memory Types\n",
    "- Constant memory for read-only data\n",
    "- Texture memory for spatial locality\n",
    "- When to use each memory type\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Resources\n",
    "- [Device Memory Access](../../cuda-programming-guide/03-advanced/device-memory-access.md)\n",
    "- [Performance Optimization](../../cuda-programming-guide/03-advanced/performance-optimization.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
