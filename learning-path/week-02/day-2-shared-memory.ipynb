{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d6fbcb",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 2: Shared Memory - Your On-Chip Scratchpad\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapati3/cuda-lab/blob/main/learning-path/week-02/day-2-shared-memory.ipynb)\n",
    "\n",
    "## Learning Philosophy\n",
    "\n",
    "> **CUDA C++ First, Python/Numba as Optional Backup**\n",
    "\n",
    "This notebook shows:\n",
    "1. **CUDA C++ code** - The PRIMARY implementation you should learn\n",
    "2. **Python/Numba code** - OPTIONAL for quick interactive testing in Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab/Local Setup - Run this first!\n",
    "# Python/Numba is OPTIONAL - for quick interactive testing only\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"\\nâš ï¸  Remember: CUDA C++ code is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c8897",
   "metadata": {},
   "source": [
    "# Day 2: Shared Memory - Your On-Chip Scratchpad\n",
    "\n",
    "Yesterday you learned that non-coalesced access kills performance. Today's solution: **shared memory**!\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand shared memory architecture\n",
    "- Declare and use shared memory (`__shared__` in CUDA C++)\n",
    "- Master thread synchronization (`__syncthreads()`)\n",
    "- Implement tile-based algorithms\n",
    "- Fix the matrix transpose problem\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is Shared Memory?\n",
    "\n",
    "```\n",
    "GPU Memory Hierarchy:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                Streaming Multiprocessor (SM)   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚    REGISTERS (per thread, ~255 each)     â”‚  â”‚  â† Fastest\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚    SHARED MEMORY (per block, 48-164 KB)  â”‚  â”‚  â† Very fast (~5 cycles)\n",
    "â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚\n",
    "â”‚  â”‚    â”‚ Block 0 threads can share data  â”‚   â”‚  â”‚\n",
    "â”‚  â”‚    â”‚ Thread 0 â†â†’ Thread 1 â†â†’ Thread Nâ”‚   â”‚  â”‚\n",
    "â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â†• (~400 cycles latency)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           GLOBAL MEMORY (4-80 GB)              â”‚  â† Slow but large\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Properties:\n",
    "| Property | Global Memory | Shared Memory |\n",
    "|----------|---------------|---------------|\n",
    "| Size | 4-80 GB | 48-164 KB per SM |\n",
    "| Latency | ~400 cycles | ~5 cycles |\n",
    "| Bandwidth | 200-900 GB/s | ~10 TB/s |\n",
    "| Scope | All threads | Threads in same block |\n",
    "| Lifetime | Until freed | Until block completes |\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile shared_mem_demo.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Shared memory in CUDA C++\n",
    "__global__ void sharedMemoryDemo(float* output, int n) {\n",
    "    // Static allocation (size known at compile time)\n",
    "    __shared__ float tile[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Each thread writes to shared memory\n",
    "    tile[tid] = (float)gid;\n",
    "    \n",
    "    // REQUIRED: Wait for all threads to finish writing\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Now safe to read other threads' data\n",
    "    // Read from reversed position\n",
    "    int reversed_tid = blockDim.x - 1 - tid;\n",
    "    if (gid < n) {\n",
    "        output[gid] = tile[reversed_tid];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Dynamic shared memory example\n",
    "__global__ void dynamicSharedDemo(float* input, float* output, int n) {\n",
    "    // Dynamic allocation (size passed at kernel launch)\n",
    "    extern __shared__ float dynamicTile[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load data to shared memory\n",
    "    if (gid < n) {\n",
    "        dynamicTile[tid] = input[gid];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute using shared memory\n",
    "    if (gid < n && tid > 0 && tid < blockDim.x - 1) {\n",
    "        // 3-point stencil using shared memory\n",
    "        output[gid] = (dynamicTile[tid-1] + dynamicTile[tid] + dynamicTile[tid+1]) / 3.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 256;\n",
    "    const int bytes = N * sizeof(float);\n",
    "    \n",
    "    // Allocate memory\n",
    "    float *h_input = (float*)malloc(bytes);\n",
    "    float *h_output = (float*)malloc(bytes);\n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, bytes);\n",
    "    cudaMalloc(&d_output, bytes);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_input[i] = (float)i;\n",
    "    }\n",
    "    cudaMemcpy(d_input, h_input, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    printf(\"=== Shared Memory Demonstration ===\\n\\n\");\n",
    "    \n",
    "    // Test static shared memory (reverse within block)\n",
    "    sharedMemoryDemo<<<1, 256>>>(d_output, N);\n",
    "    cudaMemcpy(h_output, d_output, bytes, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Static shared memory (block reverse):\\n\");\n",
    "    printf(\"  Input:  [0, 1, 2, 3, ...]\\n\");\n",
    "    printf(\"  Output: [%.0f, %.0f, %.0f, %.0f, ...]\\n\\n\", \n",
    "           h_output[0], h_output[1], h_output[2], h_output[3]);\n",
    "    \n",
    "    // Test dynamic shared memory (3-point stencil)\n",
    "    // Launch with dynamic shared memory size\n",
    "    size_t sharedMemSize = N * sizeof(float);\n",
    "    dynamicSharedDemo<<<1, 256, sharedMemSize>>>(d_input, d_output, N);\n",
    "    cudaMemcpy(h_output, d_output, bytes, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Dynamic shared memory (3-point stencil):\\n\");\n",
    "    printf(\"  Input[10]:  %.1f\\n\", h_input[10]);\n",
    "    printf(\"  Output[10]: %.1f (average of %.1f, %.1f, %.1f)\\n\",\n",
    "           h_output[10], h_input[9], h_input[10], h_input[11]);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    \n",
    "    printf(\"\\nâœ… Shared memory demonstration complete!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098830f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o shared_mem_demo shared_mem_demo.cu\n",
    "!./shared_mem_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318da863",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "print(f\"GPU: {device.name.decode()}\")\n",
    "print(f\"Shared memory per block: {device.MAX_SHARED_MEMORY_PER_BLOCK / 1024:.1f} KB\")\n",
    "print(f\"Max threads per block: {device.MAX_THREADS_PER_BLOCK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b8861",
   "metadata": {},
   "source": [
    "## 2. Declaring Shared Memory in Numba\n",
    "\n",
    "In Numba CUDA, we use `cuda.shared.array()` to allocate shared memory:\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def my_kernel(...):\n",
    "    # Allocate shared memory - size must be constant!\n",
    "    shared_data = cuda.shared.array(shape=256, dtype=numba.float32)\n",
    "    \n",
    "    # All threads in the block can access shared_data\n",
    "    tid = cuda.threadIdx.x\n",
    "    shared_data[tid] = ...  # Write\n",
    "    \n",
    "    cuda.syncthreads()  # Synchronize before reading\n",
    "    \n",
    "    val = shared_data[other_tid]  # Read another thread's data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import float32, int32\n",
    "\n",
    "# Basic shared memory example: Reverse an array within each block\n",
    "@cuda.jit\n",
    "def reverse_block(input_arr, output_arr):\n",
    "    \"\"\"\n",
    "    Each block reverses its portion of the array using shared memory.\n",
    "    \"\"\"\n",
    "    # Allocate shared memory for the block\n",
    "    shared = cuda.shared.array(shape=256, dtype=float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    block_size = cuda.blockDim.x\n",
    "    \n",
    "    # Load data from global memory into shared memory\n",
    "    if gid < input_arr.size:\n",
    "        shared[tid] = input_arr[gid]\n",
    "    \n",
    "    # CRITICAL: Wait for all threads to finish loading\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Read from reversed position in shared memory\n",
    "    if gid < input_arr.size:\n",
    "        reversed_tid = block_size - 1 - tid\n",
    "        output_arr[gid] = shared[reversed_tid]\n",
    "\n",
    "# Test it\n",
    "n = 256\n",
    "input_arr = np.arange(n, dtype=np.float32)\n",
    "output_arr = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "input_d = cuda.to_device(input_arr)\n",
    "output_d = cuda.to_device(output_arr)\n",
    "\n",
    "reverse_block[1, 256](input_d, output_d)\n",
    "result = output_d.copy_to_host()\n",
    "\n",
    "print(\"Input:  \", input_arr[:8], \"...\")\n",
    "print(\"Output: \", result[:8], \"...\")\n",
    "print(\"\\nâœ… Reversed correctly:\", np.array_equal(result, input_arr[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313162ac",
   "metadata": {},
   "source": [
    "## 3. The Importance of `cuda.syncthreads()`\n",
    "\n",
    "Threads in a block execute **concurrently but not simultaneously**. Without synchronization:\n",
    "\n",
    "```\n",
    "WITHOUT syncthreads():          WITH syncthreads():\n",
    "                                \n",
    "Thread 0: write shared[0]       Thread 0: write shared[0]\n",
    "Thread 1: read shared[0] âŒ     â”€â”€â”€â”€â”€ syncthreads() â”€â”€â”€â”€â”€\n",
    "          (might get old value)  Thread 1: read shared[0] âœ…\n",
    "```\n",
    "\n",
    "### Rules:\n",
    "1. **All threads in the block must reach the same `syncthreads()`**\n",
    "2. Call `syncthreads()` after writes, before reads from other threads\n",
    "3. Never put `syncthreads()` inside a conditional that not all threads take!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b843b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the need for syncthreads\n",
    "\n",
    "@cuda.jit\n",
    "def sum_neighbors_wrong(arr, out):\n",
    "    \"\"\"WRONG: No synchronization - race condition!\"\"\"\n",
    "    shared = cuda.shared.array(shape=256, dtype=float32)\n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    \n",
    "    if gid < arr.size:\n",
    "        shared[tid] = arr[gid]\n",
    "    \n",
    "    # BUG: No syncthreads! Neighbors might not have loaded yet!\n",
    "    \n",
    "    if gid < arr.size and tid > 0 and tid < 255:\n",
    "        out[gid] = shared[tid-1] + shared[tid] + shared[tid+1]\n",
    "\n",
    "@cuda.jit\n",
    "def sum_neighbors_correct(arr, out):\n",
    "    \"\"\"CORRECT: Proper synchronization\"\"\"\n",
    "    shared = cuda.shared.array(shape=256, dtype=float32)\n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    \n",
    "    if gid < arr.size:\n",
    "        shared[tid] = arr[gid]\n",
    "    \n",
    "    # Wait for all threads to load their values\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    if gid < arr.size and tid > 0 and tid < 255:\n",
    "        out[gid] = shared[tid-1] + shared[tid] + shared[tid+1]\n",
    "\n",
    "# Test both\n",
    "n = 256\n",
    "arr = np.ones(n, dtype=np.float32)\n",
    "\n",
    "out_wrong = cuda.device_array(n, dtype=np.float32)\n",
    "out_correct = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "arr_d = cuda.to_device(arr)\n",
    "\n",
    "sum_neighbors_correct[1, 256](arr_d, out_correct)\n",
    "result = out_correct.copy_to_host()\n",
    "\n",
    "print(\"Sum of neighbors (should be 3.0 for middle elements):\")\n",
    "print(f\"  Element 10: {result[10]}\")\n",
    "print(f\"  Element 100: {result[100]}\")\n",
    "print(f\"  Element 200: {result[200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec877cca",
   "metadata": {},
   "source": [
    "## 4. Tile-Based Algorithms\n",
    "\n",
    "The most common shared memory pattern: **load a tile, process it, store results**.\n",
    "\n",
    "```\n",
    "TILING PATTERN:\n",
    "\n",
    "Global Memory (slow):       Shared Memory (fast):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚ â”€â”€â†’ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ Tile 0\n",
    "â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ”‚         â†“ process\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â†“ in shared\n",
    "                               â†“ memory\n",
    "1. Load tile from global      \n",
    "2. syncthreads()              \n",
    "3. Process using shared mem   \n",
    "4. syncthreads()              \n",
    "5. Store results to global    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ca6f3",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Tiled Stencil (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0235cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stencil_tiled.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_SIZE 256\n",
    "#define RADIUS 1\n",
    "\n",
    "// Naive: 3 global memory reads per thread\n",
    "__global__ void stencilNaive(const float* input, float* output, int n) {\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (gid > 0 && gid < n - 1) {\n",
    "        output[gid] = (input[gid-1] + input[gid] + input[gid+1]) / 3.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Tiled: Load to shared memory, reuse for stencil computation\n",
    "__global__ void stencilTiled(const float* input, float* output, int n) {\n",
    "    __shared__ float tile[TILE_SIZE + 2 * RADIUS];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load main element\n",
    "    if (gid < n) {\n",
    "        tile[tid + RADIUS] = input[gid];\n",
    "    }\n",
    "    \n",
    "    // Load halo elements\n",
    "    if (tid == 0 && gid > 0) {\n",
    "        tile[0] = input[gid - 1];\n",
    "    }\n",
    "    if (tid == blockDim.x - 1 && gid < n - 1) {\n",
    "        tile[TILE_SIZE + RADIUS] = input[gid + 1];\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute stencil from shared memory\n",
    "    if (gid > 0 && gid < n - 1) {\n",
    "        output[gid] = (tile[tid] + tile[tid + 1] + tile[tid + 2]) / 3.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 10000000;\n",
    "    const size_t bytes = N * sizeof(float);\n",
    "    \n",
    "    float *h_input = (float*)malloc(bytes);\n",
    "    float *h_output = (float*)malloc(bytes);\n",
    "    \n",
    "    for (int i = 0; i < N; i++) h_input[i] = (float)i;\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, bytes);\n",
    "    cudaMalloc(&d_output, bytes);\n",
    "    cudaMemcpy(d_input, h_input, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threads = TILE_SIZE;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    printf(\"=== Stencil: Naive vs Tiled ===\\n\");\n",
    "    printf(\"Array size: %d elements\\n\\n\", N);\n",
    "    \n",
    "    // Benchmark naive\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        stencilNaive<<<blocks, threads>>>(d_input, d_output, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float naive_ms;\n",
    "    cudaEventElapsedTime(&naive_ms, start, stop);\n",
    "    \n",
    "    // Benchmark tiled\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        stencilTiled<<<blocks, threads>>>(d_input, d_output, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float tiled_ms;\n",
    "    cudaEventElapsedTime(&tiled_ms, start, stop);\n",
    "    \n",
    "    printf(\"Naive (global memory): %.3f ms\\n\", naive_ms / 100);\n",
    "    printf(\"Tiled (shared memory): %.3f ms\\n\", tiled_ms / 100);\n",
    "    printf(\"Speedup: %.2fx\\n\", naive_ms / tiled_ms);\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f189cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o stencil_tiled stencil_tiled.cu && ./stencil_tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fecdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Stencil with tiling (3-point stencil: left + center + right)\n",
    "\n",
    "TILE_SIZE = 256\n",
    "STENCIL_RADIUS = 1\n",
    "\n",
    "@cuda.jit\n",
    "def stencil_1d_naive(input_arr, output_arr, n):\n",
    "    \"\"\"Naive stencil: 3 global memory reads per thread\"\"\"\n",
    "    gid = cuda.grid(1)\n",
    "    \n",
    "    if gid > 0 and gid < n - 1:\n",
    "        # 3 global memory accesses (slow!)\n",
    "        output_arr[gid] = (input_arr[gid-1] + \n",
    "                          input_arr[gid] + \n",
    "                          input_arr[gid+1]) / 3.0\n",
    "\n",
    "@cuda.jit\n",
    "def stencil_1d_tiled(input_arr, output_arr, n):\n",
    "    \"\"\"Tiled stencil: Load once, reuse from shared memory\"\"\"\n",
    "    # Shared memory includes \"halo\" cells for stencil\n",
    "    shared = cuda.shared.array(shape=TILE_SIZE + 2 * STENCIL_RADIUS, dtype=float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    \n",
    "    # Load main tile element\n",
    "    if gid < n:\n",
    "        shared[tid + STENCIL_RADIUS] = input_arr[gid]\n",
    "    \n",
    "    # Load halo (left and right boundary elements)\n",
    "    if tid == 0 and gid > 0:\n",
    "        shared[0] = input_arr[gid - 1]  # Left halo\n",
    "    if tid == cuda.blockDim.x - 1 and gid < n - 1:\n",
    "        shared[tid + STENCIL_RADIUS + 1] = input_arr[gid + 1]  # Right halo\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Compute stencil from shared memory (fast!)\n",
    "    if gid > 0 and gid < n - 1:\n",
    "        output_arr[gid] = (shared[tid] + \n",
    "                          shared[tid + 1] + \n",
    "                          shared[tid + 2]) / 3.0\n",
    "\n",
    "def benchmark_stencil(n, iterations=100):\n",
    "    \"\"\"Compare naive vs tiled stencil\"\"\"\n",
    "    input_arr = cuda.to_device(np.random.randn(n).astype(np.float32))\n",
    "    output_naive = cuda.device_array(n, dtype=np.float32)\n",
    "    output_tiled = cuda.device_array(n, dtype=np.float32)\n",
    "    \n",
    "    threads = TILE_SIZE\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    # Warmup\n",
    "    stencil_1d_naive[blocks, threads](input_arr, output_naive, n)\n",
    "    stencil_1d_tiled[blocks, threads](input_arr, output_tiled, n)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark naive\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        stencil_1d_naive[blocks, threads](input_arr, output_naive, n)\n",
    "    cuda.synchronize()\n",
    "    naive_time = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Benchmark tiled\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        stencil_1d_tiled[blocks, threads](input_arr, output_tiled, n)\n",
    "    cuda.synchronize()\n",
    "    tiled_time = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Verify correctness\n",
    "    naive_result = output_naive.copy_to_host()\n",
    "    tiled_result = output_tiled.copy_to_host()\n",
    "    correct = np.allclose(naive_result[1:-1], tiled_result[1:-1])\n",
    "    \n",
    "    return naive_time, tiled_time, correct\n",
    "\n",
    "n = 10_000_000\n",
    "naive_time, tiled_time, correct = benchmark_stencil(n)\n",
    "\n",
    "print(f\"1D Stencil Benchmark (N = {n:,})\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Naive (global mem):  {naive_time*1000:.3f} ms\")\n",
    "print(f\"Tiled (shared mem):  {tiled_time*1000:.3f} ms\")\n",
    "print(f\"Speedup:             {naive_time/tiled_time:.2f}x\")\n",
    "print(f\"Results match:       {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f4924",
   "metadata": {},
   "source": [
    "## 5. Fixing Matrix Transpose with Shared Memory\n",
    "\n",
    "Yesterday's problem: You can't have both coalesced reads AND coalesced writes.\n",
    "\n",
    "**Solution:** Use shared memory as an intermediate buffer!\n",
    "\n",
    "```\n",
    "1. Coalesced read from input â†’ shared memory (tile)\n",
    "2. Transpose the tile in shared memory\n",
    "3. Coalesced write from shared memory â†’ output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61364af5",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Matrix Transpose with Shared Memory (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transpose_shared.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_DIM 32\n",
    "\n",
    "// Naive: coalesced reads, non-coalesced writes\n",
    "__global__ void transposeNaive(const float* input, float* output, int rows, int cols) {\n",
    "    int col = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int row = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    if (row < rows && col < cols) {\n",
    "        output[col * rows + row] = input[row * cols + col];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Shared memory: BOTH reads and writes are coalesced!\n",
    "__global__ void transposeShared(const float* input, float* output, int rows, int cols) {\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    \n",
    "    // Global indices for reading\n",
    "    int row = blockIdx.y * TILE_DIM + ty;\n",
    "    int col = blockIdx.x * TILE_DIM + tx;\n",
    "    \n",
    "    // Step 1: Coalesced read from global to shared\n",
    "    if (row < rows && col < cols) {\n",
    "        tile[ty][tx] = input[row * cols + col];\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Step 2: Compute transposed output position\n",
    "    int out_row = blockIdx.x * TILE_DIM + ty;\n",
    "    int out_col = blockIdx.y * TILE_DIM + tx;\n",
    "    \n",
    "    // Step 3: Coalesced write from shared to global\n",
    "    if (out_row < cols && out_col < rows) {\n",
    "        output[out_row * rows + out_col] = tile[tx][ty];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int ROWS = 4096;\n",
    "    const int COLS = 4096;\n",
    "    const size_t bytes = ROWS * COLS * sizeof(float);\n",
    "    \n",
    "    float *h_input = (float*)malloc(bytes);\n",
    "    float *h_output = (float*)malloc(bytes);\n",
    "    for (int i = 0; i < ROWS * COLS; i++) h_input[i] = (float)i;\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, bytes);\n",
    "    cudaMalloc(&d_output, bytes);\n",
    "    cudaMemcpy(d_input, h_input, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 threads(TILE_DIM, TILE_DIM);\n",
    "    dim3 blocks((COLS + TILE_DIM - 1) / TILE_DIM, (ROWS + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    printf(\"=== Matrix Transpose: Naive vs Shared Memory ===\\n\");\n",
    "    printf(\"Matrix: %d x %d\\n\\n\", ROWS, COLS);\n",
    "    \n",
    "    // Benchmark naive\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        transposeNaive<<<blocks, threads>>>(d_input, d_output, ROWS, COLS);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float naive_ms;\n",
    "    cudaEventElapsedTime(&naive_ms, start, stop);\n",
    "    float naive_bw = (2 * bytes * 100) / (naive_ms / 1000) / 1e9;\n",
    "    \n",
    "    // Benchmark shared\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        transposeShared<<<blocks, threads>>>(d_input, d_output, ROWS, COLS);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float shared_ms;\n",
    "    cudaEventElapsedTime(&shared_ms, start, stop);\n",
    "    float shared_bw = (2 * bytes * 100) / (shared_ms / 1000) / 1e9;\n",
    "    \n",
    "    printf(\"Naive:  %.2f ms (%.1f GB/s)\\n\", naive_ms / 100, naive_bw);\n",
    "    printf(\"Shared: %.2f ms (%.1f GB/s)\\n\", shared_ms / 100, shared_bw);\n",
    "    printf(\"Speedup: %.2fx\\n\", naive_ms / shared_ms);\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb03086",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o transpose_shared transpose_shared.cu && ./transpose_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_DIM = 32  # Must match block size for this simple version\n",
    "\n",
    "@cuda.jit\n",
    "def transpose_naive(input_mat, output_mat):\n",
    "    \"\"\"Naive transpose: coalesced reads, non-coalesced writes\"\"\"\n",
    "    col, row = cuda.grid(2)\n",
    "    rows, cols = input_mat.shape\n",
    "    \n",
    "    if row < rows and col < cols:\n",
    "        output_mat[col, row] = input_mat[row, col]\n",
    "\n",
    "@cuda.jit\n",
    "def transpose_shared(input_mat, output_mat):\n",
    "    \"\"\"\n",
    "    Shared memory transpose: BOTH reads and writes are coalesced!\n",
    "    \"\"\"\n",
    "    # Shared memory tile\n",
    "    tile = cuda.shared.array(shape=(TILE_DIM, TILE_DIM), dtype=float32)\n",
    "    \n",
    "    # Thread indices within block\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    # Global indices for reading (coalesced: threads read along row)\n",
    "    row = cuda.blockIdx.y * TILE_DIM + ty\n",
    "    col = cuda.blockIdx.x * TILE_DIM + tx\n",
    "    \n",
    "    rows, cols = input_mat.shape\n",
    "    \n",
    "    # Step 1: Coalesced read from global to shared\n",
    "    if row < rows and col < cols:\n",
    "        tile[ty, tx] = input_mat[row, col]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Step 2: Compute transposed output position\n",
    "    # Block indices are swapped for output\n",
    "    out_row = cuda.blockIdx.x * TILE_DIM + ty  # Note: blockIdx.x, not y!\n",
    "    out_col = cuda.blockIdx.y * TILE_DIM + tx  # Note: blockIdx.y, not x!\n",
    "    \n",
    "    # Step 3: Coalesced write from shared to global\n",
    "    # Note: we read from tile[tx, ty] (transposed indices!)\n",
    "    if out_row < cols and out_col < rows:\n",
    "        output_mat[out_row, out_col] = tile[tx, ty]\n",
    "\n",
    "def benchmark_transpose(size, iterations=50):\n",
    "    \"\"\"Compare naive vs shared memory transpose\"\"\"\n",
    "    input_mat = cuda.to_device(np.random.randn(size, size).astype(np.float32))\n",
    "    output_naive = cuda.device_array((size, size), dtype=np.float32)\n",
    "    output_shared = cuda.device_array((size, size), dtype=np.float32)\n",
    "    \n",
    "    threads = (TILE_DIM, TILE_DIM)\n",
    "    blocks = (math.ceil(size / TILE_DIM), math.ceil(size / TILE_DIM))\n",
    "    \n",
    "    # Warmup\n",
    "    transpose_naive[blocks, threads](input_mat, output_naive)\n",
    "    transpose_shared[blocks, threads](input_mat, output_shared)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark naive\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        transpose_naive[blocks, threads](input_mat, output_naive)\n",
    "    cuda.synchronize()\n",
    "    naive_time = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Benchmark shared\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        transpose_shared[blocks, threads](input_mat, output_shared)\n",
    "    cuda.synchronize()\n",
    "    shared_time = (time.perf_counter() - start) / iterations\n",
    "    \n",
    "    # Verify\n",
    "    naive_result = output_naive.copy_to_host()\n",
    "    shared_result = output_shared.copy_to_host()\n",
    "    input_host = input_mat.copy_to_host()\n",
    "    \n",
    "    correct_naive = np.allclose(naive_result, input_host.T)\n",
    "    correct_shared = np.allclose(shared_result, input_host.T)\n",
    "    \n",
    "    # Bandwidth\n",
    "    bytes_moved = 2 * size * size * 4  # Read + write\n",
    "    naive_bw = bytes_moved / naive_time / 1e9\n",
    "    shared_bw = bytes_moved / shared_time / 1e9\n",
    "    \n",
    "    return naive_time, shared_time, naive_bw, shared_bw, correct_naive and correct_shared\n",
    "\n",
    "# Run benchmark\n",
    "size = 4096\n",
    "naive_time, shared_time, naive_bw, shared_bw, correct = benchmark_transpose(size)\n",
    "\n",
    "print(f\"Matrix Transpose: {size} Ã— {size}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Naive:          {naive_time*1000:.3f} ms  ({naive_bw:.1f} GB/s)\")\n",
    "print(f\"Shared memory:  {shared_time*1000:.3f} ms  ({shared_bw:.1f} GB/s)\")\n",
    "print(f\"Speedup:        {naive_time/shared_time:.2f}x\")\n",
    "print(f\"Results correct: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff49bf7",
   "metadata": {},
   "source": [
    "## ğŸ¯ Exercises\n",
    "\n",
    "### Exercise 1: Shared Memory Array Shift\n",
    "Shift all elements in an array by 1 position using shared memory.\n",
    "\n",
    "### Exercise 2: 2D Tile Sum\n",
    "Sum all elements in each 16x16 tile of a 2D array.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c593704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile shared_exercises.cu\n",
    "// shared_exercises.cu - Shared memory exercises\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 256\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "// Exercise 1: Array Shift using shared memory\n",
    "// Shift all elements right by 1, with wrap-around\n",
    "__global__ void arrayShift(const float* input, float* output, int n) {\n",
    "    __shared__ float tile[BLOCK_SIZE + 1];  // Extra element for wrap\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load to shared memory\n",
    "    if (gid < n) {\n",
    "        tile[tid + 1] = input[gid];  // Shift right by storing at tid+1\n",
    "    }\n",
    "    \n",
    "    // First thread loads the last element of previous block (or wrap)\n",
    "    if (tid == 0) {\n",
    "        int prevIdx = (gid == 0) ? n - 1 : gid - 1;\n",
    "        tile[0] = input[prevIdx];\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Write shifted output\n",
    "    if (gid < n) {\n",
    "        output[gid] = tile[tid];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Exercise 2: 2D Tile Sum - Sum all elements in each 16x16 tile\n",
    "__global__ void tileSum(const float* input, float* output, int width, int height) {\n",
    "    __shared__ float tile[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    \n",
    "    // Load tile to shared memory\n",
    "    if (row < height && col < width) {\n",
    "        tile[ty][tx] = input[row * width + col];\n",
    "    } else {\n",
    "        tile[ty][tx] = 0.0f;\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Parallel reduction within tile (using thread 0)\n",
    "    if (tx == 0 && ty == 0) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < TILE_SIZE; i++) {\n",
    "            for (int j = 0; j < TILE_SIZE; j++) {\n",
    "                sum += tile[i][j];\n",
    "            }\n",
    "        }\n",
    "        output[blockIdx.y * gridDim.x + blockIdx.x] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Exercise 1: Array Shift ===\\n\");\n",
    "    const int N = 16;\n",
    "    float h_input[N], h_output[N];\n",
    "    \n",
    "    for (int i = 0; i < N; i++) h_input[i] = (float)i;\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, N * sizeof(float));\n",
    "    cudaMalloc(&d_output, N * sizeof(float));\n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    arrayShift<<<1, N>>>(d_input, d_output, N);\n",
    "    cudaMemcpy(h_output, d_output, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Input:  \");\n",
    "    for (int i = 0; i < N; i++) printf(\"%.0f \", h_input[i]);\n",
    "    printf(\"\\nOutput: \");\n",
    "    for (int i = 0; i < N; i++) printf(\"%.0f \", h_output[i]);\n",
    "    printf(\"\\n(Each element shifted right by 1)\\n\\n\");\n",
    "    \n",
    "    printf(\"=== Exercise 2: 2D Tile Sum ===\\n\");\n",
    "    const int W = 32, H = 32;\n",
    "    float h_matrix[W * H];\n",
    "    for (int i = 0; i < W * H; i++) h_matrix[i] = 1.0f;\n",
    "    \n",
    "    int numTilesX = (W + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    int numTilesY = (H + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    float *h_sums = (float*)malloc(numTilesX * numTilesY * sizeof(float));\n",
    "    \n",
    "    float *d_matrix, *d_sums;\n",
    "    cudaMalloc(&d_matrix, W * H * sizeof(float));\n",
    "    cudaMalloc(&d_sums, numTilesX * numTilesY * sizeof(float));\n",
    "    cudaMemcpy(d_matrix, h_matrix, W * H * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 threads(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocks(numTilesX, numTilesY);\n",
    "    tileSum<<<blocks, threads>>>(d_matrix, d_sums, W, H);\n",
    "    cudaMemcpy(h_sums, d_sums, numTilesX * numTilesY * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Tile sums (each 16x16 tile, all 1s):\\n\");\n",
    "    for (int i = 0; i < numTilesX * numTilesY; i++) {\n",
    "        printf(\"Tile %d: %.0f (expected 256)\\n\", i, h_sums[i]);\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_input); cudaFree(d_output);\n",
    "    cudaFree(d_matrix); cudaFree(d_sums);\n",
    "    free(h_sums);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c725f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o shared_exercises shared_exercises.cu && ./shared_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63966f3",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a846066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Array shift with shared memory\n",
    "\n",
    "@cuda.jit\n",
    "def shift_array(input_arr, output_arr, n, shift):\n",
    "    \"\"\"\n",
    "    Shift array elements by 'shift' positions.\n",
    "    output[i] = input[(i + shift) % n]\n",
    "    \n",
    "    Use shared memory to load a tile, then read from shifted positions.\n",
    "    \"\"\"\n",
    "    shared = cuda.shared.array(shape=256, dtype=float32)\n",
    "    tid = cuda.threadIdx.x\n",
    "    gid = cuda.grid(1)\n",
    "    \n",
    "    # TODO: Implement the shift using shared memory\n",
    "    # 1. Load tile into shared memory\n",
    "    # 2. syncthreads()\n",
    "    # 3. Read from shifted position in shared memory\n",
    "    # 4. Write to output\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# n = 256\n",
    "# shift = 5\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8961c07c",
   "metadata": {},
   "source": [
    "### Exercise 2: 2D Tile Sum\n",
    "\n",
    "Compute the sum of each 16Ã—16 tile in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7faf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: 2D Tile Sum\n",
    "\n",
    "TILE_SIZE_2D = 16\n",
    "\n",
    "@cuda.jit\n",
    "def tile_sum(matrix, tile_sums, rows, cols):\n",
    "    \"\"\"\n",
    "    Each block computes the sum of its 16Ã—16 tile.\n",
    "    tile_sums[block_y, block_x] = sum of that tile\n",
    "    \"\"\"\n",
    "    shared = cuda.shared.array(shape=(TILE_SIZE_2D, TILE_SIZE_2D), dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    # TODO: Implement tile sum\n",
    "    # 1. Load tile into shared memory\n",
    "    # 2. syncthreads()\n",
    "    # 3. Perform parallel reduction to sum the tile\n",
    "    # 4. Thread (0,0) writes result to tile_sums\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7922a",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### Shared Memory Essentials:\n",
    "\n",
    "1. **Declaration:**\n",
    "   ```python\n",
    "   shared = cuda.shared.array(shape=SIZE, dtype=float32)\n",
    "   ```\n",
    "\n",
    "2. **Always synchronize:**\n",
    "   ```python\n",
    "   cuda.syncthreads()  # After writes, before reads\n",
    "   ```\n",
    "\n",
    "3. **Tiling pattern:**\n",
    "   - Load tile from global â†’ shared\n",
    "   - `syncthreads()`\n",
    "   - Process in shared memory\n",
    "   - `syncthreads()` (if needed)\n",
    "   - Store to global\n",
    "\n",
    "4. **Use cases:**\n",
    "   - Data reuse (stencils, convolutions)\n",
    "   - Coalescing fix (transpose)\n",
    "   - Thread cooperation (reduction)\n",
    "\n",
    "### Performance Tips:\n",
    "- Shared memory: ~100x faster than global\n",
    "- But limited to 48-164 KB per block\n",
    "- Tomorrow: Bank conflicts can slow it down!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Next Up: Day 3 - Bank Conflicts\n",
    "- How shared memory is organized in banks\n",
    "- What causes bank conflicts\n",
    "- Padding and access pattern tricks\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”— Resources\n",
    "- [CUDA Memory Guide](../../cuda-programming-guide/02-basics/cuda-memory.md)\n",
    "- [Understanding Memory](../../cuda-programming-guide/02-basics/understanding-memory.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
