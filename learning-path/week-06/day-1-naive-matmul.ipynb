{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab901f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7a402",
   "metadata": {},
   "source": [
    "# Day 1: Naive Matrix Multiplication - The Brute Force Approach\n",
    "\n",
    "> *\"Multiply first, optimize later.\"*\n",
    "\n",
    "**The Hook:** Matrix multiplication powers everything from AI training to physics simulations. Today we'll implement the textbook algorithmâ€”and discover exactly why it needs optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. **Map** the matrix multiplication algorithm to GPU threads\n",
    "2. **Implement** a naive matmul kernel with 2D grid/block structure\n",
    "3. **Identify** why global memory access limits performance\n",
    "4. **Measure** the gap between naive and peak GPU performance\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ´ Concept Card: Brute Force Matrix Multiply\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  BRUTE FORCE: Everyone does their own shopping                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸª Imagine 1000 students cooking the same recipe:              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚     Student 1: Goes to store â†’ buys eggs, flour, milk          â”‚\n",
    "â”‚     Student 2: Goes to store â†’ buys eggs, flour, milk          â”‚\n",
    "â”‚     Student 3: Goes to store â†’ buys eggs, flour, milk          â”‚\n",
    "â”‚     ...                                                         â”‚\n",
    "â”‚     Student 1000: Goes to store â†’ buys eggs, flour, milk       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Total trips: 1000 trips for the SAME ingredients!              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  NAIVE MATRIX MULTIPLY:                                         â”‚\n",
    "â”‚  â€¢ Each thread computes one output element C[i,j]               â”‚\n",
    "â”‚  â€¢ Each thread reads entire row A[i,:] and column B[:,j]        â”‚\n",
    "â”‚  â€¢ Threads in same row RE-READ the same row of A                â”‚\n",
    "â”‚  â€¢ Threads in same column RE-READ the same column of B          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  For 1024Ã—1024 matrix:                                          â”‚\n",
    "â”‚  â€¢ Each element of A read ~1024 times                           â”‚\n",
    "â”‚  â€¢ Each element of B read ~1024 times                           â”‚\n",
    "â”‚  â€¢ Total memory traffic: ~2 billion reads for 3 million FLOPs   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ’¡ Tomorrow: We'll organize \"bulk shopping\" with tiling!       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Matrix Multiplication Basics\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "```\n",
    "C = A Ã— B\n",
    "\n",
    "Where:\n",
    "  A is M Ã— K\n",
    "  B is K Ã— N\n",
    "  C is M Ã— N\n",
    "\n",
    "Each element:\n",
    "  C[i,j] = Î£ A[i,k] * B[k,j]  for k = 0 to K-1\n",
    "\n",
    "Example (2Ã—3 @ 3Ã—2 = 2Ã—2):\n",
    "\n",
    "A = [1 2 3]    B = [1 4]    C = [1*1+2*2+3*3  1*4+2*5+3*6] = [14 32]\n",
    "    [4 5 6]        [2 5]        [4*1+5*2+6*3  4*4+5*5+6*6]   [32 77]\n",
    "                   [3 6]\n",
    "```\n",
    "\n",
    "### Parallelization Strategy\n",
    "\n",
    "```\n",
    "Key insight: Each C[i,j] is INDEPENDENT!\n",
    "\n",
    "GPU mapping:\n",
    "â€¢ One thread per output element\n",
    "â€¢ Thread (i, j) computes C[i,j]\n",
    "â€¢ 2D grid/block structure natural fit\n",
    "\n",
    "Grid dimensions:\n",
    "  gridDim.x = ceil(N / blockDim.x)\n",
    "  gridDim.y = ceil(M / blockDim.y)\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d87bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naive_matmul.cu\n",
    "// naive_matmul.cu - Basic matrix multiplication\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "\n",
    "// Naive matrix multiplication: C = A Ã— B\n",
    "// A: MÃ—K, B: KÃ—N, C: MÃ—N\n",
    "__global__ void matmul_naive(const float* A, const float* B, float* C,\n",
    "                              int M, int N, int K) {\n",
    "    // Calculate row and column for this thread\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Bounds check\n",
    "    if (row < M && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        // Dot product of row of A and column of B\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            sum += A[row * K + k] * B[k * N + col];\n",
    "        }\n",
    "        \n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int M = 1024, K = 1024, N = 1024;\n",
    "    size_t size_A = M * K * sizeof(float);\n",
    "    size_t size_B = K * N * sizeof(float);\n",
    "    size_t size_C = M * N * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_A = (float*)malloc(size_A);\n",
    "    float *h_B = (float*)malloc(size_B);\n",
    "    float *h_C = (float*)malloc(size_C);\n",
    "    \n",
    "    // Initialize matrices\n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = rand() / (float)RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = rand() / (float)RAND_MAX;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, size_A);\n",
    "    cudaMalloc(&d_B, size_B);\n",
    "    cudaMalloc(&d_C, size_C);\n",
    "    \n",
    "    // Copy to device\n",
    "    cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch kernel\n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((N + BLOCK_SIZE - 1) / BLOCK_SIZE,\n",
    "              (M + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    // Warmup\n",
    "    matmul_naive<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        matmul_naive<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    ms /= 10;  // Average\n",
    "    \n",
    "    // Calculate GFLOPS\n",
    "    double flops = 2.0 * M * N * K;  // multiply + add\n",
    "    double gflops = flops / (ms * 1e6);\n",
    "    \n",
    "    printf(\"Matrix size: %d x %d x %d\\n\", M, K, N);\n",
    "    printf(\"Time: %.3f ms\\n\", ms);\n",
    "    printf(\"Performance: %.2f GFLOPS\\n\", gflops);\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08533a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o naive_matmul naive_matmul.cu\n",
    "!./naive_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6747bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul_naive(A, B, C, M, N, K):\n",
    "    \"\"\"Naive matrix multiplication: C = A @ B\"\"\"\n",
    "    row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    \n",
    "    if row < M and col < N:\n",
    "        total = 0.0\n",
    "        for k in range(K):\n",
    "            total += A[row, k] * B[k, col]\n",
    "        C[row, col] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test naive matrix multiplication\n",
    "M, K, N = 512, 512, 512\n",
    "\n",
    "A = np.random.rand(M, K).astype(np.float32)\n",
    "B = np.random.rand(K, N).astype(np.float32)\n",
    "C = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)\n",
    "\n",
    "BLOCK_SIZE = 16\n",
    "grid = ((N + BLOCK_SIZE - 1) // BLOCK_SIZE,\n",
    "        (M + BLOCK_SIZE - 1) // BLOCK_SIZE)\n",
    "block = (BLOCK_SIZE, BLOCK_SIZE)\n",
    "\n",
    "matmul_naive[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "result = d_C.copy_to_host()\n",
    "\n",
    "# Verify\n",
    "expected = A @ B\n",
    "print(f\"Matrix sizes: A({M}Ã—{K}) @ B({K}Ã—{N}) = C({M}Ã—{N})\")\n",
    "print(f\"Correct: {'âœ“' if np.allclose(result, expected, rtol=1e-4) else 'âœ—'}\")\n",
    "print(f\"Max error: {np.max(np.abs(result - expected)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d76a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Access Analysis\n",
    "\n",
    "### Access Pattern Visualization\n",
    "\n",
    "```\n",
    "For C[row, col], thread reads:\n",
    "\n",
    "From A:                    From B:\n",
    "Row 'row' of A             Column 'col' of B\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”\n",
    "â”‚             â”‚            â”‚  â”‚  â”‚  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â† row      â”‚  â”‚colâ”‚  â”‚\n",
    "â”‚ * * * * * * â”‚            â”‚  â”‚ â†“â”‚  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚  â”‚ * â”‚  â”‚\n",
    "â”‚             â”‚            â”‚  â”‚ * â”‚  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚ * â”‚  â”‚\n",
    "                           â””â”€â”€â”´â”€â”€â”´â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Memory Coalescing\n",
    "\n",
    "```\n",
    "Adjacent threads in a warp (same row, consecutive cols):\n",
    "\n",
    "Thread 0: C[row, 0]  â†’ reads A[row, 0:K], B[0:K, 0]\n",
    "Thread 1: C[row, 1]  â†’ reads A[row, 0:K], B[0:K, 1]\n",
    "Thread 2: C[row, 2]  â†’ reads A[row, 0:K], B[0:K, 2]\n",
    "...\n",
    "\n",
    "A access: All threads read SAME row â†’ broadcast (efficient)\n",
    "B access: Threads read consecutive columns â†’ COALESCED (efficient)\n",
    "\n",
    "But HUGE redundancy:\n",
    "â€¢ Each row of A read N times (once per column of C)\n",
    "â€¢ Each column of B read M times (once per row of C)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_traffic(M, K, N):\n",
    "    \"\"\"Analyze memory traffic for naive matrix multiply.\"\"\"\n",
    "    \n",
    "    # Each C element reads entire row of A and column of B\n",
    "    reads_per_output = K + K  # K elements from A, K from B\n",
    "    total_reads = M * N * reads_per_output\n",
    "    \n",
    "    # Theoretical minimum (each A, B element read once)\n",
    "    min_reads = M * K + K * N\n",
    "    \n",
    "    # Output writes\n",
    "    writes = M * N\n",
    "    \n",
    "    # Bytes (float32)\n",
    "    bytes_read = total_reads * 4\n",
    "    bytes_min = min_reads * 4\n",
    "    bytes_write = writes * 4\n",
    "    \n",
    "    print(f\"Matrix multiply: ({M}Ã—{K}) @ ({K}Ã—{N}) = ({M}Ã—{N})\")\n",
    "    print(f\"\\nMemory reads:\")\n",
    "    print(f\"  Naive:    {bytes_read / 1e9:.2f} GB\")\n",
    "    print(f\"  Minimum:  {bytes_min / 1e6:.2f} MB\")\n",
    "    print(f\"  Overhead: {bytes_read / bytes_min:.0f}x\")\n",
    "    print(f\"\\nOperations: {2 * M * N * K / 1e9:.2f} GFLOP\")\n",
    "    print(f\"Arithmetic intensity (naive): {2 * M * N * K / bytes_read:.2f} FLOP/byte\")\n",
    "\n",
    "analyze_memory_traffic(1024, 1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02eb53f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul(M, K, N, iterations=20):\n",
    "    \"\"\"Benchmark naive matrix multiplication.\"\"\"\n",
    "    A = np.random.rand(M, K).astype(np.float32)\n",
    "    B = np.random.rand(K, N).astype(np.float32)\n",
    "    C = np.zeros((M, N), dtype=np.float32)\n",
    "    \n",
    "    d_A = cuda.to_device(A)\n",
    "    d_B = cuda.to_device(B)\n",
    "    d_C = cuda.to_device(C)\n",
    "    \n",
    "    BLOCK_SIZE = 16\n",
    "    grid = ((N + BLOCK_SIZE - 1) // BLOCK_SIZE,\n",
    "            (M + BLOCK_SIZE - 1) // BLOCK_SIZE)\n",
    "    block = (BLOCK_SIZE, BLOCK_SIZE)\n",
    "    \n",
    "    # Warmup\n",
    "    matmul_naive[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        matmul_naive[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    elapsed = (time.perf_counter() - start) / iterations * 1000  # ms\n",
    "    \n",
    "    # Calculate GFLOPS\n",
    "    flops = 2 * M * N * K\n",
    "    gflops = flops / (elapsed * 1e6)\n",
    "    \n",
    "    return elapsed, gflops\n",
    "\n",
    "print(f\"{'Size':<15} {'Time (ms)':<12} {'GFLOPS':<12}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for size in [256, 512, 1024, 2048]:\n",
    "    try:\n",
    "        ms, gflops = benchmark_matmul(size, size, size)\n",
    "        print(f\"{size}Ã—{size}Ã—{size:<8} {ms:<12.3f} {gflops:<12.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{size}Ã—{size}Ã—{size:<8} Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3f80c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Why Naive is Slow\n",
    "\n",
    "### The Problem\n",
    "\n",
    "```\n",
    "For 1024Ã—1024 matrices:\n",
    "\n",
    "Operations:     2 Ã— 1024Â³ = 2.1 billion FLOPS\n",
    "Naive reads:    1024Â² Ã— 2K = 2.1 billion reads\n",
    "Arithmetic intensity: 1 FLOP per read = TERRIBLE\n",
    "\n",
    "GPU peak: ~10 TFLOPS, Memory BW: ~500 GB/s\n",
    "Required BW for 10 TFLOPS: 10 TB/s (20x more than available!)\n",
    "\n",
    "Result: Memory-bound at ~10% of peak\n",
    "```\n",
    "\n",
    "### The Solution Preview\n",
    "\n",
    "```\n",
    "TILING with shared memory:\n",
    "\n",
    "1. Load tiles of A and B into shared memory\n",
    "2. Compute partial products using fast shared memory\n",
    "3. Repeat for all tiles\n",
    "\n",
    "With 32Ã—32 tiles:\n",
    "  Each element loaded once per tile, reused 32 times\n",
    "  32x reduction in global memory traffic!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0df0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with NumPy (uses optimized BLAS)\n",
    "def compare_with_numpy(sizes):\n",
    "    print(f\"{'Size':<12} {'Naive (ms)':<12} {'NumPy (ms)':<12} {'Ratio':<10}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for size in sizes:\n",
    "        A = np.random.rand(size, size).astype(np.float32)\n",
    "        B = np.random.rand(size, size).astype(np.float32)\n",
    "        \n",
    "        # NumPy\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(5):\n",
    "            _ = A @ B\n",
    "        numpy_ms = (time.perf_counter() - start) / 5 * 1000\n",
    "        \n",
    "        # CUDA naive\n",
    "        cuda_ms, _ = benchmark_matmul(size, size, size, iterations=5)\n",
    "        \n",
    "        print(f\"{size}Ã—{size:<8} {cuda_ms:<12.3f} {numpy_ms:<12.3f} {cuda_ms/numpy_ms:.1f}x\")\n",
    "\n",
    "compare_with_numpy([256, 512, 1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b005eb3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302936a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matmul_exercises.cu\n",
    "/*\n",
    " * CUDA C++ Matrix Multiplication Exercises\n",
    " * Exercise 1: Rectangular Matrices - Handle MÃ—K Ã— KÃ—N with M != K != N\n",
    " * Exercise 2: Block Size Tuning - Compare 8Ã—8, 16Ã—16, 32Ã—32 block sizes\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <chrono>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "// Exercise 1: Naive matmul that handles rectangular matrices\n",
    "__global__ void matmul_rectangular(const float* A, const float* B, float* C, \n",
    "                                   int M, int N, int K) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            sum += A[row * K + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Helper function to run matmul with specific block size (for Exercise 2)\n",
    "template<int BLOCK_SIZE>\n",
    "float benchmark_block_size(const float* d_A, const float* d_B, float* d_C,\n",
    "                           int M, int N, int K, int iterations) {\n",
    "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 grid((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (M + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "    \n",
    "    // Warmup\n",
    "    matmul_rectangular<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        matmul_rectangular<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    float ms = std::chrono::duration<float, std::milli>(end - start).count() / iterations;\n",
    "    return ms;\n",
    "}\n",
    "\n",
    "void verify_result(float* C, float* C_ref, int size, const char* test_name) {\n",
    "    float max_error = 0.0f;\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        float error = fabs(C[i] - C_ref[i]);\n",
    "        if (error > max_error) max_error = error;\n",
    "    }\n",
    "    printf(\"  %s: Max error = %.6f %s\\n\", test_name, max_error, \n",
    "           max_error < 0.01f ? \"âœ“\" : \"âœ—\");\n",
    "}\n",
    "\n",
    "void cpu_matmul(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    for (int i = 0; i < M; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            float sum = 0.0f;\n",
    "            for (int k = 0; k < K; k++) {\n",
    "                sum += A[i * K + k] * B[k * N + j];\n",
    "            }\n",
    "            C[i * N + j] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Exercise 1: Rectangular Matrices ===\\n\\n\");\n",
    "    \n",
    "    // Test cases with different rectangular dimensions\n",
    "    int test_cases[][3] = {\n",
    "        {128, 256, 192},   // M Ã— K, K Ã— N  â†’ C is M Ã— N\n",
    "        {256, 128, 512},\n",
    "        {512, 64, 256},\n",
    "        {100, 200, 150}    // Non-power-of-2\n",
    "    };\n",
    "    \n",
    "    for (int t = 0; t < 4; t++) {\n",
    "        int M = test_cases[t][0];\n",
    "        int K = test_cases[t][1];\n",
    "        int N = test_cases[t][2];\n",
    "        \n",
    "        printf(\"Test %d: A(%dÃ—%d) Ã— B(%dÃ—%d) = C(%dÃ—%d)\\n\", t+1, M, K, K, N, M, N);\n",
    "        \n",
    "        // Allocate host memory\n",
    "        float *h_A = (float*)malloc(M * K * sizeof(float));\n",
    "        float *h_B = (float*)malloc(K * N * sizeof(float));\n",
    "        float *h_C = (float*)malloc(M * N * sizeof(float));\n",
    "        float *h_C_ref = (float*)malloc(M * N * sizeof(float));\n",
    "        \n",
    "        // Initialize with random data\n",
    "        for (int i = 0; i < M * K; i++) h_A[i] = (float)rand() / RAND_MAX;\n",
    "        for (int i = 0; i < K * N; i++) h_B[i] = (float)rand() / RAND_MAX;\n",
    "        \n",
    "        // CPU reference\n",
    "        cpu_matmul(h_A, h_B, h_C_ref, M, N, K);\n",
    "        \n",
    "        // GPU computation\n",
    "        float *d_A, *d_B, *d_C;\n",
    "        CHECK_CUDA(cudaMalloc(&d_A, M * K * sizeof(float)));\n",
    "        CHECK_CUDA(cudaMalloc(&d_B, K * N * sizeof(float)));\n",
    "        CHECK_CUDA(cudaMalloc(&d_C, M * N * sizeof(float)));\n",
    "        \n",
    "        CHECK_CUDA(cudaMemcpy(d_A, h_A, M * K * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        CHECK_CUDA(cudaMemcpy(d_B, h_B, K * N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "        \n",
    "        dim3 block(16, 16);\n",
    "        dim3 grid((N + 15) / 16, (M + 15) / 16);\n",
    "        matmul_rectangular<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "        \n",
    "        CHECK_CUDA(cudaMemcpy(h_C, d_C, M * N * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        verify_result(h_C, h_C_ref, M * N, \"GPU vs CPU\");\n",
    "        \n",
    "        // Cleanup\n",
    "        cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "        free(h_A); free(h_B); free(h_C); free(h_C_ref);\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n=== Exercise 2: Block Size Tuning ===\\n\\n\");\n",
    "    \n",
    "    int M = 1024, K = 1024, N = 1024;\n",
    "    int iterations = 10;\n",
    "    \n",
    "    // Allocate\n",
    "    float *h_A = (float*)malloc(M * K * sizeof(float));\n",
    "    float *h_B = (float*)malloc(K * N * sizeof(float));\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    \n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = (float)rand() / RAND_MAX;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_A, M * K * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_B, K * N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_C, M * N * sizeof(float)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_A, h_A, M * K * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_B, h_B, K * N * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    printf(\"Matrix size: %d Ã— %d Ã— %d\\n\\n\", M, K, N);\n",
    "    printf(\"%-15s %-15s %-15s\\n\", \"Block Size\", \"Time (ms)\", \"GFLOPS\");\n",
    "    printf(\"%-15s %-15s %-15s\\n\", \"-----------\", \"-----------\", \"-----------\");\n",
    "    \n",
    "    float flops = 2.0f * M * N * K;  // 2 ops per multiply-add\n",
    "    \n",
    "    // Test 8Ã—8 blocks\n",
    "    float time_8 = benchmark_block_size<8>(d_A, d_B, d_C, M, N, K, iterations);\n",
    "    printf(\"%-15s %-15.3f %-15.2f\\n\", \"8 Ã— 8\", time_8, (flops / time_8) / 1e6);\n",
    "    \n",
    "    // Test 16Ã—16 blocks\n",
    "    float time_16 = benchmark_block_size<16>(d_A, d_B, d_C, M, N, K, iterations);\n",
    "    printf(\"%-15s %-15.3f %-15.2f\\n\", \"16 Ã— 16\", time_16, (flops / time_16) / 1e6);\n",
    "    \n",
    "    // Test 32Ã—32 blocks\n",
    "    float time_32 = benchmark_block_size<32>(d_A, d_B, d_C, M, N, K, iterations);\n",
    "    printf(\"%-15s %-15.3f %-15.2f\\n\", \"32 Ã— 32\", time_32, (flops / time_32) / 1e6);\n",
    "    \n",
    "    printf(\"\\nObservations:\\n\");\n",
    "    printf(\"- Larger blocks often perform better due to better cache utilization\\n\");\n",
    "    printf(\"- 32Ã—32 uses 1024 threads/block (maximum for many GPUs)\\n\");\n",
    "    printf(\"- Optimal block size depends on GPU architecture and matrix size\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63827ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o matmul_exercises matmul_exercises.cu && ./matmul_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5dc39",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Rectangular Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test with non-square matrices\n",
    "# Verify that the kernel handles M != K != N correctly\n",
    "\n",
    "test_cases = [\n",
    "    (100, 200, 150),  # M Ã— K Ã— N\n",
    "    (500, 100, 300),\n",
    "    (64, 1024, 64),\n",
    "]\n",
    "\n",
    "for M, K, N in test_cases:\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39305301",
   "metadata": {},
   "source": [
    "### Exercise 2: Block Size Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different block sizes and measure performance\n",
    "# Try: 8Ã—8, 16Ã—16, 32Ã—32\n",
    "\n",
    "block_sizes = [8, 16, 32]\n",
    "\n",
    "# Your benchmarking code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ae842",
   "metadata": {},
   "source": [
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### Performance Reality Check\n",
    "\n",
    "| Metric | Naive Matmul | Peak GPU |\n",
    "|--------|--------------|----------|\n",
    "| Memory reads per element | M + N (massive redundancy) | ~1 (with perfect caching) |\n",
    "| Arithmetic intensity | ~1 FLOP/byte | 100+ FLOP/byte possible |\n",
    "| % of peak performance | 5-10% | 80-90% (cuBLAS) |\n",
    "| Limiting factor | **Memory bandwidth** | Compute throughput |\n",
    "\n",
    "### Naive Matrix Multiplication Summary\n",
    "\n",
    "| Aspect | Value |\n",
    "|--------|-------|\n",
    "| Parallelization | One thread per output element |\n",
    "| Memory access | Huge redundancy (each element read N or M times) |\n",
    "| Arithmetic intensity | ~1 FLOP/byte (very low) |\n",
    "| Performance | ~10% of peak (memory-bound) |\n",
    "\n",
    "### CUDA C++ Key Pattern\n",
    "\n",
    "```cpp\n",
    "__global__ void matmul_naive(float* A, float* B, float* C, int M, int N, int K) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            sum += A[row * K + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸ”— Connection to Tomorrow\n",
    "\n",
    "The brute force approach works but wastes bandwidth. Tomorrow in **Day 2: Tiled Matrix Multiplication**, we'll fix this with shared memoryâ€”like organizing a group shopping trip instead of 1000 individual ones.\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "**Day 2: Tiled Matrix Multiplication** - Divide and conquer with shared memory\n",
    "- Reduce global memory traffic by TILE_SIZE factor\n",
    "- Achieve 2-10x speedup over naive\n",
    "- Master the `__syncthreads()` barrier pattern"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
