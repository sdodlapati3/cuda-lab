{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ea1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88726a4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Tiling Strategy\n",
    "\n",
    "### Why Tiling Works\n",
    "\n",
    "```\n",
    "Key insight: Data REUSE\n",
    "\n",
    "In naive approach:\n",
    "  Thread (i, j) reads A[i, :] and B[:, j]\n",
    "  Thread (i, j+1) also reads A[i, :] â† SAME DATA!\n",
    "  Thread (i+1, j) also reads B[:, j] â† SAME DATA!\n",
    "\n",
    "Solution: Load tiles into SHARED MEMORY\n",
    "  All threads in block share the data\n",
    "  Each element loaded once, used TILE_SIZE times\n",
    "```\n",
    "\n",
    "### Tiling Visualization\n",
    "\n",
    "```\n",
    "Matrix A (MÃ—K)              Matrix B (KÃ—N)\n",
    "â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "â”‚ T â”‚   â”‚   â”‚   â”‚          â”‚ T â”‚   â”‚   â”‚\n",
    "â”‚ i â”‚   â”‚   â”‚   â”‚          â”‚ i â”‚   â”‚   â”‚\n",
    "â”‚ l â”‚   â”‚   â”‚   â”‚          â”‚ l â”‚   â”‚   â”‚\n",
    "â”‚ e â”‚   â”‚   â”‚   â”‚          â”‚ e â”‚   â”‚   â”‚\n",
    "â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤          â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "â”‚   â”‚   â”‚   â”‚   â”‚          â”‚   â”‚   â”‚   â”‚\n",
    "â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "\n",
    "Block (bx, by) computes C[by*T:(by+1)*T, bx*T:(bx+1)*T]\n",
    "\n",
    "For each tile t along K dimension:\n",
    "  1. Load A[by*T:(by+1)*T, t*T:(t+1)*T] â†’ shared As\n",
    "  2. Load B[t*T:(t+1)*T, bx*T:(bx+1)*T] â†’ shared Bs\n",
    "  3. Compute partial products: As Ã— Bs\n",
    "  4. Accumulate to result\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311249f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tiled_matmul.cu\n",
    "// tiled_matmul.cu - Shared memory tiled matrix multiplication\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_SIZE 32\n",
    "\n",
    "// Tiled matrix multiplication: C = A Ã— B\n",
    "__global__ void matmul_tiled(const float* A, const float* B, float* C,\n",
    "                              int M, int N, int K) {\n",
    "    // Shared memory for tiles\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    // Thread's position in output\n",
    "    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    // Loop over tiles along K dimension\n",
    "    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; t++) {\n",
    "        // Load tile of A into shared memory\n",
    "        int a_col = t * TILE_SIZE + threadIdx.x;\n",
    "        if (row < M && a_col < K) {\n",
    "            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n",
    "        } else {\n",
    "            As[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        // Load tile of B into shared memory\n",
    "        int b_row = t * TILE_SIZE + threadIdx.y;\n",
    "        if (b_row < K && col < N) {\n",
    "            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n",
    "        } else {\n",
    "            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        // Wait for all threads to finish loading\n",
    "        __syncthreads();\n",
    "        \n",
    "        // Compute partial products\n",
    "        for (int k = 0; k < TILE_SIZE; k++) {\n",
    "            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        // Wait before loading next tile\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write result\n",
    "    if (row < M && col < N) {\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int M = 1024, K = 1024, N = 1024;\n",
    "    size_t size_A = M * K * sizeof(float);\n",
    "    size_t size_B = K * N * sizeof(float);\n",
    "    size_t size_C = M * N * sizeof(float);\n",
    "    \n",
    "    float *h_A = (float*)malloc(size_A);\n",
    "    float *h_B = (float*)malloc(size_B);\n",
    "    float *h_C = (float*)malloc(size_C);\n",
    "    \n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = rand() / (float)RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = rand() / (float)RAND_MAX;\n",
    "    \n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, size_A);\n",
    "    cudaMalloc(&d_B, size_B);\n",
    "    cudaMalloc(&d_C, size_C);\n",
    "    \n",
    "    cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE,\n",
    "              (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    \n",
    "    // Warmup\n",
    "    matmul_tiled<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        matmul_tiled<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    ms /= 10;\n",
    "    \n",
    "    double flops = 2.0 * M * N * K;\n",
    "    double gflops = flops / (ms * 1e6);\n",
    "    \n",
    "    printf(\"Tiled MatMul (%dx%d): %.3f ms, %.2f GFLOPS\\n\", TILE_SIZE, TILE_SIZE, ms, gflops);\n",
    "    \n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c19c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o tiled_matmul tiled_matmul.cu\n",
    "!./tiled_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c03865",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 16  # Must match block size\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled(A, B, C, M, N, K):\n",
    "    \"\"\"Tiled matrix multiplication using shared memory.\"\"\"\n",
    "    # Shared memory tiles\n",
    "    As = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    Bs = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    \n",
    "    # Thread indices\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    # Global row and column\n",
    "    row = cuda.blockIdx.y * TILE_SIZE + ty\n",
    "    col = cuda.blockIdx.x * TILE_SIZE + tx\n",
    "    \n",
    "    # Accumulator\n",
    "    total = 0.0\n",
    "    \n",
    "    # Number of tiles along K\n",
    "    num_tiles = (K + TILE_SIZE - 1) // TILE_SIZE\n",
    "    \n",
    "    for t in range(num_tiles):\n",
    "        # Load tile of A\n",
    "        a_col = t * TILE_SIZE + tx\n",
    "        if row < M and a_col < K:\n",
    "            As[ty, tx] = A[row, a_col]\n",
    "        else:\n",
    "            As[ty, tx] = 0.0\n",
    "        \n",
    "        # Load tile of B\n",
    "        b_row = t * TILE_SIZE + ty\n",
    "        if b_row < K and col < N:\n",
    "            Bs[ty, tx] = B[b_row, col]\n",
    "        else:\n",
    "            Bs[ty, tx] = 0.0\n",
    "        \n",
    "        # Synchronize\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Compute partial product\n",
    "        for k in range(TILE_SIZE):\n",
    "            total += As[ty, k] * Bs[k, tx]\n",
    "        \n",
    "        # Synchronize before next tile\n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    # Write result\n",
    "    if row < M and col < N:\n",
    "        C[row, col] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tiled matrix multiplication\n",
    "M, K, N = 512, 512, 512\n",
    "\n",
    "A = np.random.rand(M, K).astype(np.float32)\n",
    "B = np.random.rand(K, N).astype(np.float32)\n",
    "C = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)\n",
    "\n",
    "grid = ((N + TILE_SIZE - 1) // TILE_SIZE,\n",
    "        (M + TILE_SIZE - 1) // TILE_SIZE)\n",
    "block = (TILE_SIZE, TILE_SIZE)\n",
    "\n",
    "matmul_tiled[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "result = d_C.copy_to_host()\n",
    "\n",
    "expected = A @ B\n",
    "print(f\"Tiled MatMul: {M}Ã—{K} @ {K}Ã—{N}\")\n",
    "print(f\"Correct: {'âœ“' if np.allclose(result, expected, rtol=1e-4) else 'âœ—'}\")\n",
    "print(f\"Max error: {np.max(np.abs(result - expected)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da533b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Traffic Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tiled_traffic(M, K, N, tile_size):\n",
    "    \"\"\"Analyze memory traffic for tiled matrix multiply.\"\"\"\n",
    "    \n",
    "    # Number of blocks\n",
    "    blocks_m = (M + tile_size - 1) // tile_size\n",
    "    blocks_n = (N + tile_size - 1) // tile_size\n",
    "    blocks_k = (K + tile_size - 1) // tile_size\n",
    "    \n",
    "    # Each block loads tiles along K dimension\n",
    "    # A tile: tile_size Ã— tile_size per block Ã— blocks_k times\n",
    "    # Each A tile loaded by blocks_n column-blocks\n",
    "    # Each B tile loaded by blocks_m row-blocks\n",
    "    \n",
    "    # Total A loads: M/T Ã— K/T tiles Ã— TÂ² elements Ã— N/T blocks\n",
    "    # But wait - each A tile loaded only N/T times, not N times!\n",
    "    \n",
    "    # Tiled loads:\n",
    "    a_loads = blocks_m * blocks_k * blocks_n * tile_size * tile_size\n",
    "    b_loads = blocks_m * blocks_k * blocks_n * tile_size * tile_size\n",
    "    tiled_total = (a_loads + b_loads) * 4  # bytes\n",
    "    \n",
    "    # Actually, each block loads its own tiles:\n",
    "    # Each block: 2 Ã— K/T Ã— TÂ² loads\n",
    "    # Total blocks: (M/T) Ã— (N/T)\n",
    "    actual_tiled = blocks_m * blocks_n * 2 * blocks_k * tile_size * tile_size * 4\n",
    "    \n",
    "    # Naive loads (for comparison)\n",
    "    naive_total = M * N * (K + K) * 4\n",
    "    \n",
    "    # Minimum possible\n",
    "    min_loads = (M * K + K * N) * 4\n",
    "    \n",
    "    print(f\"Matrix multiply: ({M}Ã—{K}) @ ({K}Ã—{N}), Tile size: {tile_size}\")\n",
    "    print(f\"\\nMemory traffic comparison:\")\n",
    "    print(f\"  Naive:    {naive_total / 1e9:.2f} GB\")\n",
    "    print(f\"  Tiled:    {actual_tiled / 1e9:.3f} GB\")\n",
    "    print(f\"  Minimum:  {min_loads / 1e6:.2f} MB\")\n",
    "    print(f\"\\nReduction factor: {naive_total / actual_tiled:.1f}x\")\n",
    "    print(f\"Theoretical max:  {tile_size:.0f}x\")\n",
    "\n",
    "analyze_tiled_traffic(1024, 1024, 1024, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e1597",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae449158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive kernel for comparison\n",
    "@cuda.jit\n",
    "def matmul_naive(A, B, C, M, N, K):\n",
    "    row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    \n",
    "    if row < M and col < N:\n",
    "        total = 0.0\n",
    "        for k in range(K):\n",
    "            total += A[row, k] * B[k, col]\n",
    "        C[row, col] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbedaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_comparison(M, K, N, iterations=20):\n",
    "    \"\"\"Compare naive vs tiled performance.\"\"\"\n",
    "    A = np.random.rand(M, K).astype(np.float32)\n",
    "    B = np.random.rand(K, N).astype(np.float32)\n",
    "    C = np.zeros((M, N), dtype=np.float32)\n",
    "    \n",
    "    d_A = cuda.to_device(A)\n",
    "    d_B = cuda.to_device(B)\n",
    "    d_C = cuda.to_device(C)\n",
    "    \n",
    "    flops = 2 * M * N * K\n",
    "    \n",
    "    # Naive\n",
    "    block_naive = (16, 16)\n",
    "    grid_naive = ((N + 15) // 16, (M + 15) // 16)\n",
    "    \n",
    "    matmul_naive[grid_naive, block_naive](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        matmul_naive[grid_naive, block_naive](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    naive_ms = (time.perf_counter() - start) / iterations * 1000\n",
    "    naive_gflops = flops / (naive_ms * 1e6)\n",
    "    \n",
    "    # Tiled\n",
    "    block_tiled = (TILE_SIZE, TILE_SIZE)\n",
    "    grid_tiled = ((N + TILE_SIZE - 1) // TILE_SIZE, \n",
    "                  (M + TILE_SIZE - 1) // TILE_SIZE)\n",
    "    \n",
    "    matmul_tiled[grid_tiled, block_tiled](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        matmul_tiled[grid_tiled, block_tiled](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    tiled_ms = (time.perf_counter() - start) / iterations * 1000\n",
    "    tiled_gflops = flops / (tiled_ms * 1e6)\n",
    "    \n",
    "    return naive_ms, naive_gflops, tiled_ms, tiled_gflops\n",
    "\n",
    "print(f\"{'Size':<15} {'Naive (ms)':<12} {'Naive GFLOPS':<14} {'Tiled (ms)':<12} {'Tiled GFLOPS':<14} {'Speedup':<10}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "for size in [256, 512, 1024]:\n",
    "    try:\n",
    "        n_ms, n_gf, t_ms, t_gf = benchmark_comparison(size, size, size)\n",
    "        speedup = n_ms / t_ms\n",
    "        print(f\"{size}Ã—{size}Ã—{size:<8} {n_ms:<12.3f} {n_gf:<14.2f} {t_ms:<12.3f} {t_gf:<14.2f} {speedup:.2f}x\")\n",
    "    except Exception as e:\n",
    "        print(f\"{size}Ã—{size}Ã—{size:<8} Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b6ef5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Tile Size Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db62f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tile_sizes():\n",
    "    \"\"\"Analyze trade-offs of different tile sizes.\"\"\"\n",
    "    print(\"Tile Size Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for tile in [8, 16, 32]:\n",
    "        threads = tile * tile\n",
    "        shared_mem = 2 * tile * tile * 4  # Two tiles, float32\n",
    "        reuse = tile  # Each element reused tile times\n",
    "        \n",
    "        # Occupancy considerations\n",
    "        max_blocks_shared = 48 * 1024 // shared_mem  # 48KB typical\n",
    "        max_blocks_threads = 1024 // threads  # 1024 threads/block max\n",
    "        \n",
    "        print(f\"\\nTile {tile}Ã—{tile}:\")\n",
    "        print(f\"  Threads per block: {threads}\")\n",
    "        print(f\"  Shared memory: {shared_mem} bytes ({shared_mem/1024:.1f} KB)\")\n",
    "        print(f\"  Data reuse factor: {reuse}x\")\n",
    "        print(f\"  Max blocks (shared mem limit): {max_blocks_shared}\")\n",
    "        print(f\"  Max blocks (thread limit): {max_blocks_threads}\")\n",
    "\n",
    "analyze_tile_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7f597",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Variable Tile Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87144200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create kernels with different tile sizes and compare\n",
    "# Numba requires compile-time constants, so you'll need separate kernels\n",
    "\n",
    "# Tile size 8\n",
    "@cuda.jit\n",
    "def matmul_tiled_8(A, B, C, M, N, K):\n",
    "    pass  # Your implementation\n",
    "\n",
    "# Tile size 32\n",
    "@cuda.jit\n",
    "def matmul_tiled_32(A, B, C, M, N, K):\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828119a3",
   "metadata": {},
   "source": [
    "### Exercise 2: Rectangular Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40747c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement with non-square tiles (e.g., 32Ã—16)\n",
    "# This can sometimes improve register usage\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled_rect(A, B, C, M, N, K):\n",
    "    \"\"\"Tiled matmul with 32Ã—16 tiles.\"\"\"\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4ffb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Tiling Strategy\n",
    "\n",
    "| Aspect | Improvement |\n",
    "|--------|-------------|\n",
    "| Memory traffic | Reduced by TILE_SIZE factor |\n",
    "| Arithmetic intensity | Increased from 1 to ~TILE_SIZE FLOP/byte |\n",
    "| Performance | 2-10x speedup over naive |\n",
    "\n",
    "### CUDA C++ Key Pattern\n",
    "\n",
    "```cpp\n",
    "__shared__ float As[TILE][TILE], Bs[TILE][TILE];\n",
    "\n",
    "for (int t = 0; t < numTiles; t++) {\n",
    "    // Load tiles with bounds checking\n",
    "    As[ty][tx] = (in_bounds_A) ? A[...] : 0;\n",
    "    Bs[ty][tx] = (in_bounds_B) ? B[...] : 0;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute partial product\n",
    "    for (int k = 0; k < TILE; k++)\n",
    "        sum += As[ty][k] * Bs[k][tx];\n",
    "    __syncthreads();\n",
    "}\n",
    "```\n",
    "\n",
    "### Critical Points\n",
    "\n",
    "1. **Two syncthreads per tile** - One after load, one after compute\n",
    "2. **Bounds checking** - Pad with zeros for edge tiles\n",
    "3. **Tile size** - Balance shared memory, threads, and reuse\n",
    "\n",
    "### Next: Matrix Transpose\n",
    "Tomorrow we'll optimize matrix transpose with coalesced access patterns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
