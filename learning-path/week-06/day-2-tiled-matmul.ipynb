{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ea1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"‚ö†Ô∏è  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88726a4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Tiling Strategy\n",
    "\n",
    "### Why Tiling Works\n",
    "\n",
    "```\n",
    "Key insight: Data REUSE\n",
    "\n",
    "In naive approach:\n",
    "  Thread (i, j) reads A[i, :] and B[:, j]\n",
    "  Thread (i, j+1) also reads A[i, :] ‚Üê SAME DATA!\n",
    "  Thread (i+1, j) also reads B[:, j] ‚Üê SAME DATA!\n",
    "\n",
    "Solution: Load tiles into SHARED MEMORY\n",
    "  All threads in block share the data\n",
    "  Each element loaded once, used TILE_SIZE times\n",
    "```\n",
    "\n",
    "### Tiling Visualization\n",
    "\n",
    "```\n",
    "Matrix A (M√óK)              Matrix B (K√óN)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ T ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ          ‚îÇ T ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ i ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ          ‚îÇ i ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ l ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ          ‚îÇ l ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ e ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ          ‚îÇ e ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§          ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ          ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Block (bx, by) computes C[by*T:(by+1)*T, bx*T:(bx+1)*T]\n",
    "\n",
    "For each tile t along K dimension:\n",
    "  1. Load A[by*T:(by+1)*T, t*T:(t+1)*T] ‚Üí shared As\n",
    "  2. Load B[t*T:(t+1)*T, bx*T:(bx+1)*T] ‚Üí shared Bs\n",
    "  3. Compute partial products: As √ó Bs\n",
    "  4. Accumulate to result\n",
    "```\n",
    "\n",
    "### üî∑ CUDA C++ Implementation (Primary)\n",
    "\n",
    "### üî∂ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311249f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tiled_matmul.cu\n",
    "// tiled_matmul.cu - Shared memory tiled matrix multiplication\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_SIZE 32\n",
    "\n",
    "// Tiled matrix multiplication: C = A √ó B\n",
    "__global__ void matmul_tiled(const float* A, const float* B, float* C,\n",
    "                              int M, int N, int K) {\n",
    "    // Shared memory for tiles\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    // Thread's position in output\n",
    "    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    // Loop over tiles along K dimension\n",
    "    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; t++) {\n",
    "        // Load tile of A into shared memory\n",
    "        int a_col = t * TILE_SIZE + threadIdx.x;\n",
    "        if (row < M && a_col < K) {\n",
    "            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n",
    "        } else {\n",
    "            As[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        // Load tile of B into shared memory\n",
    "        int b_row = t * TILE_SIZE + threadIdx.y;\n",
    "        if (b_row < K && col < N) {\n",
    "            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n",
    "        } else {\n",
    "            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        // Wait for all threads to finish loading\n",
    "        __syncthreads();\n",
    "        \n",
    "        // Compute partial products\n",
    "        for (int k = 0; k < TILE_SIZE; k++) {\n",
    "            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        // Wait before loading next tile\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write result\n",
    "    if (row < M && col < N) {\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int M = 1024, K = 1024, N = 1024;\n",
    "    size_t size_A = M * K * sizeof(float);\n",
    "    size_t size_B = K * N * sizeof(float);\n",
    "    size_t size_C = M * N * sizeof(float);\n",
    "    \n",
    "    float *h_A = (float*)malloc(size_A);\n",
    "    float *h_B = (float*)malloc(size_B);\n",
    "    float *h_C = (float*)malloc(size_C);\n",
    "    \n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = rand() / (float)RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = rand() / (float)RAND_MAX;\n",
    "    \n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, size_A);\n",
    "    cudaMalloc(&d_B, size_B);\n",
    "    cudaMalloc(&d_C, size_C);\n",
    "    \n",
    "    cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE,\n",
    "              (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    \n",
    "    // Warmup\n",
    "    matmul_tiled<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        matmul_tiled<<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    ms /= 10;\n",
    "    \n",
    "    double flops = 2.0 * M * N * K;\n",
    "    double gflops = flops / (ms * 1e6);\n",
    "    \n",
    "    printf(\"Tiled MatMul (%dx%d): %.3f ms, %.2f GFLOPS\\n\", TILE_SIZE, TILE_SIZE, ms, gflops);\n",
    "    \n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c19c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o tiled_matmul tiled_matmul.cu\n",
    "!./tiled_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c03865",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 16  # Must match block size\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled(A, B, C, M, N, K):\n",
    "    \"\"\"Tiled matrix multiplication using shared memory.\"\"\"\n",
    "    # Shared memory tiles\n",
    "    As = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    Bs = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    \n",
    "    # Thread indices\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    # Global row and column\n",
    "    row = cuda.blockIdx.y * TILE_SIZE + ty\n",
    "    col = cuda.blockIdx.x * TILE_SIZE + tx\n",
    "    \n",
    "    # Accumulator\n",
    "    total = 0.0\n",
    "    \n",
    "    # Number of tiles along K\n",
    "    num_tiles = (K + TILE_SIZE - 1) // TILE_SIZE\n",
    "    \n",
    "    for t in range(num_tiles):\n",
    "        # Load tile of A\n",
    "        a_col = t * TILE_SIZE + tx\n",
    "        if row < M and a_col < K:\n",
    "            As[ty, tx] = A[row, a_col]\n",
    "        else:\n",
    "            As[ty, tx] = 0.0\n",
    "        \n",
    "        # Load tile of B\n",
    "        b_row = t * TILE_SIZE + ty\n",
    "        if b_row < K and col < N:\n",
    "            Bs[ty, tx] = B[b_row, col]\n",
    "        else:\n",
    "            Bs[ty, tx] = 0.0\n",
    "        \n",
    "        # Synchronize\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Compute partial product\n",
    "        for k in range(TILE_SIZE):\n",
    "            total += As[ty, k] * Bs[k, tx]\n",
    "        \n",
    "        # Synchronize before next tile\n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    # Write result\n",
    "    if row < M and col < N:\n",
    "        C[row, col] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tiled matrix multiplication\n",
    "M, K, N = 512, 512, 512\n",
    "\n",
    "A = np.random.rand(M, K).astype(np.float32)\n",
    "B = np.random.rand(K, N).astype(np.float32)\n",
    "C = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)\n",
    "\n",
    "grid = ((N + TILE_SIZE - 1) // TILE_SIZE,\n",
    "        (M + TILE_SIZE - 1) // TILE_SIZE)\n",
    "block = (TILE_SIZE, TILE_SIZE)\n",
    "\n",
    "matmul_tiled[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "result = d_C.copy_to_host()\n",
    "\n",
    "expected = A @ B\n",
    "print(f\"Tiled MatMul: {M}√ó{K} @ {K}√ó{N}\")\n",
    "print(f\"Correct: {'‚úì' if np.allclose(result, expected, rtol=1e-4) else '‚úó'}\")\n",
    "print(f\"Max error: {np.max(np.abs(result - expected)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da533b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Traffic Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tiled_traffic(M, K, N, tile_size):\n",
    "    \"\"\"Analyze memory traffic for tiled matrix multiply.\"\"\"\n",
    "    \n",
    "    # Number of blocks\n",
    "    blocks_m = (M + tile_size - 1) // tile_size\n",
    "    blocks_n = (N + tile_size - 1) // tile_size\n",
    "    blocks_k = (K + tile_size - 1) // tile_size\n",
    "    \n",
    "    # Each block loads tiles along K dimension\n",
    "    # A tile: tile_size √ó tile_size per block √ó blocks_k times\n",
    "    # Each A tile loaded by blocks_n column-blocks\n",
    "    # Each B tile loaded by blocks_m row-blocks\n",
    "    \n",
    "    # Total A loads: M/T √ó K/T tiles √ó T¬≤ elements √ó N/T blocks\n",
    "    # But wait - each A tile loaded only N/T times, not N times!\n",
    "    \n",
    "    # Tiled loads:\n",
    "    a_loads = blocks_m * blocks_k * blocks_n * tile_size * tile_size\n",
    "    b_loads = blocks_m * blocks_k * blocks_n * tile_size * tile_size\n",
    "    tiled_total = (a_loads + b_loads) * 4  # bytes\n",
    "    \n",
    "    # Actually, each block loads its own tiles:\n",
    "    # Each block: 2 √ó K/T √ó T¬≤ loads\n",
    "    # Total blocks: (M/T) √ó (N/T)\n",
    "    actual_tiled = blocks_m * blocks_n * 2 * blocks_k * tile_size * tile_size * 4\n",
    "    \n",
    "    # Naive loads (for comparison)\n",
    "    naive_total = M * N * (K + K) * 4\n",
    "    \n",
    "    # Minimum possible\n",
    "    min_loads = (M * K + K * N) * 4\n",
    "    \n",
    "    print(f\"Matrix multiply: ({M}√ó{K}) @ ({K}√ó{N}), Tile size: {tile_size}\")\n",
    "    print(f\"\\nMemory traffic comparison:\")\n",
    "    print(f\"  Naive:    {naive_total / 1e9:.2f} GB\")\n",
    "    print(f\"  Tiled:    {actual_tiled / 1e9:.3f} GB\")\n",
    "    print(f\"  Minimum:  {min_loads / 1e6:.2f} MB\")\n",
    "    print(f\"\\nReduction factor: {naive_total / actual_tiled:.1f}x\")\n",
    "    print(f\"Theoretical max:  {tile_size:.0f}x\")\n",
    "\n",
    "analyze_tiled_traffic(1024, 1024, 1024, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e1597",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae449158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive kernel for comparison\n",
    "@cuda.jit\n",
    "def matmul_naive(A, B, C, M, N, K):\n",
    "    row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    \n",
    "    if row < M and col < N:\n",
    "        total = 0.0\n",
    "        for k in range(K):\n",
    "            total += A[row, k] * B[k, col]\n",
    "        C[row, col] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbedaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_comparison(M, K, N, iterations=20):\n",
    "    \"\"\"Compare naive vs tiled performance.\"\"\"\n",
    "    A = np.random.rand(M, K).astype(np.float32)\n",
    "    B = np.random.rand(K, N).astype(np.float32)\n",
    "    C = np.zeros((M, N), dtype=np.float32)\n",
    "    \n",
    "    d_A = cuda.to_device(A)\n",
    "    d_B = cuda.to_device(B)\n",
    "    d_C = cuda.to_device(C)\n",
    "    \n",
    "    flops = 2 * M * N * K\n",
    "    \n",
    "    # Naive\n",
    "    block_naive = (16, 16)\n",
    "    grid_naive = ((N + 15) // 16, (M + 15) // 16)\n",
    "    \n",
    "    matmul_naive[grid_naive, block_naive](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        matmul_naive[grid_naive, block_naive](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    naive_ms = (time.perf_counter() - start) / iterations * 1000\n",
    "    naive_gflops = flops / (naive_ms * 1e6)\n",
    "    \n",
    "    # Tiled\n",
    "    block_tiled = (TILE_SIZE, TILE_SIZE)\n",
    "    grid_tiled = ((N + TILE_SIZE - 1) // TILE_SIZE, \n",
    "                  (M + TILE_SIZE - 1) // TILE_SIZE)\n",
    "    \n",
    "    matmul_tiled[grid_tiled, block_tiled](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        matmul_tiled[grid_tiled, block_tiled](d_A, d_B, d_C, M, N, K)\n",
    "    cuda.synchronize()\n",
    "    tiled_ms = (time.perf_counter() - start) / iterations * 1000\n",
    "    tiled_gflops = flops / (tiled_ms * 1e6)\n",
    "    \n",
    "    return naive_ms, naive_gflops, tiled_ms, tiled_gflops\n",
    "\n",
    "print(f\"{'Size':<15} {'Naive (ms)':<12} {'Naive GFLOPS':<14} {'Tiled (ms)':<12} {'Tiled GFLOPS':<14} {'Speedup':<10}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "for size in [256, 512, 1024]:\n",
    "    try:\n",
    "        n_ms, n_gf, t_ms, t_gf = benchmark_comparison(size, size, size)\n",
    "        speedup = n_ms / t_ms\n",
    "        print(f\"{size}√ó{size}√ó{size:<8} {n_ms:<12.3f} {n_gf:<14.2f} {t_ms:<12.3f} {t_gf:<14.2f} {speedup:.2f}x\")\n",
    "    except Exception as e:\n",
    "        print(f\"{size}√ó{size}√ó{size:<8} Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b6ef5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Tile Size Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db62f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tile_sizes():\n",
    "    \"\"\"Analyze trade-offs of different tile sizes.\"\"\"\n",
    "    print(\"Tile Size Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for tile in [8, 16, 32]:\n",
    "        threads = tile * tile\n",
    "        shared_mem = 2 * tile * tile * 4  # Two tiles, float32\n",
    "        reuse = tile  # Each element reused tile times\n",
    "        \n",
    "        # Occupancy considerations\n",
    "        max_blocks_shared = 48 * 1024 // shared_mem  # 48KB typical\n",
    "        max_blocks_threads = 1024 // threads  # 1024 threads/block max\n",
    "        \n",
    "        print(f\"\\nTile {tile}√ó{tile}:\")\n",
    "        print(f\"  Threads per block: {threads}\")\n",
    "        print(f\"  Shared memory: {shared_mem} bytes ({shared_mem/1024:.1f} KB)\")\n",
    "        print(f\"  Data reuse factor: {reuse}x\")\n",
    "        print(f\"  Max blocks (shared mem limit): {max_blocks_shared}\")\n",
    "        print(f\"  Max blocks (thread limit): {max_blocks_threads}\")\n",
    "\n",
    "analyze_tile_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7f597",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "### üî∑ CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792fb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tiled_matmul_exercises.cu\n",
    "/*\n",
    " * CUDA C++ Tiled Matrix Multiplication Exercises\n",
    " * Exercise 1: Variable Tile Size - Compare 8√ó8, 16√ó16, 32√ó32 tiles\n",
    " * Exercise 2: Rectangular Tiles - Implement non-square tiles\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <chrono>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "// Exercise 1: Tiled matmul with template tile size\n",
    "template<int TILE_SIZE>\n",
    "__global__ void matmul_tiled(const float* A, const float* B, float* C, \n",
    "                              int M, int N, int K) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int bx = blockIdx.x, by = blockIdx.y;\n",
    "    int tx = threadIdx.x, ty = threadIdx.y;\n",
    "    \n",
    "    int row = by * TILE_SIZE + ty;\n",
    "    int col = bx * TILE_SIZE + tx;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; t++) {\n",
    "        // Load A tile with bounds check\n",
    "        int a_col = t * TILE_SIZE + tx;\n",
    "        if (row < M && a_col < K)\n",
    "            As[ty][tx] = A[row * K + a_col];\n",
    "        else\n",
    "            As[ty][tx] = 0.0f;\n",
    "        \n",
    "        // Load B tile with bounds check\n",
    "        int b_row = t * TILE_SIZE + ty;\n",
    "        if (b_row < K && col < N)\n",
    "            Bs[ty][tx] = B[b_row * N + col];\n",
    "        else\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        // Compute partial product\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TILE_SIZE; k++) {\n",
    "            sum += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Exercise 2: Rectangular tiles (TILE_M √ó TILE_K for A, TILE_K √ó TILE_N for B)\n",
    "template<int TILE_M, int TILE_N, int TILE_K>\n",
    "__global__ void matmul_tiled_rect(const float* A, const float* B, float* C,\n",
    "                                   int M, int N, int K) {\n",
    "    // Shared memory for rectangular tiles\n",
    "    __shared__ float As[TILE_M][TILE_K];\n",
    "    __shared__ float Bs[TILE_K][TILE_N];\n",
    "    \n",
    "    int bx = blockIdx.x, by = blockIdx.y;\n",
    "    int tx = threadIdx.x, ty = threadIdx.y;\n",
    "    \n",
    "    int row = by * TILE_M + ty;\n",
    "    int col = bx * TILE_N + tx;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    int numTiles = (K + TILE_K - 1) / TILE_K;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; t++) {\n",
    "        // Load A tile (TILE_M √ó TILE_K)\n",
    "        // Need multiple loads if TILE_K > blockDim.x\n",
    "        for (int i = tx; i < TILE_K; i += blockDim.x) {\n",
    "            int a_col = t * TILE_K + i;\n",
    "            if (row < M && a_col < K)\n",
    "                As[ty][i] = A[row * K + a_col];\n",
    "            else\n",
    "                As[ty][i] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        // Load B tile (TILE_K √ó TILE_N)\n",
    "        for (int i = ty; i < TILE_K; i += blockDim.y) {\n",
    "            int b_row = t * TILE_K + i;\n",
    "            if (b_row < K && col < N)\n",
    "                Bs[i][tx] = B[b_row * N + col];\n",
    "            else\n",
    "                Bs[i][tx] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        // Compute partial product\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TILE_K; k++) {\n",
    "            sum += As[ty][k] * Bs[k][tx];\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "template<int TILE_SIZE>\n",
    "float benchmark_tiled(const float* d_A, const float* d_B, float* d_C,\n",
    "                      int M, int N, int K, int iterations) {\n",
    "    dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    \n",
    "    // Warmup\n",
    "    matmul_tiled<TILE_SIZE><<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        matmul_tiled<TILE_SIZE><<<grid, block>>>(d_A, d_B, d_C, M, N, K);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    return std::chrono::duration<float, std::milli>(end - start).count() / iterations;\n",
    "}\n",
    "\n",
    "void cpu_matmul(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    for (int i = 0; i < M; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            float sum = 0.0f;\n",
    "            for (int k = 0; k < K; k++) {\n",
    "                sum += A[i * K + k] * B[k * N + j];\n",
    "            }\n",
    "            C[i * N + j] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int M = 1024, K = 1024, N = 1024;\n",
    "    int iterations = 10;\n",
    "    \n",
    "    printf(\"=== Exercise 1: Variable Tile Size Comparison ===\\n\\n\");\n",
    "    printf(\"Matrix size: %d √ó %d √ó %d\\n\\n\", M, K, N);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    size_t size_A = M * K * sizeof(float);\n",
    "    size_t size_B = K * N * sizeof(float);\n",
    "    size_t size_C = M * N * sizeof(float);\n",
    "    \n",
    "    float *h_A = (float*)malloc(size_A);\n",
    "    float *h_B = (float*)malloc(size_B);\n",
    "    float *h_C = (float*)malloc(size_C);\n",
    "    float *h_C_ref = (float*)malloc(size_C);\n",
    "    \n",
    "    srand(42);\n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = (float)rand() / RAND_MAX;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    CHECK_CUDA(cudaMalloc(&d_A, size_A));\n",
    "    CHECK_CUDA(cudaMalloc(&d_B, size_B));\n",
    "    CHECK_CUDA(cudaMalloc(&d_C, size_C));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    float flops = 2.0f * M * N * K;\n",
    "    \n",
    "    printf(\"%-15s %-15s %-15s %-20s\\n\", \"Tile Size\", \"Time (ms)\", \"GFLOPS\", \"Shared Mem (KB)\");\n",
    "    printf(\"%-15s %-15s %-15s %-20s\\n\", \"----------\", \"----------\", \"----------\", \"--------------\");\n",
    "    \n",
    "    // 8√ó8 tiles\n",
    "    float time_8 = benchmark_tiled<8>(d_A, d_B, d_C, M, N, K, iterations);\n",
    "    printf(\"%-15s %-15.3f %-15.2f %-20.2f\\n\", \"8 √ó 8\", time_8, \n",
    "           (flops / time_8) / 1e6, 2.0f * 8 * 8 * sizeof(float) / 1024.0f);\n",
    "    \n",
    "    // 16√ó16 tiles\n",
    "    float time_16 = benchmark_tiled<16>(d_A, d_B, d_C, M, N, K, iterations);\n",
    "    printf(\"%-15s %-15.3f %-15.2f %-20.2f\\n\", \"16 √ó 16\", time_16,\n",
    "           (flops / time_16) / 1e6, 2.0f * 16 * 16 * sizeof(float) / 1024.0f);\n",
    "    \n",
    "    // 32√ó32 tiles\n",
    "    float time_32 = benchmark_tiled<32>(d_A, d_B, d_C, M, N, K, iterations);\n",
    "    printf(\"%-15s %-15.3f %-15.2f %-20.2f\\n\", \"32 √ó 32\", time_32,\n",
    "           (flops / time_32) / 1e6, 2.0f * 32 * 32 * sizeof(float) / 1024.0f);\n",
    "    \n",
    "    // Verify 16√ó16 result\n",
    "    matmul_tiled<16><<<dim3((N+15)/16, (M+15)/16), dim3(16, 16)>>>(d_A, d_B, d_C, M, N, K);\n",
    "    CHECK_CUDA(cudaMemcpy(h_C, d_C, size_C, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // CPU reference (only for small verification)\n",
    "    printf(\"\\nVerifying correctness (first 4x4 corner)...\\n\");\n",
    "    cpu_matmul(h_A, h_B, h_C_ref, M, N, K);\n",
    "    float max_error = 0.0f;\n",
    "    for (int i = 0; i < M * N; i++) {\n",
    "        float err = fabs(h_C[i] - h_C_ref[i]);\n",
    "        if (err > max_error) max_error = err;\n",
    "    }\n",
    "    printf(\"Max error: %.6f %s\\n\", max_error, max_error < 0.01f ? \"‚úì\" : \"‚úó\");\n",
    "    \n",
    "    printf(\"\\n=== Exercise 2: Rectangular Tiles ===\\n\\n\");\n",
    "    \n",
    "    // Test rectangular tile configuration: 32√ó16 output tile, 16 K-dimension\n",
    "    const int TILE_M = 32, TILE_N = 16, TILE_K = 16;\n",
    "    \n",
    "    dim3 block_rect(TILE_N, TILE_M);  // tx handles N, ty handles M\n",
    "    dim3 grid_rect((N + TILE_N - 1) / TILE_N, (M + TILE_M - 1) / TILE_M);\n",
    "    \n",
    "    printf(\"Rectangular tile configuration: %d √ó %d output, %d K-stride\\n\", TILE_M, TILE_N, TILE_K);\n",
    "    printf(\"Block dimensions: %d √ó %d = %d threads\\n\", TILE_N, TILE_M, TILE_N * TILE_M);\n",
    "    printf(\"Shared memory: %.2f KB\\n\", (TILE_M * TILE_K + TILE_K * TILE_N) * sizeof(float) / 1024.0f);\n",
    "    \n",
    "    // Warmup and benchmark\n",
    "    matmul_tiled_rect<TILE_M, TILE_N, TILE_K><<<grid_rect, block_rect>>>(d_A, d_B, d_C, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        matmul_tiled_rect<TILE_M, TILE_N, TILE_K><<<grid_rect, block_rect>>>(d_A, d_B, d_C, M, N, K);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    float time_rect = std::chrono::duration<float, std::milli>(end - start).count() / iterations;\n",
    "    printf(\"\\nPerformance: %.3f ms, %.2f GFLOPS\\n\", time_rect, (flops / time_rect) / 1e6);\n",
    "    \n",
    "    printf(\"\\nKey observations:\\n\");\n",
    "    printf(\"- Larger tiles = more data reuse, but more shared memory\\n\");\n",
    "    printf(\"- Rectangular tiles can optimize for specific matrix shapes\\n\");\n",
    "    printf(\"- 32√ó32 often hits thread limit (1024), may reduce occupancy\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C); free(h_C_ref);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87144200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create kernels with different tile sizes and compare\n",
    "# Numba requires compile-time constants, so you'll need separate kernels\n",
    "\n",
    "# Tile size 8\n",
    "@cuda.jit\n",
    "def matmul_tiled_8(A, B, C, M, N, K):\n",
    "    pass  # Your implementation\n",
    "\n",
    "# Tile size 32\n",
    "@cuda.jit\n",
    "def matmul_tiled_32(A, B, C, M, N, K):\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828119a3",
   "metadata": {},
   "source": [
    "### Exercise 2: Rectangular Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40747c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement with non-square tiles (e.g., 32√ó16)\n",
    "# This can sometimes improve register usage\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled_rect(A, B, C, M, N, K):\n",
    "    \"\"\"Tiled matmul with 32√ó16 tiles.\"\"\"\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4ffb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Tiling Strategy\n",
    "\n",
    "| Aspect | Improvement |\n",
    "|--------|-------------|\n",
    "| Memory traffic | Reduced by TILE_SIZE factor |\n",
    "| Arithmetic intensity | Increased from 1 to ~TILE_SIZE FLOP/byte |\n",
    "| Performance | 2-10x speedup over naive |\n",
    "\n",
    "### CUDA C++ Key Pattern\n",
    "\n",
    "```cpp\n",
    "__shared__ float As[TILE][TILE], Bs[TILE][TILE];\n",
    "\n",
    "for (int t = 0; t < numTiles; t++) {\n",
    "    // Load tiles with bounds checking\n",
    "    As[ty][tx] = (in_bounds_A) ? A[...] : 0;\n",
    "    Bs[ty][tx] = (in_bounds_B) ? B[...] : 0;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute partial product\n",
    "    for (int k = 0; k < TILE; k++)\n",
    "        sum += As[ty][k] * Bs[k][tx];\n",
    "    __syncthreads();\n",
    "}\n",
    "```\n",
    "\n",
    "### Critical Points\n",
    "\n",
    "1. **Two syncthreads per tile** - One after load, one after compute\n",
    "2. **Bounds checking** - Pad with zeros for edge tiles\n",
    "3. **Tile size** - Balance shared memory, threads, and reuse\n",
    "\n",
    "### Next: Matrix Transpose\n",
    "Tomorrow we'll optimize matrix transpose with coalesced access patterns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
