{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9546b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96818885",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Transpose Problem\n",
    "\n",
    "### Memory Access Patterns\n",
    "\n",
    "```\n",
    "Matrix A (MÃ—N, row-major):    Transposed A^T (NÃ—M):\n",
    "â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”            â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚   Memory:  â”‚ 0 â”‚ 4 â”‚ 8 â”‚\n",
    "â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤   0,1,2,3  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚   4,5,6,7  â”‚ 1 â”‚ 5 â”‚ 9 â”‚\n",
    "â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤   8,9,...  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "â”‚ 8 â”‚ 9 â”‚10 â”‚11 â”‚            â”‚ 2 â”‚ 6 â”‚10 â”‚\n",
    "â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜            â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "                             â”‚ 3 â”‚ 7 â”‚11 â”‚\n",
    "                             â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "\n",
    "Naive transpose: out[j][i] = in[i][j]\n",
    "\n",
    "Problem: Either reads OR writes are strided!\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transpose.cu\n",
    "// transpose.cu - Naive vs Tiled Transpose\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "// Naive transpose: coalesced reads, strided writes\n",
    "__global__ void transpose_naive(const float* in, float* out, int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;  // column\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;  // row\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        // Read: coalesced (consecutive threads read consecutive addresses)\n",
    "        // Write: strided (consecutive threads write addresses width apart)\n",
    "        out[x * height + y] = in[y * width + x];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Tiled transpose using shared memory\n",
    "__global__ void transpose_tiled(const float* in, float* out, int width, int height) {\n",
    "    // Shared memory tile with extra column for bank conflict avoidance\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Load tile from global memory (coalesced read)\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Now x and y refer to transposed coordinates\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Write tile to global memory (coalesced write)\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            // Note: read transposed from shared memory\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int width = 2048, height = 2048;\n",
    "    size_t size = width * height * sizeof(float);\n",
    "    \n",
    "    float *h_in = (float*)malloc(size);\n",
    "    float *h_out = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < width * height; i++) h_in[i] = i;\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, size);\n",
    "    cudaMalloc(&d_out, size);\n",
    "    cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 grid((width + TILE_DIM - 1) / TILE_DIM,\n",
    "              (height + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark naive\n",
    "    transpose_naive<<<grid, block>>>(d_in, d_out, width, height);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        transpose_naive<<<grid, block>>>(d_in, d_out, width, height);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float naive_ms;\n",
    "    cudaEventElapsedTime(&naive_ms, start, stop);\n",
    "    naive_ms /= 100;\n",
    "    \n",
    "    // Benchmark tiled\n",
    "    transpose_tiled<<<grid, block>>>(d_in, d_out, width, height);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        transpose_tiled<<<grid, block>>>(d_in, d_out, width, height);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float tiled_ms;\n",
    "    cudaEventElapsedTime(&tiled_ms, start, stop);\n",
    "    tiled_ms /= 100;\n",
    "    \n",
    "    // Calculate bandwidth\n",
    "    double bytes = 2.0 * width * height * sizeof(float);  // read + write\n",
    "    double naive_bw = bytes / (naive_ms * 1e6);   // GB/s\n",
    "    double tiled_bw = bytes / (tiled_ms * 1e6);\n",
    "    \n",
    "    printf(\"Matrix: %d Ã— %d\\n\", width, height);\n",
    "    printf(\"Naive:  %.3f ms, %.1f GB/s\\n\", naive_ms, naive_bw);\n",
    "    printf(\"Tiled:  %.3f ms, %.1f GB/s\\n\", tiled_ms, tiled_bw);\n",
    "    printf(\"Speedup: %.2fx\\n\", naive_ms / tiled_ms);\n",
    "    \n",
    "    cudaFree(d_in); cudaFree(d_out);\n",
    "    free(h_in); free(h_out);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o transpose transpose.cu\n",
    "!./transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194bce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive transpose\n",
    "@cuda.jit\n",
    "def transpose_naive(input_matrix, output_matrix):\n",
    "    \"\"\"Naive transpose: coalesced reads, strided writes.\"\"\"\n",
    "    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    \n",
    "    height, width = input_matrix.shape\n",
    "    \n",
    "    if x < width and y < height:\n",
    "        output_matrix[x, y] = input_matrix[y, x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29034f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test naive transpose\n",
    "height, width = 1024, 1024\n",
    "A = np.arange(height * width, dtype=np.float32).reshape(height, width)\n",
    "B = np.zeros((width, height), dtype=np.float32)\n",
    "\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "\n",
    "block = (32, 8)\n",
    "grid = ((width + 31) // 32, (height + 31) // 32)\n",
    "\n",
    "transpose_naive[grid, block](d_A, d_B)\n",
    "result = d_B.copy_to_host()\n",
    "\n",
    "expected = A.T\n",
    "print(f\"Naive Transpose: {height}Ã—{width}\")\n",
    "print(f\"Correct: {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1957f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Tiled Transpose with Shared Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_DIM = 32\n",
    "BLOCK_ROWS = 8\n",
    "\n",
    "@cuda.jit\n",
    "def transpose_tiled(input_matrix, output_matrix):\n",
    "    \"\"\"Tiled transpose with coalesced reads AND writes.\"\"\"\n",
    "    # Shared memory with padding to avoid bank conflicts\n",
    "    tile = cuda.shared.array((TILE_DIM, TILE_DIM + 1), dtype=np.float32)\n",
    "    \n",
    "    height, width = input_matrix.shape\n",
    "    \n",
    "    x = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.y\n",
    "    \n",
    "    # Load tile (coalesced read)\n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        if x < width and (y + j) < height:\n",
    "            tile[cuda.threadIdx.y + j, cuda.threadIdx.x] = input_matrix[y + j, x]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Swap block indices for output\n",
    "    x = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.y\n",
    "    \n",
    "    # Store tile transposed (coalesced write)\n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        if x < height and (y + j) < width:\n",
    "            output_matrix[y + j, x] = tile[cuda.threadIdx.x, cuda.threadIdx.y + j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tiled transpose\n",
    "height, width = 1024, 1024\n",
    "A = np.arange(height * width, dtype=np.float32).reshape(height, width)\n",
    "B = np.zeros((width, height), dtype=np.float32)\n",
    "\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "\n",
    "block = (TILE_DIM, BLOCK_ROWS)\n",
    "grid = ((width + TILE_DIM - 1) // TILE_DIM, \n",
    "        (height + TILE_DIM - 1) // TILE_DIM)\n",
    "\n",
    "transpose_tiled[grid, block](d_A, d_B)\n",
    "result = d_B.copy_to_host()\n",
    "\n",
    "expected = A.T\n",
    "print(f\"Tiled Transpose: {height}Ã—{width}\")\n",
    "print(f\"Correct: {'âœ“' if np.allclose(result, expected) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf0e2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Bank Conflict Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_bank_conflicts():\n",
    "    \"\"\"Explain why bank conflicts occur in transpose.\"\"\"\n",
    "    print(\"Bank Conflicts in Matrix Transpose\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"Shared memory has 32 banks (bank = address % 32)\")\n",
    "    print()\n",
    "    print(\"Without padding (TILE[32][32]):\")\n",
    "    print(\"  Storing: tile[ty][tx] - consecutive threads access different banks âœ“\")\n",
    "    print(\"  Loading: tile[tx][ty] - all threads in column access SAME bank âœ—\")\n",
    "    print()\n",
    "    print(\"  Example column access (ty=0, different tx):\")\n",
    "    for tx in range(8):\n",
    "        addr = tx * 32 + 0  # tile[tx][0]\n",
    "        bank = addr % 32\n",
    "        print(f\"    Thread {tx}: tile[{tx}][0] â†’ address {addr:3d} â†’ bank {bank}\")\n",
    "    print(\"  All threads access bank 0 â†’ 32-way bank conflict!\")\n",
    "    print()\n",
    "    print(\"With padding (TILE[32][33]):\")\n",
    "    print(\"  Now tile[tx][ty] has stride 33 instead of 32\")\n",
    "    for tx in range(8):\n",
    "        addr = tx * 33 + 0  # tile[tx][0] with 33-stride\n",
    "        bank = addr % 32\n",
    "        print(f\"    Thread {tx}: tile[{tx}][0] â†’ address {addr:3d} â†’ bank {bank}\")\n",
    "    print(\"  All threads access different banks âœ“\")\n",
    "\n",
    "explain_bank_conflicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9d013",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_transpose(height, width, iterations=50):\n",
    "    \"\"\"Compare naive vs tiled transpose performance.\"\"\"\n",
    "    A = np.random.rand(height, width).astype(np.float32)\n",
    "    B_naive = np.zeros((width, height), dtype=np.float32)\n",
    "    B_tiled = np.zeros((width, height), dtype=np.float32)\n",
    "    \n",
    "    d_A = cuda.to_device(A)\n",
    "    d_B_naive = cuda.to_device(B_naive)\n",
    "    d_B_tiled = cuda.to_device(B_tiled)\n",
    "    \n",
    "    bytes_moved = 2 * height * width * 4  # Read + write\n",
    "    \n",
    "    # Naive\n",
    "    block_naive = (32, 8)\n",
    "    grid_naive = ((width + 31) // 32, (height + 31) // 32)\n",
    "    \n",
    "    transpose_naive[grid_naive, block_naive](d_A, d_B_naive)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        transpose_naive[grid_naive, block_naive](d_A, d_B_naive)\n",
    "    cuda.synchronize()\n",
    "    naive_ms = (time.perf_counter() - start) / iterations * 1000\n",
    "    naive_bw = bytes_moved / (naive_ms * 1e6)  # GB/s\n",
    "    \n",
    "    # Tiled\n",
    "    block_tiled = (TILE_DIM, BLOCK_ROWS)\n",
    "    grid_tiled = ((width + TILE_DIM - 1) // TILE_DIM, \n",
    "                  (height + TILE_DIM - 1) // TILE_DIM)\n",
    "    \n",
    "    transpose_tiled[grid_tiled, block_tiled](d_A, d_B_tiled)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        transpose_tiled[grid_tiled, block_tiled](d_A, d_B_tiled)\n",
    "    cuda.synchronize()\n",
    "    tiled_ms = (time.perf_counter() - start) / iterations * 1000\n",
    "    tiled_bw = bytes_moved / (tiled_ms * 1e6)  # GB/s\n",
    "    \n",
    "    return naive_ms, naive_bw, tiled_ms, tiled_bw\n",
    "\n",
    "print(f\"{'Size':<15} {'Naive (ms)':<12} {'Naive BW':<12} {'Tiled (ms)':<12} {'Tiled BW':<12} {'Speedup':<10}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "for size in [512, 1024, 2048]:\n",
    "    try:\n",
    "        n_ms, n_bw, t_ms, t_bw = benchmark_transpose(size, size)\n",
    "        speedup = n_ms / t_ms\n",
    "        print(f\"{size}Ã—{size:<10} {n_ms:<12.3f} {n_bw:<12.1f} {t_ms:<12.3f} {t_bw:<12.1f} {speedup:.2f}x\")\n",
    "    except Exception as e:\n",
    "        print(f\"{size}Ã—{size:<10} Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7575dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Memory Access Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_access_patterns():\n",
    "    \"\"\"Visualize memory access patterns for transpose.\"\"\"\n",
    "    print(\"Memory Access Pattern Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. NAIVE TRANSPOSE\")\n",
    "    print(\"   Thread arrangement: blockDim = (32, 8)\")\n",
    "    print(\"   Each thread handles one element\")\n",
    "    print()\n",
    "    print(\"   READ (coalesced):\")\n",
    "    print(\"   Warp threads 0-31 read in[y][0], in[y][1], ..., in[y][31]\")\n",
    "    print(\"   â†’ Addresses: base, base+4, base+8, ... (consecutive) âœ“\")\n",
    "    print()\n",
    "    print(\"   WRITE (strided):\")\n",
    "    print(\"   Warp threads 0-31 write out[0][y], out[1][y], ..., out[31][y]\")\n",
    "    print(\"   â†’ Addresses: base, base+H*4, base+2*H*4, ... (stride H) âœ—\")\n",
    "    \n",
    "    print(\"\\n2. TILED TRANSPOSE\")\n",
    "    print(\"   Step 1 - READ (coalesced):\")\n",
    "    print(\"   Same as naive - warp reads consecutive addresses âœ“\")\n",
    "    print()\n",
    "    print(\"   Step 2 - STORE to shared (no bank conflict):\")\n",
    "    print(\"   tile[ty][tx] - each thread writes different bank âœ“\")\n",
    "    print()\n",
    "    print(\"   Step 3 - LOAD from shared (no bank conflict with padding):\")\n",
    "    print(\"   tile[tx][ty+j] with TILE[32][33] padding âœ“\")\n",
    "    print()\n",
    "    print(\"   Step 4 - WRITE (coalesced):\")\n",
    "    print(\"   Swapped block indices mean consecutive threads write\")\n",
    "    print(\"   consecutive global memory addresses âœ“\")\n",
    "\n",
    "visualize_access_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59316e1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: In-place Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement in-place transpose for square matrices\n",
    "# Hint: Only transpose above diagonal, swap with below diagonal\n",
    "\n",
    "@cuda.jit\n",
    "def transpose_inplace(matrix):\n",
    "    \"\"\"In-place transpose for square matrix.\"\"\"\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332174b",
   "metadata": {},
   "source": [
    "### Exercise 2: Batched Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed58ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement batched transpose for 3D tensor\n",
    "# Shape: (batch, height, width) â†’ (batch, width, height)\n",
    "\n",
    "@cuda.jit\n",
    "def transpose_batched(input_3d, output_3d):\n",
    "    \"\"\"Batched transpose: transpose each 2D slice.\"\"\"\n",
    "    pass  # Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a17183",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "| Approach | Reads | Writes | Performance |\n",
    "|----------|-------|--------|-------------|\n",
    "| Naive | Coalesced âœ“ | Strided âœ— | Low bandwidth |\n",
    "| Tiled | Coalesced âœ“ | Coalesced âœ“ | ~Peak bandwidth |\n",
    "\n",
    "### CUDA C++ Pattern\n",
    "\n",
    "```cpp\n",
    "// Key: Extra column for bank conflict avoidance\n",
    "__shared__ float tile[TILE_DIM][TILE_DIM + 1];\n",
    "\n",
    "// Load: row-major from global\n",
    "tile[ty][tx] = in[y * width + x];\n",
    "__syncthreads();\n",
    "\n",
    "// Store: column-major to global (but looks row-major after swap)\n",
    "out[(newY) * height + newX] = tile[tx][ty];\n",
    "```\n",
    "\n",
    "### Bank Conflict Solution\n",
    "\n",
    "```\n",
    "Without padding: tile[32][32] â†’ column access = 32-way conflict\n",
    "With padding:    tile[32][33] â†’ stride 33 â†’ no conflicts\n",
    "```\n",
    "\n",
    "### Tomorrow: cuBLAS Integration\n",
    "We'll use the optimized cuBLAS library for matrix operations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
