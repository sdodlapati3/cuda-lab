{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ec7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899c68c",
   "metadata": {},
   "source": [
    "# Day 4: cuBLAS Integration - Standing on Giants' Shoulders\n",
    "\n",
    "> *\"Good engineers borrow. Great engineers use vendor libraries.\"*\n",
    "\n",
    "**The Hook:** We spent three days optimizing matrix multiply. NVIDIA's engineers spent *decades*. Today we learn when to write custom kernelsâ€”and when to trust cuBLAS.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. **Use** cuBLAS for GEMM, GEMV, and other BLAS operations\n",
    "2. **Handle** row-major to column-major layout conversion\n",
    "3. **Enable** Tensor Core acceleration for maximum performance\n",
    "4. **Decide** when to use libraries vs custom kernels\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ´ Concept Card: Standing on Giants' Shoulders\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  WHY NOT WRITE EVERYTHING YOURSELF?                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Your optimized matmul (after 3 days of work):                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚  â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚ ~30% peak       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  cuBLAS (decades of NVIDIA engineering):                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚  â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ ~95% peak       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  WHAT cuBLAS DOES THAT YOU DIDN'T:                              â”‚\n",
    "â”‚  â€¢ Multi-level tiling (register, shared, L2)                    â”‚\n",
    "â”‚  â€¢ Bank conflict-free shared memory layouts                     â”‚\n",
    "â”‚  â€¢ Tensor Core utilization (FP16/TF32)                          â”‚\n",
    "â”‚  â€¢ Auto-tuning per GPU architecture                             â”‚\n",
    "â”‚  â€¢ Software pipelining for latency hiding                       â”‚\n",
    "â”‚  â€¢ Assembly-level instruction scheduling                        â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  WHEN TO WRITE CUSTOM KERNELS:                                  â”‚\n",
    "â”‚  âœ“ Fused operations (matmul + activation + bias)                â”‚\n",
    "â”‚  âœ“ Non-standard data types or layouts                           â”‚\n",
    "â”‚  âœ“ Learning/research purposes                                   â”‚\n",
    "â”‚  âœ— Standard BLAS operations (just use cuBLAS!)                  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ğŸ’¡ \"Standing on giants' shoulders\" = using proven libraries    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: cuBLAS Overview\n",
    "\n",
    "### What is cuBLAS?\n",
    "\n",
    "```\n",
    "cuBLAS = CUDA Basic Linear Algebra Subroutines\n",
    "\n",
    "Features:\n",
    "- GPU-accelerated BLAS (levels 1, 2, 3)\n",
    "- Highly optimized for NVIDIA hardware\n",
    "- Tensor Core acceleration (GEMM)\n",
    "- Near-peak performance\n",
    "\n",
    "BLAS Levels:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Level   â”‚ Operations               â”‚ Complexity  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Level 1 â”‚ Vector-Vector (dot, axpy)â”‚ O(n)        â”‚\n",
    "â”‚ Level 2 â”‚ Matrix-Vector (gemv)     â”‚ O(nÂ²)       â”‚\n",
    "â”‚ Level 3 â”‚ Matrix-Matrix (gemm)     â”‚ O(nÂ³)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### BLAS Naming Convention\n",
    "\n",
    "```\n",
    "Example: cublasSgemm, cublasDgemm\n",
    "\n",
    "Pattern: cublas[Precision][Operation]\n",
    "\n",
    "Precision codes:\n",
    "  S = float (single precision)\n",
    "  D = double\n",
    "  C = complex float\n",
    "  Z = complex double\n",
    "  H = half precision (FP16)\n",
    "\n",
    "Common operations:\n",
    "  gemm = GEneral Matrix Multiply\n",
    "  gemv = GEneral Matrix Vector\n",
    "  axpy = A*X Plus Y\n",
    "  dot  = DOT product\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/CuPy (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba1309",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_gemm.cu\n",
    "// cublas_gemm.cu - Matrix multiplication with cuBLAS\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "\n",
    "int main() {\n",
    "    int M = 1024, N = 1024, K = 1024;\n",
    "    \n",
    "    size_t size_A = M * K * sizeof(float);\n",
    "    size_t size_B = K * N * sizeof(float);\n",
    "    size_t size_C = M * N * sizeof(float);\n",
    "    \n",
    "    // Host memory\n",
    "    float *h_A = (float*)malloc(size_A);\n",
    "    float *h_B = (float*)malloc(size_B);\n",
    "    float *h_C = (float*)malloc(size_C);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = rand() / (float)RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = rand() / (float)RAND_MAX;\n",
    "    \n",
    "    // Device memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, size_A);\n",
    "    cudaMalloc(&d_B, size_B);\n",
    "    cudaMalloc(&d_C, size_C);\n",
    "    \n",
    "    cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Create cuBLAS handle\n",
    "    cublasHandle_t handle;\n",
    "    cublasCreate(&handle);\n",
    "    \n",
    "    // GEMM: C = alpha * A * B + beta * C\n",
    "    float alpha = 1.0f;\n",
    "    float beta = 0.0f;\n",
    "    \n",
    "    // Note: cuBLAS uses column-major order!\n",
    "    // For row-major C = A * B, we compute C^T = B^T * A^T\n",
    "    // cublasSgemm(handle, transB, transA, N, M, K, ...)\n",
    "    \n",
    "    // Warmup\n",
    "    cublasSgemm(handle,\n",
    "                CUBLAS_OP_N, CUBLAS_OP_N,  // No transpose\n",
    "                N, M, K,                    // Dimensions\n",
    "                &alpha,\n",
    "                d_B, N,                     // B and leading dimension\n",
    "                d_A, K,                     // A and leading dimension\n",
    "                &beta,\n",
    "                d_C, N);                    // C and leading dimension\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        cublasSgemm(handle,\n",
    "                    CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "                    N, M, K,\n",
    "                    &alpha, d_B, N, d_A, K,\n",
    "                    &beta, d_C, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    ms /= 100;\n",
    "    \n",
    "    double flops = 2.0 * M * N * K;\n",
    "    double tflops = flops / (ms * 1e9);\n",
    "    \n",
    "    printf(\"cuBLAS SGEMM: %dÃ—%dÃ—%d\\n\", M, K, N);\n",
    "    printf(\"Time: %.3f ms\\n\", ms);\n",
    "    printf(\"Performance: %.2f TFLOPS\\n\", tflops);\n",
    "    \n",
    "    // Cleanup\n",
    "    cublasDestroy(handle);\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "// Compile: nvcc cublas_gemm.cu -o cublas_gemm -lcublas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cublas_gemm cublas_gemm.cu -lcublas\n",
    "!./cublas_gemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe718d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CuPy provides easy access to cuBLAS\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"CuPy not available - install with: pip install cupy-cuda12x\")\n",
    "    CUPY_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    # CuPy uses cuBLAS under the hood\n",
    "    M, K, N = 1024, 1024, 1024\n",
    "    \n",
    "    A = cp.random.rand(M, K).astype(cp.float32)\n",
    "    B = cp.random.rand(K, N).astype(cp.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    C = A @ B\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = cp.cuda.Event()\n",
    "    end = cp.cuda.Event()\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(100):\n",
    "        C = A @ B\n",
    "    end.record()\n",
    "    end.synchronize()\n",
    "    \n",
    "    ms = cp.cuda.get_elapsed_time(start, end) / 100\n",
    "    flops = 2 * M * N * K\n",
    "    tflops = flops / (ms * 1e9)\n",
    "    \n",
    "    print(f\"CuPy (cuBLAS) GEMM: {M}Ã—{K}Ã—{N}\")\n",
    "    print(f\"Time: {ms:.3f} ms\")\n",
    "    print(f\"Performance: {tflops:.2f} TFLOPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f861c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Row-Major vs Column-Major\n",
    "\n",
    "### The Column-Major Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea434a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_column_major():\n",
    "    \"\"\"Explain cuBLAS column-major layout.\"\"\"\n",
    "    print(\"cuBLAS Column-Major Layout\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"Row-major (C/C++, NumPy default):\")\n",
    "    print(\"  A = [[1, 2, 3],    Memory: [1,2,3,4,5,6]\")\n",
    "    print(\"       [4, 5, 6]]\")\n",
    "    print()\n",
    "    print(\"Column-major (Fortran, cuBLAS):\")\n",
    "    print(\"  A = [[1, 2, 3],    Memory: [1,4,2,5,3,6]\")\n",
    "    print(\"       [4, 5, 6]]\")\n",
    "    print()\n",
    "    print(\"Solutions for row-major data:\")\n",
    "    print(\"  1. Transpose before/after (memory overhead)\")\n",
    "    print(\"  2. Use CUBLAS_OP_T to transpose on-the-fly\")\n",
    "    print(\"  3. Compute C = B^T Ã— A^T (equals (AÃ—B)^T)\")\n",
    "    print()\n",
    "    print(\"Trick: For row-major C = A Ã— B:\")\n",
    "    print(\"  Call gemm(B, A) instead of gemm(A, B)\")\n",
    "    print(\"  Because in column-major: C^T = B^T Ã— A^T\")\n",
    "\n",
    "explain_column_major()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de7c77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: cuBLAS API Deep Dive\n",
    "\n",
    "### GEMM Parameters\n",
    "\n",
    "```cpp\n",
    "cublasStatus_t cublasSgemm(\n",
    "    cublasHandle_t handle,      // cuBLAS context\n",
    "    cublasOperation_t transa,   // CUBLAS_OP_N, CUBLAS_OP_T, CUBLAS_OP_C\n",
    "    cublasOperation_t transb,\n",
    "    int m,                      // Rows of op(A) and C\n",
    "    int n,                      // Columns of op(B) and C\n",
    "    int k,                      // Columns of op(A), rows of op(B)\n",
    "    const float *alpha,         // Scalar Î±\n",
    "    const float *A, int lda,    // A and its leading dimension\n",
    "    const float *B, int ldb,    // B and its leading dimension\n",
    "    const float *beta,          // Scalar Î²\n",
    "    float *C, int ldc           // C and its leading dimension\n",
    ");\n",
    "\n",
    "// Computes: C = Î± Ã— op(A) Ã— op(B) + Î² Ã— C\n",
    "```\n",
    "\n",
    "### Common cuBLAS Functions\n",
    "\n",
    "```cpp\n",
    "// Level 1: Vector operations\n",
    "cublasSaxpy(handle, n, &alpha, x, 1, y, 1);  // y = Î±*x + y\n",
    "cublasSdot(handle, n, x, 1, y, 1, &result);  // result = xÂ·y\n",
    "cublasSnrm2(handle, n, x, 1, &result);       // result = ||x||â‚‚\n",
    "\n",
    "// Level 2: Matrix-vector\n",
    "cublasSgemv(handle, trans, m, n, &alpha, A, lda, x, 1, &beta, y, 1);\n",
    "\n",
    "// Level 3: Matrix-matrix\n",
    "cublasSgemm(handle, transA, transB, m, n, k, ...);  // C = A*B\n",
    "cublasSsyrk(handle, uplo, trans, n, k, ...);        // C = A*A^T\n",
    "cublasStrsm(handle, side, uplo, trans, diag, ...);  // Triangular solve\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c106e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cublas_quick_reference():\n",
    "    \"\"\"Quick reference for cuBLAS operations.\"\"\"\n",
    "    print(\"cuBLAS Quick Reference\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"LEVEL 1 (vector-vector):\")\n",
    "    print(\"  Saxpy:  y = Î±*x + y              O(n)\")\n",
    "    print(\"  Sdot:   s = x^T * y              O(n)\")\n",
    "    print(\"  Snrm2:  s = ||x||_2              O(n)\")\n",
    "    print(\"  Sscal:  x = Î±*x                  O(n)\")\n",
    "    print()\n",
    "    print(\"LEVEL 2 (matrix-vector):\")\n",
    "    print(\"  Sgemv:  y = Î±*A*x + Î²*y          O(nÂ²)\")\n",
    "    print(\"  Ssymv:  y = Î±*A*x + Î²*y (symm)   O(nÂ²)\")\n",
    "    print(\"  Strsv:  x = A^{-1}*b (triangular)O(nÂ²)\")\n",
    "    print()\n",
    "    print(\"LEVEL 3 (matrix-matrix):\")\n",
    "    print(\"  Sgemm:  C = Î±*A*B + Î²*C          O(nÂ³)\")\n",
    "    print(\"  Ssyrk:  C = Î±*A*A^T + Î²*C        O(nÂ³)\")\n",
    "    print(\"  Strsm:  B = Î±*A^{-1}*B           O(nÂ³)\")\n",
    "    print()\n",
    "    print(\"Prefixes: S=float, D=double, C=complex, Z=double-complex\")\n",
    "\n",
    "cublas_quick_reference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a69281",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada50c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tiled kernel for comparison\n",
    "TILE_SIZE = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled(A, B, C, M, N, K):\n",
    "    As = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    Bs = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    \n",
    "    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y\n",
    "    row = cuda.blockIdx.y * TILE_SIZE + ty\n",
    "    col = cuda.blockIdx.x * TILE_SIZE + tx\n",
    "    \n",
    "    total = 0.0\n",
    "    \n",
    "    for t in range((K + TILE_SIZE - 1) // TILE_SIZE):\n",
    "        a_col = t * TILE_SIZE + tx\n",
    "        if row < M and a_col < K:\n",
    "            As[ty, tx] = A[row, a_col]\n",
    "        else:\n",
    "            As[ty, tx] = 0.0\n",
    "        \n",
    "        b_row = t * TILE_SIZE + ty\n",
    "        if b_row < K and col < N:\n",
    "            Bs[ty, tx] = B[b_row, col]\n",
    "        else:\n",
    "            Bs[ty, tx] = 0.0\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        for k in range(TILE_SIZE):\n",
    "            total += As[ty, k] * Bs[k, tx]\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    if row < M and col < N:\n",
    "        C[row, col] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance(sizes=[256, 512, 1024, 2048]):\n",
    "    \"\"\"Compare our kernel vs cuBLAS (via CuPy).\"\"\"\n",
    "    if not CUPY_AVAILABLE:\n",
    "        print(\"CuPy not available for cuBLAS comparison\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{'Size':<12} {'Tiled (ms)':<12} {'Tiled GFLOPS':<14} {'cuBLAS (ms)':<12} {'cuBLAS GFLOPS':<14} {'Ratio':<8}\")\n",
    "    print(\"=\" * 85)\n",
    "    \n",
    "    for size in sizes:\n",
    "        M = K = N = size\n",
    "        flops = 2 * M * N * K\n",
    "        \n",
    "        # Numba tiled kernel\n",
    "        A_np = np.random.rand(M, K).astype(np.float32)\n",
    "        B_np = np.random.rand(K, N).astype(np.float32)\n",
    "        C_np = np.zeros((M, N), dtype=np.float32)\n",
    "        \n",
    "        d_A = cuda.to_device(A_np)\n",
    "        d_B = cuda.to_device(B_np)\n",
    "        d_C = cuda.to_device(C_np)\n",
    "        \n",
    "        grid = ((N + TILE_SIZE - 1) // TILE_SIZE, (M + TILE_SIZE - 1) // TILE_SIZE)\n",
    "        block = (TILE_SIZE, TILE_SIZE)\n",
    "        \n",
    "        # Warmup\n",
    "        matmul_tiled[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        iterations = 20\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            matmul_tiled[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "        cuda.synchronize()\n",
    "        tiled_ms = (time.perf_counter() - start) / iterations * 1000\n",
    "        tiled_gflops = flops / (tiled_ms * 1e6)\n",
    "        \n",
    "        # CuPy (cuBLAS)\n",
    "        A_cp = cp.asarray(A_np)\n",
    "        B_cp = cp.asarray(B_np)\n",
    "        \n",
    "        # Warmup\n",
    "        C_cp = A_cp @ B_cp\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        \n",
    "        start_ev = cp.cuda.Event()\n",
    "        end_ev = cp.cuda.Event()\n",
    "        \n",
    "        start_ev.record()\n",
    "        for _ in range(iterations):\n",
    "            C_cp = A_cp @ B_cp\n",
    "        end_ev.record()\n",
    "        end_ev.synchronize()\n",
    "        \n",
    "        cublas_ms = cp.cuda.get_elapsed_time(start_ev, end_ev) / iterations\n",
    "        cublas_gflops = flops / (cublas_ms * 1e6)\n",
    "        \n",
    "        ratio = cublas_gflops / tiled_gflops\n",
    "        \n",
    "        print(f\"{size}Ã—{size}Ã—{size:<4} {tiled_ms:<12.3f} {tiled_gflops:<14.1f} {cublas_ms:<12.3f} {cublas_gflops:<14.1f} {ratio:.1f}x\")\n",
    "\n",
    "try:\n",
    "    compare_performance()\n",
    "except Exception as e:\n",
    "    print(f\"Comparison failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23464e4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: cuBLAS Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cublas_best_practices():\n",
    "    \"\"\"Best practices for cuBLAS usage.\"\"\"\n",
    "    print(\"cuBLAS Best Practices\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"1. HANDLE MANAGEMENT\")\n",
    "    print(\"   âœ“ Create handle once, reuse for all operations\")\n",
    "    print(\"   âœ— Don't create/destroy handle per call\")\n",
    "    print()\n",
    "    print(\"2. MEMORY LAYOUT\")\n",
    "    print(\"   cuBLAS uses column-major (Fortran) order\")\n",
    "    print(\"   For row-major (C/C++): C = A*B â†’ call gemm(B,A)\")\n",
    "    print()\n",
    "    print(\"3. MEMORY ALIGNMENT\")\n",
    "    print(\"   cudaMalloc provides aligned memory (good)\")\n",
    "    print(\"   For submatrices, ensure lda is multiple of 32\")\n",
    "    print()\n",
    "    print(\"4. BATCH OPERATIONS\")\n",
    "    print(\"   Use cublasSgemmBatched for many small GEMMs\")\n",
    "    print(\"   Use cublasSgemmStridedBatched for uniform strides\")\n",
    "    print()\n",
    "    print(\"5. TENSOR CORES (Ampere+)\")\n",
    "    print(\"   Use cublasGemmEx with CUBLAS_COMPUTE_32F_FAST_TF32\")\n",
    "    print(\"   Or half precision for maximum Tensor Core usage\")\n",
    "    print()\n",
    "    print(\"6. ERROR CHECKING\")\n",
    "    print(\"   Always check cublasStatus_t return values\")\n",
    "    print(\"   Use cublasGetStatusString for error messages\")\n",
    "\n",
    "cublas_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738428bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete cuBLAS Example\n",
    "\n",
    "### CUDA C++ Production Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f168e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_complete.cu\n",
    "// cublas_complete.cu - Production-ready cuBLAS usage\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "#define CHECK_CUBLAS(call) { \\\n",
    "    cublasStatus_t stat = call; \\\n",
    "    if (stat != CUBLAS_STATUS_SUCCESS) { \\\n",
    "        fprintf(stderr, \"cuBLAS error at %s:%d: %d\\n\", __FILE__, __LINE__, stat); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "class CuBLASGemm {\n",
    "private:\n",
    "    cublasHandle_t handle;\n",
    "    \n",
    "public:\n",
    "    CuBLASGemm() {\n",
    "        CHECK_CUBLAS(cublasCreate(&handle));\n",
    "    }\n",
    "    \n",
    "    ~CuBLASGemm() {\n",
    "        cublasDestroy(handle);\n",
    "    }\n",
    "    \n",
    "    // Row-major: C = alpha * A * B + beta * C\n",
    "    void gemm(int M, int N, int K,\n",
    "              float alpha, const float* A, const float* B,\n",
    "              float beta, float* C) {\n",
    "        // For row-major, swap A and B and swap dimensions\n",
    "        CHECK_CUBLAS(cublasSgemm(handle,\n",
    "                                  CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "                                  N, M, K,\n",
    "                                  &alpha,\n",
    "                                  B, N,\n",
    "                                  A, K,\n",
    "                                  &beta,\n",
    "                                  C, N));\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    CuBLASGemm gemm;\n",
    "    \n",
    "    int M = 1024, K = 512, N = 2048;\n",
    "    \n",
    "    // Allocate and initialize host memory\n",
    "    size_t size_A = M * K * sizeof(float);\n",
    "    size_t size_B = K * N * sizeof(float);\n",
    "    size_t size_C = M * N * sizeof(float);\n",
    "    \n",
    "    float *h_A = (float*)malloc(size_A);\n",
    "    float *h_B = (float*)malloc(size_B);\n",
    "    float *h_C = (float*)malloc(size_C);\n",
    "    \n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = rand() / (float)RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = rand() / (float)RAND_MAX;\n",
    "    \n",
    "    float *d_A, *d_B, *d_C;\n",
    "    CHECK_CUDA(cudaMalloc(&d_A, size_A));\n",
    "    CHECK_CUDA(cudaMalloc(&d_B, size_B));\n",
    "    CHECK_CUDA(cudaMalloc(&d_C, size_C));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Perform GEMM\n",
    "    gemm.gemm(M, N, K, 1.0f, d_A, d_B, 0.0f, d_C);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"cuBLAS Complete Example: %dÃ—%d @ %dÃ—%d = %dÃ—%d\\n\", M, K, K, N, M, N);\n",
    "    printf(\"GEMM completed successfully!\\n\");\n",
    "    \n",
    "    CHECK_CUDA(cudaFree(d_A));\n",
    "    CHECK_CUDA(cudaFree(d_B));\n",
    "    CHECK_CUDA(cudaFree(d_C));\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cublas_complete cublas_complete.cu -lcublas\n",
    "!./cublas_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8398b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "#define CHECK_CUBLAS(call) \\\n",
    "    do { \\\n",
    "        cublasStatus_t status = call; \\\n",
    "        if (status != CUBLAS_STATUS_SUCCESS) { \\\n",
    "            printf(\"cuBLAS Error at line %d\\n\", __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: Batched GEMM\n",
    "// ============================================================\n",
    "void testBatchedGemm() {\n",
    "    printf(\"=== Exercise 1: Batched GEMM ===\\n\");\n",
    "    \n",
    "    const int batchCount = 64;\n",
    "    const int M = 128, K = 128, N = 128;\n",
    "    const float alpha = 1.0f, beta = 0.0f;\n",
    "    \n",
    "    cublasHandle_t handle;\n",
    "    CHECK_CUBLAS(cublasCreate(&handle));\n",
    "    \n",
    "    // Allocate arrays of pointers\n",
    "    float **h_Aarray = (float**)malloc(batchCount * sizeof(float*));\n",
    "    float **h_Barray = (float**)malloc(batchCount * sizeof(float*));\n",
    "    float **h_Carray = (float**)malloc(batchCount * sizeof(float*));\n",
    "    \n",
    "    // Allocate device memory for each matrix in batch\n",
    "    for (int i = 0; i < batchCount; i++) {\n",
    "        CHECK_CUDA(cudaMalloc(&h_Aarray[i], M * K * sizeof(float)));\n",
    "        CHECK_CUDA(cudaMalloc(&h_Barray[i], K * N * sizeof(float)));\n",
    "        CHECK_CUDA(cudaMalloc(&h_Carray[i], M * N * sizeof(float)));\n",
    "    }\n",
    "    \n",
    "    // Copy pointer arrays to device\n",
    "    float **d_Aarray, **d_Barray, **d_Carray;\n",
    "    CHECK_CUDA(cudaMalloc(&d_Aarray, batchCount * sizeof(float*)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_Barray, batchCount * sizeof(float*)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_Carray, batchCount * sizeof(float*)));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_Aarray, h_Aarray, batchCount * sizeof(float*), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_Barray, h_Barray, batchCount * sizeof(float*), cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_Carray, h_Carray, batchCount * sizeof(float*), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Warmup\n",
    "    CHECK_CUBLAS(cublasSgemmBatched(handle,\n",
    "        CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "        N, M, K,\n",
    "        &alpha,\n",
    "        (const float**)d_Barray, N,\n",
    "        (const float**)d_Aarray, K,\n",
    "        &beta,\n",
    "        d_Carray, N,\n",
    "        batchCount));\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int iter = 0; iter < 100; iter++) {\n",
    "        CHECK_CUBLAS(cublasSgemmBatched(handle,\n",
    "            CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "            N, M, K,\n",
    "            &alpha,\n",
    "            (const float**)d_Barray, N,\n",
    "            (const float**)d_Aarray, K,\n",
    "            &beta,\n",
    "            d_Carray, N,\n",
    "            batchCount));\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    double gflops = (2.0 * M * N * K * batchCount * 100) / (ms * 1e6);\n",
    "    printf(\"Batch size: %d, Matrix: %dx%dx%d\\n\", batchCount, M, K, N);\n",
    "    printf(\"Time (100 iters): %.2f ms, Performance: %.2f GFLOPS\\n\\n\", ms, gflops);\n",
    "    \n",
    "    // Cleanup\n",
    "    for (int i = 0; i < batchCount; i++) {\n",
    "        cudaFree(h_Aarray[i]);\n",
    "        cudaFree(h_Barray[i]);\n",
    "        cudaFree(h_Carray[i]);\n",
    "    }\n",
    "    cudaFree(d_Aarray);\n",
    "    cudaFree(d_Barray);\n",
    "    cudaFree(d_Carray);\n",
    "    free(h_Aarray);\n",
    "    free(h_Barray);\n",
    "    free(h_Carray);\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cublasDestroy(handle);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Strided Batched GEMM (More Efficient)\n",
    "// ============================================================\n",
    "void testStridedBatchedGemm() {\n",
    "    printf(\"=== Exercise 2: Strided Batched GEMM ===\\n\");\n",
    "    \n",
    "    const int batchCount = 64;\n",
    "    const int M = 128, K = 128, N = 128;\n",
    "    const float alpha = 1.0f, beta = 0.0f;\n",
    "    \n",
    "    cublasHandle_t handle;\n",
    "    CHECK_CUBLAS(cublasCreate(&handle));\n",
    "    \n",
    "    // Single contiguous allocation for all matrices\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    size_t strideA = M * K;\n",
    "    size_t strideB = K * N;\n",
    "    size_t strideC = M * N;\n",
    "    \n",
    "    CHECK_CUDA(cudaMalloc(&d_A, strideA * batchCount * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_B, strideB * batchCount * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_C, strideC * batchCount * sizeof(float)));\n",
    "    \n",
    "    // Warmup\n",
    "    CHECK_CUBLAS(cublasSgemmStridedBatched(handle,\n",
    "        CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "        N, M, K,\n",
    "        &alpha,\n",
    "        d_B, N, strideB,\n",
    "        d_A, K, strideA,\n",
    "        &beta,\n",
    "        d_C, N, strideC,\n",
    "        batchCount));\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int iter = 0; iter < 100; iter++) {\n",
    "        CHECK_CUBLAS(cublasSgemmStridedBatched(handle,\n",
    "            CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "            N, M, K,\n",
    "            &alpha,\n",
    "            d_B, N, strideB,\n",
    "            d_A, K, strideA,\n",
    "            &beta,\n",
    "            d_C, N, strideC,\n",
    "            batchCount));\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    \n",
    "    double gflops = (2.0 * M * N * K * batchCount * 100) / (ms * 1e6);\n",
    "    printf(\"Strided batch: %d, Matrix: %dx%dx%d\\n\", batchCount, M, K, N);\n",
    "    printf(\"Time (100 iters): %.2f ms, Performance: %.2f GFLOPS\\n\\n\", ms, gflops);\n",
    "    \n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cublasDestroy(handle);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 3: GEMV (Matrix-Vector) Performance\n",
    "// ============================================================\n",
    "void testGemv() {\n",
    "    printf(\"=== Exercise 3: GEMV Performance ===\\n\");\n",
    "    \n",
    "    cublasHandle_t handle;\n",
    "    CHECK_CUBLAS(cublasCreate(&handle));\n",
    "    \n",
    "    const float alpha = 1.0f, beta = 0.0f;\n",
    "    \n",
    "    int sizes[] = {1024, 2048, 4096, 8192};\n",
    "    int numSizes = sizeof(sizes) / sizeof(sizes[0]);\n",
    "    \n",
    "    printf(\"%-10s %-15s %-15s\\n\", \"Size\", \"Time (ms)\", \"GB/s\");\n",
    "    printf(\"----------------------------------------\\n\");\n",
    "    \n",
    "    for (int s = 0; s < numSizes; s++) {\n",
    "        int M = sizes[s];\n",
    "        int N = sizes[s];\n",
    "        \n",
    "        float *d_A, *d_x, *d_y;\n",
    "        CHECK_CUDA(cudaMalloc(&d_A, M * N * sizeof(float)));\n",
    "        CHECK_CUDA(cudaMalloc(&d_x, N * sizeof(float)));\n",
    "        CHECK_CUDA(cudaMalloc(&d_y, M * sizeof(float)));\n",
    "        \n",
    "        // Warmup\n",
    "        CHECK_CUBLAS(cublasSgemv(handle, CUBLAS_OP_N, M, N, &alpha, d_A, M, d_x, 1, &beta, d_y, 1));\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());\n",
    "        \n",
    "        cudaEvent_t start, stop;\n",
    "        cudaEventCreate(&start);\n",
    "        cudaEventCreate(&stop);\n",
    "        \n",
    "        int iters = 100;\n",
    "        cudaEventRecord(start);\n",
    "        for (int i = 0; i < iters; i++) {\n",
    "            CHECK_CUBLAS(cublasSgemv(handle, CUBLAS_OP_N, M, N, &alpha, d_A, M, d_x, 1, &beta, d_y, 1));\n",
    "        }\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float ms;\n",
    "        cudaEventElapsedTime(&ms, start, stop);\n",
    "        \n",
    "        // Memory: read M*N + N, write M\n",
    "        double bytes = ((double)M * N + N + M) * sizeof(float) * iters;\n",
    "        double gbps = bytes / (ms * 1e6);\n",
    "        \n",
    "        printf(\"%-10d %-15.2f %-15.2f\\n\", M, ms, gbps);\n",
    "        \n",
    "        cudaFree(d_A);\n",
    "        cudaFree(d_x);\n",
    "        cudaFree(d_y);\n",
    "        cudaEventDestroy(start);\n",
    "        cudaEventDestroy(stop);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cublasDestroy(handle);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘              cuBLAS Exercises                                â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\\n\", prop.name);\n",
    "    \n",
    "    testBatchedGemm();\n",
    "    testStridedBatchedGemm();\n",
    "    testGemv();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26879dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cublas_exercises cublas_exercises.cu -lcublas && ./cublas_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa5e67",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/CuPy Exercises (Optional)\n",
    "\n",
    "### Exercise 1: Batched GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abcb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use CuPy to perform batched matrix multiplication\n",
    "# Compare performance: loop vs batched\n",
    "\n",
    "def batched_gemm_comparison():\n",
    "    \"\"\"Compare loop GEMM vs batched GEMM.\"\"\"\n",
    "    if not CUPY_AVAILABLE:\n",
    "        print(\"CuPy required for this exercise\")\n",
    "        return\n",
    "    \n",
    "    batch_size = 64\n",
    "    M = K = N = 128\n",
    "    \n",
    "    # Create batched matrices\n",
    "    A = cp.random.rand(batch_size, M, K).astype(cp.float32)\n",
    "    B = cp.random.rand(batch_size, K, N).astype(cp.float32)\n",
    "    \n",
    "    # Loop approach\n",
    "    C_loop = [A[i] @ B[i] for i in range(batch_size)]\n",
    "    \n",
    "    # Batched approach (CuPy uses batched GEMM automatically)\n",
    "    C_batch = A @ B  # This uses batched cuBLAS under the hood\n",
    "    \n",
    "    print(f\"Batch size: {batch_size}, Matrix size: {M}Ã—{K}Ã—{N}\")\n",
    "    # TODO: Add timing comparison\n",
    "\n",
    "batched_gemm_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5517b",
   "metadata": {},
   "source": [
    "### Exercise 2: Mixed Precision GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38113372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare float32 vs float16 GEMM performance\n",
    "# Note: float16 uses Tensor Cores on supported hardware\n",
    "\n",
    "def mixed_precision_comparison():\n",
    "    if not CUPY_AVAILABLE:\n",
    "        print(\"CuPy required\")\n",
    "        return\n",
    "    \n",
    "    M = K = N = 2048\n",
    "    \n",
    "    # Float32\n",
    "    A32 = cp.random.rand(M, K).astype(cp.float32)\n",
    "    B32 = cp.random.rand(K, N).astype(cp.float32)\n",
    "    \n",
    "    # Float16\n",
    "    A16 = A32.astype(cp.float16)\n",
    "    B16 = B32.astype(cp.float16)\n",
    "    \n",
    "    # TODO: Benchmark and compare\n",
    "    print(\"Mixed precision GEMM comparison\")\n",
    "\n",
    "mixed_precision_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afc796",
   "metadata": {},
   "source": [
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### Performance Comparison: Week 6 Journey\n",
    "\n",
    "| Implementation | % of Peak | Notes |\n",
    "|---------------|-----------|-------|\n",
    "| Naive matmul (Day 1) | 5-10% | Memory bound, massive redundancy |\n",
    "| Tiled matmul (Day 2) | 20-40% | Shared memory reuse |\n",
    "| Optimized transpose (Day 3) | 90-95% | Near peak bandwidth |\n",
    "| **cuBLAS GEMM** (Day 4) | **90-95%** | Decades of optimization |\n",
    "\n",
    "### When to Use cuBLAS\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Standard BLAS operations | Always use cuBLAS |\n",
    "| Custom fused kernels | Write custom CUDA |\n",
    "| Batch small matrices | cublasBatched variants |\n",
    "| Maximum throughput | Mixed precision + Tensor Cores |\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **cuBLAS is highly optimized** - usually 5-10x faster than custom kernels\n",
    "2. **Column-major layout** - understand the memory layout conversion\n",
    "3. **Handle management** - create once, reuse many times\n",
    "4. **Tensor Cores** - use half precision or TF32 for maximum performance\n",
    "\n",
    "### CUDA C++ Pattern\n",
    "\n",
    "```cpp\n",
    "cublasHandle_t handle;\n",
    "cublasCreate(&handle);\n",
    "\n",
    "// Row-major C = A Ã— B (with cuBLAS column-major conversion)\n",
    "cublasSgemm(handle,\n",
    "            CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "            N, M, K,         // Note: N, M, K order (column-major)\n",
    "            &alpha,\n",
    "            B, N,            // B first, ldb=N\n",
    "            A, K,            // A second, lda=K\n",
    "            &beta,\n",
    "            C, N);           // C, ldc=N\n",
    "\n",
    "cublasDestroy(handle);\n",
    "```\n",
    "\n",
    "### ğŸ”— Connection to Tomorrow\n",
    "\n",
    "cuBLAS handles standard operations, but what about advanced coordination patterns? Tomorrow we explore **Cooperative Groups**â€”CUDA's flexible API for complex synchronization beyond `__syncthreads()`.\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "**Day 5: Advanced Cooperative Groups** - Team coordination\n",
    "- Grid-wide synchronization with `grid.sync()`\n",
    "- Warp-level primitives: `cg::reduce()`, `cg::inclusive_scan()`\n",
    "- Flexible thread partitioning with `tiled_partition<N>()`\n",
    "\n",
    "---\n",
    "\n",
    "## Week 6 Complete!\n",
    "\n",
    "You've mastered the full matrix operations stack:\n",
    "1. **Naive â†’ Tiled â†’ Optimized** kernel development\n",
    "2. **Memory coalescing** and **bank conflict avoidance**\n",
    "3. **Library integration** for production code\n",
    "\n",
    "Next week: **Memory Optimization Deep Dive** (occupancy, caching, unified memory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
