{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ec7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is the PRIMARY learning material!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899c68c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: cuBLAS Overview\n",
    "\n",
    "### What is cuBLAS?\n",
    "\n",
    "```\n",
    "cuBLAS = CUDA Basic Linear Algebra Subroutines\n",
    "\n",
    "Features:\n",
    "- GPU-accelerated BLAS (levels 1, 2, 3)\n",
    "- Highly optimized for NVIDIA hardware\n",
    "- Tensor Core acceleration (GEMM)\n",
    "- Near-peak performance\n",
    "\n",
    "BLAS Levels:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Level   â”‚ Operations               â”‚ Complexity  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Level 1 â”‚ Vector-Vector (dot, axpy)â”‚ O(n)        â”‚\n",
    "â”‚ Level 2 â”‚ Matrix-Vector (gemv)     â”‚ O(nÂ²)       â”‚\n",
    "â”‚ Level 3 â”‚ Matrix-Matrix (gemm)     â”‚ O(nÂ³)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### BLAS Naming Convention\n",
    "\n",
    "```\n",
    "Example: cublasSgemm, cublasDgemm\n",
    "\n",
    "Pattern: cublas[Precision][Operation]\n",
    "\n",
    "Precision codes:\n",
    "  S = float (single precision)\n",
    "  D = double\n",
    "  C = complex float\n",
    "  Z = complex double\n",
    "  H = half precision (FP16)\n",
    "\n",
    "Common operations:\n",
    "  gemm = GEneral Matrix Multiply\n",
    "  gemv = GEneral Matrix Vector\n",
    "  axpy = A*X Plus Y\n",
    "  dot  = DOT product\n",
    "```\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "### ğŸ”¶ Python/CuPy (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba1309",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_gemm.cu\n",
    "// cublas_gemm.cu - Matrix multiplication with cuBLAS\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "\n",
    "int main() {\n",
    "    int M = 1024, N = 1024, K = 1024;\n",
    "    \n",
    "    size_t size_A = M * K * sizeof(float);\n",
    "    size_t size_B = K * N * sizeof(float);\n",
    "    size_t size_C = M * N * sizeof(float);\n",
    "    \n",
    "    // Host memory\n",
    "    float *h_A = (float*)malloc(size_A);\n",
    "    float *h_B = (float*)malloc(size_B);\n",
    "    float *h_C = (float*)malloc(size_C);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = rand() / (float)RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = rand() / (float)RAND_MAX;\n",
    "    \n",
    "    // Device memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, size_A);\n",
    "    cudaMalloc(&d_B, size_B);\n",
    "    cudaMalloc(&d_C, size_C);\n",
    "    \n",
    "    cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Create cuBLAS handle\n",
    "    cublasHandle_t handle;\n",
    "    cublasCreate(&handle);\n",
    "    \n",
    "    // GEMM: C = alpha * A * B + beta * C\n",
    "    float alpha = 1.0f;\n",
    "    float beta = 0.0f;\n",
    "    \n",
    "    // Note: cuBLAS uses column-major order!\n",
    "    // For row-major C = A * B, we compute C^T = B^T * A^T\n",
    "    // cublasSgemm(handle, transB, transA, N, M, K, ...)\n",
    "    \n",
    "    // Warmup\n",
    "    cublasSgemm(handle,\n",
    "                CUBLAS_OP_N, CUBLAS_OP_N,  // No transpose\n",
    "                N, M, K,                    // Dimensions\n",
    "                &alpha,\n",
    "                d_B, N,                     // B and leading dimension\n",
    "                d_A, K,                     // A and leading dimension\n",
    "                &beta,\n",
    "                d_C, N);                    // C and leading dimension\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Benchmark\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        cublasSgemm(handle,\n",
    "                    CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "                    N, M, K,\n",
    "                    &alpha, d_B, N, d_A, K,\n",
    "                    &beta, d_C, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    ms /= 100;\n",
    "    \n",
    "    double flops = 2.0 * M * N * K;\n",
    "    double tflops = flops / (ms * 1e9);\n",
    "    \n",
    "    printf(\"cuBLAS SGEMM: %dÃ—%dÃ—%d\\n\", M, K, N);\n",
    "    printf(\"Time: %.3f ms\\n\", ms);\n",
    "    printf(\"Performance: %.2f TFLOPS\\n\", tflops);\n",
    "    \n",
    "    // Cleanup\n",
    "    cublasDestroy(handle);\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "// Compile: nvcc cublas_gemm.cu -o cublas_gemm -lcublas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cublas_gemm cublas_gemm.cu -lcublas\n",
    "!./cublas_gemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe718d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CuPy provides easy access to cuBLAS\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"CuPy not available - install with: pip install cupy-cuda12x\")\n",
    "    CUPY_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    # CuPy uses cuBLAS under the hood\n",
    "    M, K, N = 1024, 1024, 1024\n",
    "    \n",
    "    A = cp.random.rand(M, K).astype(cp.float32)\n",
    "    B = cp.random.rand(K, N).astype(cp.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    C = A @ B\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = cp.cuda.Event()\n",
    "    end = cp.cuda.Event()\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(100):\n",
    "        C = A @ B\n",
    "    end.record()\n",
    "    end.synchronize()\n",
    "    \n",
    "    ms = cp.cuda.get_elapsed_time(start, end) / 100\n",
    "    flops = 2 * M * N * K\n",
    "    tflops = flops / (ms * 1e9)\n",
    "    \n",
    "    print(f\"CuPy (cuBLAS) GEMM: {M}Ã—{K}Ã—{N}\")\n",
    "    print(f\"Time: {ms:.3f} ms\")\n",
    "    print(f\"Performance: {tflops:.2f} TFLOPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f861c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Row-Major vs Column-Major\n",
    "\n",
    "### The Column-Major Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea434a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_column_major():\n",
    "    \"\"\"Explain cuBLAS column-major layout.\"\"\"\n",
    "    print(\"cuBLAS Column-Major Layout\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"Row-major (C/C++, NumPy default):\")\n",
    "    print(\"  A = [[1, 2, 3],    Memory: [1,2,3,4,5,6]\")\n",
    "    print(\"       [4, 5, 6]]\")\n",
    "    print()\n",
    "    print(\"Column-major (Fortran, cuBLAS):\")\n",
    "    print(\"  A = [[1, 2, 3],    Memory: [1,4,2,5,3,6]\")\n",
    "    print(\"       [4, 5, 6]]\")\n",
    "    print()\n",
    "    print(\"Solutions for row-major data:\")\n",
    "    print(\"  1. Transpose before/after (memory overhead)\")\n",
    "    print(\"  2. Use CUBLAS_OP_T to transpose on-the-fly\")\n",
    "    print(\"  3. Compute C = B^T Ã— A^T (equals (AÃ—B)^T)\")\n",
    "    print()\n",
    "    print(\"Trick: For row-major C = A Ã— B:\")\n",
    "    print(\"  Call gemm(B, A) instead of gemm(A, B)\")\n",
    "    print(\"  Because in column-major: C^T = B^T Ã— A^T\")\n",
    "\n",
    "explain_column_major()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de7c77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: cuBLAS API Deep Dive\n",
    "\n",
    "### GEMM Parameters\n",
    "\n",
    "```cpp\n",
    "cublasStatus_t cublasSgemm(\n",
    "    cublasHandle_t handle,      // cuBLAS context\n",
    "    cublasOperation_t transa,   // CUBLAS_OP_N, CUBLAS_OP_T, CUBLAS_OP_C\n",
    "    cublasOperation_t transb,\n",
    "    int m,                      // Rows of op(A) and C\n",
    "    int n,                      // Columns of op(B) and C\n",
    "    int k,                      // Columns of op(A), rows of op(B)\n",
    "    const float *alpha,         // Scalar Î±\n",
    "    const float *A, int lda,    // A and its leading dimension\n",
    "    const float *B, int ldb,    // B and its leading dimension\n",
    "    const float *beta,          // Scalar Î²\n",
    "    float *C, int ldc           // C and its leading dimension\n",
    ");\n",
    "\n",
    "// Computes: C = Î± Ã— op(A) Ã— op(B) + Î² Ã— C\n",
    "```\n",
    "\n",
    "### Common cuBLAS Functions\n",
    "\n",
    "```cpp\n",
    "// Level 1: Vector operations\n",
    "cublasSaxpy(handle, n, &alpha, x, 1, y, 1);  // y = Î±*x + y\n",
    "cublasSdot(handle, n, x, 1, y, 1, &result);  // result = xÂ·y\n",
    "cublasSnrm2(handle, n, x, 1, &result);       // result = ||x||â‚‚\n",
    "\n",
    "// Level 2: Matrix-vector\n",
    "cublasSgemv(handle, trans, m, n, &alpha, A, lda, x, 1, &beta, y, 1);\n",
    "\n",
    "// Level 3: Matrix-matrix\n",
    "cublasSgemm(handle, transA, transB, m, n, k, ...);  // C = A*B\n",
    "cublasSsyrk(handle, uplo, trans, n, k, ...);        // C = A*A^T\n",
    "cublasStrsm(handle, side, uplo, trans, diag, ...);  // Triangular solve\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c106e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cublas_quick_reference():\n",
    "    \"\"\"Quick reference for cuBLAS operations.\"\"\"\n",
    "    print(\"cuBLAS Quick Reference\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"LEVEL 1 (vector-vector):\")\n",
    "    print(\"  Saxpy:  y = Î±*x + y              O(n)\")\n",
    "    print(\"  Sdot:   s = x^T * y              O(n)\")\n",
    "    print(\"  Snrm2:  s = ||x||_2              O(n)\")\n",
    "    print(\"  Sscal:  x = Î±*x                  O(n)\")\n",
    "    print()\n",
    "    print(\"LEVEL 2 (matrix-vector):\")\n",
    "    print(\"  Sgemv:  y = Î±*A*x + Î²*y          O(nÂ²)\")\n",
    "    print(\"  Ssymv:  y = Î±*A*x + Î²*y (symm)   O(nÂ²)\")\n",
    "    print(\"  Strsv:  x = A^{-1}*b (triangular)O(nÂ²)\")\n",
    "    print()\n",
    "    print(\"LEVEL 3 (matrix-matrix):\")\n",
    "    print(\"  Sgemm:  C = Î±*A*B + Î²*C          O(nÂ³)\")\n",
    "    print(\"  Ssyrk:  C = Î±*A*A^T + Î²*C        O(nÂ³)\")\n",
    "    print(\"  Strsm:  B = Î±*A^{-1}*B           O(nÂ³)\")\n",
    "    print()\n",
    "    print(\"Prefixes: S=float, D=double, C=complex, Z=double-complex\")\n",
    "\n",
    "cublas_quick_reference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a69281",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada50c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tiled kernel for comparison\n",
    "TILE_SIZE = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_tiled(A, B, C, M, N, K):\n",
    "    As = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    Bs = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    \n",
    "    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y\n",
    "    row = cuda.blockIdx.y * TILE_SIZE + ty\n",
    "    col = cuda.blockIdx.x * TILE_SIZE + tx\n",
    "    \n",
    "    total = 0.0\n",
    "    \n",
    "    for t in range((K + TILE_SIZE - 1) // TILE_SIZE):\n",
    "        a_col = t * TILE_SIZE + tx\n",
    "        if row < M and a_col < K:\n",
    "            As[ty, tx] = A[row, a_col]\n",
    "        else:\n",
    "            As[ty, tx] = 0.0\n",
    "        \n",
    "        b_row = t * TILE_SIZE + ty\n",
    "        if b_row < K and col < N:\n",
    "            Bs[ty, tx] = B[b_row, col]\n",
    "        else:\n",
    "            Bs[ty, tx] = 0.0\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        for k in range(TILE_SIZE):\n",
    "            total += As[ty, k] * Bs[k, tx]\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    if row < M and col < N:\n",
    "        C[row, col] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance(sizes=[256, 512, 1024, 2048]):\n",
    "    \"\"\"Compare our kernel vs cuBLAS (via CuPy).\"\"\"\n",
    "    if not CUPY_AVAILABLE:\n",
    "        print(\"CuPy not available for cuBLAS comparison\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{'Size':<12} {'Tiled (ms)':<12} {'Tiled GFLOPS':<14} {'cuBLAS (ms)':<12} {'cuBLAS GFLOPS':<14} {'Ratio':<8}\")\n",
    "    print(\"=\" * 85)\n",
    "    \n",
    "    for size in sizes:\n",
    "        M = K = N = size\n",
    "        flops = 2 * M * N * K\n",
    "        \n",
    "        # Numba tiled kernel\n",
    "        A_np = np.random.rand(M, K).astype(np.float32)\n",
    "        B_np = np.random.rand(K, N).astype(np.float32)\n",
    "        C_np = np.zeros((M, N), dtype=np.float32)\n",
    "        \n",
    "        d_A = cuda.to_device(A_np)\n",
    "        d_B = cuda.to_device(B_np)\n",
    "        d_C = cuda.to_device(C_np)\n",
    "        \n",
    "        grid = ((N + TILE_SIZE - 1) // TILE_SIZE, (M + TILE_SIZE - 1) // TILE_SIZE)\n",
    "        block = (TILE_SIZE, TILE_SIZE)\n",
    "        \n",
    "        # Warmup\n",
    "        matmul_tiled[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        iterations = 20\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            matmul_tiled[grid, block](d_A, d_B, d_C, M, N, K)\n",
    "        cuda.synchronize()\n",
    "        tiled_ms = (time.perf_counter() - start) / iterations * 1000\n",
    "        tiled_gflops = flops / (tiled_ms * 1e6)\n",
    "        \n",
    "        # CuPy (cuBLAS)\n",
    "        A_cp = cp.asarray(A_np)\n",
    "        B_cp = cp.asarray(B_np)\n",
    "        \n",
    "        # Warmup\n",
    "        C_cp = A_cp @ B_cp\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        \n",
    "        start_ev = cp.cuda.Event()\n",
    "        end_ev = cp.cuda.Event()\n",
    "        \n",
    "        start_ev.record()\n",
    "        for _ in range(iterations):\n",
    "            C_cp = A_cp @ B_cp\n",
    "        end_ev.record()\n",
    "        end_ev.synchronize()\n",
    "        \n",
    "        cublas_ms = cp.cuda.get_elapsed_time(start_ev, end_ev) / iterations\n",
    "        cublas_gflops = flops / (cublas_ms * 1e6)\n",
    "        \n",
    "        ratio = cublas_gflops / tiled_gflops\n",
    "        \n",
    "        print(f\"{size}Ã—{size}Ã—{size:<4} {tiled_ms:<12.3f} {tiled_gflops:<14.1f} {cublas_ms:<12.3f} {cublas_gflops:<14.1f} {ratio:.1f}x\")\n",
    "\n",
    "try:\n",
    "    compare_performance()\n",
    "except Exception as e:\n",
    "    print(f\"Comparison failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23464e4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: cuBLAS Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cublas_best_practices():\n",
    "    \"\"\"Best practices for cuBLAS usage.\"\"\"\n",
    "    print(\"cuBLAS Best Practices\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"1. HANDLE MANAGEMENT\")\n",
    "    print(\"   âœ“ Create handle once, reuse for all operations\")\n",
    "    print(\"   âœ— Don't create/destroy handle per call\")\n",
    "    print()\n",
    "    print(\"2. MEMORY LAYOUT\")\n",
    "    print(\"   cuBLAS uses column-major (Fortran) order\")\n",
    "    print(\"   For row-major (C/C++): C = A*B â†’ call gemm(B,A)\")\n",
    "    print()\n",
    "    print(\"3. MEMORY ALIGNMENT\")\n",
    "    print(\"   cudaMalloc provides aligned memory (good)\")\n",
    "    print(\"   For submatrices, ensure lda is multiple of 32\")\n",
    "    print()\n",
    "    print(\"4. BATCH OPERATIONS\")\n",
    "    print(\"   Use cublasSgemmBatched for many small GEMMs\")\n",
    "    print(\"   Use cublasSgemmStridedBatched for uniform strides\")\n",
    "    print()\n",
    "    print(\"5. TENSOR CORES (Ampere+)\")\n",
    "    print(\"   Use cublasGemmEx with CUBLAS_COMPUTE_32F_FAST_TF32\")\n",
    "    print(\"   Or half precision for maximum Tensor Core usage\")\n",
    "    print()\n",
    "    print(\"6. ERROR CHECKING\")\n",
    "    print(\"   Always check cublasStatus_t return values\")\n",
    "    print(\"   Use cublasGetStatusString for error messages\")\n",
    "\n",
    "cublas_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738428bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete cuBLAS Example\n",
    "\n",
    "### CUDA C++ Production Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f168e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cublas_complete.cu\n",
    "// cublas_complete.cu - Production-ready cuBLAS usage\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "#define CHECK_CUBLAS(call) { \\\n",
    "    cublasStatus_t stat = call; \\\n",
    "    if (stat != CUBLAS_STATUS_SUCCESS) { \\\n",
    "        fprintf(stderr, \"cuBLAS error at %s:%d: %d\\n\", __FILE__, __LINE__, stat); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "class CuBLASGemm {\n",
    "private:\n",
    "    cublasHandle_t handle;\n",
    "    \n",
    "public:\n",
    "    CuBLASGemm() {\n",
    "        CHECK_CUBLAS(cublasCreate(&handle));\n",
    "    }\n",
    "    \n",
    "    ~CuBLASGemm() {\n",
    "        cublasDestroy(handle);\n",
    "    }\n",
    "    \n",
    "    // Row-major: C = alpha * A * B + beta * C\n",
    "    void gemm(int M, int N, int K,\n",
    "              float alpha, const float* A, const float* B,\n",
    "              float beta, float* C) {\n",
    "        // For row-major, swap A and B and swap dimensions\n",
    "        CHECK_CUBLAS(cublasSgemm(handle,\n",
    "                                  CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "                                  N, M, K,\n",
    "                                  &alpha,\n",
    "                                  B, N,\n",
    "                                  A, K,\n",
    "                                  &beta,\n",
    "                                  C, N));\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    CuBLASGemm gemm;\n",
    "    \n",
    "    int M = 1024, K = 512, N = 2048;\n",
    "    \n",
    "    // Allocate and initialize host memory\n",
    "    size_t size_A = M * K * sizeof(float);\n",
    "    size_t size_B = K * N * sizeof(float);\n",
    "    size_t size_C = M * N * sizeof(float);\n",
    "    \n",
    "    float *h_A = (float*)malloc(size_A);\n",
    "    float *h_B = (float*)malloc(size_B);\n",
    "    float *h_C = (float*)malloc(size_C);\n",
    "    \n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = rand() / (float)RAND_MAX;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = rand() / (float)RAND_MAX;\n",
    "    \n",
    "    float *d_A, *d_B, *d_C;\n",
    "    CHECK_CUDA(cudaMalloc(&d_A, size_A));\n",
    "    CHECK_CUDA(cudaMalloc(&d_B, size_B));\n",
    "    CHECK_CUDA(cudaMalloc(&d_C, size_C));\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Perform GEMM\n",
    "    gemm.gemm(M, N, K, 1.0f, d_A, d_B, 0.0f, d_C);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"cuBLAS Complete Example: %dÃ—%d @ %dÃ—%d = %dÃ—%d\\n\", M, K, K, N, M, N);\n",
    "    printf(\"GEMM completed successfully!\\n\");\n",
    "    \n",
    "    CHECK_CUDA(cudaFree(d_A));\n",
    "    CHECK_CUDA(cudaFree(d_B));\n",
    "    CHECK_CUDA(cudaFree(d_C));\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o cublas_complete cublas_complete.cu -lcublas\n",
    "!./cublas_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8398b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Batched GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abcb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use CuPy to perform batched matrix multiplication\n",
    "# Compare performance: loop vs batched\n",
    "\n",
    "def batched_gemm_comparison():\n",
    "    \"\"\"Compare loop GEMM vs batched GEMM.\"\"\"\n",
    "    if not CUPY_AVAILABLE:\n",
    "        print(\"CuPy required for this exercise\")\n",
    "        return\n",
    "    \n",
    "    batch_size = 64\n",
    "    M = K = N = 128\n",
    "    \n",
    "    # Create batched matrices\n",
    "    A = cp.random.rand(batch_size, M, K).astype(cp.float32)\n",
    "    B = cp.random.rand(batch_size, K, N).astype(cp.float32)\n",
    "    \n",
    "    # Loop approach\n",
    "    C_loop = [A[i] @ B[i] for i in range(batch_size)]\n",
    "    \n",
    "    # Batched approach (CuPy uses batched GEMM automatically)\n",
    "    C_batch = A @ B  # This uses batched cuBLAS under the hood\n",
    "    \n",
    "    print(f\"Batch size: {batch_size}, Matrix size: {M}Ã—{K}Ã—{N}\")\n",
    "    # TODO: Add timing comparison\n",
    "\n",
    "batched_gemm_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5517b",
   "metadata": {},
   "source": [
    "### Exercise 2: Mixed Precision GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38113372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare float32 vs float16 GEMM performance\n",
    "# Note: float16 uses Tensor Cores on supported hardware\n",
    "\n",
    "def mixed_precision_comparison():\n",
    "    if not CUPY_AVAILABLE:\n",
    "        print(\"CuPy required\")\n",
    "        return\n",
    "    \n",
    "    M = K = N = 2048\n",
    "    \n",
    "    # Float32\n",
    "    A32 = cp.random.rand(M, K).astype(cp.float32)\n",
    "    B32 = cp.random.rand(K, N).astype(cp.float32)\n",
    "    \n",
    "    # Float16\n",
    "    A16 = A32.astype(cp.float16)\n",
    "    B16 = B32.astype(cp.float16)\n",
    "    \n",
    "    # TODO: Benchmark and compare\n",
    "    print(\"Mixed precision GEMM comparison\")\n",
    "\n",
    "mixed_precision_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afc796",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### When to Use cuBLAS\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Standard BLAS operations | Always use cuBLAS |\n",
    "| Custom fused kernels | Write custom CUDA |\n",
    "| Batch small matrices | cublasBatched variants |\n",
    "| Maximum throughput | Mixed precision + Tensor Cores |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **cuBLAS is highly optimized** - usually 5-10x faster than custom kernels\n",
    "2. **Column-major layout** - understand the memory layout conversion\n",
    "3. **Handle management** - create once, reuse many times\n",
    "4. **Tensor Cores** - use half precision or TF32 for maximum performance\n",
    "\n",
    "### CUDA C++ Pattern\n",
    "\n",
    "```cpp\n",
    "cublasHandle_t handle;\n",
    "cublasCreate(&handle);\n",
    "\n",
    "// Row-major C = A Ã— B\n",
    "cublasSgemm(handle,\n",
    "            CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "            N, M, K,         // Note: N, M, K order\n",
    "            &alpha,\n",
    "            B, N,            // B first, ldb=N\n",
    "            A, K,            // A second, lda=K\n",
    "            &beta,\n",
    "            C, N);           // C, ldc=N\n",
    "\n",
    "cublasDestroy(handle);\n",
    "```\n",
    "\n",
    "### Week 6 Complete!\n",
    "Next week: Memory Optimization Deep Dive (occupancy, caching, unified memory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
