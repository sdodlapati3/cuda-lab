{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7df2282",
   "metadata": {},
   "source": [
    "## Beyond `__syncthreads()`: The Cooperative Groups API\n",
    "\n",
    "Cooperative Groups provides a flexible, composable API for thread synchronization and collective operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_basics_advanced.cu\n",
    "#include <stdio.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/reduce.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void cgDemo() {\n",
    "    // Get thread block group\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    // Partition into tiles of 32 threads (warp-aligned)\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    // Partition into tiles of 16 threads\n",
    "    cg::thread_block_tile<16> half_warp = cg::tiled_partition<16>(block);\n",
    "    \n",
    "    // Get coalesced threads (active threads in warp)\n",
    "    cg::coalesced_group active = cg::coalesced_threads();\n",
    "    \n",
    "    if (block.thread_rank() == 0) {\n",
    "        printf(\"Block size: %d\\n\", block.size());\n",
    "        printf(\"Warp tile size: %d\\n\", warp.size());\n",
    "        printf(\"Half-warp tile size: %d\\n\", half_warp.size());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    cgDemo<<<1, 128>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_basics_advanced.cu -o cg_basics_advanced && ./cg_basics_advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6581634",
   "metadata": {},
   "source": [
    "## CG Collective Operations: `reduce()`\n",
    "\n",
    "Hardware-accelerated reductions with `cg::reduce()` - much faster than manual shuffle reductions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bea1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_reduce.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/reduce.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void cgReduceKernel(int* input, int* output, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    int tid = block.thread_rank();\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load value\n",
    "    int val = (idx < n) ? input[idx] : 0;\n",
    "    \n",
    "    // Warp-level reduction using CG (hardware-accelerated on CC 8.0+)\n",
    "    int warp_sum = cg::reduce(warp, val, cg::plus<int>());\n",
    "    \n",
    "    // First thread in each warp has the sum\n",
    "    __shared__ int warp_sums[32];  // Max 32 warps per block\n",
    "    if (warp.thread_rank() == 0) {\n",
    "        warp_sums[tid / 32] = warp_sum;\n",
    "    }\n",
    "    block.sync();\n",
    "    \n",
    "    // Final reduction of warp sums (first warp only)\n",
    "    if (tid < 32) {\n",
    "        int num_warps = (blockDim.x + 31) / 32;\n",
    "        val = (tid < num_warps) ? warp_sums[tid] : 0;\n",
    "        int block_sum = cg::reduce(warp, val, cg::plus<int>());\n",
    "        \n",
    "        if (tid == 0) {\n",
    "            output[blockIdx.x] = block_sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    int h_input[N], h_output[8];\n",
    "    \n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1;  // Sum should be N\n",
    "    \n",
    "    int *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, N * sizeof(int));\n",
    "    cudaMalloc(&d_output, 8 * sizeof(int));\n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cgReduceKernel<<<8, 128>>>(d_input, d_output, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_output, d_output, 8 * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    int total = 0;\n",
    "    for (int i = 0; i < 8; i++) total += h_output[i];\n",
    "    printf(\"Sum of %d ones = %d (expected %d)\\n\", N, total, N);\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_reduce.cu -o cg_reduce && ./cg_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf19f51",
   "metadata": {},
   "source": [
    "## CG Scan Operations: `inclusive_scan()` and `exclusive_scan()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bded12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_scan.cu\n",
    "#include <stdio.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/scan.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void cgScanKernel(int* input, int* incl_output, int* excl_output, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    int tid = block.thread_rank();\n",
    "    int val = (tid < n) ? input[tid] : 0;\n",
    "    \n",
    "    // Inclusive scan: each element includes itself\n",
    "    // Input:  [1, 2, 3, 4]\n",
    "    // Output: [1, 3, 6, 10]\n",
    "    int incl = cg::inclusive_scan(block, val, cg::plus<int>());\n",
    "    \n",
    "    // Exclusive scan: each element excludes itself\n",
    "    // Input:  [1, 2, 3, 4]\n",
    "    // Output: [0, 1, 3, 6]\n",
    "    int excl = cg::exclusive_scan(block, val, cg::plus<int>());\n",
    "    \n",
    "    if (tid < n) {\n",
    "        incl_output[tid] = incl;\n",
    "        excl_output[tid] = excl;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 8;\n",
    "    int h_input[N] = {1, 2, 3, 4, 5, 6, 7, 8};\n",
    "    int h_incl[N], h_excl[N];\n",
    "    \n",
    "    int *d_input, *d_incl, *d_excl;\n",
    "    cudaMalloc(&d_input, N * sizeof(int));\n",
    "    cudaMalloc(&d_incl, N * sizeof(int));\n",
    "    cudaMalloc(&d_excl, N * sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cgScanKernel<<<1, N>>>(d_input, d_incl, d_excl, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_incl, d_incl, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_excl, d_excl, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Input:          \");\n",
    "    for (int i = 0; i < N; i++) printf(\"%3d \", h_input[i]);\n",
    "    printf(\"\\nInclusive scan: \");\n",
    "    for (int i = 0; i < N; i++) printf(\"%3d \", h_incl[i]);\n",
    "    printf(\"\\nExclusive scan: \");\n",
    "    for (int i = 0; i < N; i++) printf(\"%3d \", h_excl[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_incl);\n",
    "    cudaFree(d_excl);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7037b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_scan.cu -o cg_scan && ./cg_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178cbd05",
   "metadata": {},
   "source": [
    "## Warp-Aggregated Atomics Pattern\n",
    "\n",
    "Reduce atomic contention by having one thread per warp perform the atomic operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_warp_atomics.cu\n",
    "#include <stdio.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/reduce.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "// Naive: every thread does atomic - HIGH CONTENTION\n",
    "__device__ int naiveAtomicInc(int* counter) {\n",
    "    return atomicAdd(counter, 1);\n",
    "}\n",
    "\n",
    "// Optimized: warp-aggregated atomic - MUCH FASTER\n",
    "__device__ int warpAggregatedAtomicInc(int* counter) {\n",
    "    cg::coalesced_group active = cg::coalesced_threads();\n",
    "    \n",
    "    int warp_res;\n",
    "    if (active.thread_rank() == 0) {\n",
    "        // Only leader does the atomic\n",
    "        warp_res = atomicAdd(counter, active.size());\n",
    "    }\n",
    "    \n",
    "    // Broadcast result and compute individual values\n",
    "    warp_res = active.shfl(warp_res, 0);\n",
    "    return warp_res + active.thread_rank();\n",
    "}\n",
    "\n",
    "__global__ void testNaive(int* counter, int* results, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        results[idx] = naiveAtomicInc(counter);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void testWarpAgg(int* counter, int* results, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        results[idx] = warpAggregatedAtomicInc(counter);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1000000;\n",
    "    int *d_counter, *d_results;\n",
    "    \n",
    "    cudaMalloc(&d_counter, sizeof(int));\n",
    "    cudaMalloc(&d_results, N * sizeof(int));\n",
    "    \n",
    "    // Benchmark naive\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaMemset(d_counter, 0, sizeof(int));\n",
    "    cudaEventRecord(start);\n",
    "    testNaive<<<(N+255)/256, 256>>>(d_counter, d_results, N);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float naive_ms;\n",
    "    cudaEventElapsedTime(&naive_ms, start, stop);\n",
    "    \n",
    "    // Benchmark warp-aggregated\n",
    "    cudaMemset(d_counter, 0, sizeof(int));\n",
    "    cudaEventRecord(start);\n",
    "    testWarpAgg<<<(N+255)/256, 256>>>(d_counter, d_results, N);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float warp_ms;\n",
    "    cudaEventElapsedTime(&warp_ms, start, stop);\n",
    "    \n",
    "    printf(\"Naive atomics:          %.3f ms\\n\", naive_ms);\n",
    "    printf(\"Warp-aggregated atomics: %.3f ms\\n\", warp_ms);\n",
    "    printf(\"Speedup: %.1fx\\n\", naive_ms / warp_ms);\n",
    "    \n",
    "    cudaFree(d_counter);\n",
    "    cudaFree(d_results);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f9b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_warp_atomics.cu -o cg_warp_atomics && ./cg_warp_atomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92ffc8",
   "metadata": {},
   "source": [
    "## Grid-Level Synchronization\n",
    "\n",
    "Cooperative kernel launch allows synchronizing ALL threads across the entire grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b332d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_grid_sync.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void gridSyncKernel(int* data, int n) {\n",
    "    cg::grid_group grid = cg::this_grid();\n",
    "    \n",
    "    int idx = grid.thread_rank();\n",
    "    \n",
    "    // Phase 1: Each thread writes its index\n",
    "    if (idx < n) {\n",
    "        data[idx] = idx;\n",
    "    }\n",
    "    \n",
    "    // Grid-wide synchronization - ALL threads wait here\n",
    "    grid.sync();\n",
    "    \n",
    "    // Phase 2: Read neighbor's value (guaranteed to be written)\n",
    "    if (idx < n && idx > 0) {\n",
    "        data[idx] += data[idx - 1];\n",
    "    }\n",
    "    \n",
    "    if (idx == 0) {\n",
    "        printf(\"Grid sync completed across %lu threads\\n\", grid.size());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Check for cooperative launch support\n",
    "    int dev = 0;\n",
    "    int supportsCoopLaunch = 0;\n",
    "    cudaDeviceGetAttribute(&supportsCoopLaunch, cudaDevAttrCooperativeLaunch, dev);\n",
    "    \n",
    "    if (!supportsCoopLaunch) {\n",
    "        printf(\"Device does not support cooperative launch\\n\");\n",
    "        return 0;\n",
    "    }\n",
    "    \n",
    "    const int N = 1024;\n",
    "    int* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(int));\n",
    "    \n",
    "    // Use cooperative launch\n",
    "    void* args[] = {&d_data, (void*)&N};\n",
    "    \n",
    "    int numBlocks;\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, gridSyncKernel, 128, 0);\n",
    "    \n",
    "    int numSMs;\n",
    "    cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, dev);\n",
    "    \n",
    "    dim3 grid(numSMs * numBlocks);\n",
    "    dim3 block(128);\n",
    "    \n",
    "    cudaLaunchCooperativeKernel((void*)gridSyncKernel, grid, block, args);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int h_data[N];\n",
    "    cudaMemcpy(h_data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Results: data[0]=%d, data[1]=%d, data[10]=%d\\n\",\n",
    "           h_data[0], h_data[1], h_data[10]);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_grid_sync.cu -o cg_grid_sync && ./cg_grid_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496bf24a",
   "metadata": {},
   "source": [
    "## CG Reduction Operators\n",
    "\n",
    "| Operator | Returns |\n",
    "|----------|--------|\n",
    "| `cg::plus<T>()` | Sum |\n",
    "| `cg::less<T>()` | Minimum |\n",
    "| `cg::greater<T>()` | Maximum |\n",
    "| `cg::bit_and<T>()` | Bitwise AND |\n",
    "| `cg::bit_or<T>()` | Bitwise OR |\n",
    "| `cg::bit_xor<T>()` | Bitwise XOR |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e273f9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercises\n",
    "\n",
    "### ðŸ”· CUDA C++ Exercises (Primary)\n",
    "\n",
    "Complete these exercises to practice advanced cooperative groups concepts:\n",
    "\n",
    "**Exercise 1: Multi-Level Reduction**\n",
    "- Implement a hierarchical reduction using both warp-level and block-level cooperative groups\n",
    "- Use `cg::reduce()` for warp-level and shared memory for block-level\n",
    "\n",
    "**Exercise 2: Warp-Aggregated Histogram**\n",
    "- Implement a histogram using warp-aggregated atomics\n",
    "- Compare performance with naive atomic operations\n",
    "\n",
    "**Exercise 3: Grid-Wide Barrier**\n",
    "- Implement a multi-pass algorithm using `grid.sync()`\n",
    "- Example: multi-pass smoothing filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile advanced_cg_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/reduce.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Multi-Level Hierarchical Reduction\n",
    "// TODO: Complete the kernel to perform reduction at warp and block levels\n",
    "// =============================================================================\n",
    "__global__ void hierarchicalReduction(float* input, float* output, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    extern __shared__ float sdata[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load data\n",
    "    float val = (gid < n) ? input[gid] : 0.0f;\n",
    "    \n",
    "    // TODO: Exercise 1a - Use cg::reduce() for warp-level reduction\n",
    "    // Hint: float warp_sum = cg::reduce(warp, val, cg::plus<float>());\n",
    "    float warp_sum = 0.0f;  // Replace with cg::reduce\n",
    "    \n",
    "    // TODO: Exercise 1b - Store warp results to shared memory (lane 0 only)\n",
    "    // Hint: if (warp.thread_rank() == 0) sdata[warp_id] = warp_sum;\n",
    "    \n",
    "    block.sync();\n",
    "    \n",
    "    // TODO: Exercise 1c - Final reduction across warps (first warp only)\n",
    "    // Reduce the warp sums stored in shared memory\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        output[blockIdx.x] = sdata[0];  // Store block result\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Warp-Aggregated Histogram\n",
    "// TODO: Implement efficient histogram with leader election\n",
    "// =============================================================================\n",
    "#define NUM_BINS 256\n",
    "\n",
    "__global__ void warpAggregatedHistogram(unsigned char* data, int* histogram, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    __shared__ int local_hist[NUM_BINS];\n",
    "    \n",
    "    // Initialize shared histogram\n",
    "    for (int i = threadIdx.x; i < NUM_BINS; i += blockDim.x) {\n",
    "        local_hist[i] = 0;\n",
    "    }\n",
    "    block.sync();\n",
    "    \n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (gid < n) {\n",
    "        unsigned char bin = data[gid];\n",
    "        \n",
    "        // TODO: Exercise 2a - Find threads with matching bins using ballot\n",
    "        // Hint: unsigned int mask = __ballot_sync(0xffffffff, true);\n",
    "        //       unsigned int peers = ... find threads with same bin value\n",
    "        \n",
    "        // TODO: Exercise 2b - Count matching threads\n",
    "        // Hint: int count = __popc(peers);\n",
    "        \n",
    "        // TODO: Exercise 2c - Elect leader (lowest lane) to do atomic\n",
    "        // Hint: int leader = __ffs(peers) - 1;\n",
    "        //       if (warp.thread_rank() == leader) atomicAdd(...)\n",
    "        \n",
    "        // Naive version (replace with aggregated version above):\n",
    "        atomicAdd(&local_hist[bin], 1);\n",
    "    }\n",
    "    \n",
    "    block.sync();\n",
    "    \n",
    "    // Merge to global histogram\n",
    "    for (int i = threadIdx.x; i < NUM_BINS; i += blockDim.x) {\n",
    "        if (local_hist[i] > 0) {\n",
    "            atomicAdd(&histogram[i], local_hist[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Grid-Wide Multi-Pass Smoothing\n",
    "// TODO: Use grid.sync() for iterative algorithm\n",
    "// =============================================================================\n",
    "__global__ void multiPassSmooth(float* data, float* temp, int n, int passes) {\n",
    "    cg::grid_group grid = cg::this_grid();\n",
    "    \n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    for (int pass = 0; pass < passes; pass++) {\n",
    "        // TODO: Exercise 3a - Apply smoothing (average of neighbors)\n",
    "        if (tid > 0 && tid < n - 1) {\n",
    "            // Read from data, write to temp\n",
    "            temp[tid] = (data[tid-1] + data[tid] + data[tid+1]) / 3.0f;\n",
    "        } else if (tid < n) {\n",
    "            temp[tid] = data[tid];  // Boundary\n",
    "        }\n",
    "        \n",
    "        // TODO: Exercise 3b - Synchronize entire grid\n",
    "        // Hint: grid.sync();\n",
    "        \n",
    "        // TODO: Exercise 3c - Swap pointers (copy temp back to data)\n",
    "        if (tid < n) {\n",
    "            data[tid] = temp[tid];\n",
    "        }\n",
    "        \n",
    "        // TODO: Exercise 3d - Another grid sync before next pass\n",
    "        grid.sync();\n",
    "    }\n",
    "}\n",
    "\n",
    "// Test Exercise 1: Hierarchical Reduction\n",
    "void testHierarchicalReduction() {\n",
    "    printf(\"=== Exercise 1: Hierarchical Reduction ===\\n\");\n",
    "    \n",
    "    int n = 1024 * 1024;\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    float *h_input = (float*)malloc(n * sizeof(float));\n",
    "    float *h_output = (float*)malloc(numBlocks * sizeof(float));\n",
    "    \n",
    "    float expected = 0.0f;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_input[i] = 1.0f;\n",
    "        expected += h_input[i];\n",
    "    }\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, numBlocks * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_input, h_input, n * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    int warpsPerBlock = blockSize / 32;\n",
    "    int sharedSize = warpsPerBlock * sizeof(float);\n",
    "    \n",
    "    hierarchicalReduction<<<numBlocks, blockSize, sharedSize>>>(d_input, d_output, n);\n",
    "    CHECK_CUDA(cudaGetLastError());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_output, d_output, numBlocks * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    float total = 0.0f;\n",
    "    for (int i = 0; i < numBlocks; i++) {\n",
    "        total += h_output[i];\n",
    "    }\n",
    "    \n",
    "    printf(\"Expected: %.0f, Got: %.0f\\n\", expected, total);\n",
    "    printf(\"Status: %s\\n\\n\", (fabsf(total - expected) < 1e-3) ? \"PASS\" : \"NEEDS WORK\");\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "}\n",
    "\n",
    "// Test Exercise 2: Warp-Aggregated Histogram\n",
    "void testWarpHistogram() {\n",
    "    printf(\"=== Exercise 2: Warp-Aggregated Histogram ===\\n\");\n",
    "    \n",
    "    int n = 1024 * 1024;\n",
    "    \n",
    "    unsigned char *h_data = (unsigned char*)malloc(n);\n",
    "    int *h_hist = (int*)calloc(NUM_BINS, sizeof(int));\n",
    "    int expected[NUM_BINS] = {0};\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_data[i] = rand() % NUM_BINS;\n",
    "        expected[h_data[i]]++;\n",
    "    }\n",
    "    \n",
    "    unsigned char *d_data;\n",
    "    int *d_hist;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n));\n",
    "    CHECK_CUDA(cudaMalloc(&d_hist, NUM_BINS * sizeof(int)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, n, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemset(d_hist, 0, NUM_BINS * sizeof(int)));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    warpAggregatedHistogram<<<numBlocks, blockSize>>>(d_data, d_hist, n);\n",
    "    CHECK_CUDA(cudaGetLastError());\n",
    "    \n",
    "    CHECK_CUDA(cudaMemcpy(h_hist, d_hist, NUM_BINS * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < NUM_BINS; i++) {\n",
    "        if (h_hist[i] != expected[i]) {\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"Histogram check: %s\\n\", correct ? \"PASS\" : \"NEEDS WORK\");\n",
    "    printf(\"Sample bins - [0]: expected=%d, got=%d\\n\\n\", expected[0], h_hist[0]);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_hist);\n",
    "    free(h_data);\n",
    "    free(h_hist);\n",
    "}\n",
    "\n",
    "// Test Exercise 3: Grid-Wide Smoothing (requires cooperative launch)\n",
    "void testGridSmooth() {\n",
    "    printf(\"=== Exercise 3: Grid-Wide Smoothing ===\\n\");\n",
    "    \n",
    "    int deviceId;\n",
    "    cudaGetDevice(&deviceId);\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, deviceId);\n",
    "    \n",
    "    if (!prop.cooperativeLaunch) {\n",
    "        printf(\"Device does not support cooperative launch\\n\");\n",
    "        printf(\"Status: SKIPPED\\n\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    int n = 1024;\n",
    "    int passes = 3;\n",
    "    \n",
    "    float *h_data = (float*)malloc(n * sizeof(float));\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_data[i] = (i == n/2) ? 100.0f : 0.0f;  // Spike in middle\n",
    "    }\n",
    "    \n",
    "    float *d_data, *d_temp;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_temp, n * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, n * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    void* args[] = {&d_data, &d_temp, &n, &passes};\n",
    "    \n",
    "    cudaError_t err = cudaLaunchCooperativeKernel(\n",
    "        (void*)multiPassSmooth,\n",
    "        dim3(numBlocks), dim3(blockSize),\n",
    "        args, 0, 0\n",
    "    );\n",
    "    \n",
    "    if (err == cudaSuccess) {\n",
    "        CHECK_CUDA(cudaMemcpy(h_data, d_data, n * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "        \n",
    "        printf(\"After %d smoothing passes:\\n\", passes);\n",
    "        printf(\"Center region: [%.2f, %.2f, %.2f, %.2f, %.2f]\\n\",\n",
    "               h_data[n/2-2], h_data[n/2-1], h_data[n/2], h_data[n/2+1], h_data[n/2+2]);\n",
    "        printf(\"Status: Check if spike has spread\\n\\n\");\n",
    "    } else {\n",
    "        printf(\"Cooperative launch failed: %s\\n\", cudaGetErrorString(err));\n",
    "        printf(\"Status: NEEDS WORK\\n\\n\");\n",
    "    }\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_temp);\n",
    "    free(h_data);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘     Advanced Cooperative Groups Exercises                    â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    testHierarchicalReduction();\n",
    "    testWarpHistogram();\n",
    "    testGridSmooth();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"Complete the TODOs in each exercise and re-run to verify!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7aa6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -rdc=true -lcudadevrt -o advanced_cg_exercises advanced_cg_exercises.cu && ./advanced_cg_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628ea0f",
   "metadata": {},
   "source": [
    "### ðŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises use Numba for GPU programming. While Numba doesn't support all cooperative groups features, you can practice the concepts:\n",
    "\n",
    "```python\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# Exercise: Implement warp-level primitives simulation\n",
    "# Numba supports some warp shuffle operations\n",
    "\n",
    "@cuda.jit\n",
    "def warp_reduce_sum(data, result):\n",
    "    \"\"\"\n",
    "    Use cuda.shfl_down_sync to implement warp reduction\n",
    "    Numba equivalent of cg::reduce for a warp\n",
    "    \"\"\"\n",
    "    tid = cuda.threadIdx.x\n",
    "    lane = tid % 32\n",
    "    \n",
    "    val = data[cuda.blockIdx.x * cuda.blockDim.x + tid]\n",
    "    \n",
    "    # TODO: Implement warp shuffle reduction\n",
    "    # for offset in [16, 8, 4, 2, 1]:\n",
    "    #     val += cuda.shfl_down_sync(0xffffffff, val, offset)\n",
    "    \n",
    "    if lane == 0:\n",
    "        cuda.atomic.add(result, 0, val)\n",
    "```\n",
    "\n",
    "**Note**: For full cooperative groups functionality including `grid.sync()`, use CUDA C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608ff66",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **`cg::reduce()`** - Hardware-accelerated on CC 8.0+\n",
    "2. **`cg::inclusive_scan()` / `cg::exclusive_scan()`** - Parallel prefix operations\n",
    "3. **Warp-aggregated atomics** - Reduce contention dramatically\n",
    "4. **Grid sync** - Requires `cudaLaunchCooperativeKernel`\n",
    "5. **Flexible partitioning** - `tiled_partition<N>()` for any power-of-2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
