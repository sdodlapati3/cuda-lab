{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7df2282",
   "metadata": {},
   "source": [
    "## Beyond `__syncthreads()`: The Cooperative Groups API\n",
    "\n",
    "Cooperative Groups provides a flexible, composable API for thread synchronization and collective operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_basics_advanced.cu\n",
    "#include <stdio.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/reduce.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void cgDemo() {\n",
    "    // Get thread block group\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    // Partition into tiles of 32 threads (warp-aligned)\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    // Partition into tiles of 16 threads\n",
    "    cg::thread_block_tile<16> half_warp = cg::tiled_partition<16>(block);\n",
    "    \n",
    "    // Get coalesced threads (active threads in warp)\n",
    "    cg::coalesced_group active = cg::coalesced_threads();\n",
    "    \n",
    "    if (block.thread_rank() == 0) {\n",
    "        printf(\"Block size: %d\\n\", block.size());\n",
    "        printf(\"Warp tile size: %d\\n\", warp.size());\n",
    "        printf(\"Half-warp tile size: %d\\n\", half_warp.size());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    cgDemo<<<1, 128>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_basics_advanced.cu -o cg_basics_advanced && ./cg_basics_advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6581634",
   "metadata": {},
   "source": [
    "## CG Collective Operations: `reduce()`\n",
    "\n",
    "Hardware-accelerated reductions with `cg::reduce()` - much faster than manual shuffle reductions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bea1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_reduce.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/reduce.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void cgReduceKernel(int* input, int* output, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    \n",
    "    int tid = block.thread_rank();\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load value\n",
    "    int val = (idx < n) ? input[idx] : 0;\n",
    "    \n",
    "    // Warp-level reduction using CG (hardware-accelerated on CC 8.0+)\n",
    "    int warp_sum = cg::reduce(warp, val, cg::plus<int>());\n",
    "    \n",
    "    // First thread in each warp has the sum\n",
    "    __shared__ int warp_sums[32];  // Max 32 warps per block\n",
    "    if (warp.thread_rank() == 0) {\n",
    "        warp_sums[tid / 32] = warp_sum;\n",
    "    }\n",
    "    block.sync();\n",
    "    \n",
    "    // Final reduction of warp sums (first warp only)\n",
    "    if (tid < 32) {\n",
    "        int num_warps = (blockDim.x + 31) / 32;\n",
    "        val = (tid < num_warps) ? warp_sums[tid] : 0;\n",
    "        int block_sum = cg::reduce(warp, val, cg::plus<int>());\n",
    "        \n",
    "        if (tid == 0) {\n",
    "            output[blockIdx.x] = block_sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1024;\n",
    "    int h_input[N], h_output[8];\n",
    "    \n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1;  // Sum should be N\n",
    "    \n",
    "    int *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, N * sizeof(int));\n",
    "    cudaMalloc(&d_output, 8 * sizeof(int));\n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cgReduceKernel<<<8, 128>>>(d_input, d_output, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_output, d_output, 8 * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    int total = 0;\n",
    "    for (int i = 0; i < 8; i++) total += h_output[i];\n",
    "    printf(\"Sum of %d ones = %d (expected %d)\\n\", N, total, N);\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_reduce.cu -o cg_reduce && ./cg_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf19f51",
   "metadata": {},
   "source": [
    "## CG Scan Operations: `inclusive_scan()` and `exclusive_scan()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bded12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_scan.cu\n",
    "#include <stdio.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/scan.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void cgScanKernel(int* input, int* incl_output, int* excl_output, int n) {\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    \n",
    "    int tid = block.thread_rank();\n",
    "    int val = (tid < n) ? input[tid] : 0;\n",
    "    \n",
    "    // Inclusive scan: each element includes itself\n",
    "    // Input:  [1, 2, 3, 4]\n",
    "    // Output: [1, 3, 6, 10]\n",
    "    int incl = cg::inclusive_scan(block, val, cg::plus<int>());\n",
    "    \n",
    "    // Exclusive scan: each element excludes itself\n",
    "    // Input:  [1, 2, 3, 4]\n",
    "    // Output: [0, 1, 3, 6]\n",
    "    int excl = cg::exclusive_scan(block, val, cg::plus<int>());\n",
    "    \n",
    "    if (tid < n) {\n",
    "        incl_output[tid] = incl;\n",
    "        excl_output[tid] = excl;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 8;\n",
    "    int h_input[N] = {1, 2, 3, 4, 5, 6, 7, 8};\n",
    "    int h_incl[N], h_excl[N];\n",
    "    \n",
    "    int *d_input, *d_incl, *d_excl;\n",
    "    cudaMalloc(&d_input, N * sizeof(int));\n",
    "    cudaMalloc(&d_incl, N * sizeof(int));\n",
    "    cudaMalloc(&d_excl, N * sizeof(int));\n",
    "    \n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cgScanKernel<<<1, N>>>(d_input, d_incl, d_excl, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_incl, d_incl, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_excl, d_excl, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    printf(\"Input:          \");\n",
    "    for (int i = 0; i < N; i++) printf(\"%3d \", h_input[i]);\n",
    "    printf(\"\\nInclusive scan: \");\n",
    "    for (int i = 0; i < N; i++) printf(\"%3d \", h_incl[i]);\n",
    "    printf(\"\\nExclusive scan: \");\n",
    "    for (int i = 0; i < N; i++) printf(\"%3d \", h_excl[i]);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_incl);\n",
    "    cudaFree(d_excl);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7037b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_scan.cu -o cg_scan && ./cg_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178cbd05",
   "metadata": {},
   "source": [
    "## Warp-Aggregated Atomics Pattern\n",
    "\n",
    "Reduce atomic contention by having one thread per warp perform the atomic operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_warp_atomics.cu\n",
    "#include <stdio.h>\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/reduce.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "// Naive: every thread does atomic - HIGH CONTENTION\n",
    "__device__ int naiveAtomicInc(int* counter) {\n",
    "    return atomicAdd(counter, 1);\n",
    "}\n",
    "\n",
    "// Optimized: warp-aggregated atomic - MUCH FASTER\n",
    "__device__ int warpAggregatedAtomicInc(int* counter) {\n",
    "    cg::coalesced_group active = cg::coalesced_threads();\n",
    "    \n",
    "    int warp_res;\n",
    "    if (active.thread_rank() == 0) {\n",
    "        // Only leader does the atomic\n",
    "        warp_res = atomicAdd(counter, active.size());\n",
    "    }\n",
    "    \n",
    "    // Broadcast result and compute individual values\n",
    "    warp_res = active.shfl(warp_res, 0);\n",
    "    return warp_res + active.thread_rank();\n",
    "}\n",
    "\n",
    "__global__ void testNaive(int* counter, int* results, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        results[idx] = naiveAtomicInc(counter);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void testWarpAgg(int* counter, int* results, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        results[idx] = warpAggregatedAtomicInc(counter);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1000000;\n",
    "    int *d_counter, *d_results;\n",
    "    \n",
    "    cudaMalloc(&d_counter, sizeof(int));\n",
    "    cudaMalloc(&d_results, N * sizeof(int));\n",
    "    \n",
    "    // Benchmark naive\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaMemset(d_counter, 0, sizeof(int));\n",
    "    cudaEventRecord(start);\n",
    "    testNaive<<<(N+255)/256, 256>>>(d_counter, d_results, N);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float naive_ms;\n",
    "    cudaEventElapsedTime(&naive_ms, start, stop);\n",
    "    \n",
    "    // Benchmark warp-aggregated\n",
    "    cudaMemset(d_counter, 0, sizeof(int));\n",
    "    cudaEventRecord(start);\n",
    "    testWarpAgg<<<(N+255)/256, 256>>>(d_counter, d_results, N);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float warp_ms;\n",
    "    cudaEventElapsedTime(&warp_ms, start, stop);\n",
    "    \n",
    "    printf(\"Naive atomics:          %.3f ms\\n\", naive_ms);\n",
    "    printf(\"Warp-aggregated atomics: %.3f ms\\n\", warp_ms);\n",
    "    printf(\"Speedup: %.1fx\\n\", naive_ms / warp_ms);\n",
    "    \n",
    "    cudaFree(d_counter);\n",
    "    cudaFree(d_results);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f9b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_warp_atomics.cu -o cg_warp_atomics && ./cg_warp_atomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92ffc8",
   "metadata": {},
   "source": [
    "## Grid-Level Synchronization\n",
    "\n",
    "Cooperative kernel launch allows synchronizing ALL threads across the entire grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b332d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cg_grid_sync.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "__global__ void gridSyncKernel(int* data, int n) {\n",
    "    cg::grid_group grid = cg::this_grid();\n",
    "    \n",
    "    int idx = grid.thread_rank();\n",
    "    \n",
    "    // Phase 1: Each thread writes its index\n",
    "    if (idx < n) {\n",
    "        data[idx] = idx;\n",
    "    }\n",
    "    \n",
    "    // Grid-wide synchronization - ALL threads wait here\n",
    "    grid.sync();\n",
    "    \n",
    "    // Phase 2: Read neighbor's value (guaranteed to be written)\n",
    "    if (idx < n && idx > 0) {\n",
    "        data[idx] += data[idx - 1];\n",
    "    }\n",
    "    \n",
    "    if (idx == 0) {\n",
    "        printf(\"Grid sync completed across %lu threads\\n\", grid.size());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Check for cooperative launch support\n",
    "    int dev = 0;\n",
    "    int supportsCoopLaunch = 0;\n",
    "    cudaDeviceGetAttribute(&supportsCoopLaunch, cudaDevAttrCooperativeLaunch, dev);\n",
    "    \n",
    "    if (!supportsCoopLaunch) {\n",
    "        printf(\"Device does not support cooperative launch\\n\");\n",
    "        return 0;\n",
    "    }\n",
    "    \n",
    "    const int N = 1024;\n",
    "    int* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(int));\n",
    "    \n",
    "    // Use cooperative launch\n",
    "    void* args[] = {&d_data, (void*)&N};\n",
    "    \n",
    "    int numBlocks;\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, gridSyncKernel, 128, 0);\n",
    "    \n",
    "    int numSMs;\n",
    "    cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, dev);\n",
    "    \n",
    "    dim3 grid(numSMs * numBlocks);\n",
    "    dim3 block(128);\n",
    "    \n",
    "    cudaLaunchCooperativeKernel((void*)gridSyncKernel, grid, block, args);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    int h_data[N];\n",
    "    cudaMemcpy(h_data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    printf(\"Results: data[0]=%d, data[1]=%d, data[10]=%d\\n\",\n",
    "           h_data[0], h_data[1], h_data[10]);\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc cg_grid_sync.cu -o cg_grid_sync && ./cg_grid_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496bf24a",
   "metadata": {},
   "source": [
    "## CG Reduction Operators\n",
    "\n",
    "| Operator | Returns |\n",
    "|----------|--------|\n",
    "| `cg::plus<T>()` | Sum |\n",
    "| `cg::less<T>()` | Minimum |\n",
    "| `cg::greater<T>()` | Maximum |\n",
    "| `cg::bit_and<T>()` | Bitwise AND |\n",
    "| `cg::bit_or<T>()` | Bitwise OR |\n",
    "| `cg::bit_xor<T>()` | Bitwise XOR |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608ff66",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **`cg::reduce()`** - Hardware-accelerated on CC 8.0+\n",
    "2. **`cg::inclusive_scan()` / `cg::exclusive_scan()`** - Parallel prefix operations\n",
    "3. **Warp-aggregated atomics** - Reduce contention dramatically\n",
    "4. **Grid sync** - Requires `cudaLaunchCooperativeKernel`\n",
    "5. **Flexible partitioning** - `tiled_partition<N>()` for any power-of-2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
