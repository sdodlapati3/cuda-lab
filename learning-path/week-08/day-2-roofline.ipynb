{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53766c20",
   "metadata": {},
   "source": [
    "## 1. The Roofline Model Concept\n",
    "\n",
    "### What is the Roofline Model?\n",
    "\n",
    "The roofline model is a **visual performance model** that shows:\n",
    "- Maximum achievable performance for a given arithmetic intensity\n",
    "- Whether a kernel is compute-bound or memory-bound\n",
    "- How far from peak performance you are\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "```\n",
    "          ^\n",
    "  GFLOPS  |                    _______________  Peak Compute\n",
    "          |                  /\n",
    "          |                /\n",
    "          |              /   Memory-Bound Region\n",
    "          |            /\n",
    "          |          /\n",
    "          |        /       Ridge Point\n",
    "          |      /        â†“\n",
    "          |    /     * Kernel A (memory-bound)\n",
    "          |  /\n",
    "          |/______________ Compute-Bound Region ___________\n",
    "          |                           * Kernel B (compute-bound)\n",
    "          +---------------------------------------------> \n",
    "                    Arithmetic Intensity (FLOP/Byte)\n",
    "```\n",
    "\n",
    "### Definitions\n",
    "\n",
    "| Term | Definition | Formula |\n",
    "|------|------------|--------|\n",
    "| **Arithmetic Intensity (AI)** | Compute work per byte transferred | FLOP / Bytes |\n",
    "| **Peak Compute** | Max FLOPS the GPU can perform | From spec |\n",
    "| **Peak Bandwidth** | Max memory throughput | From spec |\n",
    "| **Ridge Point** | AI where compute = memory bound | Peak_FLOPS / Peak_BW |\n",
    "\n",
    "### The Roofline Equation\n",
    "\n",
    "$$\\text{Achievable FLOPS} = \\min(\\text{Peak FLOPS}, \\text{Peak BW} \\times \\text{AI})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d380c",
   "metadata": {},
   "source": [
    "### ðŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "## 2. Calculating Arithmetic Intensity\n",
    "\n",
    "### CUDA C++ Examples with AI Calculations\n",
    "\n",
    "The following kernels demonstrate different arithmetic intensities:\n",
    "- **Vector Add**: AI â‰ˆ 0.083 (very memory-bound)\n",
    "- **SAXPY**: AI â‰ˆ 0.167 (memory-bound)\n",
    "- **Dot Product**: AI â‰ˆ 0.375 (memory-bound)\n",
    "- **Matrix Multiply**: AI scales with N (compute-bound for large N)\n",
    "- **3D Stencil**: AI â‰ˆ 0.75-3 (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90533668",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile arithmetic_intensity.cu\n",
    "// arithmetic_intensity.cu - Calculate AI for different kernels\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 1: Vector Add\n",
    "// FLOP: N (one add per element)\n",
    "// Bytes: 3N * 4 (read 2 vectors, write 1 vector, float32)\n",
    "// AI = N / (12N) = 1/12 â‰ˆ 0.083 FLOP/Byte  â†’ VERY memory-bound\n",
    "//=============================================================================\n",
    "__global__ void vectorAdd(float* c, const float* a, const float* b, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];  // 1 FLOP, 12 bytes\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 2: SAXPY (y = a*x + y)\n",
    "// FLOP: 2N (mul + add per element)\n",
    "// Bytes: 3N * 4 (read x, read y, write y) - ignoring scalar a (cached)\n",
    "// AI = 2N / (12N) = 1/6 â‰ˆ 0.167 FLOP/Byte  â†’ Memory-bound\n",
    "//=============================================================================\n",
    "__global__ void saxpy(float* y, const float* x, float a, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        y[idx] = a * x[idx] + y[idx];  // 2 FLOP, 12 bytes\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 3: Dot Product (reduction)\n",
    "// FLOP: 2N (mul + add per element) + N (reduction adds)\n",
    "// Bytes: 2N * 4 (read 2 vectors)\n",
    "// AI = 3N / (8N) â‰ˆ 0.375 FLOP/Byte  â†’ Memory-bound\n",
    "//=============================================================================\n",
    "__global__ void dotProduct(float* result, const float* a, const float* b, \n",
    "                           int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? a[idx] * b[idx] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduction\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(result, sdata[0]);\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 4: Matrix Multiply (naive)\n",
    "// For NÃ—N matrices:\n",
    "// FLOP: 2NÂ³ (NÂ³ multiplies + NÂ³ adds)\n",
    "// Bytes: 3NÂ² * 4 (read A, B; write C) - minimum\n",
    "// AI = 2NÂ³ / (12NÂ²) = N/6 FLOP/Byte  â†’ Scales with N!\n",
    "// For N=1024: AI â‰ˆ 170 FLOP/Byte â†’ Compute-bound\n",
    "//=============================================================================\n",
    "__global__ void matrixMul(float* C, const float* A, const float* B, \n",
    "                          int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < N && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            sum += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 5: Stencil (7-point 3D)\n",
    "// FLOP per point: 7 reads, 6 adds, 1 write = 6 FLOP\n",
    "// Bytes per point: ~8 bytes (with caching) to 32 bytes (no cache)\n",
    "// AI â‰ˆ 0.75 - 3 FLOP/Byte â†’ Memory-bound to balanced\n",
    "//=============================================================================\n",
    "__global__ void stencil3D(float* out, const float* in, \n",
    "                          int nx, int ny, int nz) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;\n",
    "    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;\n",
    "    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;\n",
    "    \n",
    "    if (i < nx-1 && j < ny-1 && k < nz-1) {\n",
    "        int idx = i + j * nx + k * nx * ny;\n",
    "        \n",
    "        out[idx] = in[idx] +\n",
    "                   in[idx - 1] + in[idx + 1] +\n",
    "                   in[idx - nx] + in[idx + nx] +\n",
    "                   in[idx - nx*ny] + in[idx + nx*ny];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Arithmetic Intensity Examples:\\n\");\n",
    "    printf(\"================================\\n\");\n",
    "    printf(\"Vector Add:    AI â‰ˆ 0.08 FLOP/Byte (memory-bound)\\n\");\n",
    "    printf(\"SAXPY:         AI â‰ˆ 0.17 FLOP/Byte (memory-bound)\\n\");\n",
    "    printf(\"Dot Product:   AI â‰ˆ 0.38 FLOP/Byte (memory-bound)\\n\");\n",
    "    printf(\"MatMul 1024:   AI â‰ˆ 170 FLOP/Byte  (compute-bound)\\n\");\n",
    "    printf(\"3D Stencil:    AI â‰ˆ 0.75-3 FLOP/Byte (balanced)\\n\");\n",
    "    printf(\"\\nRidge point (typical GPU): ~10-20 FLOP/Byte\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o arithmetic_intensity arithmetic_intensity.cu\n",
    "!./arithmetic_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ebf7d",
   "metadata": {},
   "source": [
    "## 3. Building a Roofline Chart\n",
    "\n",
    "### GPU Specifications\n",
    "\n",
    "| GPU | Peak FP32 | Peak BW | Ridge Point |\n",
    "|-----|-----------|---------|-------------|\n",
    "| RTX 3090 | 35.6 TFLOPS | 936 GB/s | 38 FLOP/Byte |\n",
    "| A100 | 19.5 TFLOPS | 2039 GB/s | 9.6 FLOP/Byte |\n",
    "| V100 | 15.7 TFLOPS | 900 GB/s | 17.4 FLOP/Byte |\n",
    "| T4 | 8.1 TFLOPS | 320 GB/s | 25.3 FLOP/Byte |\n",
    "\n",
    "### Calculate Ridge Point\n",
    "\n",
    "```\n",
    "Ridge Point = Peak Compute / Peak Bandwidth\n",
    "\n",
    "Example for A100:\n",
    "Ridge = 19.5 TFLOPS / 2039 GB/s = 9.6 FLOP/Byte\n",
    "\n",
    "Interpretation:\n",
    "- AI < 9.6: Memory-bound\n",
    "- AI > 9.6: Compute-bound\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798e140",
   "metadata": {},
   "source": [
    "## 4. Nsight Compute Roofline Analysis\n",
    "\n",
    "### Generate Roofline with ncu\n",
    "\n",
    "```bash\n",
    "# Generate roofline analysis\n",
    "ncu --set roofline -o roofline_report ./my_program\n",
    "\n",
    "# Open in GUI to see visual roofline\n",
    "ncu-ui roofline_report.ncu-rep\n",
    "```\n",
    "\n",
    "### Interpreting the Roofline Chart\n",
    "\n",
    "```\n",
    "In Nsight Compute GUI:\n",
    "\n",
    "1. Look for your kernel's position (dot on the chart)\n",
    "2. Check which roof it's closest to:\n",
    "   - Below slanted line (memory roof) â†’ Memory-bound\n",
    "   - Below horizontal line (compute roof) â†’ Compute-bound\n",
    "3. Distance from roof = optimization potential\n",
    "\n",
    "Multiple roofs may appear:\n",
    "- L1 cache roofline (highest bandwidth)\n",
    "- L2 cache roofline\n",
    "- DRAM roofline (lowest bandwidth)\n",
    "- FP32 roofline\n",
    "- FP64 roofline\n",
    "```\n",
    "\n",
    "### Command-Line Roofline Metrics\n",
    "\n",
    "```bash\n",
    "# Get metrics for manual roofline calculation\n",
    "ncu --metrics \\\n",
    "  sm__sass_thread_inst_executed_op_fadd_pred_on.sum,\\\n",
    "  sm__sass_thread_inst_executed_op_fmul_pred_on.sum,\\\n",
    "  sm__sass_thread_inst_executed_op_ffma_pred_on.sum,\\\n",
    "  dram__bytes.sum,\\\n",
    "  gpu__time_duration.avg \\\n",
    "  ./my_program\n",
    "\n",
    "# Calculate:\n",
    "# FLOPS = FADD + FMUL + 2*FFMA\n",
    "# AI = FLOPS / dram__bytes.sum\n",
    "# Performance = FLOPS / time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02e22e",
   "metadata": {},
   "source": [
    "### ðŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "## 5. CUDA C++ Roofline Test Program\n",
    "\n",
    "This program measures actual performance across different arithmetic intensities to verify the roofline model. By varying the number of compute iterations per memory access, we can sweep from memory-bound to compute-bound regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile roofline_test.cu\n",
    "// roofline_test.cu - Measure roofline position\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <chrono>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "// SAXPY kernel - known AI\n",
    "__global__ void saxpy(float* y, const float* x, float a, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        y[idx] = a * x[idx] + y[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Compute-heavy kernel - adjustable AI\n",
    "__global__ void computeHeavy(float* output, const float* input, \n",
    "                              int n, int compute_iters) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = input[idx];\n",
    "        \n",
    "        // Increase compute per byte by iterating\n",
    "        for (int i = 0; i < compute_iters; i++) {\n",
    "            val = val * 1.00001f + 0.00001f;  // 2 FLOP\n",
    "        }\n",
    "        \n",
    "        output[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "void measureKernel(const char* name, int n, int compute_iters, \n",
    "                   float* d_in, float* d_out) {\n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // Warm up\n",
    "    computeHeavy<<<gridSize, blockSize>>>(d_out, d_in, n, compute_iters);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    // Timing\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int iterations = 100;\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        computeHeavy<<<gridSize, blockSize>>>(d_out, d_in, n, compute_iters);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ms = 0;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    float avgTime = ms / iterations;\n",
    "    \n",
    "    // Calculate metrics\n",
    "    long long flops = (long long)n * compute_iters * 2;  // 2 FLOP per iter\n",
    "    long long bytes = (long long)n * 2 * sizeof(float);  // Read + Write\n",
    "    \n",
    "    float ai = (float)flops / bytes;\n",
    "    float gflops = flops / (avgTime * 1e6);\n",
    "    float bandwidth = bytes / (avgTime * 1e6);  // GB/s\n",
    "    \n",
    "    printf(\"%s (iters=%d):\\n\", name, compute_iters);\n",
    "    printf(\"  Time: %.3f ms\\n\", avgTime);\n",
    "    printf(\"  AI: %.2f FLOP/Byte\\n\", ai);\n",
    "    printf(\"  Performance: %.2f GFLOPS\\n\", gflops);\n",
    "    printf(\"  Bandwidth: %.2f GB/s\\n\", bandwidth);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 24;  // 16M elements\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    CHECK_CUDA(cudaMalloc(&d_in, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_out, N * sizeof(float)));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_in = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_in[i] = 1.0f;\n",
    "    CHECK_CUDA(cudaMemcpy(d_in, h_in, N * sizeof(float), \n",
    "                          cudaMemcpyHostToDevice));\n",
    "    \n",
    "    printf(\"Roofline Test - Varying Arithmetic Intensity\\n\");\n",
    "    printf(\"=============================================\\n\\n\");\n",
    "    \n",
    "    // Test different AI levels\n",
    "    // AI = (2 * iters) / 8 = iters / 4\n",
    "    measureKernel(\"Low AI\", N, 1, d_in, d_out);      // AI = 0.25\n",
    "    measureKernel(\"Medium AI\", N, 10, d_in, d_out);  // AI = 2.5\n",
    "    measureKernel(\"High AI\", N, 100, d_in, d_out);   // AI = 25\n",
    "    measureKernel(\"Very High AI\", N, 1000, d_in, d_out);  // AI = 250\n",
    "    \n",
    "    printf(\"Observations:\\n\");\n",
    "    printf(\"- Low AI: Limited by memory bandwidth\\n\");\n",
    "    printf(\"- High AI: Limited by compute throughput\\n\");\n",
    "    printf(\"- Performance scales with AI until hitting compute roof\\n\");\n",
    "    \n",
    "    delete[] h_in;\n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -lineinfo -o roofline_test roofline_test.cu\n",
    "!./roofline_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ae0c4",
   "metadata": {},
   "source": [
    "### ðŸ”¶ Python/Numba (Optional - Quick Testing)\n",
    "\n",
    "## 6. Python/Numba Optional Backup\n",
    "\n",
    "Let's visualize the roofline model in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib numba -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2fd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roofline(peak_compute_gflops, peak_bandwidth_gb_s, kernels=None):\n",
    "    \"\"\"\n",
    "    Plot a roofline model.\n",
    "    \n",
    "    Parameters:\n",
    "    - peak_compute_gflops: Peak compute throughput in GFLOPS\n",
    "    - peak_bandwidth_gb_s: Peak memory bandwidth in GB/s\n",
    "    - kernels: List of (name, AI, achieved_gflops) tuples\n",
    "    \"\"\"\n",
    "    # Calculate ridge point\n",
    "    ridge_point = peak_compute_gflops / peak_bandwidth_gb_s\n",
    "    \n",
    "    # AI range for plotting\n",
    "    ai = np.logspace(-2, 3, 1000)  # 0.01 to 1000 FLOP/Byte\n",
    "    \n",
    "    # Roofline: min(peak_compute, peak_bw * AI)\n",
    "    roofline = np.minimum(peak_compute_gflops, peak_bandwidth_gb_s * ai)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Roofline\n",
    "    plt.loglog(ai, roofline, 'b-', linewidth=3, label='Roofline')\n",
    "    \n",
    "    # Ridge point\n",
    "    plt.axvline(x=ridge_point, color='g', linestyle='--', linewidth=1,\n",
    "                label=f'Ridge Point ({ridge_point:.1f} FLOP/Byte)')\n",
    "    \n",
    "    # Peak lines\n",
    "    plt.axhline(y=peak_compute_gflops, color='r', linestyle=':', linewidth=1,\n",
    "                label=f'Peak Compute ({peak_compute_gflops:.0f} GFLOPS)')\n",
    "    \n",
    "    # Plot kernel points if provided\n",
    "    colors = ['red', 'orange', 'purple', 'brown', 'pink']\n",
    "    if kernels:\n",
    "        for i, (name, kernel_ai, kernel_perf) in enumerate(kernels):\n",
    "            color = colors[i % len(colors)]\n",
    "            plt.scatter(kernel_ai, kernel_perf, s=200, c=color, \n",
    "                       marker='*', zorder=5)\n",
    "            plt.annotate(name, (kernel_ai, kernel_perf), \n",
    "                        textcoords=\"offset points\", \n",
    "                        xytext=(10, 10), fontsize=10, color=color)\n",
    "    \n",
    "    # Shading for regions\n",
    "    plt.fill_between(ai[ai < ridge_point], 0.1, roofline[ai < ridge_point],\n",
    "                     alpha=0.1, color='blue', label='Memory-bound region')\n",
    "    plt.fill_between(ai[ai >= ridge_point], 0.1, roofline[ai >= ridge_point],\n",
    "                     alpha=0.1, color='red', label='Compute-bound region')\n",
    "    \n",
    "    plt.xlabel('Arithmetic Intensity (FLOP/Byte)', fontsize=12)\n",
    "    plt.ylabel('Performance (GFLOPS)', fontsize=12)\n",
    "    plt.title(f'Roofline Model\\nPeak: {peak_compute_gflops} GFLOPS, '\n",
    "              f'Bandwidth: {peak_bandwidth_gb_s} GB/s', fontsize=14)\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.3)\n",
    "    plt.xlim(0.01, 1000)\n",
    "    plt.ylim(1, peak_compute_gflops * 2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ridge_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7897cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: T4 GPU (Google Colab)\n",
    "PEAK_COMPUTE = 8100  # GFLOPS (FP32)\n",
    "PEAK_BANDWIDTH = 320  # GB/s\n",
    "\n",
    "# Example kernels with their AI and measured performance\n",
    "kernels = [\n",
    "    (\"Vector Add\", 0.083, 50),      # Very memory-bound\n",
    "    (\"SAXPY\", 0.167, 100),          # Memory-bound\n",
    "    (\"Dot Product\", 0.375, 200),    # Memory-bound\n",
    "    (\"3D Stencil\", 1.5, 400),       # Balanced\n",
    "    (\"MatMul 1024\", 170, 5000),     # Compute-bound\n",
    "]\n",
    "\n",
    "ridge = plot_roofline(PEAK_COMPUTE, PEAK_BANDWIDTH, kernels)\n",
    "print(f\"Ridge Point: {ridge:.2f} FLOP/Byte\")\n",
    "print(f\"\\nKernels below ridge point are memory-bound\")\n",
    "print(f\"Kernels above ridge point are compute-bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1091422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AI for common operations\n",
    "\n",
    "def calculate_ai(name, flops, bytes_transferred):\n",
    "    ai = flops / bytes_transferred\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  FLOP: {flops}\")\n",
    "    print(f\"  Bytes: {bytes_transferred}\")\n",
    "    print(f\"  AI: {ai:.3f} FLOP/Byte\")\n",
    "    print()\n",
    "    return ai\n",
    "\n",
    "N = 1024  # Vector/matrix size\n",
    "\n",
    "print(\"Arithmetic Intensity Calculations\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Vector Add: c = a + b\n",
    "calculate_ai(\"Vector Add (N elements)\",\n",
    "             flops=N,  # N additions\n",
    "             bytes_transferred=3 * N * 4)  # 3 vectors Ã— N Ã— 4 bytes\n",
    "\n",
    "# SAXPY: y = a*x + y  \n",
    "calculate_ai(\"SAXPY\",\n",
    "             flops=2 * N,  # N muls + N adds\n",
    "             bytes_transferred=3 * N * 4)  # x, y(read), y(write)\n",
    "\n",
    "# Matrix Multiply: C = A Ã— B (naive)\n",
    "calculate_ai(f\"Matrix Multiply ({N}Ã—{N})\",\n",
    "             flops=2 * N**3,  # NÂ³ muls + NÂ³ adds\n",
    "             bytes_transferred=3 * N**2 * 4)  # A, B, C\n",
    "\n",
    "# Convolution 3Ã—3\n",
    "calculate_ai(\"3Ã—3 Convolution (per pixel)\",\n",
    "             flops=9 * 2,  # 9 muls + 9 adds\n",
    "             bytes_transferred=10 * 4)  # 9 reads + 1 write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d649e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate kernel performance across AI spectrum\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "@cuda.jit\n",
    "def variable_ai_kernel(output, input_arr, iters):\n",
    "    \"\"\"Kernel with variable AI based on iteration count\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < input_arr.size:\n",
    "        val = input_arr[idx]\n",
    "        for i in range(iters):\n",
    "            val = val * 1.00001 + 0.00001  # 2 FLOP per iteration\n",
    "        output[idx] = val\n",
    "\n",
    "if cuda.is_available():\n",
    "    N = 1 << 22  # 4M elements\n",
    "    h_input = np.ones(N, dtype=np.float32)\n",
    "    d_input = cuda.to_device(h_input)\n",
    "    d_output = cuda.device_array_like(h_input)\n",
    "    \n",
    "    threads = 256\n",
    "    blocks = (N + threads - 1) // threads\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for iters in [1, 5, 10, 50, 100, 500, 1000]:\n",
    "        # Warm up\n",
    "        variable_ai_kernel[blocks, threads](d_output, d_input, iters)\n",
    "        cuda.synchronize()\n",
    "        \n",
    "        # Time it\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(10):\n",
    "            variable_ai_kernel[blocks, threads](d_output, d_input, iters)\n",
    "        cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / 10\n",
    "        \n",
    "        # Calculate metrics\n",
    "        flops = N * iters * 2\n",
    "        bytes_trans = N * 2 * 4  # Read + Write, float32\n",
    "        ai = flops / bytes_trans\n",
    "        gflops = flops / elapsed / 1e9\n",
    "        \n",
    "        results.append((ai, gflops))\n",
    "        print(f\"Iterations: {iters:4d}, AI: {ai:7.2f}, Performance: {gflops:8.2f} GFLOPS\")\n",
    "    \n",
    "    # Plot measured vs roofline\n",
    "    ais, perfs = zip(*results)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Theoretical roofline (T4 specs)\n",
    "    ai_range = np.logspace(-1, 3, 100)\n",
    "    roofline = np.minimum(8100, 320 * ai_range)\n",
    "    plt.loglog(ai_range, roofline, 'b-', linewidth=2, label='Roofline')\n",
    "    \n",
    "    # Measured points\n",
    "    plt.scatter(ais, perfs, s=100, c='red', marker='o', label='Measured', zorder=5)\n",
    "    \n",
    "    plt.xlabel('Arithmetic Intensity (FLOP/Byte)')\n",
    "    plt.ylabel('Performance (GFLOPS)')\n",
    "    plt.title('Measured Performance vs Roofline')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No CUDA GPU available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a08a6",
   "metadata": {},
   "source": [
    "## 7. Optimization Guidance from Roofline\n",
    "\n",
    "### If Memory-Bound (below ridge point)\n",
    "\n",
    "1. **Improve memory access patterns**\n",
    "   - Ensure coalescing\n",
    "   - Reduce bank conflicts\n",
    "   \n",
    "2. **Use memory hierarchy**\n",
    "   - Cache in shared memory\n",
    "   - Use texture/constant memory\n",
    "   \n",
    "3. **Reduce memory traffic**\n",
    "   - Compute redundant values\n",
    "   - Compress data\n",
    "   \n",
    "4. **Increase arithmetic intensity**\n",
    "   - Fuse kernels\n",
    "   - Compute more per load\n",
    "\n",
    "### If Compute-Bound (above ridge point)\n",
    "\n",
    "1. **Increase parallelism**\n",
    "   - More threads\n",
    "   - Instruction-level parallelism\n",
    "   \n",
    "2. **Reduce instruction latency**\n",
    "   - Use intrinsics\n",
    "   - Avoid divergence\n",
    "   \n",
    "3. **Use specialized units**\n",
    "   - Tensor cores for matrix ops\n",
    "   - FP16 for doubled throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8eeb7",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### Roofline Essentials\n",
    "\n",
    "1. **Arithmetic Intensity (AI)** = FLOP / Bytes transferred\n",
    "2. **Ridge Point** = Peak Compute / Peak Bandwidth\n",
    "3. **Below ridge** = Memory-bound, optimize memory access\n",
    "4. **Above ridge** = Compute-bound, optimize compute\n",
    "\n",
    "### Common AI Values\n",
    "\n",
    "| Operation | AI (FLOP/Byte) | Bound |\n",
    "|-----------|----------------|-------|\n",
    "| Vector copy | 0 | Memory |\n",
    "| Vector add | 0.08 | Memory |\n",
    "| SAXPY | 0.17 | Memory |\n",
    "| Dot product | 0.38 | Memory |\n",
    "| Stencil | 0.5-3 | Memory/Balanced |\n",
    "| GEMM (large) | 50-200 | Compute |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. Calculate theoretical AI before optimizing\n",
    "2. Use `ncu --set roofline` for measured roofline\n",
    "3. Focus optimization on actual bottleneck\n",
    "4. Kernel fusion can increase AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967250e",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "### Exercise 1: Calculate AI\n",
    "Calculate the arithmetic intensity for a 5-point 2D stencil:\n",
    "```cpp\n",
    "out[i][j] = in[i][j] + in[i-1][j] + in[i+1][j] + in[i][j-1] + in[i][j+1];\n",
    "```\n",
    "\n",
    "### Exercise 2: Roofline Position\n",
    "Given a GPU with:\n",
    "- Peak compute: 15 TFLOPS\n",
    "- Peak bandwidth: 500 GB/s\n",
    "\n",
    "Where on the roofline would a kernel with AI=5 FLOP/Byte appear?\n",
    "\n",
    "### Exercise 3: Profile with ncu\n",
    "Use Nsight Compute to generate a roofline analysis for one of your kernels.\n",
    "\n",
    "### Exercise 4: Increase AI\n",
    "Modify a memory-bound kernel to increase its arithmetic intensity by fusing multiple operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9daab",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Today you learned:\n",
    "- The **roofline model** visualizes performance bounds\n",
    "- **Arithmetic intensity** determines if you're memory or compute bound\n",
    "- **Ridge point** is the boundary between regions\n",
    "- Use `ncu --set roofline` for visual analysis\n",
    "- Optimization strategy depends on which roof limits you\n",
    "\n",
    "**Next**: Day 3 - Nsight Systems for application-level profiling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
