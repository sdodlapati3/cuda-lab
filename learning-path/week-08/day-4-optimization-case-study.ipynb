{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed42ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "## Part 1: Case Study - Softmax Optimization\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Softmax is ubiquitous in deep learning:\n",
    "\n",
    "```\n",
    "softmax(x_i) = exp(x_i - max(x)) / sum(exp(x_j - max(x)))\n",
    "\n",
    "Three passes needed:\n",
    "1. Find max(x)           - Reduction\n",
    "2. Compute sum(exp(...)) - Map + Reduction  \n",
    "3. Divide by sum         - Map\n",
    "```\n",
    "\n",
    "### CUDA C++ Optimization Journey\n",
    "\n",
    "The following code demonstrates the optimization journey from naive to online algorithm:\n",
    "- **Version 0**: Naive (three separate kernels)\n",
    "- **Version 1**: Fused (single kernel, multiple passes)\n",
    "- **Version 2**: Online algorithm (single pass using running max/sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile softmax_optimization.cu\n",
    "// softmax_optimization.cu - Complete optimization case study\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <float.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Version 0: Naive (three separate kernels)\n",
    "__global__ void findMax(const float* input, float* maxVal, int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load and find max\n",
    "    sdata[tid] = (i < n) ? input[i] : -FLT_MAX;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduce\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicMax((int*)maxVal, __float_as_int(sdata[0]));\n",
    "}\n",
    "\n",
    "__global__ void computeExpSum(const float* input, float maxVal, \n",
    "                              float* expSum, int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (i < n) ? expf(input[i] - maxVal) : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(expSum, sdata[0]);\n",
    "}\n",
    "\n",
    "__global__ void applySoftmax(const float* input, float maxVal, \n",
    "                             float expSum, float* output, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        output[i] = expf(input[i] - maxVal) / expSum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Version 1: Fused (single pass per row)\n",
    "__global__ void softmaxFused(const float* input, float* output, \n",
    "                             int rows, int cols) {\n",
    "    extern __shared__ float sdata[];\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    const float* rowIn = input + row * cols;\n",
    "    float* rowOut = output + row * cols;\n",
    "    \n",
    "    // Step 1: Find max (single pass over row)\n",
    "    float localMax = -FLT_MAX;\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        localMax = fmaxf(localMax, rowIn[i]);\n",
    "    }\n",
    "    sdata[tid] = localMax;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduce to find row max\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);\n",
    "        __syncthreads();\n",
    "    }\n",
    "    float rowMax = sdata[0];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Step 2: Compute exp and sum\n",
    "    float localSum = 0.0f;\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        localSum += expf(rowIn[i] - rowMax);\n",
    "    }\n",
    "    sdata[tid] = localSum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    float rowSum = sdata[0];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Step 3: Apply softmax\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        rowOut[i] = expf(rowIn[i] - rowMax) / rowSum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Version 2: Online softmax (single pass)\n",
    "__global__ void softmaxOnline(const float* input, float* output,\n",
    "                              int rows, int cols) {\n",
    "    extern __shared__ float sdata[];\n",
    "    float* smaxes = sdata;\n",
    "    float* ssums = sdata + blockDim.x;\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    const float* rowIn = input + row * cols;\n",
    "    float* rowOut = output + row * cols;\n",
    "    \n",
    "    // Online algorithm: track max and sum in single pass\n",
    "    float localMax = -FLT_MAX;\n",
    "    float localSum = 0.0f;\n",
    "    \n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        float val = rowIn[i];\n",
    "        if (val > localMax) {\n",
    "            // Rescale existing sum\n",
    "            localSum *= expf(localMax - val);\n",
    "            localMax = val;\n",
    "        }\n",
    "        localSum += expf(val - localMax);\n",
    "    }\n",
    "    \n",
    "    smaxes[tid] = localMax;\n",
    "    ssums[tid] = localSum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduce (combining max and sum together)\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            float maxA = smaxes[tid], maxB = smaxes[tid + s];\n",
    "            float sumA = ssums[tid], sumB = ssums[tid + s];\n",
    "            \n",
    "            if (maxA > maxB) {\n",
    "                ssums[tid] = sumA + sumB * expf(maxB - maxA);\n",
    "                smaxes[tid] = maxA;\n",
    "            } else {\n",
    "                ssums[tid] = sumA * expf(maxA - maxB) + sumB;\n",
    "                smaxes[tid] = maxB;\n",
    "            }\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    float rowMax = smaxes[0];\n",
    "    float rowSum = ssums[0];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Apply softmax\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        rowOut[i] = expf(rowIn[i] - rowMax) / rowSum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int rows = 1024, cols = 4096;\n",
    "    size_t size = rows * cols * sizeof(float);\n",
    "    \n",
    "    float *h_input = (float*)malloc(size);\n",
    "    float *h_output = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < rows * cols; i++) {\n",
    "        h_input[i] = (float)(rand() % 100) / 10.0f - 5.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, size);\n",
    "    cudaMalloc(&d_output, size);\n",
    "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Benchmark each version\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int sharedSize = blockSize * sizeof(float);\n",
    "    \n",
    "    // Version 1: Fused\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        softmaxFused<<<rows, blockSize, sharedSize>>>(d_input, d_output, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float fusedTime;\n",
    "    cudaEventElapsedTime(&fusedTime, start, stop);\n",
    "    \n",
    "    // Version 2: Online\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        softmaxOnline<<<rows, blockSize, 2 * sharedSize>>>(d_input, d_output, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float onlineTime;\n",
    "    cudaEventElapsedTime(&onlineTime, start, stop);\n",
    "    \n",
    "    printf(\"=== Softmax Optimization Results ===\\n\");\n",
    "    printf(\"Matrix: %d x %d\\n\", rows, cols);\n",
    "    printf(\"Fused (3 passes):  %.2f ms\\n\", fusedTime / 100);\n",
    "    printf(\"Online (1 pass):   %.2f ms\\n\", onlineTime / 100);\n",
    "    printf(\"Speedup: %.2fx\\n\", fusedTime / onlineTime);\n",
    "    \n",
    "    // Calculate bandwidth\n",
    "    float bytes = 2.0f * rows * cols * sizeof(float); // Read + write\n",
    "    printf(\"Online bandwidth: %.1f GB/s\\n\", \n",
    "           bytes / (onlineTime / 100 * 1e6));\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3300eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o softmax_optimization softmax_optimization.cu\n",
    "!./softmax_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb6931",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Optimization Stages\n",
    "\n",
    "### Stage-by-Stage Analysis\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Version 0: Naive (3 separate kernels)                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Problem: 3 kernel launches, 3 global memory passes              â”‚\n",
    "â”‚ Memory: Read input 3x, write output 1x + intermediates          â”‚\n",
    "â”‚ Launch: Overhead for 3 kernels Ã— rows                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Version 1: Fused (1 kernel, 3 passes per row)                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Improvement: Single kernel, row-level parallelism               â”‚\n",
    "â”‚ Memory: Still 3 passes over row data                            â”‚\n",
    "â”‚ Problem: Re-reading row 3 times from global memory              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Version 2: Online Algorithm (1 pass)                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Improvement: Compute max and sum simultaneously!                â”‚\n",
    "â”‚ Memory: Read input 2x (once for max+sum, once for output)       â”‚\n",
    "â”‚ Trick: Rescale running sum when new max found                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173edb4",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Implementation (OPTIONAL - for verification)\n",
    "\n",
    "@cuda.jit\n",
    "def softmax_naive(input_arr, output_arr, n):\n",
    "    \"\"\"Naive: Each thread handles one element (bad!)\"\"\"\n",
    "    i = cuda.grid(1)\n",
    "    if i >= n:\n",
    "        return\n",
    "    \n",
    "    # Each thread finds global max (terrible!)\n",
    "    max_val = input_arr[0]\n",
    "    for j in range(n):\n",
    "        if input_arr[j] > max_val:\n",
    "            max_val = input_arr[j]\n",
    "    \n",
    "    # Each thread computes sum (terrible!)\n",
    "    exp_sum = 0.0\n",
    "    for j in range(n):\n",
    "        exp_sum += math.exp(input_arr[j] - max_val)\n",
    "    \n",
    "    # Each thread writes its element\n",
    "    output_arr[i] = math.exp(input_arr[i] - max_val) / exp_sum\n",
    "\n",
    "@cuda.jit\n",
    "def softmax_shared(input_arr, output_arr, n):\n",
    "    \"\"\"Better: Use shared memory for reductions\"\"\"\n",
    "    sdata = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + tid\n",
    "    \n",
    "    # Load to shared memory\n",
    "    sdata[tid] = input_arr[i] if i < n else -1e38\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Find block max\n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s and sdata[tid + s] > sdata[tid]:\n",
    "            sdata[tid] = sdata[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    block_max = sdata[0]\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Compute exp sum\n",
    "    sdata[tid] = math.exp(input_arr[i] - block_max) if i < n else 0.0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s:\n",
    "            sdata[tid] += sdata[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    exp_sum = sdata[0]\n",
    "    \n",
    "    if i < n:\n",
    "        output_arr[i] = math.exp(input_arr[i] - block_max) / exp_sum\n",
    "\n",
    "# Test\n",
    "n = 256\n",
    "x = np.random.randn(n).astype(np.float32)\n",
    "y_gpu = np.zeros_like(x)\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y_gpu)\n",
    "\n",
    "softmax_shared[1, 256](d_x, d_y, n)\n",
    "y_gpu = d_y.copy_to_host()\n",
    "\n",
    "# Compare with NumPy\n",
    "y_cpu = np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n",
    "\n",
    "print(f\"Max error: {np.max(np.abs(y_gpu - y_cpu)):.2e}\")\n",
    "print(f\"Sum of softmax: {np.sum(y_gpu):.6f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89612274",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Profiling-Driven Optimization\n",
    "\n",
    "### Optimization Workflow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    PROFILING WORKFLOW                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. BASELINE                                                    â”‚\n",
    "â”‚     â””â”€ Run ncu on naive version                                 â”‚\n",
    "â”‚     â””â”€ Record: time, bandwidth, occupancy, bottleneck           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  2. IDENTIFY BOTTLENECK                                         â”‚\n",
    "â”‚     â”œâ”€ Memory bound? â†’ Improve access patterns                  â”‚\n",
    "â”‚     â”œâ”€ Compute bound? â†’ Reduce instructions                     â”‚\n",
    "â”‚     â””â”€ Latency bound? â†’ Increase occupancy                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  3. OPTIMIZE                                                    â”‚\n",
    "â”‚     â””â”€ Apply ONE optimization                                   â”‚\n",
    "â”‚     â””â”€ Verify correctness!                                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  4. MEASURE                                                     â”‚\n",
    "â”‚     â””â”€ Re-profile                                               â”‚\n",
    "â”‚     â””â”€ Compare metrics                                          â”‚\n",
    "â”‚     â””â”€ If improved, keep. If not, revert.                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  5. REPEAT until satisfied                                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Metrics to Track\n",
    "\n",
    "```cpp\n",
    "// Profile command for each version\n",
    "// ncu --set full -o softmax_v0 ./softmax_opt  # Baseline\n",
    "// ncu --set full -o softmax_v1 ./softmax_opt  # After opt 1\n",
    "\n",
    "/*\n",
    "Track these metrics across versions:\n",
    "\n",
    "| Metric                  | V0     | V1     | V2     |\n",
    "|-------------------------|--------|--------|--------|\n",
    "| Time (ms)               | 5.2    | 2.1    | 1.4    |\n",
    "| Memory Throughput (%)   | 45%    | 72%    | 89%    |\n",
    "| L2 Hit Rate             | 12%    | 35%    | 68%    |\n",
    "| Occupancy               | 25%    | 50%    | 75%    |\n",
    "| Achieved Bandwidth      | 180    | 290    | 380    |\n",
    "| Theoretical BW          | 400    | 400    | 400    |\n",
    "| Efficiency              | 45%    | 73%    | 95%    |\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ce6b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Complete Optimization Checklist\n",
    "\n",
    "### The GPU Optimization Checklist\n",
    "\n",
    "```\n",
    "â–¡ MEMORY ACCESS\n",
    "  â”œâ”€ â–¡ Coalesced global memory access?\n",
    "  â”œâ”€ â–¡ Shared memory for reuse?\n",
    "  â”œâ”€ â–¡ Bank conflict free?\n",
    "  â””â”€ â–¡ Minimize global memory traffic?\n",
    "\n",
    "â–¡ PARALLELISM\n",
    "  â”œâ”€ â–¡ Sufficient threads (>= 10K)?\n",
    "  â”œâ”€ â–¡ Good occupancy (>= 50%)?\n",
    "  â”œâ”€ â–¡ Load balanced work?\n",
    "  â””â”€ â–¡ Avoid thread divergence?\n",
    "\n",
    "â–¡ KERNEL DESIGN\n",
    "  â”œâ”€ â–¡ Fused operations (reduce launches)?\n",
    "  â”œâ”€ â–¡ Grid-stride loops for flexibility?\n",
    "  â”œâ”€ â–¡ Warp primitives where applicable?\n",
    "  â””â”€ â–¡ Appropriate block size?\n",
    "\n",
    "â–¡ SYNCHRONIZATION\n",
    "  â”œâ”€ â–¡ Minimize __syncthreads()?\n",
    "  â”œâ”€ â–¡ Use warp-level sync when possible?\n",
    "  â””â”€ â–¡ Atomics only when necessary?\n",
    "\n",
    "â–¡ PROFILING\n",
    "  â”œâ”€ â–¡ Identified bottleneck?\n",
    "  â”œâ”€ â–¡ Measured before/after?\n",
    "  â””â”€ â–¡ Compared to theoretical peak?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95ce70",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: LayerNorm Optimization\n",
    "\n",
    "Apply the same optimization journey to LayerNorm:\n",
    "\n",
    "```cpp\n",
    "// LayerNorm: y = (x - mean) / sqrt(var + eps) * gamma + beta\n",
    "\n",
    "// TODO: Implement these versions:\n",
    "// V0: Separate kernels for mean, variance, normalize\n",
    "// V1: Fused single kernel\n",
    "// V2: Online algorithm (Welford's method)\n",
    "\n",
    "__global__ void layernormOnline(const float* input, float* output,\n",
    "                                const float* gamma, const float* beta,\n",
    "                                int rows, int cols, float eps) {\n",
    "    // Welford's online algorithm for mean and variance\n",
    "    // Your implementation here\n",
    "}\n",
    "```\n",
    "\n",
    "### Exercise 2: Profile and Document\n",
    "\n",
    "For your softmax implementation:\n",
    "1. Run `ncu` on each version\n",
    "2. Create a table of metrics\n",
    "3. Identify the bottleneck at each stage\n",
    "4. Calculate achieved vs theoretical bandwidth\n",
    "\n",
    "### Exercise 3: Vectorized Memory Access\n",
    "\n",
    "```cpp\n",
    "// Add vectorized loads to improve bandwidth\n",
    "__global__ void softmaxVectorized(const float4* input, float4* output,\n",
    "                                  int rows, int cols) {\n",
    "    // Load 4 floats at once with float4\n",
    "    // Process and store\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dad4ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  OPTIMIZATION WORKFLOW                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. Profile first - never guess!                                â”‚\n",
    "â”‚  2. One optimization at a time                                  â”‚\n",
    "â”‚  3. Measure before and after                                    â”‚\n",
    "â”‚  4. Document your journey                                       â”‚\n",
    "â”‚  5. Compare to theoretical limits                               â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Key Techniques Used:                                           â”‚\n",
    "â”‚  â€¢ Kernel fusion (reduce launches)                              â”‚\n",
    "â”‚  â€¢ Online algorithms (reduce passes)                            â”‚\n",
    "â”‚  â€¢ Shared memory (reduce global access)                         â”‚\n",
    "â”‚  â€¢ Warp primitives (reduce sync overhead)                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Day 5:** Practice & checkpoint quiz for Week 8!\n",
    "\n",
    "**Week 9:** We'll explore CUDA Streams & Concurrency for overlapping computation with data transfer."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
