{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"âš ï¸  CUDA C++ is PRIMARY. Python/Numba for quick testing only.\")\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed42ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "## Part 1: Case Study - Softmax Optimization\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Softmax is ubiquitous in deep learning:\n",
    "\n",
    "```\n",
    "softmax(x_i) = exp(x_i - max(x)) / sum(exp(x_j - max(x)))\n",
    "\n",
    "Three passes needed:\n",
    "1. Find max(x)           - Reduction\n",
    "2. Compute sum(exp(...)) - Map + Reduction  \n",
    "3. Divide by sum         - Map\n",
    "```\n",
    "\n",
    "### CUDA C++ Optimization Journey\n",
    "\n",
    "The following code demonstrates the optimization journey from naive to online algorithm:\n",
    "- **Version 0**: Naive (three separate kernels)\n",
    "- **Version 1**: Fused (single kernel, multiple passes)\n",
    "- **Version 2**: Online algorithm (single pass using running max/sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile softmax_optimization.cu\n",
    "// softmax_optimization.cu - Complete optimization case study\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <float.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Version 0: Naive (three separate kernels)\n",
    "__global__ void findMax(const float* input, float* maxVal, int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load and find max\n",
    "    sdata[tid] = (i < n) ? input[i] : -FLT_MAX;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduce\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicMax((int*)maxVal, __float_as_int(sdata[0]));\n",
    "}\n",
    "\n",
    "__global__ void computeExpSum(const float* input, float maxVal, \n",
    "                              float* expSum, int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (i < n) ? expf(input[i] - maxVal) : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) atomicAdd(expSum, sdata[0]);\n",
    "}\n",
    "\n",
    "__global__ void applySoftmax(const float* input, float maxVal, \n",
    "                             float expSum, float* output, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        output[i] = expf(input[i] - maxVal) / expSum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Version 1: Fused (single pass per row)\n",
    "__global__ void softmaxFused(const float* input, float* output, \n",
    "                             int rows, int cols) {\n",
    "    extern __shared__ float sdata[];\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    const float* rowIn = input + row * cols;\n",
    "    float* rowOut = output + row * cols;\n",
    "    \n",
    "    // Step 1: Find max (single pass over row)\n",
    "    float localMax = -FLT_MAX;\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        localMax = fmaxf(localMax, rowIn[i]);\n",
    "    }\n",
    "    sdata[tid] = localMax;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduce to find row max\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);\n",
    "        __syncthreads();\n",
    "    }\n",
    "    float rowMax = sdata[0];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Step 2: Compute exp and sum\n",
    "    float localSum = 0.0f;\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        localSum += expf(rowIn[i] - rowMax);\n",
    "    }\n",
    "    sdata[tid] = localSum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    float rowSum = sdata[0];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Step 3: Apply softmax\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        rowOut[i] = expf(rowIn[i] - rowMax) / rowSum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Version 2: Online softmax (single pass)\n",
    "__global__ void softmaxOnline(const float* input, float* output,\n",
    "                              int rows, int cols) {\n",
    "    extern __shared__ float sdata[];\n",
    "    float* smaxes = sdata;\n",
    "    float* ssums = sdata + blockDim.x;\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    const float* rowIn = input + row * cols;\n",
    "    float* rowOut = output + row * cols;\n",
    "    \n",
    "    // Online algorithm: track max and sum in single pass\n",
    "    float localMax = -FLT_MAX;\n",
    "    float localSum = 0.0f;\n",
    "    \n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        float val = rowIn[i];\n",
    "        if (val > localMax) {\n",
    "            // Rescale existing sum\n",
    "            localSum *= expf(localMax - val);\n",
    "            localMax = val;\n",
    "        }\n",
    "        localSum += expf(val - localMax);\n",
    "    }\n",
    "    \n",
    "    smaxes[tid] = localMax;\n",
    "    ssums[tid] = localSum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduce (combining max and sum together)\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            float maxA = smaxes[tid], maxB = smaxes[tid + s];\n",
    "            float sumA = ssums[tid], sumB = ssums[tid + s];\n",
    "            \n",
    "            if (maxA > maxB) {\n",
    "                ssums[tid] = sumA + sumB * expf(maxB - maxA);\n",
    "                smaxes[tid] = maxA;\n",
    "            } else {\n",
    "                ssums[tid] = sumA * expf(maxA - maxB) + sumB;\n",
    "                smaxes[tid] = maxB;\n",
    "            }\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    float rowMax = smaxes[0];\n",
    "    float rowSum = ssums[0];\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Apply softmax\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        rowOut[i] = expf(rowIn[i] - rowMax) / rowSum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int rows = 1024, cols = 4096;\n",
    "    size_t size = rows * cols * sizeof(float);\n",
    "    \n",
    "    float *h_input = (float*)malloc(size);\n",
    "    float *h_output = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize\n",
    "    for (int i = 0; i < rows * cols; i++) {\n",
    "        h_input[i] = (float)(rand() % 100) / 10.0f - 5.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, size);\n",
    "    cudaMalloc(&d_output, size);\n",
    "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Benchmark each version\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int sharedSize = blockSize * sizeof(float);\n",
    "    \n",
    "    // Version 1: Fused\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        softmaxFused<<<rows, blockSize, sharedSize>>>(d_input, d_output, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float fusedTime;\n",
    "    cudaEventElapsedTime(&fusedTime, start, stop);\n",
    "    \n",
    "    // Version 2: Online\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        softmaxOnline<<<rows, blockSize, 2 * sharedSize>>>(d_input, d_output, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float onlineTime;\n",
    "    cudaEventElapsedTime(&onlineTime, start, stop);\n",
    "    \n",
    "    printf(\"=== Softmax Optimization Results ===\\n\");\n",
    "    printf(\"Matrix: %d x %d\\n\", rows, cols);\n",
    "    printf(\"Fused (3 passes):  %.2f ms\\n\", fusedTime / 100);\n",
    "    printf(\"Online (1 pass):   %.2f ms\\n\", onlineTime / 100);\n",
    "    printf(\"Speedup: %.2fx\\n\", fusedTime / onlineTime);\n",
    "    \n",
    "    // Calculate bandwidth\n",
    "    float bytes = 2.0f * rows * cols * sizeof(float); // Read + write\n",
    "    printf(\"Online bandwidth: %.1f GB/s\\n\", \n",
    "           bytes / (onlineTime / 100 * 1e6));\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3300eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -o softmax_optimization softmax_optimization.cu\n",
    "!./softmax_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb6931",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Optimization Stages\n",
    "\n",
    "### Stage-by-Stage Analysis\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Version 0: Naive (3 separate kernels)                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Problem: 3 kernel launches, 3 global memory passes              â”‚\n",
    "â”‚ Memory: Read input 3x, write output 1x + intermediates          â”‚\n",
    "â”‚ Launch: Overhead for 3 kernels Ã— rows                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Version 1: Fused (1 kernel, 3 passes per row)                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Improvement: Single kernel, row-level parallelism               â”‚\n",
    "â”‚ Memory: Still 3 passes over row data                            â”‚\n",
    "â”‚ Problem: Re-reading row 3 times from global memory              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Version 2: Online Algorithm (1 pass)                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Improvement: Compute max and sum simultaneously!                â”‚\n",
    "â”‚ Memory: Read input 2x (once for max+sum, once for output)       â”‚\n",
    "â”‚ Trick: Rescale running sum when new max found                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173edb4",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/Numba Implementation (OPTIONAL - for verification)\n",
    "\n",
    "@cuda.jit\n",
    "def softmax_naive(input_arr, output_arr, n):\n",
    "    \"\"\"Naive: Each thread handles one element (bad!)\"\"\"\n",
    "    i = cuda.grid(1)\n",
    "    if i >= n:\n",
    "        return\n",
    "    \n",
    "    # Each thread finds global max (terrible!)\n",
    "    max_val = input_arr[0]\n",
    "    for j in range(n):\n",
    "        if input_arr[j] > max_val:\n",
    "            max_val = input_arr[j]\n",
    "    \n",
    "    # Each thread computes sum (terrible!)\n",
    "    exp_sum = 0.0\n",
    "    for j in range(n):\n",
    "        exp_sum += math.exp(input_arr[j] - max_val)\n",
    "    \n",
    "    # Each thread writes its element\n",
    "    output_arr[i] = math.exp(input_arr[i] - max_val) / exp_sum\n",
    "\n",
    "@cuda.jit\n",
    "def softmax_shared(input_arr, output_arr, n):\n",
    "    \"\"\"Better: Use shared memory for reductions\"\"\"\n",
    "    sdata = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + tid\n",
    "    \n",
    "    # Load to shared memory\n",
    "    sdata[tid] = input_arr[i] if i < n else -1e38\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Find block max\n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s and sdata[tid + s] > sdata[tid]:\n",
    "            sdata[tid] = sdata[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    block_max = sdata[0]\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Compute exp sum\n",
    "    sdata[tid] = math.exp(input_arr[i] - block_max) if i < n else 0.0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s:\n",
    "            sdata[tid] += sdata[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "    \n",
    "    exp_sum = sdata[0]\n",
    "    \n",
    "    if i < n:\n",
    "        output_arr[i] = math.exp(input_arr[i] - block_max) / exp_sum\n",
    "\n",
    "# Test\n",
    "n = 256\n",
    "x = np.random.randn(n).astype(np.float32)\n",
    "y_gpu = np.zeros_like(x)\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y_gpu)\n",
    "\n",
    "softmax_shared[1, 256](d_x, d_y, n)\n",
    "y_gpu = d_y.copy_to_host()\n",
    "\n",
    "# Compare with NumPy\n",
    "y_cpu = np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n",
    "\n",
    "print(f\"Max error: {np.max(np.abs(y_gpu - y_cpu)):.2e}\")\n",
    "print(f\"Sum of softmax: {np.sum(y_gpu):.6f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89612274",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Profiling-Driven Optimization\n",
    "\n",
    "### Optimization Workflow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    PROFILING WORKFLOW                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. BASELINE                                                    â”‚\n",
    "â”‚     â””â”€ Run ncu on naive version                                 â”‚\n",
    "â”‚     â””â”€ Record: time, bandwidth, occupancy, bottleneck           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  2. IDENTIFY BOTTLENECK                                         â”‚\n",
    "â”‚     â”œâ”€ Memory bound? â†’ Improve access patterns                  â”‚\n",
    "â”‚     â”œâ”€ Compute bound? â†’ Reduce instructions                     â”‚\n",
    "â”‚     â””â”€ Latency bound? â†’ Increase occupancy                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  3. OPTIMIZE                                                    â”‚\n",
    "â”‚     â””â”€ Apply ONE optimization                                   â”‚\n",
    "â”‚     â””â”€ Verify correctness!                                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  4. MEASURE                                                     â”‚\n",
    "â”‚     â””â”€ Re-profile                                               â”‚\n",
    "â”‚     â””â”€ Compare metrics                                          â”‚\n",
    "â”‚     â””â”€ If improved, keep. If not, revert.                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  5. REPEAT until satisfied                                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Metrics to Track\n",
    "\n",
    "```cpp\n",
    "// Profile command for each version\n",
    "// ncu --set full -o softmax_v0 ./softmax_opt  # Baseline\n",
    "// ncu --set full -o softmax_v1 ./softmax_opt  # After opt 1\n",
    "\n",
    "/*\n",
    "Track these metrics across versions:\n",
    "\n",
    "| Metric                  | V0     | V1     | V2     |\n",
    "|-------------------------|--------|--------|--------|\n",
    "| Time (ms)               | 5.2    | 2.1    | 1.4    |\n",
    "| Memory Throughput (%)   | 45%    | 72%    | 89%    |\n",
    "| L2 Hit Rate             | 12%    | 35%    | 68%    |\n",
    "| Occupancy               | 25%    | 50%    | 75%    |\n",
    "| Achieved Bandwidth      | 180    | 290    | 380    |\n",
    "| Theoretical BW          | 400    | 400    | 400    |\n",
    "| Efficiency              | 45%    | 73%    | 95%    |\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ce6b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Complete Optimization Checklist\n",
    "\n",
    "### The GPU Optimization Checklist\n",
    "\n",
    "```\n",
    "â–¡ MEMORY ACCESS\n",
    "  â”œâ”€ â–¡ Coalesced global memory access?\n",
    "  â”œâ”€ â–¡ Shared memory for reuse?\n",
    "  â”œâ”€ â–¡ Bank conflict free?\n",
    "  â””â”€ â–¡ Minimize global memory traffic?\n",
    "\n",
    "â–¡ PARALLELISM\n",
    "  â”œâ”€ â–¡ Sufficient threads (>= 10K)?\n",
    "  â”œâ”€ â–¡ Good occupancy (>= 50%)?\n",
    "  â”œâ”€ â–¡ Load balanced work?\n",
    "  â””â”€ â–¡ Avoid thread divergence?\n",
    "\n",
    "â–¡ KERNEL DESIGN\n",
    "  â”œâ”€ â–¡ Fused operations (reduce launches)?\n",
    "  â”œâ”€ â–¡ Grid-stride loops for flexibility?\n",
    "  â”œâ”€ â–¡ Warp primitives where applicable?\n",
    "  â””â”€ â–¡ Appropriate block size?\n",
    "\n",
    "â–¡ SYNCHRONIZATION\n",
    "  â”œâ”€ â–¡ Minimize __syncthreads()?\n",
    "  â”œâ”€ â–¡ Use warp-level sync when possible?\n",
    "  â””â”€ â–¡ Atomics only when necessary?\n",
    "\n",
    "â–¡ PROFILING\n",
    "  â”œâ”€ â–¡ Identified bottleneck?\n",
    "  â”œâ”€ â–¡ Measured before/after?\n",
    "  â””â”€ â–¡ Compared to theoretical peak?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95ce70",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ab9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile optimization_exercises.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <float.h>\n",
    "\n",
    "#define CHECK_CUDA(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 1: LayerNorm Optimization Journey\n",
    "// ============================================================\n",
    "\n",
    "// V0: Naive - Multiple passes\n",
    "__global__ void layernormV0_mean(const float* input, float* means, int cols) {\n",
    "    int row = blockIdx.x;\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        sum += input[row * cols + i];\n",
    "    }\n",
    "    means[row] = sum / cols;\n",
    "}\n",
    "\n",
    "__global__ void layernormV0_var(const float* input, const float* means, \n",
    "                                 float* vars, int cols) {\n",
    "    int row = blockIdx.x;\n",
    "    float mean = means[row];\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        float diff = input[row * cols + i] - mean;\n",
    "        sum += diff * diff;\n",
    "    }\n",
    "    vars[row] = sum / cols;\n",
    "}\n",
    "\n",
    "__global__ void layernormV0_normalize(const float* input, float* output,\n",
    "                                       const float* means, const float* vars,\n",
    "                                       const float* gamma, const float* beta,\n",
    "                                       int cols, float eps) {\n",
    "    int row = blockIdx.x;\n",
    "    float mean = means[row];\n",
    "    float rstd = rsqrtf(vars[row] + eps);\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        int idx = row * cols + i;\n",
    "        output[idx] = (input[idx] - mean) * rstd * gamma[i] + beta[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// V1: Fused - Single kernel using parallel reduction\n",
    "__global__ void layernormV1_fused(const float* input, float* output,\n",
    "                                   const float* gamma, const float* beta,\n",
    "                                   int rows, int cols, float eps) {\n",
    "    extern __shared__ float smem[];\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Load and compute sum for mean\n",
    "    float sum = 0.0f;\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        sum += input[row * cols + i];\n",
    "    }\n",
    "    smem[tid] = sum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduce to compute mean\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) smem[tid] += smem[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    float mean = smem[0] / cols;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Compute variance\n",
    "    float var_sum = 0.0f;\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        float diff = input[row * cols + i] - mean;\n",
    "        var_sum += diff * diff;\n",
    "    }\n",
    "    smem[tid] = var_sum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) smem[tid] += smem[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    float rstd = rsqrtf(smem[0] / cols + eps);\n",
    "    \n",
    "    // Normalize\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        int idx = row * cols + i;\n",
    "        output[idx] = (input[idx] - mean) * rstd * gamma[i] + beta[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// V2: Online algorithm (Welford's method) - single pass for mean & variance\n",
    "__global__ void layernormV2_welford(const float* input, float* output,\n",
    "                                     const float* gamma, const float* beta,\n",
    "                                     int rows, int cols, float eps) {\n",
    "    extern __shared__ float smem[];\n",
    "    float* smem_mean = smem;\n",
    "    float* smem_m2 = smem + blockDim.x;\n",
    "    int* smem_count = (int*)(smem + 2 * blockDim.x);\n",
    "    \n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Welford's online algorithm for each thread\n",
    "    float mean = 0.0f;\n",
    "    float m2 = 0.0f;\n",
    "    int count = 0;\n",
    "    \n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        count++;\n",
    "        float x = input[row * cols + i];\n",
    "        float delta = x - mean;\n",
    "        mean += delta / count;\n",
    "        float delta2 = x - mean;\n",
    "        m2 += delta * delta2;\n",
    "    }\n",
    "    \n",
    "    smem_mean[tid] = mean;\n",
    "    smem_m2[tid] = m2;\n",
    "    smem_count[tid] = count;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Parallel reduction with Welford's merge\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s && smem_count[tid + s] > 0) {\n",
    "            int n_a = smem_count[tid];\n",
    "            int n_b = smem_count[tid + s];\n",
    "            int n = n_a + n_b;\n",
    "            float delta = smem_mean[tid + s] - smem_mean[tid];\n",
    "            smem_mean[tid] = (n_a * smem_mean[tid] + n_b * smem_mean[tid + s]) / n;\n",
    "            smem_m2[tid] += smem_m2[tid + s] + delta * delta * n_a * n_b / n;\n",
    "            smem_count[tid] = n;\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    float final_mean = smem_mean[0];\n",
    "    float rstd = rsqrtf(smem_m2[0] / cols + eps);\n",
    "    \n",
    "    // Normalize\n",
    "    for (int i = tid; i < cols; i += blockDim.x) {\n",
    "        int idx = row * cols + i;\n",
    "        output[idx] = (input[idx] - final_mean) * rstd * gamma[i] + beta[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "void benchmarkLayerNorm() {\n",
    "    printf(\"=== Exercise 1: LayerNorm Optimization Journey ===\\n\");\n",
    "    \n",
    "    const int rows = 1024;\n",
    "    const int cols = 2048;\n",
    "    const float eps = 1e-5f;\n",
    "    \n",
    "    float *d_input, *d_output, *d_gamma, *d_beta, *d_means, *d_vars;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, rows * cols * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, rows * cols * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_gamma, cols * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_beta, cols * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_means, rows * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_vars, rows * sizeof(float)));\n",
    "    \n",
    "    // Initialize gamma=1, beta=0\n",
    "    float* h_gamma = (float*)malloc(cols * sizeof(float));\n",
    "    float* h_beta = (float*)malloc(cols * sizeof(float));\n",
    "    for (int i = 0; i < cols; i++) { h_gamma[i] = 1.0f; h_beta[i] = 0.0f; }\n",
    "    cudaMemcpy(d_gamma, h_gamma, cols * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_beta, h_beta, cols * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int iters = 100;\n",
    "    \n",
    "    // V0: Naive multi-pass\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iters; i++) {\n",
    "        layernormV0_mean<<<rows, 1>>>(d_input, d_means, cols);\n",
    "        layernormV0_var<<<rows, 1>>>(d_input, d_means, d_vars, cols);\n",
    "        layernormV0_normalize<<<rows, 1>>>(d_input, d_output, d_means, d_vars,\n",
    "                                           d_gamma, d_beta, cols, eps);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float v0Ms;\n",
    "    cudaEventElapsedTime(&v0Ms, start, stop);\n",
    "    \n",
    "    // V1: Fused\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iters; i++) {\n",
    "        layernormV1_fused<<<rows, 256, 256 * sizeof(float)>>>(\n",
    "            d_input, d_output, d_gamma, d_beta, rows, cols, eps);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float v1Ms;\n",
    "    cudaEventElapsedTime(&v1Ms, start, stop);\n",
    "    \n",
    "    // V2: Welford\n",
    "    size_t smemSize = 2 * 256 * sizeof(float) + 256 * sizeof(int);\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iters; i++) {\n",
    "        layernormV2_welford<<<rows, 256, smemSize>>>(\n",
    "            d_input, d_output, d_gamma, d_beta, rows, cols, eps);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float v2Ms;\n",
    "    cudaEventElapsedTime(&v2Ms, start, stop);\n",
    "    \n",
    "    printf(\"V0 (Naive multi-pass): %.2f ms\\n\", v0Ms);\n",
    "    printf(\"V1 (Fused):            %.2f ms (%.2fx speedup)\\n\", v1Ms, v0Ms / v1Ms);\n",
    "    printf(\"V2 (Welford):          %.2f ms (%.2fx speedup)\\n\\n\", v2Ms, v0Ms / v2Ms);\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_gamma);\n",
    "    cudaFree(d_beta);\n",
    "    cudaFree(d_means);\n",
    "    cudaFree(d_vars);\n",
    "    free(h_gamma);\n",
    "    free(h_beta);\n",
    "}\n",
    "\n",
    "// ============================================================\n",
    "// Exercise 2: Vectorized Memory Access\n",
    "// ============================================================\n",
    "\n",
    "__global__ void softmaxNaive(const float* input, float* output, int rows, int cols) {\n",
    "    int row = blockIdx.x;\n",
    "    \n",
    "    // Find max\n",
    "    float maxVal = -FLT_MAX;\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        maxVal = fmaxf(maxVal, input[row * cols + i]);\n",
    "    }\n",
    "    \n",
    "    // Compute exp and sum\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        sum += expf(input[row * cols + i] - maxVal);\n",
    "    }\n",
    "    \n",
    "    // Normalize\n",
    "    for (int i = 0; i < cols; i++) {\n",
    "        int idx = row * cols + i;\n",
    "        output[idx] = expf(input[idx] - maxVal) / sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void softmaxVectorized(const float4* input, float4* output, int rows, int cols4) {\n",
    "    int row = blockIdx.x;\n",
    "    int cols = cols4 * 4;\n",
    "    \n",
    "    // Find max using float4\n",
    "    float maxVal = -FLT_MAX;\n",
    "    for (int i = 0; i < cols4; i++) {\n",
    "        float4 val = input[row * cols4 + i];\n",
    "        maxVal = fmaxf(maxVal, val.x);\n",
    "        maxVal = fmaxf(maxVal, val.y);\n",
    "        maxVal = fmaxf(maxVal, val.z);\n",
    "        maxVal = fmaxf(maxVal, val.w);\n",
    "    }\n",
    "    \n",
    "    // Compute exp and sum\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < cols4; i++) {\n",
    "        float4 val = input[row * cols4 + i];\n",
    "        sum += expf(val.x - maxVal);\n",
    "        sum += expf(val.y - maxVal);\n",
    "        sum += expf(val.z - maxVal);\n",
    "        sum += expf(val.w - maxVal);\n",
    "    }\n",
    "    \n",
    "    float rsum = 1.0f / sum;\n",
    "    \n",
    "    // Normalize\n",
    "    for (int i = 0; i < cols4; i++) {\n",
    "        float4 val = input[row * cols4 + i];\n",
    "        float4 result;\n",
    "        result.x = expf(val.x - maxVal) * rsum;\n",
    "        result.y = expf(val.y - maxVal) * rsum;\n",
    "        result.z = expf(val.z - maxVal) * rsum;\n",
    "        result.w = expf(val.w - maxVal) * rsum;\n",
    "        output[row * cols4 + i] = result;\n",
    "    }\n",
    "}\n",
    "\n",
    "void benchmarkVectorized() {\n",
    "    printf(\"=== Exercise 2: Vectorized Memory Access ===\\n\");\n",
    "    \n",
    "    const int rows = 1024;\n",
    "    const int cols = 2048;  // Must be divisible by 4\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, rows * cols * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, rows * cols * sizeof(float)));\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    int iters = 100;\n",
    "    \n",
    "    // Naive\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iters; i++) {\n",
    "        softmaxNaive<<<rows, 1>>>(d_input, d_output, rows, cols);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float naiveMs;\n",
    "    cudaEventElapsedTime(&naiveMs, start, stop);\n",
    "    \n",
    "    // Vectorized\n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < iters; i++) {\n",
    "        softmaxVectorized<<<rows, 1>>>((float4*)d_input, (float4*)d_output, rows, cols / 4);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float vecMs;\n",
    "    cudaEventElapsedTime(&vecMs, start, stop);\n",
    "    \n",
    "    printf(\"Naive:      %.2f ms\\n\", naiveMs);\n",
    "    printf(\"Vectorized: %.2f ms (%.2fx speedup)\\n\\n\", vecMs, naiveMs / vecMs);\n",
    "    \n",
    "    // Calculate bandwidth\n",
    "    float dataBytes = rows * cols * sizeof(float) * 2 * iters;  // read + write\n",
    "    printf(\"Naive bandwidth:      %.2f GB/s\\n\", dataBytes / (naiveMs * 1e6));\n",
    "    printf(\"Vectorized bandwidth: %.2f GB/s\\n\\n\", dataBytes / (vecMs * 1e6));\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n\");\n",
    "    printf(\"â•‘           Optimization Case Study Exercises                  â•‘\\n\");\n",
    "    printf(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Memory Bandwidth: %.0f GB/s (theoretical)\\n\\n\", \n",
    "           2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) / 1e6);\n",
    "    \n",
    "    benchmarkLayerNorm();\n",
    "    benchmarkVectorized();\n",
    "    \n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    printf(\"                    All exercises completed!\\n\");\n",
    "    printf(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o optimization_exercises optimization_exercises.cu && ./optimization_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ec086",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "### Exercise 1: LayerNorm Optimization\n",
    "\n",
    "Apply the same optimization journey to LayerNorm:\n",
    "\n",
    "```cpp\n",
    "// LayerNorm: y = (x - mean) / sqrt(var + eps) * gamma + beta\n",
    "\n",
    "// TODO: Implement these versions:\n",
    "// V0: Separate kernels for mean, variance, normalize\n",
    "// V1: Fused single kernel\n",
    "// V2: Online algorithm (Welford's method)\n",
    "\n",
    "__global__ void layernormOnline(const float* input, float* output,\n",
    "                                const float* gamma, const float* beta,\n",
    "                                int rows, int cols, float eps) {\n",
    "    // Welford's online algorithm for mean and variance\n",
    "    // Your implementation here\n",
    "}\n",
    "```\n",
    "\n",
    "### Exercise 2: Profile and Document\n",
    "\n",
    "For your softmax implementation:\n",
    "1. Run `ncu` on each version\n",
    "2. Create a table of metrics\n",
    "3. Identify the bottleneck at each stage\n",
    "4. Calculate achieved vs theoretical bandwidth\n",
    "\n",
    "### Exercise 3: Vectorized Memory Access\n",
    "\n",
    "```cpp\n",
    "// Add vectorized loads to improve bandwidth\n",
    "__global__ void softmaxVectorized(const float4* input, float4* output,\n",
    "                                  int rows, int cols) {\n",
    "    // Load 4 floats at once with float4\n",
    "    // Process and store\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dad4ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  OPTIMIZATION WORKFLOW                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. Profile first - never guess!                                â”‚\n",
    "â”‚  2. One optimization at a time                                  â”‚\n",
    "â”‚  3. Measure before and after                                    â”‚\n",
    "â”‚  4. Document your journey                                       â”‚\n",
    "â”‚  5. Compare to theoretical limits                               â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Key Techniques Used:                                           â”‚\n",
    "â”‚  â€¢ Kernel fusion (reduce launches)                              â”‚\n",
    "â”‚  â€¢ Online algorithms (reduce passes)                            â”‚\n",
    "â”‚  â€¢ Shared memory (reduce global access)                         â”‚\n",
    "â”‚  â€¢ Warp primitives (reduce sync overhead)                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Day 5:** Practice & checkpoint quiz for Week 8!\n",
    "\n",
    "**Week 9:** We'll explore CUDA Streams & Concurrency for overlapping computation with data transfer."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
