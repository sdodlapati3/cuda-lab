{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875ec8aa",
   "metadata": {},
   "source": [
    "## 1. Nsight Systems Overview\n",
    "\n",
    "### Nsight Systems vs Nsight Compute\n",
    "\n",
    "| Feature | Nsight Systems | Nsight Compute |\n",
    "|---------|---------------|----------------|\n",
    "| **Scope** | Application-level | Kernel-level |\n",
    "| **View** | Timeline | Metrics |\n",
    "| **Use For** | CPU-GPU interaction | Kernel optimization |\n",
    "| **Overhead** | Low | High |\n",
    "| **Profiling** | All kernels at once | One kernel at a time |\n",
    "\n",
    "### When to Use Nsight Systems\n",
    "\n",
    "- Finding **where time is spent** in your application\n",
    "- Identifying **CPU bottlenecks** between kernel launches\n",
    "- Analyzing **memory transfer patterns**\n",
    "- Detecting **synchronization issues**\n",
    "- Understanding **stream and concurrency** behavior\n",
    "- Profiling **multi-GPU** applications\n",
    "\n",
    "### Installation Check\n",
    "\n",
    "```bash\n",
    "# Check if nsys is installed\n",
    "nsys --version\n",
    "\n",
    "# Typical output:\n",
    "# NVIDIA Nsight Systems version 2024.1.1.59\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55da65",
   "metadata": {},
   "source": [
    "### ðŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "## 2. CUDA C++ Test Application\n",
    "\n",
    "This application demonstrates various execution patterns that affect timeline behavior:\n",
    "- **Pattern 1**: Synchronous execution (BAD - sync after every kernel)\n",
    "- **Pattern 2**: Asynchronous execution (GOOD - single sync at end)\n",
    "- **Pattern 3**: CPU-GPU overlap opportunity\n",
    "- **Pattern 4**: Memory transfer patterns (pageable vs pinned)\n",
    "- **Pattern 5**: Multiple CUDA streams for concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_profiling.cu\n",
    "// app_profiling.cu - Application with various patterns to profile\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <chrono>\n",
    "#include <thread>\n",
    "\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "               cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernels\n",
    "//=============================================================================\n",
    "__global__ void processKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = data[idx];\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            val = val * 1.01f + 0.01f;\n",
    "        }\n",
    "        data[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void reduceKernel(float* output, const float* input, int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? input[idx] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) output[blockIdx.x] = sdata[0];\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Pattern 1: Synchronous execution (BAD - many sync points)\n",
    "//=============================================================================\n",
    "void patternSynchronous(float* d_data, int n, int iterations) {\n",
    "    printf(\"Pattern 1: Synchronous (sync after every kernel)\\n\");\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        processKernel<<<gridSize, blockSize>>>(d_data, n);\n",
    "        CHECK_CUDA(cudaDeviceSynchronize());  // Sync every time!\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Pattern 2: Asynchronous execution (GOOD - batch sync)\n",
    "//=============================================================================\n",
    "void patternAsynchronous(float* d_data, int n, int iterations) {\n",
    "    printf(\"Pattern 2: Asynchronous (sync at end)\\n\");\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        processKernel<<<gridSize, blockSize>>>(d_data, n);\n",
    "        // No sync inside loop!\n",
    "    }\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());  // Single sync at end\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Pattern 3: CPU work between kernels (potential overlap)\n",
    "//=============================================================================\n",
    "void cpuWork(int duration_us) {\n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    while (true) {\n",
    "        auto now = std::chrono::high_resolution_clock::now();\n",
    "        auto elapsed = std::chrono::duration_cast<std::chrono::microseconds>(\n",
    "            now - start).count();\n",
    "        if (elapsed >= duration_us) break;\n",
    "    }\n",
    "}\n",
    "\n",
    "void patternOverlap(float* d_data, int n, int iterations) {\n",
    "    printf(\"Pattern 3: CPU-GPU overlap opportunity\\n\");\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        processKernel<<<gridSize, blockSize>>>(d_data, n);\n",
    "        cpuWork(100);  // Simulated CPU work (could overlap)\n",
    "    }\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Pattern 4: Memory transfer patterns\n",
    "//=============================================================================\n",
    "void patternMemoryTransfers(int n) {\n",
    "    printf(\"Pattern 4: Memory transfers\\n\");\n",
    "    \n",
    "    float *h_data, *d_data;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Pageable memory (slow)\n",
    "    h_data = new float[n];\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, size));\n",
    "    \n",
    "    // Multiple small transfers (inefficient)\n",
    "    printf(\"  Small transfers (pageable)...\\n\");\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        CHECK_CUDA(cudaMemcpy(d_data, h_data, size/10, \n",
    "                              cudaMemcpyHostToDevice));\n",
    "        CHECK_CUDA(cudaMemcpy(h_data, d_data, size/10,\n",
    "                              cudaMemcpyDeviceToHost));\n",
    "    }\n",
    "    \n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "    \n",
    "    // Pinned memory (fast)\n",
    "    CHECK_CUDA(cudaMallocHost(&h_data, size));\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, size));\n",
    "    \n",
    "    // Single large transfer (efficient)\n",
    "    printf(\"  Large transfer (pinned)...\\n\");\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    cudaFreeHost(h_data);\n",
    "    cudaFree(d_data);\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Pattern 5: Stream concurrency\n",
    "//=============================================================================\n",
    "void patternStreams(float* d_data, int n) {\n",
    "    printf(\"Pattern 5: Multiple streams\\n\");\n",
    "    \n",
    "    const int NUM_STREAMS = 4;\n",
    "    cudaStream_t streams[NUM_STREAMS];\n",
    "    \n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        CHECK_CUDA(cudaStreamCreate(&streams[i]));\n",
    "    }\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int chunkSize = n / NUM_STREAMS;\n",
    "    int gridSize = (chunkSize + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // Launch work on multiple streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        float* d_chunk = d_data + i * chunkSize;\n",
    "        for (int j = 0; j < 5; j++) {\n",
    "            processKernel<<<gridSize, blockSize, 0, streams[i]>>>(\n",
    "                d_chunk, chunkSize);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Sync all streams\n",
    "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
    "        CHECK_CUDA(cudaStreamSynchronize(streams[i]));\n",
    "        CHECK_CUDA(cudaStreamDestroy(streams[i]));\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Main\n",
    "//=============================================================================\n",
    "int main() {\n",
    "    const int N = 1 << 20;  // 1M elements\n",
    "    const int ITERATIONS = 20;\n",
    "    \n",
    "    float* d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMemset(d_data, 0, N * sizeof(float)));\n",
    "    \n",
    "    printf(\"\\n=== Application Profiling Demo ===\\n\\n\");\n",
    "    \n",
    "    patternSynchronous(d_data, N, ITERATIONS);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    patternAsynchronous(d_data, N, ITERATIONS);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    patternOverlap(d_data, N, ITERATIONS);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    patternMemoryTransfers(N);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    patternStreams(d_data, N);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    printf(\"=== Done! Profile with nsys to see timeline ===\\n\");\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d019811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -lineinfo -o app_profiling app_profiling.cu\n",
    "!./app_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b5b65",
   "metadata": {},
   "source": [
    "## 3. Basic Nsight Systems Usage\n",
    "\n",
    "### Command-Line Profiling\n",
    "\n",
    "```bash\n",
    "# Basic profiling\n",
    "nsys profile ./app_profiling\n",
    "\n",
    "# Save report with custom name\n",
    "nsys profile -o my_report ./app_profiling\n",
    "\n",
    "# Profile with CUDA trace\n",
    "nsys profile --trace=cuda ./app_profiling\n",
    "\n",
    "# Include NVTX annotations\n",
    "nsys profile --trace=cuda,nvtx ./app_profiling\n",
    "\n",
    "# CPU sampling\n",
    "nsys profile --sample=cpu ./app_profiling\n",
    "```\n",
    "\n",
    "### View Report\n",
    "\n",
    "```bash\n",
    "# Open report in GUI\n",
    "nsys-ui my_report.nsys-rep\n",
    "\n",
    "# Generate statistics summary\n",
    "nsys stats my_report.nsys-rep\n",
    "\n",
    "# Export to specific format\n",
    "nsys export --type=sqlite my_report.nsys-rep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a504e4",
   "metadata": {},
   "source": [
    "## 4. Understanding the Timeline\n",
    "\n",
    "### Timeline Components\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ CPU Thread (main)                                               â”‚\n",
    "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘        â”‚\n",
    "â”‚     â†‘       â†‘       â†‘                                           â”‚\n",
    "â”‚   sync    sync    sync (expensive!)                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ CUDA API Calls                                                  â”‚\n",
    "â”‚ â–“â–“â–“ â–“â–“â–“ â–“â–“â–“ â–“â–“â–“ â–“â–“â–“ â–“â–“â–“ â–“â–“â–“                                    â”‚\n",
    "â”‚ Launch Launch Launch...                                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ GPU Compute (Stream 0)                                          â”‚\n",
    "â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â”‚\n",
    "â”‚   kernel  kernel  kernel  kernel  kernel  kernel  kernel        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ GPU Memory                                                      â”‚\n",
    "â”‚ â–’â–’â–’â–’â–’â–’â–’â–’â–’                                                       â”‚\n",
    "â”‚ H2D transfer                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "| Pattern | Indicates |\n",
    "|---------|----------|\n",
    "| Gaps in GPU timeline | CPU bottleneck or synchronization |\n",
    "| Many small kernels | Kernel launch overhead |\n",
    "| Large memory regions | Transfer-heavy code |\n",
    "| Non-overlapping streams | Missed concurrency opportunity |\n",
    "| Long API calls | Blocking operations |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63921a",
   "metadata": {},
   "source": [
    "### ðŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "## 5. NVTX Annotations\n",
    "\n",
    "NVTX (NVIDIA Tools Extension) allows you to add custom annotations to mark regions in your code. These annotations appear in Nsight Systems timeline for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063930c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nvtx_example.cu\n",
    "// nvtx_example.cu - Add custom annotations\n",
    "#include <cuda_runtime.h>\n",
    "#include <nvtx3/nvToolsExt.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Helper macro for NVTX ranges\n",
    "#define NVTX_RANGE_PUSH(name) nvtxRangePushA(name)\n",
    "#define NVTX_RANGE_POP() nvtxRangePop()\n",
    "\n",
    "__global__ void computeKernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] = data[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "void initializeData(float* h_data, int n) {\n",
    "    NVTX_RANGE_PUSH(\"Initialize\");\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_data[i] = (float)i;\n",
    "    }\n",
    "    NVTX_RANGE_POP();\n",
    "}\n",
    "\n",
    "void processData(float* d_data, int n) {\n",
    "    NVTX_RANGE_PUSH(\"Process\");\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (n + blockSize - 1) / blockSize;\n",
    "    \n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        char marker[32];\n",
    "        snprintf(marker, sizeof(marker), \"Iteration %d\", i);\n",
    "        NVTX_RANGE_PUSH(marker);\n",
    "        \n",
    "        computeKernel<<<gridSize, blockSize>>>(d_data, n);\n",
    "        \n",
    "        NVTX_RANGE_POP();\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    NVTX_RANGE_POP();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int N = 1 << 20;\n",
    "    \n",
    "    NVTX_RANGE_PUSH(\"Application\");\n",
    "    \n",
    "    // Allocate\n",
    "    NVTX_RANGE_PUSH(\"Allocation\");\n",
    "    float* h_data = new float[N];\n",
    "    float* d_data;\n",
    "    cudaMalloc(&d_data, N * sizeof(float));\n",
    "    NVTX_RANGE_POP();\n",
    "    \n",
    "    // Initialize\n",
    "    initializeData(h_data, N);\n",
    "    \n",
    "    // Transfer\n",
    "    NVTX_RANGE_PUSH(\"H2D Transfer\");\n",
    "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    NVTX_RANGE_POP();\n",
    "    \n",
    "    // Process\n",
    "    processData(d_data, N);\n",
    "    \n",
    "    // Transfer back\n",
    "    NVTX_RANGE_PUSH(\"D2H Transfer\");\n",
    "    cudaMemcpy(h_data, d_data, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    NVTX_RANGE_POP();\n",
    "    \n",
    "    // Cleanup\n",
    "    NVTX_RANGE_PUSH(\"Cleanup\");\n",
    "    delete[] h_data;\n",
    "    cudaFree(d_data);\n",
    "    NVTX_RANGE_POP();\n",
    "    \n",
    "    NVTX_RANGE_POP();  // Application\n",
    "    \n",
    "    printf(\"Done! Profile with: nsys profile --trace=cuda,nvtx ./nvtx_example\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caacb974",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -lnvToolsExt -o nvtx_example nvtx_example.cu\n",
    "!./nvtx_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d0f6c",
   "metadata": {},
   "source": [
    "## 6. Common Issues and Solutions\n",
    "\n",
    "### Issue 1: GPU Idle Time (Gaps in Timeline)\n",
    "\n",
    "**Symptoms in timeline:**\n",
    "```\n",
    "GPU: â–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ\n",
    "         ^gap   ^gap   ^gap\n",
    "```\n",
    "\n",
    "**Causes:**\n",
    "- Excessive `cudaDeviceSynchronize()` calls\n",
    "- CPU work blocking kernel launch\n",
    "- Serial kernel dependencies\n",
    "\n",
    "**Solution:**\n",
    "```cpp\n",
    "// Bad: sync after each kernel\n",
    "for (int i = 0; i < N; i++) {\n",
    "    kernel<<<grid, block>>>(...);\n",
    "    cudaDeviceSynchronize();  // Creates gap!\n",
    "}\n",
    "\n",
    "// Good: single sync at end\n",
    "for (int i = 0; i < N; i++) {\n",
    "    kernel<<<grid, block>>>(...);\n",
    "}\n",
    "cudaDeviceSynchronize();  // One sync\n",
    "```\n",
    "\n",
    "### Issue 2: Memory Transfer Overhead\n",
    "\n",
    "**Symptoms:**\n",
    "- Large memory transfer regions\n",
    "- Many small transfers\n",
    "\n",
    "**Solution:**\n",
    "```cpp\n",
    "// Bad: many small transfers\n",
    "for (int i = 0; i < 100; i++) {\n",
    "    cudaMemcpy(d_data + i*chunk, h_data + i*chunk, \n",
    "               chunk_size, cudaMemcpyHostToDevice);\n",
    "}\n",
    "\n",
    "// Good: single large transfer\n",
    "cudaMemcpy(d_data, h_data, total_size, cudaMemcpyHostToDevice);\n",
    "```\n",
    "\n",
    "### Issue 3: No Stream Overlap\n",
    "\n",
    "**Symptoms:**\n",
    "- Multiple streams but sequential execution\n",
    "\n",
    "**Common cause:** Implicit synchronization\n",
    "\n",
    "```cpp\n",
    "// These cause implicit sync:\n",
    "cudaMalloc(...)           // Syncs\n",
    "cudaMemcpy(...)           // Syncs (use Async version)\n",
    "cudaDeviceSynchronize()   // Explicit sync\n",
    "\n",
    "// Use async versions for overlap:\n",
    "cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caba06",
   "metadata": {},
   "source": [
    "## 7. Statistics Analysis\n",
    "\n",
    "### Generate Statistics\n",
    "\n",
    "```bash\n",
    "# Profile and get stats\n",
    "nsys profile -o report ./app_profiling\n",
    "nsys stats report.nsys-rep\n",
    "```\n",
    "\n",
    "### Key Statistics Tables\n",
    "\n",
    "```\n",
    "CUDA API Statistics:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ API                 â”‚ Count   â”‚ Avg (ns) â”‚ Total %  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ cudaDeviceSynchronizeâ”‚ 100    â”‚ 50000    â”‚ 45.2%    â”‚  â† Too many syncs!\n",
    "â”‚ cudaLaunchKernel    â”‚ 100    â”‚ 5000     â”‚ 4.5%     â”‚\n",
    "â”‚ cudaMemcpy          â”‚ 20     â”‚ 100000   â”‚ 18.1%    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "GPU Kernel Statistics:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Kernel             â”‚ Count   â”‚ Avg (us) â”‚ Total %  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ processKernel      â”‚ 80     â”‚ 125      â”‚ 85.3%    â”‚\n",
    "â”‚ reduceKernel       â”‚ 20     â”‚ 50       â”‚ 14.7%    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Memory Transfer Statistics:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Transfer Type      â”‚ Count   â”‚ Size     â”‚ BW (GB/s)â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ H2D (Pageable)     â”‚ 10     â”‚ 1 MB     â”‚ 5.2      â”‚  â† Slow!\n",
    "â”‚ H2D (Pinned)       â”‚ 1      â”‚ 10 MB    â”‚ 12.1     â”‚  â† Fast\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f07ca",
   "metadata": {},
   "source": [
    "### ðŸ”¶ Python/Numba (Optional - Quick Testing)\n",
    "\n",
    "## 8. Python/Numba Optional Backup\n",
    "\n",
    "Simulate timeline concepts in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe23ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a815491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def process_kernel(data):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < data.size:\n",
    "        val = data[idx]\n",
    "        for i in range(100):\n",
    "            val = val * 1.01 + 0.01\n",
    "        data[idx] = val\n",
    "\n",
    "# Check GPU\n",
    "if not cuda.is_available():\n",
    "    print(\"No CUDA GPU available\")\n",
    "else:\n",
    "    print(f\"GPU: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef4328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare synchronous vs asynchronous patterns\n",
    "\n",
    "if cuda.is_available():\n",
    "    N = 1 << 20  # 1M elements\n",
    "    ITERATIONS = 20\n",
    "    \n",
    "    d_data = cuda.device_array(N, dtype=np.float32)\n",
    "    \n",
    "    threads = 256\n",
    "    blocks = (N + threads - 1) // threads\n",
    "    \n",
    "    # Warm up\n",
    "    process_kernel[blocks, threads](d_data)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Pattern 1: Sync after every kernel (BAD)\n",
    "    start = time.perf_counter()\n",
    "    for i in range(ITERATIONS):\n",
    "        process_kernel[blocks, threads](d_data)\n",
    "        cuda.synchronize()  # Sync every time!\n",
    "    sync_time = time.perf_counter() - start\n",
    "    \n",
    "    # Pattern 2: Single sync at end (GOOD)\n",
    "    start = time.perf_counter()\n",
    "    for i in range(ITERATIONS):\n",
    "        process_kernel[blocks, threads](d_data)\n",
    "    cuda.synchronize()  # Single sync\n",
    "    async_time = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"Synchronous pattern:  {sync_time*1000:.2f} ms\")\n",
    "    print(f\"Asynchronous pattern: {async_time*1000:.2f} ms\")\n",
    "    print(f\"Speedup: {sync_time/async_time:.2f}x\")\n",
    "    print(f\"\\nâ†’ Removing unnecessary syncs can significantly improve performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b55bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize timeline-like execution pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def visualize_timeline(pattern_name, cpu_events, gpu_events):\n",
    "    \"\"\"Visualize CPU and GPU timeline\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    \n",
    "    # CPU timeline\n",
    "    for start, end, label, color in cpu_events:\n",
    "        ax.barh(1, end-start, left=start, height=0.4, color=color, \n",
    "                edgecolor='black', linewidth=0.5)\n",
    "        ax.text((start+end)/2, 1, label, ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # GPU timeline\n",
    "    for start, end, label, color in gpu_events:\n",
    "        ax.barh(0, end-start, left=start, height=0.4, color=color,\n",
    "                edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticklabels(['GPU', 'CPU'])\n",
    "    ax.set_xlabel('Time (arbitrary units)')\n",
    "    ax.set_title(f'Timeline: {pattern_name}')\n",
    "    ax.set_xlim(-0.5, max(e[1] for e in cpu_events + gpu_events) + 0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Pattern 1: Synchronous (gaps in GPU)\n",
    "cpu_sync = [(0, 0.5, 'L', 'lightblue'), (0.5, 1.0, 'S', 'red'),\n",
    "            (1.5, 2.0, 'L', 'lightblue'), (2.0, 2.5, 'S', 'red'),\n",
    "            (3.0, 3.5, 'L', 'lightblue'), (3.5, 4.0, 'S', 'red'),\n",
    "            (4.5, 5.0, 'L', 'lightblue'), (5.0, 5.5, 'S', 'red')]\n",
    "gpu_sync = [(0.5, 1.5, 'K', 'green'), (2.0, 3.0, 'K', 'green'),\n",
    "            (3.5, 4.5, 'K', 'green'), (5.0, 6.0, 'K', 'green')]\n",
    "\n",
    "visualize_timeline('Synchronous (sync after each kernel)', cpu_sync, gpu_sync)\n",
    "\n",
    "# Pattern 2: Asynchronous (continuous GPU)\n",
    "cpu_async = [(0, 0.3, 'L', 'lightblue'), (0.3, 0.6, 'L', 'lightblue'),\n",
    "             (0.6, 0.9, 'L', 'lightblue'), (0.9, 1.2, 'L', 'lightblue'),\n",
    "             (4.0, 4.5, 'S', 'red')]\n",
    "gpu_async = [(0.3, 1.3, 'K', 'green'), (1.3, 2.3, 'K', 'green'),\n",
    "             (2.3, 3.3, 'K', 'green'), (3.3, 4.3, 'K', 'green')]\n",
    "\n",
    "visualize_timeline('Asynchronous (single sync at end)', cpu_async, gpu_async)\n",
    "\n",
    "print(\"Legend: L=Launch, S=Sync, K=Kernel\")\n",
    "print(\"\\nNotice the gaps in Pattern 1 vs continuous execution in Pattern 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f3002",
   "metadata": {},
   "source": [
    "## 9. Best Practices for Application Profiling\n",
    "\n",
    "### Profiling Workflow\n",
    "\n",
    "1. **Start with Nsight Systems** to understand application-level behavior\n",
    "2. **Look for gaps** in GPU timeline (idle time)\n",
    "3. **Check API call durations** for blocking operations\n",
    "4. **Identify hot kernels** for detailed analysis\n",
    "5. **Use Nsight Compute** on specific kernels for optimization\n",
    "\n",
    "### Checklist\n",
    "\n",
    "- [ ] GPU timeline is mostly busy (minimal gaps)\n",
    "- [ ] Minimize `cudaDeviceSynchronize()` calls\n",
    "- [ ] Use pinned memory for transfers\n",
    "- [ ] Batch small transfers into larger ones\n",
    "- [ ] Use streams for overlapping operations\n",
    "- [ ] CPU work overlaps with GPU execution\n",
    "\n",
    "### Command Quick Reference\n",
    "\n",
    "```bash\n",
    "# Basic profile\n",
    "nsys profile ./app\n",
    "\n",
    "# Full tracing\n",
    "nsys profile --trace=cuda,nvtx,osrt ./app\n",
    "\n",
    "# With CPU sampling\n",
    "nsys profile --sample=cpu --trace=cuda ./app\n",
    "\n",
    "# Statistics only\n",
    "nsys stats report.nsys-rep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548bd2",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "### Exercise 1: Profile Sync Points\n",
    "Profile the `app_profiling` program and identify how many sync points exist.\n",
    "\n",
    "### Exercise 2: Add NVTX Markers\n",
    "Add NVTX annotations to your own CUDA application to mark:\n",
    "- Initialization phase\n",
    "- Main compute loop\n",
    "- Cleanup phase\n",
    "\n",
    "### Exercise 3: Identify Bottleneck\n",
    "Profile an application and determine if it's:\n",
    "- GPU-bound (GPU busy, CPU waiting)\n",
    "- CPU-bound (GPU idle, CPU busy)\n",
    "- Transfer-bound (large memory transfer regions)\n",
    "\n",
    "### Exercise 4: Optimize Sync Points\n",
    "Take a program with excessive syncs and optimize by batching operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873c5d1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Today you learned:\n",
    "- **Nsight Systems** profiles application-level behavior\n",
    "- **Timeline view** shows CPU-GPU interaction\n",
    "- **Gaps in GPU timeline** indicate synchronization or CPU bottlenecks\n",
    "- **NVTX annotations** add custom markers for clarity\n",
    "- **Statistics** reveal API call patterns and kernel durations\n",
    "\n",
    "### Key Commands\n",
    "```bash\n",
    "nsys profile ./app           # Profile\n",
    "nsys stats report.nsys-rep   # Statistics\n",
    "nsys-ui report.nsys-rep      # GUI view\n",
    "```\n",
    "\n",
    "**Next**: Day 4 - Systematic bottleneck analysis combining all profiling techniques"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
