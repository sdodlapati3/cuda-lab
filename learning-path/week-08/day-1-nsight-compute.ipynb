{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb63438",
   "metadata": {},
   "source": [
    "# Day 1: Nsight Compute - The Kernel Microscope ğŸ”¬\n",
    "\n",
    "> *\"Ever watched a doctor diagnose a patient? They don't guess - they run tests, check vitals, and look at scans. Your GPU kernels deserve the same thorough examination.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "1. **Profile** CUDA kernels using Nsight Compute command-line interface\n",
    "2. **Interpret** key metrics: SM throughput, memory throughput, occupancy\n",
    "3. **Identify** whether a kernel is compute-bound or memory-bound\n",
    "4. **Diagnose** common performance bottlenecks from profiler output\n",
    "5. **Generate** and analyze profiler reports for optimization guidance\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Nsight Compute\n",
    "\n",
    "### What is Nsight Compute?\n",
    "\n",
    "Nsight Compute is NVIDIA's **kernel-level profiler** that provides:\n",
    "- Detailed metrics for each kernel launch\n",
    "- Source-level analysis\n",
    "- Roofline analysis\n",
    "- Guided optimization recommendations\n",
    "\n",
    "### When to Use It\n",
    "\n",
    "| Tool | Use Case |\n",
    "|------|----------|\n",
    "| **Nsight Compute** | Optimizing individual kernels |\n",
    "| **Nsight Systems** | Understanding application timeline, CPU-GPU interaction |\n",
    "\n",
    "### Installation Check\n",
    "\n",
    "```bash\n",
    "# Check if ncu is installed\n",
    "ncu --version\n",
    "\n",
    "# Typical output:\n",
    "# NVIDIA (R) Nsight Compute Command Line Profiler\n",
    "# Copyright (c) 2018-2024 NVIDIA Corporation\n",
    "# Version 2024.1.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad1b85",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Concept Card: The Kernel Microscope\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              NSIGHT COMPUTE = KERNEL MICROSCOPE                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   ğŸ”¬ Just like a microscope reveals cellular details...         â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   Your Code        â†’  What You See                              â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
    "â”‚   kernel<<<...>>>  â†’  \"It runs in 5ms\"                          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   ğŸ”¬ Nsight Compute reveals the INTERNAL machinery:             â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚   â”‚ SM Activity â”‚  â”‚Memory Flow  â”‚  â”‚ Occupancy   â”‚            â”‚\n",
    "â”‚   â”‚ 45% compute â”‚  â”‚ 80% BW used â”‚  â”‚ 62.5% SMs   â”‚            â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   THE DIAGNOSIS ANALOGY:                                        â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚   â”‚  Doctor               â”‚  Nsight Compute                â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚\n",
    "â”‚   â”‚  Blood pressure       â”‚  Memory throughput             â”‚   â”‚\n",
    "â”‚   â”‚  Heart rate           â”‚  SM throughput                 â”‚   â”‚\n",
    "â”‚   â”‚  Temperature          â”‚  Occupancy                     â”‚   â”‚\n",
    "â”‚   â”‚  X-ray/MRI            â”‚  Source correlation            â”‚   â”‚\n",
    "â”‚   â”‚  Prescription         â”‚  Optimization recommendations  â”‚   â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   ğŸ’Š Don't guess - PROFILE! Let the data guide optimization.   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Week 7 Connection**: We measured kernel time. Now we see *inside* that time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a34739",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "## 2. CUDA C++ Test Kernels\n",
    "\n",
    "Let's create kernels with different performance characteristics to profile. The code below demonstrates:\n",
    "- **Memory-Bound**: Simple copy kernel\n",
    "- **Compute-Bound**: Heavy math operations\n",
    "- **Non-Coalesced**: Column-major access pattern\n",
    "- **Bank Conflicts**: Naive reduction with conflicts\n",
    "- **Low Occupancy**: High register usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d52cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile profiling_targets.cu\n",
    "// profiling_targets.cu - Kernels to profile\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <chrono>\n",
    "\n",
    "// Utility macro\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "               cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 1: Memory-Bound (Simple Copy)\n",
    "//=============================================================================\n",
    "__global__ void copyKernel(float* dst, const float* src, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        dst[idx] = src[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 2: Compute-Bound (Heavy Math)\n",
    "//=============================================================================\n",
    "__global__ void computeKernel(float* output, const float* input, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = input[idx];\n",
    "        \n",
    "        // Heavy compute: transcendentals and iterations\n",
    "        #pragma unroll 10\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            val = sinf(val) + cosf(val);\n",
    "            val = sqrtf(fabsf(val) + 1.0f);\n",
    "            val = expf(-val * 0.001f);\n",
    "        }\n",
    "        \n",
    "        output[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 3: Non-Coalesced Access (Column Major)\n",
    "//=============================================================================\n",
    "__global__ void nonCoalescedKernel(float* dst, const float* src, \n",
    "                                    int rows, int cols) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < rows * cols) {\n",
    "        int row = idx / cols;\n",
    "        int col = idx % cols;\n",
    "        \n",
    "        // Column-major read (non-coalesced)\n",
    "        int src_idx = col * rows + row;\n",
    "        dst[idx] = src[src_idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 4: Bank Conflicts (Naive Reduction)\n",
    "//=============================================================================\n",
    "__global__ void bankConflictReduction(float* output, const float* input, int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? input[idx] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Naive reduction with bank conflicts\n",
    "    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
    "        if (tid % (2 * stride) == 0) {\n",
    "            sdata[tid] += sdata[tid + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        output[blockIdx.x] = sdata[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 5: Low Occupancy (High Register Usage)\n",
    "//=============================================================================\n",
    "__global__ void lowOccupancyKernel(float* output, const float* input, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Force high register usage\n",
    "    float r0 = input[idx % n], r1 = r0, r2 = r0, r3 = r0;\n",
    "    float r4 = r0, r5 = r0, r6 = r0, r7 = r0;\n",
    "    float r8 = r0, r9 = r0, r10 = r0, r11 = r0;\n",
    "    float r12 = r0, r13 = r0, r14 = r0, r15 = r0;\n",
    "    float r16 = r0, r17 = r0, r18 = r0, r19 = r0;\n",
    "    float r20 = r0, r21 = r0, r22 = r0, r23 = r0;\n",
    "    float r24 = r0, r25 = r0, r26 = r0, r27 = r0;\n",
    "    float r28 = r0, r29 = r0, r30 = r0, r31 = r0;\n",
    "    \n",
    "    // Keep all registers live\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        r0 = r1 + r2; r1 = r2 + r3; r2 = r3 + r4; r3 = r4 + r5;\n",
    "        r4 = r5 + r6; r5 = r6 + r7; r6 = r7 + r8; r7 = r8 + r9;\n",
    "        r8 = r9 + r10; r9 = r10 + r11; r10 = r11 + r12; r11 = r12 + r13;\n",
    "        r12 = r13 + r14; r13 = r14 + r15; r14 = r15 + r16; r15 = r16 + r17;\n",
    "        r16 = r17 + r18; r17 = r18 + r19; r18 = r19 + r20; r19 = r20 + r21;\n",
    "        r20 = r21 + r22; r21 = r22 + r23; r22 = r23 + r24; r23 = r24 + r25;\n",
    "        r24 = r25 + r26; r25 = r26 + r27; r26 = r27 + r28; r27 = r28 + r29;\n",
    "        r28 = r29 + r30; r29 = r30 + r31; r30 = r31 + r0; r31 = r0 + r1;\n",
    "    }\n",
    "    \n",
    "    if (idx < n) {\n",
    "        output[idx] = r0 + r1 + r2 + r3 + r4 + r5 + r6 + r7 +\n",
    "                      r8 + r9 + r10 + r11 + r12 + r13 + r14 + r15 +\n",
    "                      r16 + r17 + r18 + r19 + r20 + r21 + r22 + r23 +\n",
    "                      r24 + r25 + r26 + r27 + r28 + r29 + r30 + r31;\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Main - Run all kernels\n",
    "//=============================================================================\n",
    "int main() {\n",
    "    const int N = 1 << 22;  // 4M elements\n",
    "    const int ROWS = 4096;\n",
    "    const int COLS = 1024;\n",
    "    \n",
    "    // Allocate memory\n",
    "    float *d_input, *d_output, *d_src, *d_dst;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_src, ROWS * COLS * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_dst, ROWS * COLS * sizeof(float)));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_input = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_input[i] = (float)(i % 1000) / 1000.0f;\n",
    "    CHECK_CUDA(cudaMemcpy(d_input, h_input, N * sizeof(float), \n",
    "                          cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_src, h_input, ROWS * COLS * sizeof(float),\n",
    "                          cudaMemcpyHostToDevice));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (N + blockSize - 1) / blockSize;\n",
    "    int gridSize2D = (ROWS * COLS + blockSize - 1) / blockSize;\n",
    "    \n",
    "    printf(\"Running profiling target kernels...\\n\\n\");\n",
    "    \n",
    "    // Run each kernel\n",
    "    printf(\"1. Copy Kernel (Memory-Bound)\\n\");\n",
    "    copyKernel<<<gridSize, blockSize>>>(d_output, d_input, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"2. Compute Kernel (Compute-Bound)\\n\");\n",
    "    computeKernel<<<gridSize, blockSize>>>(d_output, d_input, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"3. Non-Coalesced Kernel\\n\");\n",
    "    nonCoalescedKernel<<<gridSize2D, blockSize>>>(d_dst, d_src, ROWS, COLS);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"4. Bank Conflict Reduction\\n\");\n",
    "    bankConflictReduction<<<gridSize, blockSize>>>(d_output, d_input, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"5. Low Occupancy Kernel\\n\");\n",
    "    lowOccupancyKernel<<<gridSize, blockSize>>>(d_output, d_input, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"\\nAll kernels complete. Use ncu to profile!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    delete[] h_input;\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_src);\n",
    "    cudaFree(d_dst);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d772ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -lineinfo -o profiling_targets profiling_targets.cu\n",
    "!./profiling_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f860ae6",
   "metadata": {},
   "source": [
    "## 3. Basic Nsight Compute Usage\n",
    "\n",
    "### Command Line Profiling\n",
    "\n",
    "```bash\n",
    "# Basic profiling - profile all kernels\n",
    "ncu ./profiling_targets\n",
    "\n",
    "# Profile specific kernel by name\n",
    "ncu --kernel-name copyKernel ./profiling_targets\n",
    "\n",
    "# Profile specific kernel by launch number\n",
    "ncu --launch-skip 0 --launch-count 1 ./profiling_targets\n",
    "\n",
    "# Save report to file\n",
    "ncu -o profile_report ./profiling_targets\n",
    "# Creates profile_report.ncu-rep (open in Nsight Compute GUI)\n",
    "```\n",
    "\n",
    "### Profiling Sections\n",
    "\n",
    "```bash\n",
    "# Full analysis (all sections)\n",
    "ncu --set full ./profiling_targets\n",
    "\n",
    "# Roofline analysis only\n",
    "ncu --set roofline ./profiling_targets\n",
    "\n",
    "# Memory analysis\n",
    "ncu --section MemoryWorkloadAnalysis ./profiling_targets\n",
    "\n",
    "# Compute analysis\n",
    "ncu --section ComputeWorkloadAnalysis ./profiling_targets\n",
    "\n",
    "# Occupancy analysis\n",
    "ncu --section Occupancy ./profiling_targets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe10d9e",
   "metadata": {},
   "source": [
    "## 4. Key Metrics Explained\n",
    "\n",
    "### Throughput Metrics\n",
    "\n",
    "| Metric | Description | Good Value |\n",
    "|--------|-------------|------------|\n",
    "| **SM Throughput** | % of SM compute cycles used | >60% compute-bound |\n",
    "| **Memory Throughput** | % of peak memory bandwidth | >60% memory-bound |\n",
    "| **L1 Hit Rate** | Cache hit rate for L1 | Higher = better |\n",
    "| **L2 Hit Rate** | Cache hit rate for L2 | Higher = better |\n",
    "\n",
    "### Occupancy Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Theoretical Occupancy** | Max warps per SM / hardware limit |\n",
    "| **Achieved Occupancy** | Actual average active warps |\n",
    "| **Active Warps** | Number of resident warps |\n",
    "\n",
    "### Memory Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Global Load Efficiency** | Useful bytes / total bytes loaded |\n",
    "| **Global Store Efficiency** | Useful bytes / total bytes stored |\n",
    "| **Shared Bank Conflicts** | Conflicts per shared memory access |\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "```\n",
    "Example ncu output for copyKernel:\n",
    "\n",
    "Section: GPU Speed Of Light Throughput\n",
    "  DRAM Throughput             95.2%\n",
    "  SM Throughput               12.3%\n",
    "\n",
    "Interpretation:\n",
    "- High DRAM (95.2%) + Low SM (12.3%) = MEMORY-BOUND\n",
    "- This is expected for a simple copy kernel\n",
    "- Optimization: focus on memory access patterns, not compute\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746b834",
   "metadata": {},
   "source": [
    "## 5. Practical Profiling Workflow\n",
    "\n",
    "### Step 1: Quick Overview\n",
    "\n",
    "```bash\n",
    "# Get quick summary of all kernels\n",
    "ncu --target-processes all ./profiling_targets 2>&1 | head -100\n",
    "```\n",
    "\n",
    "### Step 2: Identify Bottleneck Type\n",
    "\n",
    "```bash\n",
    "# Focus on speed-of-light analysis\n",
    "ncu --section SpeedOfLight --kernel-name computeKernel ./profiling_targets\n",
    "```\n",
    "\n",
    "Expected output pattern:\n",
    "```\n",
    "copyKernel:    Memory ~95%, Compute ~12%  â†’ Memory-bound\n",
    "computeKernel: Memory ~15%, Compute ~85%  â†’ Compute-bound\n",
    "```\n",
    "\n",
    "### Step 3: Deep Dive Based on Bottleneck\n",
    "\n",
    "**For memory-bound kernels:**\n",
    "```bash\n",
    "ncu --section MemoryWorkloadAnalysis \\\n",
    "    --section MemoryWorkloadAnalysis_Chart \\\n",
    "    --kernel-name copyKernel ./profiling_targets\n",
    "```\n",
    "\n",
    "**For compute-bound kernels:**\n",
    "```bash\n",
    "ncu --section ComputeWorkloadAnalysis \\\n",
    "    --section WarpStateStatistics \\\n",
    "    --kernel-name computeKernel ./profiling_targets\n",
    "```\n",
    "\n",
    "### Step 4: Check for Common Issues\n",
    "\n",
    "```bash\n",
    "# Check for non-coalesced access\n",
    "ncu --metrics l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum,\\\n",
    "l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum \\\n",
    "    --kernel-name nonCoalescedKernel ./profiling_targets\n",
    "\n",
    "# Calculate coalescing efficiency:\n",
    "# Efficiency = requests / sectors\n",
    "# Perfect = 1.0, Non-coalesced < 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939ac09",
   "metadata": {},
   "source": [
    "## 6. Specific Metric Queries\n",
    "\n",
    "### Custom Metric Collection\n",
    "\n",
    "```bash\n",
    "# Occupancy metrics\n",
    "ncu --metrics sm__warps_active.avg.pct_of_peak_sustained_active \\\n",
    "    ./profiling_targets\n",
    "\n",
    "# Memory throughput\n",
    "ncu --metrics dram__bytes.sum.per_second \\\n",
    "    ./profiling_targets\n",
    "\n",
    "# Compute throughput\n",
    "ncu --metrics sm__sass_thread_inst_executed_op_fadd_pred_on.sum,\\\n",
    "sm__sass_thread_inst_executed_op_fmul_pred_on.sum,\\\n",
    "sm__sass_thread_inst_executed_op_ffma_pred_on.sum \\\n",
    "    ./profiling_targets\n",
    "\n",
    "# Shared memory bank conflicts\n",
    "ncu --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_shared.sum \\\n",
    "    --kernel-name bankConflictReduction ./profiling_targets\n",
    "```\n",
    "\n",
    "### Common Useful Metrics\n",
    "\n",
    "```bash\n",
    "# One-liner for key metrics\n",
    "ncu --metrics \\\n",
    "sm__throughput.avg.pct_of_peak_sustained_elapsed,\\\n",
    "dram__throughput.avg.pct_of_peak_sustained_elapsed,\\\n",
    "sm__warps_active.avg.pct_of_peak_sustained_active,\\\n",
    "l1tex__t_sector_hit_rate.pct \\\n",
    "    ./profiling_targets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2334bb",
   "metadata": {},
   "source": [
    "### ğŸ”· CUDA C++ Implementation (Primary)\n",
    "\n",
    "## 7. Comparing Kernel Versions\n",
    "\n",
    "The following code compares naive vs optimized matrix transpose:\n",
    "- **Naive transpose**: Non-coalesced writes to global memory\n",
    "- **Optimized transpose**: Uses shared memory tile with padding to avoid bank conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17247d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile kernel_comparison.cu\n",
    "// kernel_comparison.cu - Compare optimized vs naive\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Naive transpose (non-coalesced writes)\n",
    "__global__ void transposeNaive(float* out, const float* in, \n",
    "                                int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        out[x * height + y] = in[y * width + x];  // Non-coalesced write\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized transpose (coalesced with shared memory)\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "__global__ void transposeCoalesced(float* out, const float* in,\n",
    "                                    int width, int height) {\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];  // +1 for bank conflicts\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Coalesced read into shared memory\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Coalesced write from shared memory\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;  // Transpose block position\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int WIDTH = 4096;\n",
    "    const int HEIGHT = 4096;\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, WIDTH * HEIGHT * sizeof(float));\n",
    "    cudaMalloc(&d_out, WIDTH * HEIGHT * sizeof(float));\n",
    "    \n",
    "    dim3 blockNaive(32, 32);\n",
    "    dim3 gridNaive((WIDTH + 31) / 32, (HEIGHT + 31) / 32);\n",
    "    \n",
    "    dim3 blockCoalesced(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 gridCoalesced((WIDTH + TILE_DIM - 1) / TILE_DIM,\n",
    "                       (HEIGHT + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    printf(\"Running naive transpose...\\n\");\n",
    "    transposeNaive<<<gridNaive, blockNaive>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Running coalesced transpose...\\n\");\n",
    "    transposeCoalesced<<<gridCoalesced, blockCoalesced>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Done. Compare with ncu!\\n\");\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -lineinfo -o kernel_comparison kernel_comparison.cu\n",
    "!./kernel_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46482b",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba (Optional - Quick Testing)\n",
    "\n",
    "## 8. Python/Numba Optional Backup\n",
    "\n",
    "Since Nsight Compute is a command-line tool, we can use Python for simulating profiling concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86075c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install numba numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import cuda\n",
    "import math\n",
    "\n",
    "# Check GPU availability\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"GPU: {device.name}\")\n",
    "    print(f\"Compute Capability: {device.compute_capability}\")\n",
    "    print(f\"Total Memory: {device.total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No CUDA GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate profiling by timing different kernel types\n",
    "\n",
    "@cuda.jit\n",
    "def copy_kernel(dst, src):\n",
    "    \"\"\"Memory-bound kernel - simple copy\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < src.size:\n",
    "        dst[idx] = src[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def compute_kernel(output, input_arr):\n",
    "    \"\"\"Compute-bound kernel - heavy math\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < input_arr.size:\n",
    "        val = input_arr[idx]\n",
    "        for i in range(100):\n",
    "            val = math.sin(val) + math.cos(val)\n",
    "            val = math.sqrt(abs(val) + 1.0)\n",
    "            val = math.exp(-val * 0.001)\n",
    "        output[idx] = val\n",
    "\n",
    "# Test data\n",
    "N = 1 << 20  # 1M elements\n",
    "h_input = np.random.rand(N).astype(np.float32)\n",
    "h_output = np.zeros_like(h_input)\n",
    "\n",
    "# Device arrays\n",
    "d_input = cuda.to_device(h_input)\n",
    "d_output = cuda.device_array_like(h_input)\n",
    "\n",
    "# Launch config\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "# Warm up\n",
    "copy_kernel[blocks_per_grid, threads_per_block](d_output, d_input)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Time copy kernel (memory-bound)\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    copy_kernel[blocks_per_grid, threads_per_block](d_output, d_input)\n",
    "cuda.synchronize()\n",
    "copy_time = (time.perf_counter() - start) / 100\n",
    "\n",
    "# Time compute kernel (compute-bound)  \n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    compute_kernel[blocks_per_grid, threads_per_block](d_output, d_input)\n",
    "cuda.synchronize()\n",
    "compute_time = (time.perf_counter() - start) / 10\n",
    "\n",
    "# Analysis\n",
    "bytes_transferred = 2 * N * 4  # Read + Write, float32\n",
    "copy_bandwidth = bytes_transferred / copy_time / 1e9\n",
    "\n",
    "print(f\"Copy Kernel:\")\n",
    "print(f\"  Time: {copy_time*1000:.3f} ms\")\n",
    "print(f\"  Effective Bandwidth: {copy_bandwidth:.2f} GB/s\")\n",
    "print(f\"  â†’ Memory-bound (simple memory operations)\")\n",
    "\n",
    "print(f\"\\nCompute Kernel:\")\n",
    "print(f\"  Time: {compute_time*1000:.3f} ms\")\n",
    "print(f\"  Time Ratio (Compute/Copy): {compute_time/copy_time:.1f}x\")\n",
    "print(f\"  â†’ Compute-bound (heavy math operations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5909f5e",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### Nsight Compute Essentials\n",
    "\n",
    "1. **Basic profiling**: `ncu ./my_program`\n",
    "2. **Kernel selection**: `--kernel-name KernelName`\n",
    "3. **Save reports**: `-o report_name`\n",
    "4. **Key sections**:\n",
    "   - `SpeedOfLight` - Quick bottleneck identification\n",
    "   - `MemoryWorkloadAnalysis` - Memory access patterns\n",
    "   - `ComputeWorkloadAnalysis` - Compute utilization\n",
    "   - `Occupancy` - Thread parallelism\n",
    "\n",
    "### Bottleneck Identification\n",
    "\n",
    "| SM Throughput | Memory Throughput | Bottleneck |\n",
    "|---------------|-------------------|------------|\n",
    "| High | Low | Compute-bound |\n",
    "| Low | High | Memory-bound |\n",
    "| Low | Low | Latency-bound or low occupancy |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Compile with `-lineinfo`** for source correlation\n",
    "2. **Start with overview**, then deep dive\n",
    "3. **Compare before/after** optimization\n",
    "4. **Focus on one bottleneck at a time**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9404a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Exercises\n",
    "\n",
    "### ğŸ”· CUDA C++ Exercises (Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nsight_compute_exercises.cu\n",
    "/*\n",
    " * Nsight Compute Exercises\n",
    " * Week 8, Day 1: Kernel-Level Profiling Exercises\n",
    " * \n",
    " * Profile these kernels with ncu to analyze performance metrics.\n",
    " * \n",
    " * Compile: nvcc -arch=sm_75 -o nsight_compute_exercises nsight_compute_exercises.cu\n",
    " * Profile: ncu --set detailed ./nsight_compute_exercises\n",
    " */\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "            exit(1); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "#define N (1 << 22)  // 4M elements\n",
    "#define BLOCK_SIZE 256\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 1: Coalesced vs Non-Coalesced Memory Access\n",
    "// =============================================================================\n",
    "// Profile these kernels and compare global load/store efficiency\n",
    "\n",
    "__global__ void coalescedCopy(float* dst, const float* src, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        dst[idx] = src[idx];  // Coalesced access\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void stridedCopy(float* dst, const float* src, int n, int stride) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int strided_idx = (idx * stride) % n;\n",
    "    if (idx < n) {\n",
    "        dst[idx] = src[strided_idx];  // Strided access - poor coalescing\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 2: Occupancy Analysis\n",
    "// =============================================================================\n",
    "// Compare occupancy between high and low register usage kernels\n",
    "\n",
    "// High occupancy kernel - minimal registers\n",
    "__global__ void highOccupancyKernel(float* out, const float* in, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        out[idx] = in[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Low occupancy kernel - many registers\n",
    "__global__ void lowOccupancyKernel(float* out, const float* in, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        // Force register pressure with local variables\n",
    "        float r0 = in[idx];\n",
    "        float r1 = r0 * 1.1f, r2 = r0 * 1.2f, r3 = r0 * 1.3f, r4 = r0 * 1.4f;\n",
    "        float r5 = r1 * 1.1f, r6 = r2 * 1.2f, r7 = r3 * 1.3f, r8 = r4 * 1.4f;\n",
    "        float r9 = r5 * 1.1f, r10 = r6 * 1.2f, r11 = r7 * 1.3f, r12 = r8 * 1.4f;\n",
    "        float r13 = r9 * 1.1f, r14 = r10 * 1.2f, r15 = r11 * 1.3f, r16 = r12 * 1.4f;\n",
    "        out[idx] = r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + \n",
    "                   r9 + r10 + r11 + r12 + r13 + r14 + r15 + r16;\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 3: Shared Memory Bank Conflicts\n",
    "// =============================================================================\n",
    "// Analyze bank conflicts with ncu\n",
    "\n",
    "#define TILE_SIZE 32\n",
    "\n",
    "__global__ void noBankConflicts(float* out, const float* in, int n) {\n",
    "    __shared__ float tile[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "    \n",
    "    if (row < n && col < n) {\n",
    "        tile[ty][tx] = in[row * n + col];  // Row-major - no conflicts\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (row < n && col < n) {\n",
    "        out[row * n + col] = tile[ty][tx];\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void withBankConflicts(float* out, const float* in, int n) {\n",
    "    __shared__ float tile[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int row = blockIdx.y * TILE_SIZE + ty;\n",
    "    int col = blockIdx.x * TILE_SIZE + tx;\n",
    "    \n",
    "    if (row < n && col < n) {\n",
    "        tile[tx][ty] = in[row * n + col];  // Column access - bank conflicts!\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (row < n && col < n) {\n",
    "        out[row * n + col] = tile[tx][ty];\n",
    "    }\n",
    "}\n",
    "\n",
    "// =============================================================================\n",
    "// Exercise 4: Compute vs Memory Bound Analysis\n",
    "// =============================================================================\n",
    "// Profile and determine which is compute-bound vs memory-bound\n",
    "\n",
    "__global__ void memoryBoundKernel(float* out, const float* in1, const float* in2, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        out[idx] = in1[idx] + in2[idx];  // Low arithmetic intensity\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void computeBoundKernel(float* out, const float* in, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = in[idx];\n",
    "        // High compute with single load\n",
    "        #pragma unroll\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            val = sinf(val) * cosf(val) + sqrtf(fabsf(val));\n",
    "        }\n",
    "        out[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "void runExercise1() {\n",
    "    printf(\"\\n=== Exercise 1: Memory Coalescing Analysis ===\\n\");\n",
    "    printf(\"Profile command:\\n\");\n",
    "    printf(\"  ncu --metrics l1tex__t_bytes_pipe_lsu_mem_global_op_ld.sum,\");\n",
    "    printf(\"l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum \");\n",
    "    printf(\"--kernel-name-base demangled ./nsight_compute_exercises\\n\\n\");\n",
    "    \n",
    "    float *d_src, *d_dst;\n",
    "    CUDA_CHECK(cudaMalloc(&d_src, N * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc(&d_dst, N * sizeof(float)));\n",
    "    \n",
    "    int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    // Coalesced copy\n",
    "    coalescedCopy<<<blocks, BLOCK_SIZE>>>(d_dst, d_src, N);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ coalescedCopy completed\\n\");\n",
    "    \n",
    "    // Strided copy (stride=32 causes worst case)\n",
    "    stridedCopy<<<blocks, BLOCK_SIZE>>>(d_dst, d_src, N, 32);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ stridedCopy completed\\n\");\n",
    "    \n",
    "    cudaFree(d_src);\n",
    "    cudaFree(d_dst);\n",
    "}\n",
    "\n",
    "void runExercise2() {\n",
    "    printf(\"\\n=== Exercise 2: Occupancy Analysis ===\\n\");\n",
    "    printf(\"Profile command:\\n\");\n",
    "    printf(\"  ncu --metrics sm__warps_active.avg.pct_of_peak_sustained_active,\");\n",
    "    printf(\"launch__registers_per_thread ./nsight_compute_exercises\\n\\n\");\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    CUDA_CHECK(cudaMalloc(&d_in, N * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc(&d_out, N * sizeof(float)));\n",
    "    \n",
    "    int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    highOccupancyKernel<<<blocks, BLOCK_SIZE>>>(d_out, d_in, N);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ highOccupancyKernel completed\\n\");\n",
    "    \n",
    "    lowOccupancyKernel<<<blocks, BLOCK_SIZE>>>(d_out, d_in, N);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ lowOccupancyKernel completed\\n\");\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "}\n",
    "\n",
    "void runExercise3() {\n",
    "    printf(\"\\n=== Exercise 3: Shared Memory Bank Conflicts ===\\n\");\n",
    "    printf(\"Profile command:\\n\");\n",
    "    printf(\"  ncu --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum,\");\n",
    "    printf(\"l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_st.sum \");\n",
    "    printf(\"./nsight_compute_exercises\\n\\n\");\n",
    "    \n",
    "    int MATRIX_SIZE = 1024;\n",
    "    float *d_in, *d_out;\n",
    "    CUDA_CHECK(cudaMalloc(&d_in, MATRIX_SIZE * MATRIX_SIZE * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc(&d_out, MATRIX_SIZE * MATRIX_SIZE * sizeof(float)));\n",
    "    \n",
    "    dim3 blocks2D((MATRIX_SIZE + TILE_SIZE - 1) / TILE_SIZE, \n",
    "                  (MATRIX_SIZE + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    dim3 threads2D(TILE_SIZE, TILE_SIZE);\n",
    "    \n",
    "    noBankConflicts<<<blocks2D, threads2D>>>(d_out, d_in, MATRIX_SIZE);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ noBankConflicts completed\\n\");\n",
    "    \n",
    "    withBankConflicts<<<blocks2D, threads2D>>>(d_out, d_in, MATRIX_SIZE);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ withBankConflicts completed\\n\");\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "}\n",
    "\n",
    "void runExercise4() {\n",
    "    printf(\"\\n=== Exercise 4: Compute vs Memory Bound ===\\n\");\n",
    "    printf(\"Profile command:\\n\");\n",
    "    printf(\"  ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,\");\n",
    "    printf(\"gpu__dram_throughput.avg.pct_of_peak_sustained_elapsed \");\n",
    "    printf(\"./nsight_compute_exercises\\n\\n\");\n",
    "    \n",
    "    float *d_in1, *d_in2, *d_out;\n",
    "    CUDA_CHECK(cudaMalloc(&d_in1, N * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc(&d_in2, N * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc(&d_out, N * sizeof(float)));\n",
    "    \n",
    "    int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    memoryBoundKernel<<<blocks, BLOCK_SIZE>>>(d_out, d_in1, d_in2, N);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ memoryBoundKernel completed (expect high memory throughput %%)\\n\");\n",
    "    \n",
    "    computeBoundKernel<<<blocks, BLOCK_SIZE>>>(d_out, d_in1, N);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    printf(\"âœ“ computeBoundKernel completed (expect high SM throughput %%)\\n\");\n",
    "    \n",
    "    cudaFree(d_in1);\n",
    "    cudaFree(d_in2);\n",
    "    cudaFree(d_out);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Nsight Compute Profiling Exercises\\n\");\n",
    "    printf(\"===================================\\n\");\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
    "    printf(\"Device: %s\\n\", prop.name);\n",
    "    printf(\"Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "    \n",
    "    runExercise1();\n",
    "    runExercise2();\n",
    "    runExercise3();\n",
    "    runExercise4();\n",
    "    \n",
    "    printf(\"\\n=== Profile the full application ===\\n\");\n",
    "    printf(\"Full analysis: ncu --set full -o ncu_report ./nsight_compute_exercises\\n\");\n",
    "    printf(\"Open in GUI:   ncu-ui ncu_report.ncu-rep\\n\");\n",
    "    printf(\"\\nAll exercises completed successfully!\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o nsight_compute_exercises nsight_compute_exercises.cu && ./nsight_compute_exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12dc73",
   "metadata": {},
   "source": [
    "### ğŸ”¶ Python/Numba Exercises (Optional)\n",
    "\n",
    "The following exercises use Python with Numba for profiling concepts. While the CUDA C++ exercises above are the primary learning material, these provide an alternative approach using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a466ea",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary & Key Takeaways\n",
    "\n",
    "### What You Learned Today\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **Kernel Microscope** | Nsight Compute reveals internal kernel behavior |\n",
    "| **SM Throughput** | How busy are your compute units? |\n",
    "| **Memory Throughput** | How saturated is memory bandwidth? |\n",
    "| **Occupancy** | Are enough warps hiding latency? |\n",
    "| **Bound Detection** | Compare SM vs Memory % to find bottleneck |\n",
    "\n",
    "### ğŸ“‹ Nsight Compute Profiling Checklist\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚               KERNEL PROFILING CHECKLIST                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  â–¡ Run baseline:  ncu ./myapp                                   â”‚\n",
    "â”‚  â–¡ Check SM throughput percentage (compute saturation)          â”‚\n",
    "â”‚  â–¡ Check Memory throughput percentage (bandwidth saturation)    â”‚\n",
    "â”‚  â–¡ Check Occupancy (warp scheduling efficiency)                 â”‚\n",
    "â”‚  â–¡ Identify bottleneck: Higher % = likely bottleneck            â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  DIAGNOSIS QUICK REFERENCE:                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚  SM High, Mem Low   â†’ Compute-bound (more math!)        â”‚   â”‚\n",
    "â”‚  â”‚  SM Low, Mem High   â†’ Memory-bound (optimize access)    â”‚   â”‚\n",
    "â”‚  â”‚  Both Low           â†’ Latency-bound (increase occupancy)â”‚   â”‚\n",
    "â”‚  â”‚  Both High          â†’ Great! Near peak performance      â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  â–¡ Use --section SpeedOfLight for quick summary                 â”‚\n",
    "â”‚  â–¡ Save report: ncu -o report.ncu-rep ./myapp                   â”‚\n",
    "â”‚  â–¡ Open in GUI for source correlation                           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® What's Next?\n",
    "\n",
    "**Day 2: Roofline Analysis** - We'll visualize performance limits:\n",
    "- Where is your kernel on the roofline?\n",
    "- Are you hitting the compute ceiling or memory ceiling?\n",
    "- The roofline model makes optimization strategy crystal clear!\n",
    "\n",
    "*The microscope shows the details; the roofline shows the limits.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
