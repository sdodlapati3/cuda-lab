{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb63438",
   "metadata": {},
   "source": [
    "## 1. Introduction to Nsight Compute\n",
    "\n",
    "### What is Nsight Compute?\n",
    "\n",
    "Nsight Compute is NVIDIA's **kernel-level profiler** that provides:\n",
    "- Detailed metrics for each kernel launch\n",
    "- Source-level analysis\n",
    "- Roofline analysis\n",
    "- Guided optimization recommendations\n",
    "\n",
    "### When to Use It\n",
    "\n",
    "| Tool | Use Case |\n",
    "|------|----------|\n",
    "| **Nsight Compute** | Optimizing individual kernels |\n",
    "| **Nsight Systems** | Understanding application timeline, CPU-GPU interaction |\n",
    "\n",
    "### Installation Check\n",
    "\n",
    "```bash\n",
    "# Check if ncu is installed\n",
    "ncu --version\n",
    "\n",
    "# Typical output:\n",
    "# NVIDIA (R) Nsight Compute Command Line Profiler\n",
    "# Copyright (c) 2018-2024 NVIDIA Corporation\n",
    "# Version 2024.1.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a34739",
   "metadata": {},
   "source": [
    "## 2. CUDA C++ Test Kernels\n",
    "\n",
    "Let's create kernels with different performance characteristics to profile. The code below demonstrates:\n",
    "- **Memory-Bound**: Simple copy kernel\n",
    "- **Compute-Bound**: Heavy math operations\n",
    "- **Non-Coalesced**: Column-major access pattern\n",
    "- **Bank Conflicts**: Naive reduction with conflicts\n",
    "- **Low Occupancy**: High register usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d52cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile profiling_targets.cu\n",
    "// profiling_targets.cu - Kernels to profile\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <chrono>\n",
    "\n",
    "// Utility macro\n",
    "#define CHECK_CUDA(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "               cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 1: Memory-Bound (Simple Copy)\n",
    "//=============================================================================\n",
    "__global__ void copyKernel(float* dst, const float* src, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        dst[idx] = src[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 2: Compute-Bound (Heavy Math)\n",
    "//=============================================================================\n",
    "__global__ void computeKernel(float* output, const float* input, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float val = input[idx];\n",
    "        \n",
    "        // Heavy compute: transcendentals and iterations\n",
    "        #pragma unroll 10\n",
    "        for (int i = 0; i < 100; i++) {\n",
    "            val = sinf(val) + cosf(val);\n",
    "            val = sqrtf(fabsf(val) + 1.0f);\n",
    "            val = expf(-val * 0.001f);\n",
    "        }\n",
    "        \n",
    "        output[idx] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 3: Non-Coalesced Access (Column Major)\n",
    "//=============================================================================\n",
    "__global__ void nonCoalescedKernel(float* dst, const float* src, \n",
    "                                    int rows, int cols) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < rows * cols) {\n",
    "        int row = idx / cols;\n",
    "        int col = idx % cols;\n",
    "        \n",
    "        // Column-major read (non-coalesced)\n",
    "        int src_idx = col * rows + row;\n",
    "        dst[idx] = src[src_idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 4: Bank Conflicts (Naive Reduction)\n",
    "//=============================================================================\n",
    "__global__ void bankConflictReduction(float* output, const float* input, int n) {\n",
    "    __shared__ float sdata[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (idx < n) ? input[idx] : 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Naive reduction with bank conflicts\n",
    "    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
    "        if (tid % (2 * stride) == 0) {\n",
    "            sdata[tid] += sdata[tid + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        output[blockIdx.x] = sdata[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Kernel 5: Low Occupancy (High Register Usage)\n",
    "//=============================================================================\n",
    "__global__ void lowOccupancyKernel(float* output, const float* input, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Force high register usage\n",
    "    float r0 = input[idx % n], r1 = r0, r2 = r0, r3 = r0;\n",
    "    float r4 = r0, r5 = r0, r6 = r0, r7 = r0;\n",
    "    float r8 = r0, r9 = r0, r10 = r0, r11 = r0;\n",
    "    float r12 = r0, r13 = r0, r14 = r0, r15 = r0;\n",
    "    float r16 = r0, r17 = r0, r18 = r0, r19 = r0;\n",
    "    float r20 = r0, r21 = r0, r22 = r0, r23 = r0;\n",
    "    float r24 = r0, r25 = r0, r26 = r0, r27 = r0;\n",
    "    float r28 = r0, r29 = r0, r30 = r0, r31 = r0;\n",
    "    \n",
    "    // Keep all registers live\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        r0 = r1 + r2; r1 = r2 + r3; r2 = r3 + r4; r3 = r4 + r5;\n",
    "        r4 = r5 + r6; r5 = r6 + r7; r6 = r7 + r8; r7 = r8 + r9;\n",
    "        r8 = r9 + r10; r9 = r10 + r11; r10 = r11 + r12; r11 = r12 + r13;\n",
    "        r12 = r13 + r14; r13 = r14 + r15; r14 = r15 + r16; r15 = r16 + r17;\n",
    "        r16 = r17 + r18; r17 = r18 + r19; r18 = r19 + r20; r19 = r20 + r21;\n",
    "        r20 = r21 + r22; r21 = r22 + r23; r22 = r23 + r24; r23 = r24 + r25;\n",
    "        r24 = r25 + r26; r25 = r26 + r27; r26 = r27 + r28; r27 = r28 + r29;\n",
    "        r28 = r29 + r30; r29 = r30 + r31; r30 = r31 + r0; r31 = r0 + r1;\n",
    "    }\n",
    "    \n",
    "    if (idx < n) {\n",
    "        output[idx] = r0 + r1 + r2 + r3 + r4 + r5 + r6 + r7 +\n",
    "                      r8 + r9 + r10 + r11 + r12 + r13 + r14 + r15 +\n",
    "                      r16 + r17 + r18 + r19 + r20 + r21 + r22 + r23 +\n",
    "                      r24 + r25 + r26 + r27 + r28 + r29 + r30 + r31;\n",
    "    }\n",
    "}\n",
    "\n",
    "//=============================================================================\n",
    "// Main - Run all kernels\n",
    "//=============================================================================\n",
    "int main() {\n",
    "    const int N = 1 << 22;  // 4M elements\n",
    "    const int ROWS = 4096;\n",
    "    const int COLS = 1024;\n",
    "    \n",
    "    // Allocate memory\n",
    "    float *d_input, *d_output, *d_src, *d_dst;\n",
    "    CHECK_CUDA(cudaMalloc(&d_input, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_output, N * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_src, ROWS * COLS * sizeof(float)));\n",
    "    CHECK_CUDA(cudaMalloc(&d_dst, ROWS * COLS * sizeof(float)));\n",
    "    \n",
    "    // Initialize\n",
    "    float* h_input = new float[N];\n",
    "    for (int i = 0; i < N; i++) h_input[i] = (float)(i % 1000) / 1000.0f;\n",
    "    CHECK_CUDA(cudaMemcpy(d_input, h_input, N * sizeof(float), \n",
    "                          cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA(cudaMemcpy(d_src, h_input, ROWS * COLS * sizeof(float),\n",
    "                          cudaMemcpyHostToDevice));\n",
    "    \n",
    "    int blockSize = 256;\n",
    "    int gridSize = (N + blockSize - 1) / blockSize;\n",
    "    int gridSize2D = (ROWS * COLS + blockSize - 1) / blockSize;\n",
    "    \n",
    "    printf(\"Running profiling target kernels...\\n\\n\");\n",
    "    \n",
    "    // Run each kernel\n",
    "    printf(\"1. Copy Kernel (Memory-Bound)\\n\");\n",
    "    copyKernel<<<gridSize, blockSize>>>(d_output, d_input, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"2. Compute Kernel (Compute-Bound)\\n\");\n",
    "    computeKernel<<<gridSize, blockSize>>>(d_output, d_input, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"3. Non-Coalesced Kernel\\n\");\n",
    "    nonCoalescedKernel<<<gridSize2D, blockSize>>>(d_dst, d_src, ROWS, COLS);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"4. Bank Conflict Reduction\\n\");\n",
    "    bankConflictReduction<<<gridSize, blockSize>>>(d_output, d_input, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"5. Low Occupancy Kernel\\n\");\n",
    "    lowOccupancyKernel<<<gridSize, blockSize>>>(d_output, d_input, N);\n",
    "    CHECK_CUDA(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"\\nAll kernels complete. Use ncu to profile!\\n\");\n",
    "    \n",
    "    // Cleanup\n",
    "    delete[] h_input;\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_src);\n",
    "    cudaFree(d_dst);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d772ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -lineinfo -o profiling_targets profiling_targets.cu\n",
    "!./profiling_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f860ae6",
   "metadata": {},
   "source": [
    "## 3. Basic Nsight Compute Usage\n",
    "\n",
    "### Command Line Profiling\n",
    "\n",
    "```bash\n",
    "# Basic profiling - profile all kernels\n",
    "ncu ./profiling_targets\n",
    "\n",
    "# Profile specific kernel by name\n",
    "ncu --kernel-name copyKernel ./profiling_targets\n",
    "\n",
    "# Profile specific kernel by launch number\n",
    "ncu --launch-skip 0 --launch-count 1 ./profiling_targets\n",
    "\n",
    "# Save report to file\n",
    "ncu -o profile_report ./profiling_targets\n",
    "# Creates profile_report.ncu-rep (open in Nsight Compute GUI)\n",
    "```\n",
    "\n",
    "### Profiling Sections\n",
    "\n",
    "```bash\n",
    "# Full analysis (all sections)\n",
    "ncu --set full ./profiling_targets\n",
    "\n",
    "# Roofline analysis only\n",
    "ncu --set roofline ./profiling_targets\n",
    "\n",
    "# Memory analysis\n",
    "ncu --section MemoryWorkloadAnalysis ./profiling_targets\n",
    "\n",
    "# Compute analysis\n",
    "ncu --section ComputeWorkloadAnalysis ./profiling_targets\n",
    "\n",
    "# Occupancy analysis\n",
    "ncu --section Occupancy ./profiling_targets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe10d9e",
   "metadata": {},
   "source": [
    "## 4. Key Metrics Explained\n",
    "\n",
    "### Throughput Metrics\n",
    "\n",
    "| Metric | Description | Good Value |\n",
    "|--------|-------------|------------|\n",
    "| **SM Throughput** | % of SM compute cycles used | >60% compute-bound |\n",
    "| **Memory Throughput** | % of peak memory bandwidth | >60% memory-bound |\n",
    "| **L1 Hit Rate** | Cache hit rate for L1 | Higher = better |\n",
    "| **L2 Hit Rate** | Cache hit rate for L2 | Higher = better |\n",
    "\n",
    "### Occupancy Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Theoretical Occupancy** | Max warps per SM / hardware limit |\n",
    "| **Achieved Occupancy** | Actual average active warps |\n",
    "| **Active Warps** | Number of resident warps |\n",
    "\n",
    "### Memory Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Global Load Efficiency** | Useful bytes / total bytes loaded |\n",
    "| **Global Store Efficiency** | Useful bytes / total bytes stored |\n",
    "| **Shared Bank Conflicts** | Conflicts per shared memory access |\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "```\n",
    "Example ncu output for copyKernel:\n",
    "\n",
    "Section: GPU Speed Of Light Throughput\n",
    "  DRAM Throughput             95.2%\n",
    "  SM Throughput               12.3%\n",
    "\n",
    "Interpretation:\n",
    "- High DRAM (95.2%) + Low SM (12.3%) = MEMORY-BOUND\n",
    "- This is expected for a simple copy kernel\n",
    "- Optimization: focus on memory access patterns, not compute\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746b834",
   "metadata": {},
   "source": [
    "## 5. Practical Profiling Workflow\n",
    "\n",
    "### Step 1: Quick Overview\n",
    "\n",
    "```bash\n",
    "# Get quick summary of all kernels\n",
    "ncu --target-processes all ./profiling_targets 2>&1 | head -100\n",
    "```\n",
    "\n",
    "### Step 2: Identify Bottleneck Type\n",
    "\n",
    "```bash\n",
    "# Focus on speed-of-light analysis\n",
    "ncu --section SpeedOfLight --kernel-name computeKernel ./profiling_targets\n",
    "```\n",
    "\n",
    "Expected output pattern:\n",
    "```\n",
    "copyKernel:    Memory ~95%, Compute ~12%  → Memory-bound\n",
    "computeKernel: Memory ~15%, Compute ~85%  → Compute-bound\n",
    "```\n",
    "\n",
    "### Step 3: Deep Dive Based on Bottleneck\n",
    "\n",
    "**For memory-bound kernels:**\n",
    "```bash\n",
    "ncu --section MemoryWorkloadAnalysis \\\n",
    "    --section MemoryWorkloadAnalysis_Chart \\\n",
    "    --kernel-name copyKernel ./profiling_targets\n",
    "```\n",
    "\n",
    "**For compute-bound kernels:**\n",
    "```bash\n",
    "ncu --section ComputeWorkloadAnalysis \\\n",
    "    --section WarpStateStatistics \\\n",
    "    --kernel-name computeKernel ./profiling_targets\n",
    "```\n",
    "\n",
    "### Step 4: Check for Common Issues\n",
    "\n",
    "```bash\n",
    "# Check for non-coalesced access\n",
    "ncu --metrics l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum,\\\n",
    "l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum \\\n",
    "    --kernel-name nonCoalescedKernel ./profiling_targets\n",
    "\n",
    "# Calculate coalescing efficiency:\n",
    "# Efficiency = requests / sectors\n",
    "# Perfect = 1.0, Non-coalesced < 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939ac09",
   "metadata": {},
   "source": [
    "## 6. Specific Metric Queries\n",
    "\n",
    "### Custom Metric Collection\n",
    "\n",
    "```bash\n",
    "# Occupancy metrics\n",
    "ncu --metrics sm__warps_active.avg.pct_of_peak_sustained_active \\\n",
    "    ./profiling_targets\n",
    "\n",
    "# Memory throughput\n",
    "ncu --metrics dram__bytes.sum.per_second \\\n",
    "    ./profiling_targets\n",
    "\n",
    "# Compute throughput\n",
    "ncu --metrics sm__sass_thread_inst_executed_op_fadd_pred_on.sum,\\\n",
    "sm__sass_thread_inst_executed_op_fmul_pred_on.sum,\\\n",
    "sm__sass_thread_inst_executed_op_ffma_pred_on.sum \\\n",
    "    ./profiling_targets\n",
    "\n",
    "# Shared memory bank conflicts\n",
    "ncu --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_shared.sum \\\n",
    "    --kernel-name bankConflictReduction ./profiling_targets\n",
    "```\n",
    "\n",
    "### Common Useful Metrics\n",
    "\n",
    "```bash\n",
    "# One-liner for key metrics\n",
    "ncu --metrics \\\n",
    "sm__throughput.avg.pct_of_peak_sustained_elapsed,\\\n",
    "dram__throughput.avg.pct_of_peak_sustained_elapsed,\\\n",
    "sm__warps_active.avg.pct_of_peak_sustained_active,\\\n",
    "l1tex__t_sector_hit_rate.pct \\\n",
    "    ./profiling_targets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2334bb",
   "metadata": {},
   "source": [
    "## 7. Comparing Kernel Versions\n",
    "\n",
    "The following code compares naive vs optimized matrix transpose:\n",
    "- **Naive transpose**: Non-coalesced writes to global memory\n",
    "- **Optimized transpose**: Uses shared memory tile with padding to avoid bank conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17247d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile kernel_comparison.cu\n",
    "// kernel_comparison.cu - Compare optimized vs naive\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Naive transpose (non-coalesced writes)\n",
    "__global__ void transposeNaive(float* out, const float* in, \n",
    "                                int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        out[x * height + y] = in[y * width + x];  // Non-coalesced write\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized transpose (coalesced with shared memory)\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "\n",
    "__global__ void transposeCoalesced(float* out, const float* in,\n",
    "                                    int width, int height) {\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];  // +1 for bank conflicts\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Coalesced read into shared memory\n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < width && (y + j) < height) {\n",
    "            tile[threadIdx.y + j][threadIdx.x] = in[(y + j) * width + x];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Coalesced write from shared memory\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;  // Transpose block position\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
    "        if (x < height && (y + j) < width) {\n",
    "            out[(y + j) * height + x] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int WIDTH = 4096;\n",
    "    const int HEIGHT = 4096;\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, WIDTH * HEIGHT * sizeof(float));\n",
    "    cudaMalloc(&d_out, WIDTH * HEIGHT * sizeof(float));\n",
    "    \n",
    "    dim3 blockNaive(32, 32);\n",
    "    dim3 gridNaive((WIDTH + 31) / 32, (HEIGHT + 31) / 32);\n",
    "    \n",
    "    dim3 blockCoalesced(TILE_DIM, BLOCK_ROWS);\n",
    "    dim3 gridCoalesced((WIDTH + TILE_DIM - 1) / TILE_DIM,\n",
    "                       (HEIGHT + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    printf(\"Running naive transpose...\\n\");\n",
    "    transposeNaive<<<gridNaive, blockNaive>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Running coalesced transpose...\\n\");\n",
    "    transposeCoalesced<<<gridCoalesced, blockCoalesced>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Done. Compare with ncu!\\n\");\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 -lineinfo -o kernel_comparison kernel_comparison.cu\n",
    "!./kernel_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46482b",
   "metadata": {},
   "source": [
    "## 8. Python/Numba Optional Backup\n",
    "\n",
    "Since Nsight Compute is a command-line tool, we can use Python for simulating profiling concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86075c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install numba numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import cuda\n",
    "import math\n",
    "\n",
    "# Check GPU availability\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"GPU: {device.name}\")\n",
    "    print(f\"Compute Capability: {device.compute_capability}\")\n",
    "    print(f\"Total Memory: {device.total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No CUDA GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate profiling by timing different kernel types\n",
    "\n",
    "@cuda.jit\n",
    "def copy_kernel(dst, src):\n",
    "    \"\"\"Memory-bound kernel - simple copy\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < src.size:\n",
    "        dst[idx] = src[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def compute_kernel(output, input_arr):\n",
    "    \"\"\"Compute-bound kernel - heavy math\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < input_arr.size:\n",
    "        val = input_arr[idx]\n",
    "        for i in range(100):\n",
    "            val = math.sin(val) + math.cos(val)\n",
    "            val = math.sqrt(abs(val) + 1.0)\n",
    "            val = math.exp(-val * 0.001)\n",
    "        output[idx] = val\n",
    "\n",
    "# Test data\n",
    "N = 1 << 20  # 1M elements\n",
    "h_input = np.random.rand(N).astype(np.float32)\n",
    "h_output = np.zeros_like(h_input)\n",
    "\n",
    "# Device arrays\n",
    "d_input = cuda.to_device(h_input)\n",
    "d_output = cuda.device_array_like(h_input)\n",
    "\n",
    "# Launch config\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "# Warm up\n",
    "copy_kernel[blocks_per_grid, threads_per_block](d_output, d_input)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Time copy kernel (memory-bound)\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    copy_kernel[blocks_per_grid, threads_per_block](d_output, d_input)\n",
    "cuda.synchronize()\n",
    "copy_time = (time.perf_counter() - start) / 100\n",
    "\n",
    "# Time compute kernel (compute-bound)  \n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    compute_kernel[blocks_per_grid, threads_per_block](d_output, d_input)\n",
    "cuda.synchronize()\n",
    "compute_time = (time.perf_counter() - start) / 10\n",
    "\n",
    "# Analysis\n",
    "bytes_transferred = 2 * N * 4  # Read + Write, float32\n",
    "copy_bandwidth = bytes_transferred / copy_time / 1e9\n",
    "\n",
    "print(f\"Copy Kernel:\")\n",
    "print(f\"  Time: {copy_time*1000:.3f} ms\")\n",
    "print(f\"  Effective Bandwidth: {copy_bandwidth:.2f} GB/s\")\n",
    "print(f\"  → Memory-bound (simple memory operations)\")\n",
    "\n",
    "print(f\"\\nCompute Kernel:\")\n",
    "print(f\"  Time: {compute_time*1000:.3f} ms\")\n",
    "print(f\"  Time Ratio (Compute/Copy): {compute_time/copy_time:.1f}x\")\n",
    "print(f\"  → Compute-bound (heavy math operations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5909f5e",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### Nsight Compute Essentials\n",
    "\n",
    "1. **Basic profiling**: `ncu ./my_program`\n",
    "2. **Kernel selection**: `--kernel-name KernelName`\n",
    "3. **Save reports**: `-o report_name`\n",
    "4. **Key sections**:\n",
    "   - `SpeedOfLight` - Quick bottleneck identification\n",
    "   - `MemoryWorkloadAnalysis` - Memory access patterns\n",
    "   - `ComputeWorkloadAnalysis` - Compute utilization\n",
    "   - `Occupancy` - Thread parallelism\n",
    "\n",
    "### Bottleneck Identification\n",
    "\n",
    "| SM Throughput | Memory Throughput | Bottleneck |\n",
    "|---------------|-------------------|------------|\n",
    "| High | Low | Compute-bound |\n",
    "| Low | High | Memory-bound |\n",
    "| Low | Low | Latency-bound or low occupancy |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Compile with `-lineinfo`** for source correlation\n",
    "2. **Start with overview**, then deep dive\n",
    "3. **Compare before/after** optimization\n",
    "4. **Focus on one bottleneck at a time**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9404a4",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "### Exercise 1: Profile a Reduction\n",
    "Profile the bank conflict reduction kernel. What is the bank conflict count?\n",
    "\n",
    "```bash\n",
    "ncu --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_shared.sum \\\n",
    "    --kernel-name bankConflictReduction ./profiling_targets\n",
    "```\n",
    "\n",
    "### Exercise 2: Measure Occupancy\n",
    "Profile the low occupancy kernel and report:\n",
    "1. Theoretical occupancy\n",
    "2. Achieved occupancy\n",
    "3. Register count per thread\n",
    "\n",
    "### Exercise 3: Coalescing Analysis\n",
    "Compare global load efficiency between `copyKernel` and `nonCoalescedKernel`.\n",
    "\n",
    "### Exercise 4: Create a Full Report\n",
    "Generate a comprehensive report and open in GUI:\n",
    "```bash\n",
    "ncu --set full -o full_analysis ./profiling_targets\n",
    "ncu-ui full_analysis.ncu-rep  # Open in GUI\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a466ea",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Today you learned:\n",
    "- **Nsight Compute** is NVIDIA's kernel-level profiler\n",
    "- Key metrics: **SM/Memory Throughput**, **Occupancy**, **Efficiency**\n",
    "- Identify bottlenecks by comparing throughput percentages\n",
    "- Use `--section` for focused analysis\n",
    "- Save reports with `-o` for later analysis in GUI\n",
    "\n",
    "**Next**: Day 2 - Roofline Analysis for systematic performance modeling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
